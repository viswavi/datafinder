# pip install accelerate
import argparse
import jsonlines
import numpy as np
import time
from transformers import AutoTokenizer, OPTForCausalLM
import torch
from tqdm import tqdm

parser = argparse.ArgumentParser()
parser.add_argument("--abstracts", default="/home/vijayv/train_abstracts.txt")
parser.add_argument("--output-file", default="/home/vijayv/train_abstracts_parsed_by_galactica.jsonl")
parser.add_argument("--batch-size", type=int, default=18)

def load_model(PAD_TOKEN="Ä dexamethasone"):
    tokenizer = AutoTokenizer.from_pretrained("facebook/galactica-6.7b")
    tokenizer.add_special_tokens({'pad_token': PAD_TOKEN})
    tokenizer.padding_side = 'left'

    model = OPTForCausalLM.from_pretrained("facebook/galactica-6.7b", device_map="auto", torch_dtype=torch.float16)
    model.config.pad_token_id = tokenizer.vocab[PAD_TOKEN]
    return tokenizer, model, PAD_TOKEN

def load_prompt():
prompt_text = """Given an abstract from an artificial intelligence paper:
1) Extract keyphrases regarding the task (e.g. image classification), data modality (e.g. images or speech), domain (e.g. biomedical or aerial), training style (unsupervised, semi-supervised, fully supervised, or reinforcement learning), text length (sentence-level or paragraph-level), language required (e.g. English)
2) Write a brief, single-sentence summary containing these relevant keyphrases. This summary must describe the task studied in the paper.

Abstract:
We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).

Output: (Task | Modality | Domain | Training Style | Text Length |  Language Required | Single-Sentence Summary)
Task: question generation
Modality: text
Domain: N/A
Training Style: fully supervised
Text Length: paragraph-level
Language Required: N/A
Single-Sentence Summary: We propose an improved end-to-end system for automatic question generation.
--

Abstract:
We present a self-supervised approach to estimate flow in camera image and top-view grid map sequences using fully convolutional neural networks in the domain of automated driving. We extend existing approaches for self-supervised optical flow estimation by adding a regularizer expressing motion consistency assuming a static environment. However, as this assumption is violated for other moving traffic participants we also estimate a mask to scale this regularization. Adding a regularization towards motion consistency improves convergence and flow estimation accuracy. Furthermore, we scale the errors due to spatial flow inconsistency by a mask that we derive from the motion mask. This improves accuracy in regions where the flow drastically changes due to a better separation between static and dynamic environment. We apply our approach to optical flow estimation from camera image sequences, validate on odometry estimation and suggest a method to iteratively increase optical flow estimation accuracy using the generated motion masks. Finally, we provide quantitative and qualitative results based on the KITTI odometry and tracking benchmark for scene flow estimation based on grid map sequences. We show that we can improve accuracy and convergence when applying motion and spatial consistency regularization.

Output: (Task | Modality | Domain | Training Style | Text Length |  Language Required | Single-Sentence Summary)
Task: optical flow estimation
Modality: images and top-view grid map sequences
Domain: autonomous driving
Training Style: unsupervised
Text Length: N/A
Language Required: N/A
Single-Sentence Summary: A system for self-supervised optical flow estimation from images and top-down maps.
--

Abstract:
In this paper, we study the actor-action semantic segmentation problem, which requires joint labeling of both actor and action categories in video frames. One major challenge for this task is that when an actor performs an action, different body parts of the actor provide different types of cues for the action category and may receive inconsistent action labeling when they are labeled independently. To address this issue, we propose an end-to-end region-based actor-action segmentation approach which relies on region masks from an instance segmentation algorithm. Our main novelty is to avoid labeling pixels in a region mask independently - instead we assign a single action label to these pixels to achieve consistent action labeling. When a pixel belongs to multiple region masks, max pooling is applied to resolve labeling conflicts. Our approach uses a two-stream network as the front-end (which learns features capturing both appearance and motion information), and uses two region-based segmentation networks as the back-end (which takes the fused features from the two-stream network as the input and predicts actor-action labeling). Experiments on the A2D dataset demonstrate that both the region-based segmentation strategy and the fused features from the two-stream network contribute to the performance improvements. The proposed approach outperforms the state-of-the-art results by more than 8% in mean class accuracy, and more than 5% in mean class IOU, which validates its effectiveness.

Output: (Task | Modality | Domain | Training Style | Text Length |  Language Required | Single-Sentence Summary)
Task: actor-action semantic segmentation
Modality: video
Domain: N/A
Training Style: fully supervised
Text Length: N/A
Language Required: N/A
Single-Sentence Summary: I want to train a supervised model for actor-action semantic segmentation from video.
--

    """
    return prompt_text

def construct_instance_prompt(abstract):
    prompt = load_prompt()
    input_text = """{}Abstract:
    {}""".format(prompt, abstract)
    return input_text

def tokenize_batch(tokenizer, text_batch, prompt_suffix_tokenized, max_tokens=1500):
    tokenizer_output = tokenizer(text_batch, return_tensors="pt", padding=True, truncation=True, max_length=max_tokens - prompt_suffix_tokenized.input_ids.shape[1])
    input_ids = torch.cat([tokenizer_output.input_ids, prompt_suffix_tokenized.input_ids], dim=1)
    attention_mask = torch.cat([tokenizer_output.attention_mask, prompt_suffix_tokenized.attention_mask], dim=1)
    return input_ids.to("cuda"), attention_mask.to("cuda")

def run_model(model, batch_token_ids, batch_attention_masks):
    generated_token_ids = model.generate(batch_token_ids, attention_mask=batch_attention_masks, max_new_tokens=67, length_penalty=0.15, do_sample=True, top_p=0.8)
    batch_generations = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True)
    return batch_generations


def remove_prompt(prompt, generated_text):
    generated_chunks = generated_text.split("""
    --""")
    match_found = False
    for chunk in reversed(generated_chunks):
        if "Single-Sentence Summary:" in chunk and "Language Required:" in chunk and "Text Length:" in chunk and "Training Style:" in chunk:    
            match_found = True    
            break
    if match_found:
        return chunk.strip()
    else:
        return None

def parse_fields(text, abstract):
    out_dict = {"abstract": abstract}
    if text is None:
        out_dict["parsing_successful"] = "False"
    else:
        out_dict["parsing_successful"] = "True"

    for line in text.split("\n"):
        stripped_line = line.strip()
        for field in ["Task", "Modality", "Domain", "Training Style", "Text Length", "Language Required", "Single-Sentence Summary"]:
            if f"{field}:" in stripped_line:
                field_value = stripped_line[len(f"{field}:"):].strip()
                out_dict[field] = field_value
    return out_dict

if __name__ == "__main__":
    args = parser.parse_args()
    abstracts_file = open(args.abstracts)
    abstracts = [a for a in abstracts_file.read().split("\n") if len(a.split()) > 0]
    galactica_tags_and_queries = []

    batch_raw_abstracts = [[]]
    batch_formatted_abstracts = [[]]
    for abstract in abstracts:
        batch_raw_abstracts[-1].append(abstract)
        batch_formatted_abstracts[-1].append(construct_instance_prompt(abstract))
        if len(batch_formatted_abstracts[-1]) == args.batch_size:
            batch_raw_abstracts.append([])
            batch_formatted_abstracts.append([])

    tokenizer, model, pad_token = load_model()

    prompt_prefix_length = tokenizer(load_prompt(), return_tensors="pt").input_ids.shape[1]

    prompt_suffix = ["""

    Output: (Task | Modality | Domain | Training Style | Text Length |  Language Required | Single-Sentence Summary)
    Task:""" for _ in range(args.batch_size)]
    prompt_suffix_tokenized = tokenizer(prompt_suffix, return_tensors="pt")

    final_batch_prompt_suffix = ["""

    Output: (Task | Modality | Domain | Training Style | Text Length |  Language Required | Single-Sentence Summary)
    Task:""" for _ in range(len(batch_raw_abstracts[-1]))]
    final_batch_prompt_suffix_tokenized = tokenizer(final_batch_prompt_suffix, return_tensors="pt")



    with open(args.output_file, 'w') as file:
        writer = jsonlines.Writer(file, flush=True)
        for batch_idx, formatted_abstracts_batch in tqdm(enumerate(batch_formatted_abstracts)):
            raw_abstracts_batch = batch_raw_abstracts[batch_idx]

            if batch_idx == len(batch_formatted_abstracts) - 1:
                batch_token_ids, batch_attention_masks = tokenize_batch(tokenizer, formatted_abstracts_batch, final_batch_prompt_suffix_tokenized)
            else:
                batch_token_ids, batch_attention_masks = tokenize_batch(tokenizer, formatted_abstracts_batch, prompt_suffix_tokenized)
            batch_generations = run_model(model, batch_token_ids, batch_attention_masks)
            generated_text_only = [remove_prompt(load_prompt(), text) for text in batch_generations]
            fields_parsed = [parse_fields(truncated_text, raw_abstracts_batch[abs_idx]) for abs_idx, truncated_text in enumerate(generated_text_only)]
            writer.write_all(fields_parsed)
        writer.close()

