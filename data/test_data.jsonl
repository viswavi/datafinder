{"documents": ["Cityscapes"], "year": 2017, "keyphrase_query": "semantic image segmentation autonomous driving", "abstract": "Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset.", "task": "semantic image segmentation", "domain": "autonomous driving", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want to build a system for semantic image segmentation for self-driving cars using large-scale supervised learning."}
{"documents": ["PROMISE12"], "year": 2016, "keyphrase_query": "3d image segmentation mri biomedical", "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.", "task": "3D image segmentation", "domain": "biomedical", "modality": "MRI", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a CNN trained end-to-end on 3D MRI volumes depicting prostates to predict segmentation for the whole volume at once."}
{"documents": ["CNN/Daily Mail", "SQuAD"], "year": 2016, "keyphrase_query": "machine comprehension text", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "task": "machine comprehension", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "I want to design a system that answers questions about paragraphs of text."}
{"documents": ["SNLI"], "year": 2016, "keyphrase_query": "sentence pair modeling text", "abstract": "Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state-of-the-art methods.", "task": "sentence pair modeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level", "query": "A deep architecture for modeling the interactions of two sentences."}
{"documents": [], "year": 2017, "keyphrase_query": "game playing games chess, go, shogi", "abstract": "The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.", "task": "game playing", "domain": "chess, go, shogi", "modality": "Games", "language": "", "training_style": "Reinforcement learning", "text_length": "", "query": "I want to build a model that learns to play games like chess and Go."}
{"documents": ["AFLW2000-3D", "Florence"], "year": 2017, "keyphrase_query": "3d face reconstruction, facial landmark localization images, faces", "abstract": "3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Code and models will be made available at http://aaronsplace.co.uk", "task": "3D face reconstruction, facial landmark localization", "domain": "faces", "modality": "Images, 3D images", "language": "", "training_style": "", "text_length": "", "query": "I want to build a model that constructs 3D representations of spaces from 2D images of faces."}
{"documents": ["COCO Captions"], "year": 2017, "keyphrase_query": "text generation", "abstract": "Automatically generating coherent and semantically meaningful text has many applications in machine translation, dialogue systems, image captioning, etc. Recently, by combining with policy gradient, Generative Adversarial Nets (GAN) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation. However, the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process. As such, it limits its success when the length of the generated text samples is long (more than 20 words). In this paper, we propose a new framework, called LeakGAN, to address the problem for long text generation. We allow the discriminative net to leak its own high-level extracted features to the generative net to further help the guidance. The generator incorporates such informative signals into all generation steps through an additional Manager module, which takes the extracted features of current generated words and outputs a latent vector to guide the Worker module for next-word generation. Our extensive experiments on synthetic data and various real-world tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios. More importantly, without any supervision, LeakGAN would be able to implicitly learn sentence structures only through the interaction between Manager and Worker.", "task": "text generation", "domain": "", "modality": "Text", "language": "", "training_style": "Semi-supervised", "text_length": "Paragraph-level", "query": "We propose a framework for using Generative Adversarial Networks for long text generation."}
{"documents": ["CIFAR-10", "CIFAR-100", "ImageNet"], "year": 2017, "keyphrase_query": "image classification", "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We present a highly modularized network architecture for image classification."}
{"documents": ["COCO", "Visual Question Answering"], "year": 2015, "keyphrase_query": "visual question answering image, text", "abstract": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.", "task": "visual question answering", "domain": "", "modality": "Image, Text", "language": "", "training_style": "", "text_length": "", "query": "We propose the task of free-form and open-ended Visual Question Answering."}
{"documents": ["CIFAR-10", "MNIST", "SVHN"], "year": 2017, "keyphrase_query": "image generation", "abstract": "In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.", "task": "image generation", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I propose a novel model for image generation."}
{"documents": ["AFW", "Helen", "LFPW"], "year": 2017, "keyphrase_query": "facial landmark localization, human pose estimation", "abstract": "In this work we use deep learning to establish dense correspondences between a 3D object model and an image \"in the wild\". We introduce \"DenseReg\", a fully-convolutional neural network (F-CNN) that densely regresses at every foreground pixel a pair of U-V template coordinates in a single feedforward pass. To train DenseReg we construct a supervision signal by combining 3D deformable model fitting and 2D landmark annotations. We define the regression task in terms of the intrinsic, U-V coordinates of a 3D deformable model that is brought into correspondence with image instances at training time. A host of other object-related tasks (e.g. part segmentation, landmark localization) are shown to be by-products of this task, and to largely improve thanks to its introduction. We obtain highly-accurate regression results by combining ideas from semantic segmentation with regression networks, yielding a 'quantized regression' architecture that first obtains a quantized estimate of position through classification, and refines it through regression of the residual. We show that such networks can boost the performance of existing state-of-the-art systems for pose estimation. Firstly, we show that our system can serve as an initialization for Statistical Deformable Models, as well as an element of cascaded architectures that jointly localize landmarks and estimate dense correspondences. We also show that the obtained dense correspondence can act as a source of 'privileged information' that complements and extends the pure landmark-level annotations, accelerating and improving the training of pose estimation networks. We report state-of-the-art performance on the challenging 300W benchmark for facial landmark localization and on the MPII and LSP datasets for human pose estimation.", "task": "facial landmark localization, human pose estimation", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "We use deep learning to construct dense correspondences between an image and 3D landmark points."}
{"documents": ["Arcade Learning Environment"], "year": 2015, "keyphrase_query": "game play", "abstract": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.", "task": "game play", "domain": "", "modality": "Game play", "language": "", "training_style": "Reinforcement learning", "text_length": "", "query": "I want to build a model that learns to play video games."}
{"documents": ["CelebA", "MNIST", "SVHN"], "year": 2016, "keyphrase_query": "disentanglement writing styles, pose lighting, discovery visual concepts image", "abstract": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.", "task": "disentanglement of writing styles, disentanglement of pose from lighting, discovery of visual concepts", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to create a method that can disentangle various traits of images."}
{"documents": ["AG News", "Amazon Fine Foods", "Amazon Product Data", "DBpedia", "YFCC100M"], "year": 2016, "keyphrase_query": "text classification", "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.", "task": "text classification", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to create a method that can efficiently perform text classification."}
{"documents": ["SNLI", "SST"], "year": 2017, "keyphrase_query": "natural language inference, sentiment analysis text", "abstract": "For years, recursive neural networks (RvNNs) have been shown to be suitable for representing text into fixed-length vectors and achieved good performance on several natural language processing tasks. However, the main drawback of RvNNs is that they require structured input, which makes data preparation and model implementation hard. In this paper, we propose Gumbel Tree-LSTM, a novel tree-structured long short-term memory architecture that learns how to compose task-specific tree structures only from plain text data efficiently. Our model uses Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically and to calculate gradients of the discrete decision. We evaluate the proposed model on natural language inference and sentiment analysis, and show that our model outperforms or is at least comparable to previous models. We also find that our model converges significantly faster than other models.", "task": "natural language inference, sentiment analysis", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level", "query": "I propose a tree-structured neural network architecture for understanding the semantics of sentences on tasks like sentiment analysis and natural language inference."}
{"documents": ["DAVIS 2016", "DAVIS 2017", "VOT2016", "VOT2018", "YouTube-VOS 2018"], "year": 2019, "keyphrase_query": "object tracking, semi-supervised video segmentation", "abstract": "In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017.", "task": "object tracking, semi-supervised video object segmentation", "domain": "", "modality": "Video", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "A simple and fast approach for both object tracking and semi-supervised video object segmentation."}
{"documents": ["BraTS 2015"], "year": 2017, "keyphrase_query": "brain lesion segmentation image biomedical", "abstract": "HIGHLIGHTSAn efficient 11‐layers deep, multi‐scale, 3D CNN architecture.A novel training strategy that significantly boosts performance.The first employment of a 3D fully connected CRF for post‐processing.State‐of‐the‐art performance on three challenging lesion segmentation tasks.New insights into the automatically learned intermediate representations. ABSTRACT We propose a dual pathway, 11‐layers deep, three‐dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in‐depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post‐processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi‐channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state‐of‐the‐art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.", "task": "brain lesion segmentation", "domain": "biomedical", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to train a 3D CNN for brain lesion segmentation"}
{"documents": ["AwA", "CUB-200-2011", "Stanford Dogs"], "year": 2014, "keyphrase_query": "fine-grained image classification", "abstract": "Despite significant recent advances in image classification, fine-grained classification remains a challenge. In the present paper, we address the zero-shot and few-shot learning scenarios as obtaining labeled data is especially difficult for fine-grained classification tasks. First, we embed state-of-the-art image descriptors in a label embedding space using side information such as attributes. We argue that learning a joint embedding space, that maximizes the compatibility between the input and output embeddings, is highly effective for zero/few-shot learning. We show empirically that such embeddings significantly outperforms the current state-of-the-art methods on two challenging datasets (Caltech-UCSD Birds and Animals with Attributes). Second, to reduce the amount of costly manual attribute annotations, we use alternate output embeddings based on the word-vector representations, obtained from large text-corpora without any supervision. We report that such unsupervised embeddings achieve encouraging results, and lead to further improvements when combined with the supervised ones.", "task": "fine-grained image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Few-shot", "text_length": "", "query": "I want to train a model for fine-grained image classification."}
{"documents": ["CIFAR-10", "CIFAR-100"], "year": 2016, "keyphrase_query": "image classification english", "abstract": "The depth of convolutional neural networks is a crucial ingredient for reduction in test errors on benchmarks like ImageNet and COCO. However, training a neural network becomes difficult with increasing depth. Problems like vanishing gradient and diminishing feature reuse are quite trivial in very deep convolutional neural networks. The notable recent contributions towards solving these problems and simplifying the training of very deep models are Residual and Highway Networks. These networks allow earlier representations (from the input or those learned in earlier layers) to flow unimpededly to later layers through skip connections. Such very deep models with hundreds or more layers have lead to a considerable decrease in test errors, on benchmarks like ImageNet and COCO. In this paper, we propose to replace the combination of ReLU and Batch Normalization with Exponential Linear Unit (ELU) in Residual Networks. Our experiments show that this not only speeds up the learning behavior in Residual Networks, but also improves the classification performance as the depth increases. Our model increases the accuracy on datasets like CIFAR-10 and CIFAR-100 by a significant margin.", "task": "image classification", "domain": "", "modality": "Image", "language": "English", "training_style": "", "text_length": "", "query": "I want to train a very deep CNN fast without compromise on performance."}
{"documents": ["BSD", "BSDS500", "Set14", "Set5"], "year": 2016, "keyphrase_query": "image super-resolution images videos", "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.", "task": "image super-resolution", "domain": "", "modality": "Images and Videos", "language": "", "training_style": "", "text_length": "", "query": "I want to train a model for real-time video/image super-resolution."}
{"documents": ["DUC 2004", "JAMUL", "JNC"], "year": 2019, "keyphrase_query": "abstractive summarization text", "abstract": "Neural encoder-decoder models have been successful in natural language generation tasks. However, real applications of abstractive summarization must consider additional constraint that a generated summary should not exceed a desired length. In this paper, we propose a simple but effective extension of a sinusoidal positional encoding (Vaswani et al., 2017) to enable neural encoder-decoder model to preserves the length constraint. Unlike in previous studies where that learn embeddings representing each length, the proposed method can generate a text of any length even if the target length is not present in training data. The experimental results show that the proposed method can not only control the generation length but also improve the ROUGE scores.", "task": "abstractive summarization", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Paragraph-level", "query": "I want to train an abstractive summarizer wherein the model preserves the length-constraint of the generated summary."}
{"documents": ["CIFAR-10", "CIFAR-100", "COCO", "SVHN"], "year": 2016, "keyphrase_query": "object detection image english", "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL", "task": "object detection", "domain": "", "modality": "Image", "language": "English", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train an efficient, wide residual network for image classification and object recognition"}
{"documents": ["COCO"], "year": 2018, "keyphrase_query": "object detection image english", "abstract": "In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code will be made available at this https URL.", "task": "object detection", "domain": "", "modality": "Image", "language": "English", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train an object detector that deals with IoU thresholds effectively."}
{"documents": ["Chairs", "KITTI", "MPI Sintel", "Middlebury"], "year": 2018, "keyphrase_query": "optical flow estimation image", "abstract": "FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that outperforms FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet.", "task": "optical flow estimation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to train a model for optical flow estimation that outperforms FlowNet2 in efficiency and accuracy."}
{"documents": ["Arcade Learning Environment"], "year": 2017, "keyphrase_query": "reinforcement learning video", "abstract": "In this work we present a new reinforcement learning agent, called Reactor (for Retrace-actor), based on an off-policy multi-step return actor-critic architecture. The agent uses a deep recurrent neural network for function approximation. The network outputs a target policy {\\pi} (the actor), an action-value Q-function (the critic) evaluating the current policy {\\pi}, and an estimated behavioral policy {\\hat \\mu} which we use for off-policy correction. The agent maintains a memory buffer filled with past experiences. The critic is trained by the multi-step off-policy Retrace algorithm and the actor is trained by a novel {\\beta}-leave-one-out policy gradient estimate (which uses both the off-policy corrected return and the estimated Q-function). The Reactor is sample-efficient thanks to the use of memory replay, and numerical efficient since it uses multi-step returns. Also both acting and learning can be parallelized. We evaluated our algorithm on 57 Atari 2600 games and demonstrate that it achieves state-of-the-art performance.", "task": "reinforcement learning", "domain": "", "modality": "Video", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I want to train a sample-efficient and numerical efficient reinforcement learning agent that achieves good performance on Atari video games."}
{"documents": ["BSD", "Set14", "Set5"], "year": 2016, "keyphrase_query": "image super resolution", "abstract": "In this paper we present seven techniques that everybody should know to improve example-based single image super resolution (SR): 1) augmentation of data, 2) use of large dictionaries with efficient search structures, 3) cascading, 4) image self-similarities, 5) back projection refinement, 6) enhanced prediction by consistency check, and 7) context reasoning. We validate our seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) and methods (i.e. A+, SRCNN, ANR, Zeyde, Yang) and achieve substantial improvements.The techniques are widely applicable and require no changes or only minor adjustments of the SR methods. Moreover, our Improved A+ (IA) method sets new state-of-the-art results outperforming A+ by up to 0.9dB on average PSNR whilst maintaining a low time complexity.", "task": "image super resolution", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to build a model for single image super resolution."}
{"documents": ["PASCAL VOC", "PASCAL VOC 2007"], "year": 2017, "keyphrase_query": "object detection image", "abstract": "Of late, weakly supervised object detection is with great importance in object recognition. Based on deep learning, weakly supervised detectors have achieved many promising results. However, compared with fully supervised detection, it is more challenging to train deep network based detectors in a weakly supervised manner. Here we formulate weakly supervised detection as a Multiple Instance Learning (MIL) problem, where instance classifiers (object detectors) are put into the network as hidden nodes. We propose a novel online instance classifier refinement algorithm to integrate MIL and the instance classifier refinement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information. More precisely, instance labels inferred from weak supervision are propagated to their spatially overlapped instances to refine instance classifier online. The iterative instance classifier refinement procedure is implemented using multiple streams in deep network, where each stream supervises its latter stream. Weakly supervised object detection experiments are carried out on the challenging PASCAL VOC 2007 and 2012 benchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the previous state-of-the-art.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "weakly supervised", "text_length": "", "query": "I want to train an object detector in a weakly supervised fashion where I only have access to image-level information and not object-level."}
{"documents": ["IJB-A", "IJB-B", "VGGFace2"], "year": 2018, "keyphrase_query": "face recognition image", "abstract": "The objective of this paper is to learn a compact representation of image sets for template-based face recognition. We make the following contributions: first, we propose a network architecture which aggregates and embeds the face descriptors produced by deep convolutional neural networks into a compact fixed-length representation. This compact representation requires minimal memory storage and enables efficient similarity computation. Second, we propose a novel GhostVLAD layer that includes ghost clusters, that do not contribute to the aggregation. We show that a quality weighting on the input faces emerges automatically such that informative images contribute more than those with low quality, and that the ghost clusters enhance the network’s ability to deal with poor quality images. Third, we explore how input feature dimension, number of clusters and different training techniques affect the recognition performance. Given this analysis, we train a network that far exceeds the state-of-the-art on the IJB-B face recognition dataset. This is currently one of the most challenging public benchmarks, and we surpass the state-of-the-art on both the identification and verification protocols.", "task": "face recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a method to learn representations of images for face recogntion"}
{"documents": ["CoNLL-2012"], "year": 2018, "keyphrase_query": "semantic role labeling text news english", "abstract": "Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-of-speech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates. Syntax is incorporated by training one attention head to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on out-of-domain data, nearly 10% reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.", "task": "semantic role labeling", "domain": "news", "modality": "Text", "language": "English", "training_style": "", "text_length": "Sentence-level", "query": "We propose a linguistically-informed approach to semantic role labeling with neural networks that uses multi-task learning"}
{"documents": ["Caltech Pedestrian Dataset", "INRIA Person"], "year": 2014, "keyphrase_query": "object detection image pedestrian", "abstract": "Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.", "task": "object detection", "domain": "pedestrian", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose an method for pedestrian detection using boosted decision trees"}
{"documents": ["CIFAR-10", "MNIST", "SVHN"], "year": 2019, "keyphrase_query": "image classification", "abstract": "In this paper, we present the Lipschitz regularization theory and algorithms for a novel Loss-Sensitive Generative Adversarial Network (LS-GAN). Specifically, it trains a loss function to distinguish between real and fake samples by designated margins, while learning a generator alternately to produce realistic samples by minimizing their losses. The LS-GAN further regularizes its loss function with a Lipschitz regularity condition on the density of real data, yielding a regularized model that can better generalize to produce new data from a reasonable number of training examples than the classic GAN. We will further present a Generalized LS-GAN (GLS-GAN) and show it contains a large family of regularized GAN models, including both LS-GAN and Wasserstein GAN, as its special cases. Compared with the other GAN models, we will conduct experiments to show both LS-GAN and GLS-GAN exhibit competitive ability in generating new images in terms of the Minimum Reconstruction Error (MRE) assessed on a separate test set. We further extend the LS-GAN to a conditional form for supervised and semi-supervised learning problems, and demonstrate its outstanding performance on image classification tasks.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "supervised training,semi-supervised", "text_length": "", "query": "We propose a GAN-based approach to image classification in supervised and semi-supervised settings"}
{"documents": [], "year": 2016, "keyphrase_query": "targeted aspect-based sentiment analysis text social media", "abstract": "© 1963-2018 ACL. In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis that assumes a single entity per document and targeted sentiment analysis that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units of text often mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform in this case a QA platform, is used for fine-grained opinion mining. Text coming from QA platforms is far less constrained compared to text from review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks.", "task": "targeted aspect-based sentiment analysis", "domain": "social media", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "I want to develop a model for targeted aspect-based sentiment analysis on data from a question answering social media platform for discussing urban neighbourhoods"}
{"documents": ["CIFAR-10", "CIFAR-100", "MNIST", "SVHN"], "year": 2014, "keyphrase_query": "classification image", "abstract": "Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce \"companion objective\" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).", "task": "classification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a method to train a neural network with both an output-level loss and a layer-level objective for classification tasks"}
{"documents": ["BSD", "Set14", "Set5", "Urban100"], "year": 2016, "keyphrase_query": "image super-resolution", "abstract": "We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/ vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.", "task": "image super-resolution", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a method to use deeply recursive neural networks for image super-resolution using recursive supervision and skip connections"}
{"documents": ["MR", "SST"], "year": 2018, "keyphrase_query": "sentiment classification text", "abstract": "Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge. In this paper, we propose a Multi-sentiment-resource Enhanced Attention Network (MEAN) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge (e.g., sentiment lexicon, negation words, intensity words) into the deep neural network via attention mechanisms. By using various types of sentiment resources, MEAN utilizes sentiment-relevant information from different representation subspaces, which makes it more effective to capture the overall semantics of the sentiment, negation and intensity words for sentiment prediction. The experimental results demonstrate that MEAN has robust superiority over strong competitors.", "task": "sentiment classification", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We propose a method to integrate linguistic knowledge into the deep neural network via attention mechanisms for sentiment classification"}
{"documents": [], "year": 2015, "keyphrase_query": "neuronal structure segmentation,cell tracking image", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .", "task": "neuronal structure segmentation,cell tracking", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a method that uses data augmentation to perform neuronal structure segmentation and cell tracking from very few training images"}
{"documents": ["WMT 2014"], "year": 2014, "keyphrase_query": "machine translation english,french", "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "task": "machine translation", "domain": "", "modality": "", "language": "English,French", "training_style": "", "text_length": "", "query": "I want to develop a model that uses encoder-decoder neural network models for machine translation"}
{"documents": ["Cityscapes", "ImageNet", "KAIST Multispectral Pedestrian Detection Benchmark"], "year": 2016, "keyphrase_query": "synthesizing photos label maps,reconstructing objects edge maps,colorizing images", "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.", "task": "synthesizing photos from label maps,reconstructing objects from edge maps,colorizing images", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose using conditional adversarial networks as a general-purpose solution to image-to-image translation problems such as synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images"}
{"documents": ["Cityscapes", "GTA5", "MNIST", "SVHN", "SYNTHIA"], "year": 2017, "keyphrase_query": "domain adaptation visual recognition prediction image", "abstract": "Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.", "task": "domain adaptation for visual recognition and prediction", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We propose a new adversarial domain adaptation model that leverages cycle-consistency and task-specific losses."}
{"documents": ["WMT 2014"], "year": 2017, "keyphrase_query": "machine translation text", "abstract": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new\"super-separable\"convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.", "task": "machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "A new deep neural network architecture for machine translation using depthwise separable convolutions"}
{"documents": [], "year": 2012, "keyphrase_query": "transportation mode classification gps positional data", "abstract": "Understanding travel behaviour and travel demand is of constant importance to transportation communities and agencies in every country. Nowadays, attempts have been made to automatically infer transportation modes from positional data, such as the data collected by using GPS devices so that the cost in time and budget of conventional travel diary survey could be significantly reduced. Some limitations, however, exist in the literature, in aspects of data collection (sample size selected, duration of study, granularity of data), selection of variables (or combination of variables), and method of inference (the number of transportation modes to be used in the learning). This paper therefore, attempts to fully understand these aspects in the process of inference. We aim to solve a classification problem of GPS data into different transportation modes (car, walk, cycle, underground, train and bus). We first study the variables that could contribute positively to this classification, and statistically quantify their discriminatory power. We then introduce a novel approach to carry out this inference using a framework based on Support Vector Machines (SVMs) classification. The framework was tested using coarse-grained GPS data, which has been avoided in previous studies, achieving a promising accuracy of 88% with a Kappa statistic reflecting almost perfect agreement.", "task": "transportation mode classification", "domain": "transportation", "modality": "GPS positional data", "language": "", "training_style": "", "text_length": "", "query": "A novel approach using Support Vector Machine to classify mode of transportation from GPS positional data"}
{"documents": [], "year": 2015, "keyphrase_query": "semantic matching text", "abstract": "Semantic matching is of central importance to many natural language tasks \\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.", "task": "semantic matching", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level", "query": "A model for semantic matching using convolutional model"}
{"documents": ["COCO", "MNIST"], "year": 2015, "keyphrase_query": "image generation natural language description text", "abstract": "Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.", "task": "image generation from natural language description", "domain": "", "modality": "text and image", "language": "", "training_style": "", "text_length": "", "query": "A model that generates images from natural language description by iteratively drawing patches"}
{"documents": ["CoNLL 2002", "CoNLL-2003"], "year": 2016, "keyphrase_query": "named entity recognition text", "abstract": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.", "task": "named entity recognition", "domain": "", "modality": "Text", "language": "", "training_style": "supervised and unsupervised", "text_length": "", "query": "Two new neural architectures for named entity recognition which rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora"}
{"documents": ["T-LESS"], "year": 2019, "keyphrase_query": "object detection, 6d pose estimation image", "abstract": "We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results.", "task": "object detection, 6D pose estimation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "A real-time RGB-based pipeline based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization"}
{"documents": ["IJB-A", "PRID2011", "iLIDS-VID"], "year": 2017, "keyphrase_query": "set recognition image", "abstract": "This paper targets on the problem of set to set recognition, which learns the metric between two image sets. Images in each set belong to the same identity. Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at this https URL", "task": "set to set recognition", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "A quality-aware network for set-to-set recognition of images"}
{"documents": ["DuReader", "MS MARCO"], "year": 2018, "keyphrase_query": "multi-passage machine reading comprehension text english, chinese", "abstract": "Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine. Compared with MRC on a single passage, multi-passage MRC is more challenging, since we are likely to get multiple confusing answer candidates from different passages. To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations. Specifically, we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings.", "task": "multi-passage machine reading comprehension", "domain": "", "modality": "Text", "language": "English, Chinese", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "We propose a model for reading comprehension with multiple evidence passages."}
{"documents": ["CIFAR-10", "CelebA", "ImageNet", "LSUN"], "year": 2016, "keyphrase_query": "modeling natural images", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "task": "modeling natural images", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I want to learn a probabilistic model of images."}
{"documents": ["ImageNet", "PASCAL VOC", "Places365"], "year": 2017, "keyphrase_query": "image classification", "abstract": "In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a method for image classification with a unique network topology."}
{"documents": ["300W", "AFLW2000-3D", "CelebA"], "year": 2018, "keyphrase_query": "face alignment, 3d reconstruction image", "abstract": "As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.", "task": "face alignment, 3D reconstruction", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We propose a model for facial analysis based on learning a 3D morphable model."}
{"documents": ["FB15k", "NELL", "WN18RR"], "year": 2018, "keyphrase_query": "knowledge base completion graphs", "abstract": "Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.", "task": "knowledge base completion", "domain": "", "modality": "Graphs", "language": "", "training_style": "Reinforcement Learning", "text_length": "", "query": "We propose a method for performing graph search using reinforcement learning and Monte Carlo tree search, with applications to knowledge graph completion."}
{"documents": ["BSD", "Set14", "Set5"], "year": 2016, "keyphrase_query": "single image super-resolution", "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.", "task": "single image super-resolution", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a method for single image super-resolution using convolutional neural networks."}
{"documents": ["300W", "AFW", "Helen", "IJB-A", "IJB-B"], "year": 2017, "keyphrase_query": "face pose regression image", "abstract": "We show how a simple convolutional neural network (CNN) can be trained to accurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose, directly from image intensities. We further explain how this FacePoseNet (FPN) can be used to align faces in 2D and 3D as an alternative to explicit facial landmark detection for these tasks. We claim that in many cases the standard means of measuring landmark detector accuracy can be misleading when comparing different face alignments. Instead, we compare our FPN with existing methods by evaluating how they affect face recognition accuracy on the IJB-A and IJB-B benchmarks: using the same recognition pipeline, but varying the face alignment method. Our results show that (a) better landmark detection accuracy measured on the 300W benchmark does not necessarily imply better face recognition accuracy. (b) Our FPN provides superior 2D and 3D face alignment on both benchmarks. Finally, (c), FPN aligns faces at a small fraction of the computational cost of comparably accurate landmark detectors. For many purposes, FPN is thus a far faster and far more accurate face alignment method than using facial landmark detectors.", "task": "face pose regression", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a CNN-based method to estimate the pose of a face in 6 degrees of freedom."}
{"documents": ["CIFAR-FS", "mini-Imagenet"], "year": 2019, "keyphrase_query": "image classification", "abstract": "Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, meta-learning typically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically, \"meta\" refers to training multiple tasks, and \"transfer\" is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum for MTL. We conduct experiments using (5-class, 1-shot) and (5-class, 5-shot) recognition tasks on two challenging few-shot learning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons to related works validate that our meta-transfer learning approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Few-shot", "text_length": "", "query": "We propose a method for few-shot learning called meta transfer learning."}
{"documents": ["WMT 2015"], "year": 2016, "keyphrase_query": "machine translation text", "abstract": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.", "task": "machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "We propose a method for character-level machine translation."}
{"documents": ["FB15k", "WN18"], "year": 2016, "keyphrase_query": "knowledge base link prediction graph", "abstract": "In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.", "task": "knowledge base link prediction", "domain": "", "modality": "Knowledge Graph", "language": "", "training_style": "", "text_length": "", "query": "We propose a method for knowledge graph link prediction based on complex embeddings."}
{"documents": ["CIFAR-10", "CIFAR-100", "MNIST", "SVHN"], "year": 2013, "keyphrase_query": "image processing", "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "task": "image processing", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a method for regularizing convolutional neural networks for image classification."}
{"documents": ["Penn Treebank", "WMT 2014"], "year": 2017, "keyphrase_query": "machine translation text english-to-german, english-to-french", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "task": "machine translation", "domain": "", "modality": "Text", "language": "English-to-German, English-to-French", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a method for modeling sequences using attention, with applications to machine translation."}
{"documents": ["WMT 2014"], "year": 2014, "keyphrase_query": "machine translation text", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "task": "Machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a novel neural network architecture that encodes source text into a fixed-length vector representation before subsequently decoding it out into target text. "}
{"documents": ["BSDS500", "DIV2K", "LIVE1", "Set14", "Set5"], "year": 2018, "keyphrase_query": "image reconstruction", "abstract": "We propose a simple, interpretable framework for solving a wide range of image reconstruction problems such as denoising and deconvolution. Given a corrupted input image, the model synthesizes a spatially varying linear filter which, when applied to the input image, reconstructs the desired output. The model parameters are learned using supervised or self-supervised training. We test this model on three tasks: non-uniform motion blur removal, lossy-compression artifact reduction and single image super resolution. We demonstrate that our model substantially outperforms state-of-the-art methods on all these tasks and is significantly faster than optimization-based approaches to deconvolution. Unlike models that directly predict output pixel values, the predicted filter flow is controllable and interpretable, which we demonstrate by visualizing the space of predicted filters for different tasks.", "task": "Image reconstruction", "domain": "", "modality": "Image", "language": "", "training_style": "Both supervised and semi-supervised", "text_length": "", "query": "We propose a  fast, controllable, and  interpretable method for image reconstruction that achieves state-of-the-art results by learning spatially variable linear filters."}
{"documents": ["IEMOCAP", "Multimodal Opinionlevel Sentiment Intensity"], "year": 2018, "keyphrase_query": "multimodal sentiment analysis video", "abstract": "Multimodal sentiment analysis is a very actively growing field of research. A promising area of opportunity in this field is to improve the multimodal fusion mechanism. We present a novel feature fusion strategy that proceeds in a hierarchical fashion, first fusing the modalities two in two and only then fusing all three modalities. On multimodal sentiment analysis of individual utterances, our strategy outperforms conventional concatenation of features by 1%, which amounts to 5% reduction in error rate. On utterance-level multimodal sentiment analysis of multi-utterance video clips, for which current state-of-the-art techniques incorporate contextual information from other utterances of the same clip, our hierarchical fusion gives up to 2.4% (almost 10% error rate reduction) over currently used concatenation. The implementation of our method is publicly available in the form of open-source code.", "task": "Multimodal sentiment analysis", "domain": "", "modality": "Video", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a novel feature fusion strategy for multimodal sentiment analysis that fuses features hierarchically, leading to a 5% reduction in error rate over standard concatenation. "}
{"documents": ["CIFAR-10", "CIFAR-100", "ImageNet", "MNIST"], "year": 2015, "keyphrase_query": "image classification", "abstract": "We introduce the\"exponential linear unit\"(ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.", "task": "Image Classification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a novel activation function for deep neural networks that leads to faster learning and better performance. "}
{"documents": ["Penn Treebank", "WikiText-2"], "year": 2018, "keyphrase_query": "language modeling, machine translation, headline generation text newswire, wikipedia english", "abstract": "This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: this https URL.", "task": "Language modeling, machine translation, headline generation", "domain": "newswire, wikipedia text", "modality": "Text", "language": "English", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a novel, RNN-based language model that uses activations from intermediate layers in addition to the final layer."}
{"documents": ["Arcade Learning Environment"], "year": 2018, "keyphrase_query": "reinforcement learning atari games", "abstract": "In this paper we introduce a simple approach for exploration in reinforcement learning (RL) that allows us to develop theoretically justified algorithms in the tabular case but that is also extendable to settings where function approximation is required. Our approach is based on the successor representation (SR), which was originally introduced as a representation defining state generalization by the similarity of successor states. Here we show that the norm of the SR, while it is being learned, can be used as a reward bonus to incentivize exploration. In order to better understand this transient behavior of the norm of the SR we introduce the substochastic successor representation (SSR) and we show that it implicitly counts the number of times each state (or feature) has been observed. We use this result to introduce an algorithm that performs as well as some theoretically sample-efficient approaches. Finally, we extend these ideas to a deep RL algorithm and show that it achieves state-of-the-art performance in Atari 2600 games when in a low sample-complexity regime.", "task": "Reinforcement Learning", "domain": "games", "modality": "Atari Games", "language": "", "training_style": "Reinforcement learning", "text_length": "", "query": "We propose a method for exploration in reinforcement learning based on the successor representation and use it in a deep reinforcement learning that achieves state-of-the-art performance on  Atari 2600 games when in a low sample-complexity regime."}
{"documents": ["WFLW"], "year": 2018, "keyphrase_query": "face detection image", "abstract": "Faster RCNN has achieved great success for generic object detection including PASCAL object detection and MS COCO object detection. In this report, we propose a detailed designed Faster RCNN method named FDNet1.0 for face detection. Several techniques were employed including multi-scale training, multi-scale testing, light-designed RCNN, some tricks for inference and a vote-based ensemble method. Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset (easy set, medium set, hard set).", "task": "Face detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a FasterRCNN-based method for face detection."}
{"documents": ["CUHK03", "DukeMTMC-reID", "Market-1501"], "year": 2017, "keyphrase_query": "person search image", "abstract": "Existing person re-identification benchmarks and methods mainly focus on matching cropped pedestrian images between queries and candidates. However, it is different from real-world scenarios where the annotations of pedestrian bounding boxes are unavailable and the target person needs to be searched from a gallery of whole scene images. To close the gap, we propose a new deep learning framework for person search. Instead of breaking it down into two separate tasks---pedestrian detection and person re-identification, we jointly handle both aspects in a single convolutional neural network. An Online Instance Matching (OIM) loss function is proposed to train the network effectively, which is scalable to datasets with numerous identities. To validate our approach, we collect and annotate a large-scale benchmark dataset for person search. It contains 18,184 images, 8,432 identities, and 96,143 pedestrian bounding boxes. Experiments show that our framework outperforms other separate approaches, and the proposed OIM loss function converges much faster and better than the conventional Softmax loss.", "task": "person search", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "End-to-end deep learning for person search."}
{"documents": ["Hutter Prize", "IAM", "Penn Treebank"], "year": 2016, "keyphrase_query": "character-level language modelling, handwriting sequence text", "abstract": "Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.", "task": "character-level language modelling, handwriting sequence modelling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "In this paper, we propose an end-to-end multiscale recurrent neural network architecture for character-level language modelling and handwriting sequence modelling."}
{"documents": ["QM9"], "year": 2017, "keyphrase_query": "molecular property prediction molecules", "abstract": "Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.", "task": "molecular property prediction", "domain": "", "modality": "Molecules", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We describe a new supervised learning framework for molecule prediction that is invariant to molecular symmetries."}
{"documents": ["Cityscapes", "PASCAL VOC"], "year": 2018, "keyphrase_query": "semantic image segmentation", "abstract": "Most existing methods of semantic segmentation still suffer from two aspects of challenges: intra-class inconsistency and inter-class indistinction. To tackle these two problems, we propose a Discriminative Feature Network (DFN), which contains two sub-networks: Smooth Network and Border Network. Specifically, to handle the intra-class inconsistency problem, we specially design a Smooth Network with Channel Attention Block and global average pooling to select the more discriminative features. Furthermore, we propose a Border Network to make the bilateral features of boundary distinguishable with deep semantic boundary supervision. Based on our proposed DFN, we achieve state-of-the-art performance 86.2% mean IOU on PASCAL VOC 2012 and 80.3% mean IOU on Cityscapes dataset.", "task": "semantic image segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a discriminative feature network to improve semantic segmentation."}
{"documents": ["WMT 2014"], "year": 2016, "keyphrase_query": "natural language generation text", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.", "task": "natural language generation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We use actor-critic methods to learn to generate language by directly optimizing for task-specific scores like BLEU."}
{"documents": ["Chairs", "FlyingThings3D", "KITTI", "MPI Sintel", "Middlebury", "UCF101"], "year": 2017, "keyphrase_query": "optical flow estimation image, video", "abstract": "The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.", "task": "optical flow estimation", "domain": "", "modality": "Image, Video", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We present a better machine learning algorithm for optical flow estimation."}
{"documents": ["COCO-Stuff", "CamVid", "Cityscapes"], "year": 2017, "keyphrase_query": "semantic segmentation image open domain", "abstract": "We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.", "task": "semantic segmentation", "domain": "open domain", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to develop a supervised semantic segmentatioon model for images"}
{"documents": ["SICK", "SNLI"], "year": 2015, "keyphrase_query": "natural language entailment text", "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "task": "Natural language entailment", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "I want to develop a supervised model for sentence-pair entailment"}
{"documents": ["CIFAR-10", "MNIST", "SVHN", "smallNORB"], "year": 2015, "keyphrase_query": "image classification", "abstract": "We present a neural network architecture and training method designed to enable very rapid training and low implementation complexity. Due to its training speed and the absence of iteratively-tuned parameters, the method has strong potential for applications requiring frequent retraining or online training. The approach is characterized by (a) convolutional filters based on biologically inspired visual processing filters, (b) randomly-valued classifier-stage input weights, (c) use of least squares regression to train the classifier output weights in a single batch, and (d) linear classifier-stage output units. We demonstrate the efficacy of the method by applying it to image classification. Our results match existing state-of-the-art results on the MNIST (0.37% error) and NORB-small (2.2% error) image classification databases, but with very fast training times compared to standard deep network approaches. The network's performance on the Google Street View House Number (SVHN) (4% error) database is also competitive with state-of-the art methods.", "task": "Image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to develop a supervised image classification model that trains very fast and can be used in an online setting"}
{"documents": ["CIFAR-10", "CIFAR-100", "ImageNet"], "year": 2015, "keyphrase_query": "classification, approximiation", "abstract": "Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. ::: ::: Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.", "task": "classification, approximiation", "domain": "", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to develop a method that helps supervised models converge faster and generates better representations"}
{"documents": ["MovieLens"], "year": 2014, "keyphrase_query": "matrix completion, recommender systems graph", "abstract": "The problem of finding the missing values of a matrix given a few of its entries, called matrix completion, has gathered a lot of attention in the recent years. Although the problem under the standard low rank assumption is NP-hard, Cand\\`es and Recht showed that it can be exactly relaxed if the number of observed entries is sufficiently large. In this work, we introduce a novel matrix completion model that makes use of proximity information about rows and columns by assuming they form communities. This assumption makes sense in several real-world problems like in recommender systems, where there are communities of people sharing preferences, while products form clusters that receive similar ratings. Our main goal is thus to find a low-rank solution that is structured by the proximities of rows and columns encoded by graphs. We borrow ideas from manifold learning to constrain our solution to be smooth on these graphs, in order to implicitly force row and column proximities. Our matrix recovery model is formulated as a convex non-smooth optimization problem, for which a well-posed iterative scheme is provided. We study and evaluate the proposed matrix completion on synthetic and real data, showing that the proposed structured low-rank recovery model outperforms the standard matrix completion model in many situations.", "task": "Matrix completion, recommender systems", "domain": "", "modality": "Graph", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I want to develop a matrix completion method that  can be used for recommender systems"}
{"documents": ["Arcade Learning Environment"], "year": 2012, "keyphrase_query": "evaluation ai technology development interactive games gaming", "abstract": "In this extended abstract we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by presenting a benchmark set of domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. We conclude with a brief update on the latest ALE developments. All of the software, including the benchmark agents, is publicly available.", "task": "evaluation of AI technology development", "domain": "gaming", "modality": "interactive games", "language": "", "training_style": "reinforcement learning", "text_length": "", "query": "I want to develop a framework to evaluate the development of platform-independent AI technologies"}
{"documents": [], "year": 2016, "keyphrase_query": "depiction-invariant object recognition image", "abstract": "Current state of the art object recognition architectures achieve impressive performance but are typically specialized for a single depictive style (e.g. photos only, sketches only). In this paper, we present SwiDeN: our Convolutional Neural Network (CNN) architecture which recognizes objects regardless of how they are visually depicted (line drawing, realistic shaded drawing, photograph etc.). In SwiDeN, we utilize a novel `deep' depictive style-based switching mechanism which appropriately addresses the depiction-specific and depiction-invariant aspects of the problem. We compare SwiDeN with alternative architectures and prior work on a 50-category Photo-Art dataset containing objects depicted in multiple styles. Experimental results show that SwiDeN outperforms other approaches for the depiction-invariant object recognition problem.", "task": "depiction-invariant object recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to develop a supervised depiction-invariant object detection model"}
{"documents": ["AFEW-VA", "RAF-DB", "SFEW"], "year": 2018, "keyphrase_query": "facial expression recognition image images", "abstract": "Classifying facial expressions into different categories requires capturing regional distortions of facial landmarks. We believe that second-order statistics such as covariance is better able to capture such distortions in regional facial fea- tures. In this work, we explore the benefits of using a man- ifold network structure for covariance pooling to improve facial expression recognition. In particular, we first employ such kind of manifold networks in conjunction with tradi- tional convolutional networks for spatial pooling within in- dividual image feature maps in an end-to-end deep learning manner. By doing so, we are able to achieve a recognition accuracy of 58.14% on the validation set of Static Facial Expressions in the Wild (SFEW 2.0) and 87.0% on the vali- dation set of Real-World Affective Faces (RAF) Database. Both of these results are the best results we are aware of. Besides, we leverage covariance pooling to capture the tem- poral evolution of per-frame features for video-based facial expression recognition. Our reported results demonstrate the advantage of pooling image-set features temporally by stacking the designed manifold network of covariance pool-ing on top of convolutional network layers.", "task": "facial expression recognition", "domain": "facial images", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to develop a model for supervised facial recognition from images and videos"}
{"documents": ["FLIC", "MPII Human Pose"], "year": 2016, "keyphrase_query": "human pose estimation image humans", "abstract": "This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a “stacked hourglass” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.", "task": "human pose estimation", "domain": "humans", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "This paper used a novel convolutional architecture using supervised training on images to obtain state of the art results on human pose estimation task"}
{"documents": ["Arcade Learning Environment"], "year": 2017, "keyphrase_query": "exploration reinforcement learning", "abstract": "We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.", "task": "exploration in reinforcement learning", "domain": "", "modality": "", "language": "", "training_style": "reinforcement learning", "text_length": "", "query": "A novel, efficient count-based exploration algorithm to scalably estimate uncertainty in unvisited states in reinforcement learning tasks"}
{"documents": ["CIFAR-10", "CIFAR-100", "ImageNet"], "year": 2014, "keyphrase_query": "object recognition image", "abstract": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.", "task": "object recognition", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "This paper introduced a more efficient CNN architecture for object recognition"}
{"documents": ["CIFAR-10", "CelebA", "ImageNet", "LSUN", "MNIST"], "year": 2016, "keyphrase_query": "draw samples target probability distributions", "abstract": "We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. As an application of our method, we propose an amortized MLE algorithm for training deep energy model, where a neural sampler is adaptively trained to approximate the likelihood function. Our method mimics an adversarial game between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results.", "task": "draw samples from target probability distributions", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "The paper proposed a simple algorithm to train stochastic neural networks to mimic target distributions"}
{"documents": ["COCO", "PASCAL VOC"], "year": 2018, "keyphrase_query": "multi-person pose estimation image humans", "abstract": "In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose estimation architecture that combines a multi-task model with a novel assignment method. MultiPoseNet can jointly handle person detection, keypoint detection, person segmentation and pose estimation problems. The novel assignment method is implemented by the Pose Residual Network (PRN) which receives keypoint and person detections, and produces accurate poses by assigning keypoints to person instances. On the COCO keypoints dataset, our pose estimation method outperforms all previous bottom-up methods both in accuracy (+4-point mAP over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster. Our method is the fastest real time system with 23 frames/sec. Source code is available at: https://github.com/mkocabas/pose-residual-network", "task": "multi-person pose estimation", "domain": "humans", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "An efficient algorithm to perform multi-person pose estimation on images."}
{"documents": ["English Web Treebank", "OntoNotes 5.0", "Penn Treebank"], "year": 2016, "keyphrase_query": "part-of-speech tagging, dependency parsing, sentence compression text", "abstract": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.", "task": "part-of-speech tagging, dependency parsing, sentence compression", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "A globally normalized neural network that achieves state-of-the-art results on part of speech tagging, dependency parsing and sentence compression on texts"}
{"documents": ["CIFAR-10"], "year": 2012, "keyphrase_query": "automatic optimization algorithms", "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.", "task": "automatic optimization of algorithms", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "A Bayesian optimization based algorithm optimization approach to improve automatic tuning of algorithms and architecture hyperparameters in machine learning"}
{"documents": ["20NewsGroups", "RCV1", "WikiQA"], "year": 2015, "keyphrase_query": "generative document modelling, supervised question answering text", "abstract": "Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.", "task": "generative document modelling, supervised question answering", "domain": "", "modality": "Text", "language": "", "training_style": "variational inference", "text_length": "sentence-level and paragraph-level", "query": "A generic variational inference framework for generative and conditional modeling of text, both sentence-level and paragraph level"}
{"documents": ["INRIA Holidays Dataset", "Oxford5k", "Paris6k"], "year": 2016, "keyphrase_query": "image retrieval", "abstract": "Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in many computer vision tasks. However, this achievement is preceded by extreme manual annotation in order to perform either training from scratch or fine-tuning for the target task. In this work, we propose to fine-tune CNN for image retrieval from a large collection of unordered images in a fully automated manner. We employ state-of-the-art retrieval and Structure-from-Motion (SfM) methods to obtain 3D models, which are used to guide the selection of the training data for CNN fine-tuning. We show that both hard positive and hard negative examples enhance the final performance in particular object retrieval with compact codes.", "task": "image retrieval", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "An approach that automates the fine-tuning of CNN architectures for image-retrieval from a large unordered collection of images using various heuristics"}
{"documents": ["CBT", "CNN/Daily Mail", "SearchQA"], "year": 2016, "keyphrase_query": "cloze-style question answering text", "abstract": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets.", "task": "cloze-style question answering", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We propose a simple model to answer text comprehension questions by choosing answers from the context."}
{"documents": ["COCO-QA", "Visual Question Answering"], "year": 2015, "keyphrase_query": "visual question answering image text", "abstract": "We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks. To explore the strength and weakness of the trained model, we also provide an interactive web demo and open-source code. .", "task": "visual question answering", "domain": "", "modality": "image and text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a simple baseline for visual question answering."}
{"documents": ["Penn Treebank", "Text8"], "year": 2015, "keyphrase_query": "question answering, language modeling text", "abstract": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.", "task": "question answering, language modeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We introduce a model that improves on diverse tasks, including question answering and language modeling. "}
{"documents": ["SST", "TREC-10"], "year": 2015, "keyphrase_query": "sentiment classification, question text", "abstract": "Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.", "task": "sentiment classification, question classification", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a model to improve sentence representation and text classification, and evaluate this model on downstream sentiment classification and question classification tasks."}
{"documents": ["ImageNet", "Omniglot", "Penn Treebank"], "year": 2016, "keyphrase_query": "image classification, part speech tagging, dependency parsing", "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.", "task": "image classification, part of speech tagging, dependency parsing", "domain": "", "modality": "", "language": "", "training_style": "Few-shot", "text_length": "", "query": "We propose a framework for few-shot learning on vision and language tasks."}
{"documents": ["ImageNet"], "year": 2013, "keyphrase_query": "image classification, localization, object detection", "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", "task": "image classification, localization, object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We present a unified framework for image localization, detection, and classification."}
{"documents": ["Penn Treebank", "WikiText-2"], "year": 2019, "keyphrase_query": "word-level language modeling text", "abstract": "Although SGD requires shuffling the training data between epochs, currently none of the word-level language modeling systems do this. Naively shuffling all sentences in the training data would not permit the model to learn inter-sentence dependencies. Here we present a method that partially shuffles the training data between epochs. This method makes each batch random, while keeping most sentence ordering intact. It achieves new state of the art results on word-level language modeling on both the Penn Treebank and WikiText-2 datasets.", "task": "word-level language modeling", "domain": "", "modality": "Text", "language": "", "training_style": "Self-supervised", "text_length": "Sentence-level", "query": "Sentence order preserving, partial data shuffling method for batch randomization for word-level language modeling"}
{"documents": ["FB15k", "FB15k-237", "WN18", "WN18RR"], "year": 2019, "keyphrase_query": "link prediction text", "abstract": "Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively straightforward but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms previous state-of-the-art models across standard link prediction datasets, acting as a strong baseline for more elaborate models. We show that TuckER is a fully expressive model, derive sufficient bounds on its embedding dimensionalities and demonstrate that several previously introduced linear models can be viewed as special cases of TuckER.", "task": "link prediction", "domain": "", "modality": "Text", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "Tucker decomposition-based linear model for knowledge graph link prediction."}
{"documents": ["SNLI"], "year": 2015, "keyphrase_query": "entailment recognition text", "abstract": "While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.", "task": "entailment recognition", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "A neural model with word-by-word neural attention for entailment recognition."}
{"documents": ["SQuAD", "TriviaQA"], "year": 2017, "keyphrase_query": "machine comprehension text", "abstract": "Machine comprehension(MC) style question answering is a representative problem in natural language processing. Previous methods rarely spend time on the improvement of encoding layer, especially the embedding of syntactic information and name entity of the words, which are very crucial to the quality of encoding. Moreover, existing attention methods represent each query word as a vector or use a single vector to represent the whole query sentence, neither of them can handle the proper weight of the key words in query sentence. In this paper, we introduce a novel neural network architecture called Multi-layer Embedding with Memory Network(MEMEN) for machine reading task. In the encoding layer, we employ classic skip-gram model to the syntactic and semantic information of the words to train a new kind of embedding layer. We also propose a memory network of full-orientation matching of the query and passage to catch more pivotal information. Experiments show that our model has competitive results both from the perspectives of precision and efficiency in Stanford Question Answering Dataset(SQuAD) among all published results and achieves the state-of-the-art results on TriviaQA dataset.", "task": "machine comprehension", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to build a memory network with query passage orientation matching for machine comprehension."}
{"documents": ["CBT", "CNN/Daily Mail", "Who-did-What"], "year": 2016, "keyphrase_query": "cloze-style question answering text news", "abstract": "In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at this https URL", "task": "cloze-style question answering", "domain": "News", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "A multi-hop architecture with a novel intermediate document and query representation attention for cloze-style question answering."}
{"documents": ["CUB-200-2011"], "year": 2014, "keyphrase_query": "fine-grained categorization image", "abstract": "Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.", "task": "fine-grained categorization", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "Model for fine-grained bird category prediction from pose normalized representations using deep convolutional features over bottom up region proposals, and learned object and part detectors, and geometric constraints."}
{"documents": ["CoNLL-2014 Shared Task: Grammatical Error Correction", "FCE"], "year": 2017, "keyphrase_query": "error detection text", "abstract": "Shortage of available training data is holding back progress in the area of automated error detection. This paper investigates two alternative methods for artificially generating writing errors, in order to create additional resources. We propose treating error generation as a machine translation task, where grammatically correct text is translated to contain errors. In addition, we explore a system for extracting textual patterns from an annotated corpus, which can then be used to insert errors into grammatically correct sentences. Our experiments show that the inclusion of artificially generated errors significantly improves error detection accuracy on both FCE and CoNLL 2014 datasets.", "task": "error detection", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "Machine translation formulation and system for extracting textual patterns for creation of artificial data to improve error detection."}
{"documents": ["Arcade Learning Environment"], "year": 2015, "keyphrase_query": "reinforcement learning", "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.", "task": "reinforcement learning", "domain": "", "modality": "", "language": "", "training_style": "Reinforcement Learning", "text_length": "", "query": "Novel dueling neural networks based architecture for model-free reinforcement learning"}
{"documents": ["CUHK03", "DukeMTMC-reID", "Market-1501"], "year": 2017, "keyphrase_query": "person re-identification image", "abstract": "This paper proposes the SVDNet for retrieval problems, with focus on the application of person re-identification (reID). We view each weight vector within a fully connected (FC) layer in a convolutional neuron network (CNN) as a projection basis. It is observed that the weight vectors are usually highly correlated. This problem leads to correlations among entries of the FC descriptor, and compromises the retrieval performance based on the Euclidean distance. To address the problem, this paper proposes to optimize the deep representation learning process with Singular Vector Decomposition (SVD). Specifically, with the restraint and relaxation iteration (RRI) training scheme, we are able to iteratively integrate the orthogonality constraint in CNN training, yielding the so-called SVDNet. We conduct experiments on the Market-1501, CUHK03, and DukeMTMC-reID datasets, and show that RRI effectively reduces the correlation among the projection vectors, produces more discriminative FC descriptors, and significantly improves the re-ID accuracy. On the Market-1501 dataset, for instance, rank-1 accuracy is improved from 55.3% to 80.5% for CaffeNet, and from 73.8% to 82.3% for ResNet-50.", "task": "person re-identification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "SVD-augmented representation learning of CNNs for person re-identification."}
{"documents": ["Omniglot", "mini-Imagenet"], "year": 2017, "keyphrase_query": "meta-learning, few-shot image classification, regression, reinforcement", "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.", "task": "meta-learning, few-shot image classification, few-shot regression, reinforcement learning", "domain": "", "modality": "", "language": "", "training_style": "Few-shot", "text_length": "", "query": "Model-agnostic meta-learning for few shot generalization"}
{"documents": ["Amazon Review", "SST", "SemEval-2018 Task 1"], "year": 2018, "keyphrase_query": "multidimensional emotion classification, sentiment text", "abstract": "Multi-emotion sentiment classification is a natural language processing (NLP) problem with valuable use cases on real-world data. We demonstrate that large-scale unsupervised language modeling combined with finetuning offers a practical solution to this task on difficult datasets, including those with label class imbalance and domain-specific context. By training an attention-based Transformer network (Vaswani et al. 2017) on 40GB of text (Amazon reviews) (McAuley et al. 2015) and fine-tuning on the training set, our model achieves a 0.69 F1 score on the SemEval Task 1:E-c multi-dimensional emotion classification problem (Mohammad et al. 2018), based on the Plutchik wheel of emotions (Plutchik 1979). These results are competitive with state of the art models, including strong F1 scores on difficult (emotion) categories such as Fear (0.73), Disgust (0.77) and Anger (0.78), as well as competitive results on rare categories such as Anticipation (0.42) and Surprise (0.37). Furthermore, we demonstrate our application on a real world text classification task. We create a narrowly collected text dataset of real tweets on several topics, and show that our finetuned model outperforms general purpose commercially available APIs for sentiment and multidimensional emotion classification on this dataset by a significant margin. We also perform a variety of additional studies, investigating properties of deep learning architectures, datasets and algorithms for achieving practical multidimensional sentiment classification. Overall, we find that unsupervised language modeling and finetuning is a simple framework for achieving high quality results on real-world sentiment classification.", "task": "multidimensional emotion classification, sentiment classification", "domain": "", "modality": "Text", "language": "", "training_style": "Unsupervised, Supervised training/finetuning", "text_length": "", "query": "Improving multi-dimensional emotion and sentiment classification through large scale unsupervised transformer based language models"}
{"documents": ["BC2GM", "BC4CHEMD", "BC5CDR", "JNLPBA", "NCBI Disease"], "year": 2018, "keyphrase_query": "biomedical named entity recognition text", "abstract": "Finding biomedical named entities is one of the most essential tasks in biomedical text mining. Recently, deep learning-based approaches have been applied to biomedical named entity recognition (BioNER) and showed promising results. However, as deep learning approaches need an abundant amount of training data, a lack of data can hinder performance. BioNER datasets are scarce resources and each dataset covers only a small subset of entity types. Furthermore, many bio entities are polysemous, which is one of the major obstacles in named entity recognition. To address the lack of data and the entity type misclassification problem, we propose CollaboNet which utilizes a combination of multiple NER models. In CollaboNet, models trained on a different dataset are connected to each other so that a target model obtains information from other collaborator models to reduce false positives. Every model is an expert on their target entity type and takes turns serving as a target and a collaborator model during training time. The experimental results show that CollaboNet can be used to greatly reduce the number of false positives and misclassified entities including polysemous words. CollaboNet achieved state-of-the-art performance in terms of precision, recall and F1 score. We demonstrated the benefits of combining multiple models for BioNER. Our model has successfully reduced the number of misclassified entities and improved the performance by leveraging multiple datasets annotated for different entity types. Given the state-of-the-art performance of our model, we believe that CollaboNet can improve the accuracy of downstream biomedical text mining applications such as bio-entity relation extraction.", "task": "biomedical named entity recognition", "domain": "biomedical", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "A combination of expert deep neural networks for biomedical NER."}
{"documents": ["Criteo"], "year": 2017, "keyphrase_query": "ctr prediction raw features", "abstract": "Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.", "task": "CTR prediction", "domain": "", "modality": "raw features", "language": "", "training_style": "", "text_length": "", "query": "We propose a new model for learning sophisticated feature interactions behind user behaviors."}
{"documents": ["WMT 2015"], "year": 2015, "keyphrase_query": "neural machine translation text", "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English!German and English!Russian by up to 1.1 and 1.3 BLEU, respectively.", "task": "neural machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce a new approach to neural machine translation which encodes rare and unknown words as sequences of sub-word units."}
{"documents": ["Multi-Domain Sentiment Dataset v2.0"], "year": 2018, "keyphrase_query": "distributional correspondence indexing text", "abstract": "This paper introduces PyDCI, a new implementation of Distributional Correspondence Indexing (DCI) written in Python. DCI is a transfer learning method for cross-domain and cross-lingual text classification for which we had provided an implementation (here called JaDCI) built on top of JaTeCS, a Java framework for text classification. PyDCI is a stand-alone version of DCI that exploits scikit-learn and the SciPy stack. We here report on new experiments that we have carried out in order to test PyDCI, and in which we use as baselines new high-performing methods that have appeared after DCI was originally proposed. These experiments show that, thanks to a few subtle ways in which we have improved DCI, PyDCI outperforms both JaDCI and the above-mentioned high-performing methods, and delivers the best known results on the two popular benchmarks on which we had tested DCI, i.e., MultiDomainSentiment (a.k.a. MDS -- for cross-domain adaptation) and Webis-CLS-10 (for cross-lingual adaptation). PyDCI, together with the code allowing to replicate our experiments, is available at https://github.com/AlexMoreo/pydci .", "task": "Distributional Correspondence Indexing", "domain": "", "modality": "Text", "language": "", "training_style": "Transfer learning", "text_length": "", "query": "We introduce a transfer learning method for cross-domain and cross-lingual text classification."}
{"documents": ["ModelNet"], "year": 2016, "keyphrase_query": "3d object recognition geometry representation", "abstract": "Building discriminative representations for 3D data has been an important task in computer graphics and computer vision research. Convolutional Neural Networks (CNNs) have shown to operate on 2D images with great success for a variety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a plausible and promising next step. Unfortunately, the computational complexity of 3D CNNs grows cubically with respect to voxel resolution. Moreover, since most 3D geometry representations are boundary based, occupied regions do not increase proportionately with the size of the discretization, resulting in wasted computation. In this work, we represent 3D spaces as volumetric fields, and propose a novel design that employs field probing filters to efficiently extract features from them. Each field probing filter is a set of probing points -- sensors that perceive the space. Our learning algorithm optimizes not only the weights associated with the probing points, but also their locations, which deforms the shape of the probing filters and adaptively distributes them in 3D space. The optimized probing points sense the 3D space \"intelligently\", rather than operating blindly over the entire domain. We show that field probing is significantly more efficient than 3DCNNs, while providing state-of-the-art performance, on classification tasks for 3D object recognition benchmark datasets.", "task": "3D object recognition", "domain": "", "modality": "3D geometry representation", "language": "", "training_style": "", "text_length": "", "query": "We propose an extension of convolutional neural networks into 3D space by representing 3D spaces as volumetric fields and employing field probing filters to efficiently extract features from 3D objects."}
{"documents": ["CIFAR-10", "CelebA", "LSUN", "STL-10"], "year": 2018, "keyphrase_query": "image generation", "abstract": "Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the repulsive loss function. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization.", "task": "image generation", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We enhance MMD-GAN by proposing a repulsive loss function and a bounded Gaussian kernel."}
{"documents": ["BUCC", "ParaCrawl", "United Nations Parallel Corpus"], "year": 2018, "keyphrase_query": "machine translation text english, french, german", "abstract": "Machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. In this paper, we propose a new method for this task based on multilingual sentence embeddings. Our approach uses an encoder-decoder trained over an initial parallel corpus to build multilingual sentence representations, which are then incorporated into a new margin-based method to score, mine and filter parallel sentences. In contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. Our experiments show large improvements over existing methods. We outperform the best published results on the BUCC shared task on parallel corpus mining by more than 10 F1 points. We also improve the precision from 48.9 to 83.3 on the reconstruction of 11.3M English-French sentence pairs of the UN corpus. Finally, filtering the English-German ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version.", "task": "machine translation", "domain": "", "modality": "Text", "language": "English, French, German", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "We propose a new approach to multilingual translation based on multilingual sentence embeddings."}
{"documents": ["CUHK03", "DukeMTMC-reID", "Market-1501"], "year": 2017, "keyphrase_query": "person re-identification image", "abstract": "The main contribution of this paper is a simple semi-supervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at this https URL.", "task": "person re-identification", "domain": "", "modality": "Image", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We propose a semi-supervised pipeline by leveraging GAN-generated data and verify the pipeline on the task of person re-identification."}
{"documents": ["IWSLT 2019", "WMT 2014", "WMT 2016"], "year": 2017, "keyphrase_query": "machine translation text", "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English-German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.", "task": "machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level, paragraph-level", "query": "We propose a machine translation model that avoids the autoregressive property and produces outputs in parallel, allowing an order of magnitude lower latency during inference."}
{"documents": ["Ali-CCP", "Amazon Review", "NYUv2", "PASCAL Context", "PASCAL VOC"], "year": 2016, "keyphrase_query": "semantic image segmentation", "abstract": "Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information; specifically, we explore `patch-patch' context between image regions, and `patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset.", "task": "semantic image segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I explore methods to improve semantic image segmentation via incorporating contextual information."}
{"documents": ["MovieLens"], "year": 2018, "keyphrase_query": "click-through rate prediction online advertising", "abstract": "Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.", "task": "click-through rate prediction", "domain": "online advertising", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to develop industrial deep networks hundreds of millions of parameters for click-through rate prediction in online advertising."}
{"documents": ["Switchboard-1 Corpus"], "year": 2016, "keyphrase_query": "conversational speech recognition", "abstract": "Conversational speech recognition has served as a flagship speech recognition task since the release of the Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcribers is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state of the art, and edges past the human benchmark, achieving error rates of 5.8% and 11.0%, respectively. The key to our system's performance is the use of various convolutional and LSTM acoustic model architectures, combined with a novel spatial smoothing method and lattice-free MMI acoustic training, multiple recurrent neural network language modeling approaches, and a systematic use of system combination.", "task": "conversational speech recognition", "domain": "", "modality": "speech", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I developed a conversational speech recognition system using convolutional and LSTM acoustic model architectures that achieve lower error rates than human transcribers."}
{"documents": ["ASPEC", "IMDb Movie Reviews", "IWSLT2015"], "year": 2017, "keyphrase_query": "text compression, machine translation, sentiment analysis,", "abstract": "Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.", "task": "text compression, machine translation, sentiment analysis, ", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a multi-codebook quantization-based method for compressing word embeddings."}
{"documents": ["CoNLL-2000", "CoNLL-2003", "Penn Treebank", "Universal Dependencies"], "year": 2017, "keyphrase_query": "pos tagging text", "abstract": "Adversarial training (AT) is a powerful regularization method for neural networks, aiming to achieve robustness to input perturbations. Yet, the specific effects of the robustness obtained by AT are still unclear in the context of natural language processing. In this paper, we propose and analyze a neural POS tagging model that exploits adversarial training (AT). In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies (UD) dataset (28 languages), we find that AT not only improves the overall tagging accuracy, but also 1) largely prevents overfitting in low resource languages and 2) boosts tagging accuracy for rare / unseen words. The proposed POS tagger achieves state-of-the-art performance on nearly all of the languages in UD v1.2. We also demonstrate that 3) the improved tagging performance by AT contributes to the downstream task of dependency parsing, and that 4) AT helps the model to learn cleaner word and internal representations. These positive results motivate further use of AT for natural language tasks.", "task": "POS tagging", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "Adversarial training (AT) of neural POS tagging models prevents overfitting in low resource languages and improves downstream dependency parsing."}
{"documents": ["CompCars"], "year": 2015, "keyphrase_query": "car model classification, verification, attribute prediction image", "abstract": "This paper aims to highlight vision related tasks centered around “car”, which has been largely neglected by vision community in comparison to other objects. We show that there are still many interesting car-related problems and applications, which are not yet well explored and researched. To facilitate future car-related research, in this paper we present our on-going effort in collecting a large-scale dataset, “CompCars”, that covers not only different car views, but also their different internal and external parts, and rich attributes. Importantly, the dataset is constructed with a cross-modality nature, containing a surveillance-nature set and a web-nature set. We further demonstrate a few important applications exploiting the dataset, namely car model classification, car model verification, and attribute prediction. We also discuss specific challenges of the car-related problems and other potential applications that worth further investigations. The latest dataset can be downloaded at http://mmlab.ie.cuhk.edu.hk/ datasets/comp_cars/index.html", "task": " car model classification, car model verification, attribute prediction", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We want a dataset that covers rich attributes of cars not captured or studied in the vision community for tasks such as car model classification, car model verification, and attribute prediction."}
{"documents": ["DukeMTMC-reID", "Market-1501"], "year": 2018, "keyphrase_query": "person re-identification image", "abstract": "Being a cross-camera retrieval task, person re-identification suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle can serve as a data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against over-fitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which over-fitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of over-fitting. We also report competitive accuracy compared with the state of the art. Code is available at: https://github.com/zhunzhong07/CamStyle", "task": "person re-identification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to improve the person re-identification task by explicitly adapting to different camera styles."}
{"documents": [], "year": 2018, "keyphrase_query": "query understanding, text classification", "abstract": "Understanding search queries is a hard problem as it involves dealing with \"word salad\" text ubiquitously issued by users. However, if a query resembles a well-formed question, a natural language processing pipeline is able to perform more accurate interpretation, thus reducing downstream compounding errors. Hence, identifying whether or not a query is well formed can enhance query understanding. Here, we introduce a new task of identifying a well-formed natural language question. We construct and release a dataset of 25,100 publicly available questions classified into well-formed and non-wellformed categories and report an accuracy of 70.7% on the test set. We also show that our classifier can be used to improve the performance of neural sequence-to-sequence models for generating questions for reading comprehension.", "task": "query understanding, text classification", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want to perform text classification for query understanding, to determine whether questions are well-formed or not."}
{"documents": ["CoNLL-2009", "CoNLL-2012"], "year": 2019, "keyphrase_query": "semantic role labeling text", "abstract": "Semantic role labeling (SRL) aims to discover the predicateargument structure of a sentence. End-to-end SRL without syntactic input has received great attention. However, most of them focus on either span-based or dependency-based semantic representation form and only show specific model optimization respectively. Meanwhile, handling these two SRL tasks uniformly was less successful. This paper presents an end-to-end model for both dependency and span SRL with a unified argument representation to deal with two different types of argument annotations in a uniform fashion. Furthermore, we jointly predict all predicates and arguments, especially including long-term ignored predicate identification subtask. Our single model achieves new state-of-the-art results on both span (CoNLL 2005, 2012) and dependency (CoNLL 2008, 2009) SRL benchmarks.", "task": "semantic role labeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level", "query": "I want to design a single unified model for both span-based and dependency-based input representations for semantic role labeling"}
{"documents": ["Arcade Learning Environment"], "year": 2015, "keyphrase_query": "atari 2600 games", "abstract": "We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.", "task": "Atari 2600 games", "domain": "", "modality": "games", "language": "", "training_style": "", "text_length": "", "query": "I want to build a distributed architecture for deep reinforcement learning in 49 Atari 2600 games."}
{"documents": ["COCO", "Cityscapes"], "year": 2020, "keyphrase_query": "instance segmentation, object recognition image", "abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron .", "task": "instance segmentation, object recognition", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to build a system for instance segmentation and object detection that can quickly detect objects and provide object masks."}
{"documents": [], "year": 2018, "keyphrase_query": "speech synthesis audio", "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and $F_0$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.", "task": "speech synthesis", "domain": "", "modality": "audio", "language": "", "training_style": "", "text_length": "", "query": "I want to make a neural network for synthesizing speech directly from text."}
{"documents": ["WMT 2014", "WMT 2016 News"], "year": 2016, "keyphrase_query": "machine translation text english, czech, german, romanian, russian", "abstract": "We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3–11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.12", "task": "machine translation", "domain": "", "modality": "Text", "language": "English, Czech, German, Romanian, Russian", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to build a machine translation system for news articles in english, czech, german, romanian, and russian."}
{"documents": [], "year": 2014, "keyphrase_query": "network classification networks", "abstract": "We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.", "task": "network classification", "domain": "", "modality": "networks", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I want to learn representations of vertices in a social network that will be useful for network classification."}
{"documents": ["Hutter Prize", "Penn Treebank", "WikiText-2"], "year": 2018, "keyphrase_query": "language modeling text", "abstract": "Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our past decode regularization (PDR) method achieves state-of-the-art word level perplexity on the Penn Treebank (55.6) and WikiText-2 (63.5) datasets and bits-per-character on the Penn Treebank Character (1.169) dataset for character level language modeling. Using dynamic evaluation, we also achieve the first sub 50 perplexity of 49.3 on the Penn Treebank test set.", "task": "language modeling", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "Proposed method regularizes character level language modeling with LSTMs."}
{"documents": ["PASCAL Context", "PASCAL VOC"], "year": 2015, "keyphrase_query": "semantic segmentation image", "abstract": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at this https URL .", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a method for adding global context into semantic segmentation models based on convolutional neural networks."}
{"documents": ["SimpleQuestions", "WikiMovies", "WikiQA"], "year": 2016, "keyphrase_query": "question answering text movies", "abstract": "Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark.", "task": "question answering", "domain": "Movies", "modality": "Text", "language": "", "training_style": "", "text_length": "document-level", "query": "We propose key-value memory networks for document level information extraction and question answering."}
{"documents": ["AFAD", "CACD", "MORPH", "UTKFace"], "year": 2019, "keyphrase_query": "ordinal regression image faces", "abstract": "While extraordinary progress has been made towards developing neural network architectures for classification tasks, commonly used loss functions such as the multi-category cross entropy loss are inadequate for ranking and ordinal regression problems. To address this issue, approaches have been developed that transform ordinal target variables series of binary classification tasks, resulting in robust ranking algorithms with good generalization performance. However, to model ordinal information appropriately, ideally, a rank-monotonic prediction function is required such that confidence scores are ordered and consistent. We propose a new framework (Consistent Rank Logits, CORAL) with theoretical guarantees for rank-monotonicity and consistent confidence scores. Through parameter sharing, our framework benefits from low training complexity and can easily be implemented to extend common convolutional neural network classifiers for ordinal regression tasks. Furthermore, our empirical results support the proposed theory and show a substantial improvement compared to the current state-of-the-art ordinal regression method for age prediction from face images.", "task": "ordinal regression", "domain": "faces", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose the CORAL framework for ordinal regression for age prediction from facial images."}
{"documents": ["CIFAR-10", "CIFAR-100"], "year": 2014, "keyphrase_query": "sequence classification", "abstract": "Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.", "task": "sequence classification", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "We propose Deep Attention Selective Network (DasNet) that allows for increased sensitivity in sequence processing."}
{"documents": ["Criteo", "iPinYou"], "year": 2016, "keyphrase_query": "click rate conversion image, audio web search, personalised recommendation, online advertising", "abstract": "Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users’ ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.", "task": "Click Through Rate and Conversion Rate", "domain": "web search, personalised recommendation, and online advertising", "modality": "Image, Audio", "language": "", "training_style": "", "text_length": "", "query": "We propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users’ ad clicks, and three feature transformation methods."}
{"documents": ["CNN/Daily Mail"], "year": 2016, "keyphrase_query": "document comprehension text news english", "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6% and 76.6% on these two datasets, exceeding current state-of-the-art results by 7-10% and approaching what we believe is the ceiling for performance on this task.", "task": "Document Comprehension", "domain": "News", "modality": "Text", "language": "English", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "We investigate what levels of document understanding are required for the novel CNN and Daily News reading comprehension task."}
{"documents": ["PASCAL VOC 2007"], "year": 2016, "keyphrase_query": "object detection image", "abstract": "Weakly supervised learning of object detection is an important problem in image understanding that still does not have a satisfactory solution. In this paper, we address this problem by exploiting the power of deep convolutional neural networks pre-trained on large-scale image-level classification tasks. We propose a weakly supervised deep detection architecture that modifies one such network to operate at the level of image regions, performing simultaneously region selection and classification. Trained as an image classifier, the architecture implicitly learns object detectors that are better than alternative weakly supervised detection systems on the PASCAL VOC data. The model, which is a simple and elegant end-to-end architecture, outperforms standard data augmentation and fine-tuning techniques for the task of image-level classification as well.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a weakly supervised deep detection architecture that modifies one such network to operate at the level of image regions, performing simultaneously region selection and classification. "}
{"documents": ["YCB-Video"], "year": 2017, "keyphrase_query": "6d object pose estimation video robotics", "abstract": "Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at this https URL.", "task": "6D Object Pose Estimation", "domain": "Robotics", "modality": "Video", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation."}
{"documents": ["COCO", "DAQUAR", "Visual Question Answering"], "year": 2015, "keyphrase_query": "visual question answering images text", "abstract": "We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single \"hop\" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3].", "task": "visual question answering", "domain": "", "modality": "images and text", "language": "", "training_style": "", "text_length": "", "query": "We aim to improve visual question answering by better modeling spatial inference."}
{"documents": ["KITTI", "SUN RGB-D"], "year": 2018, "keyphrase_query": "3d object detection images, point clouds", "abstract": "We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multi-stage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and application-agnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors. We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform better or on-par with the state-of-the-art on these diverse datasets without any dataset-specific model tuning.", "task": "3D object detection", "domain": "", "modality": "images, point clouds", "language": "", "training_style": "", "text_length": "", "query": "I want to build a 3D object detection system that can use both image and 3D point cloud information."}
{"documents": ["CelebA", "LFW"], "year": 2015, "keyphrase_query": "face recognition image", "abstract": "This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts. (3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.", "task": "face recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a model for face recognition with better robustness to occlusions."}
{"documents": ["CIFAR-10", "CelebA", "ImageNet"], "year": 2018, "keyphrase_query": "image generation, super resolution", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.", "task": "image generation, image super resolution", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to train an image generation model that can generate images or be used for image super resolution."}
{"documents": ["CelebA", "MNIST", "SVHN"], "year": 2016, "keyphrase_query": "image generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "task": "image generation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to develop a method that can transfer samples between different visual domains to generate images of previously unseen entities."}
{"documents": ["CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings", "Penn Treebank"], "year": 2018, "keyphrase_query": "part-of-speech tagging, morphological text", "abstract": "The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with learned and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations. In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states. We present results on part-of-speech and morphological tagging with state-of-the-art performance on a number of languages.", "task": "part-of-speech tagging, morphological tagging", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level", "query": "We investigate models that use recurrent neural networks with sentence-level context for part-of-speech tagging and morphological tagging."}
{"documents": ["INRIA Holidays Dataset", "ImageNet", "YFCC100M"], "year": 2019, "keyphrase_query": "image classification, object retrieval", "abstract": "MultiGrain is a network architecture producing compact vector representations that are suited both for image classification and particular object retrieval. It builds on a standard classification trunk. The top of the network produces an embedding containing coarse and fine-grained information, so that images can be recognized based on the object class, particular object, or if they are distorted copies. Our joint training is simple: we minimize a cross-entropy loss for classification and a ranking loss that determines if two images are identical up to data augmentation, with no need for additional labels. A key component of MultiGrain is a pooling layer that takes advantage of high-resolution images with a network trained at a lower resolution. ::: When fed to a linear classifier, the learned embeddings provide state-of-the-art classification accuracy. For instance, we obtain 79.4% top-1 accuracy with a ResNet-50 learned on Imagenet, which is a +1.8% absolute improvement over the AutoAugment method. When compared with the cosine similarity, the same embeddings perform on par with the state-of-the-art for image retrieval at moderate resolutions.", "task": "image classification, object retrieval", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a supervised image classification model"}
{"documents": ["CIFAR-10", "ImageNet", "MNIST"], "year": 2016, "keyphrase_query": "image modeling", "abstract": "Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.", "task": "image modeling", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I want to develop an unsupervised image model to predict pixels using datasets of natural images like ImageNet"}
{"documents": ["CIFAR-10", "CIFAR-100", "ImageNet"], "year": 2016, "keyphrase_query": "image classification", "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I would like to develop an image-classification model that can generalize well."}
{"documents": ["WikiBio"], "year": 2016, "keyphrase_query": "concept-to-text generation wikipedia", "abstract": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.", "task": "concept-to-text generation", "domain": "Wikipedia", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "I would like to develop a model for concept-to-text generation that performs well on large, rich domains."}
{"documents": ["CUHK01", "CUHK03", "Market-1501", "PRID2011", "VIPeR"], "year": 2016, "keyphrase_query": "person re-identification image", "abstract": "Most existing person re-identification (re-id) methods focus on learning the optimal distance metrics across camera views. Typically a person's appearance is represented using features of thousands of dimensions, whilst only hundreds of training samples are available due to the difficulties in collecting matched training images. With the number of training samples much smaller than the feature dimension, the existing methods thus face the classic small sample size (SSS) problem and have to resort to dimensionality reduction techniques and/or matrix regularisation, which lead to loss of discriminative power. In this work, we propose to overcome the SSS problem in re-id distance metric learning by matching people in a discriminative null space of the training data. In this null space, images of the same person are collapsed into a single point thus minimising the within-class scatter to the extreme and maximising the relative between-class separation simultaneously. Importantly, it has a fixed dimension, a closed-form solution and is very efficient to compute. Extensive experiments carried out on five person re-identification benchmarks including VIPeR, PRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beats the state-of-the-art alternatives, often by a big margin.", "task": "person re-identification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I would like to develop a supervised model for person re-identification where the distance metric is based on discriminative null space."}
{"documents": [], "year": 2017, "keyphrase_query": "individual-level causal inference", "abstract": "There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a \"balanced\" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.", "task": "individual-level causal inference", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "We would like to analyze algorithms for predicting individual treatment effect in fields such as healthcare, economics and education"}
{"documents": ["CUB-200-2011", "FGVC-Aircraft", "NABirds", "Oxford 102 Flower", "Stanford Cars", "Stanford Dogs"], "year": 2018, "keyphrase_query": "fine-grained visual classification image", "abstract": "Fine-Grained Visual Classification (FGVC) datasets contain small sample sizes, along with significant intra-class variation and inter-class similarity. While prior work has addressed intra-class variation using localization and segmentation techniques, inter-class similarity may also affect feature learning and reduce classification performance. In this work, we address this problem using a novel optimization procedure for the end-to-end neural network training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces overfitting by intentionally introducing confusion in the activations. With PC regularization, we obtain state-of-the-art performance on six of the most widely-used FGVC datasets and demonstrate improved localization ability. PC is easy to implement, does not need excessive hyperparameter tuning during training, and does not add significant overhead during test time.", "task": "fine-grained visual classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I would like to develop a supervised method that reduces overfitting for fine-grained image classification in cases where sample sizes are small."}
{"documents": ["CoNLL-2014 Shared Task: Grammatical Error Correction", "FCE", "JFLEG"], "year": 2018, "keyphrase_query": "grammatical error correction text", "abstract": "Neural sequence-to-sequence (seq2seq) approaches have proven to be successful in grammatical error correction (GEC). Based on the seq2seq framework, we propose a novel fluency boost learning and inference mechanism. Fluency boosting learning generates diverse error-corrected sentence pairs during training, enabling the error correction model to learn how to improve a sentence's fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps. Combining fluency boost learning and inference with convolutional seq2seq models, our approach achieves the state-of-the-art performance: 75.72 (F_{0.5}) on CoNLL-2014 10 annotation dataset and 62.42 (GLEU) on JFLEG test set respectively, becoming the first GEC system that reaches human-level performance (72.58 for CoNLL and 62.37 for JFLEG) on both of the benchmarks.", "task": "grammatical error correction", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level", "query": "We propose a mechanism that we call fluency boost learning and inference in order to improve grammatical error correction."}
{"documents": ["FlyingChairs", "KITTI", "MPI Sintel", "Middlebury"], "year": 2017, "keyphrase_query": "optical flow estimation image", "abstract": "We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions, these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small (", "task": "optical flow estimation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We want to compute optical flow by combining a classical spatial-pyramid formulation with deep learning."}
{"documents": ["BSDS500", "MNIST", "SVHN"], "year": 2014, "keyphrase_query": "image classification, domain adaptation", "abstract": "Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). ::: As the training progresses, the approach promotes the emergence of \"deep\" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. ::: Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.", "task": "image classification, domain adaptation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We want to improve domain adaptation for image classification models."}
{"documents": ["COCO-QA", "DAQUAR", "Visual Question Answering"], "year": 2016, "keyphrase_query": "visual question answering image, text", "abstract": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.", "task": "visual question answering", "domain": "", "modality": "Image, Text", "language": "", "training_style": "", "text_length": "", "query": "I propose a neural method that learns to answer natural language questions from images."}
{"documents": ["COCO"], "year": 2018, "keyphrase_query": "object detection image", "abstract": "Despite the great success of two-stage detectors, single-stage detector is still a more elegant and efficient way, yet suffers from the two well-known disharmonies during training, i.e. the huge difference in quantity between positive and negative examples as well as between easy and hard examples. In this work, we first point out that the essential effect of the two disharmonies can be summarized in term of the gradient. Further, we propose a novel gradient harmonizing mechanism (GHM) to be a hedging for the disharmonies. The philosophy behind GHM can be easily embedded into both classification loss function like cross-entropy (CE) and regression loss function like smooth-L1 (SL1) loss. To this end, two novel loss functions called GHM-C and GHM-R are designed to balancing the gradient flow for anchor classification and bounding box refinement, respectively. Ablation study on MS COCO demonstrates that without laborious hyper-parameter tuning, both GHM-C and GHM-R can bring substantial improvement for single-stage detector. Without any whistles and bells, the proposed model achieves 41.6 mAP on COCO testdev set which surpass the state-of-the-art method, Focal Loss (FL) + SL1, by 0.8. The code1 is released to facilitate future research.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to build a better loss function for single-stage object detection."}
{"documents": ["CIFAR-10", "ImageNet"], "year": 2018, "keyphrase_query": "class-conditional image synthesis", "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.", "task": "class-conditional image synthesis", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I am trying to train a very large-scale GAN for class-conditional image synthesis."}
{"documents": ["Billion Word Benchmark", "CIFAR-10", "CelebA", "LSUN", "SVHN"], "year": 2017, "keyphrase_query": "image generation, text", "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Frechet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.", "task": "image generation, text generation", "domain": "", "modality": "", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I propose an improvement to GANs that helps with conditionally generating images, text, and other data."}
{"documents": ["SMS-WSJ", "Switchboard-1 Corpus"], "year": 2014, "keyphrase_query": "speech recognition", "abstract": "We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.", "task": "speech recognition", "domain": "", "modality": "Speech", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We present a novel end-to-end deep learning approach for speech recognition."}
{"documents": ["MS MARCO", "NarrativeQA"], "year": 2019, "keyphrase_query": "abstractive question answering text", "abstract": "This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque. The proposed model has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the NLG capability for all styles involved. This also enables our model to give an answer in the target style. Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success.", "task": "abstractive question answering", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Paragraph-level", "query": "We propose a multi-style abstractive summarization model for abstractive question answering."}
{"documents": ["SNLI"], "year": 2016, "keyphrase_query": "sentence encoding text", "abstract": "Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suer from two key technical problems that make them slow and unwieldyforlarge-scaleNLPtasks: theyusually operate on parsed sentences and they do not directly support batched computation. We address these issues by introducingtheStack-augmentedParser-Interpreter NeuralNetwork(SPINN),whichcombines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shiftreduceparser. Ourmodelsupportsbatched computation for a speedup of up to 25◊ over other tree-structured models, and its integrated parser can operate on unparsed data with little loss in accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.", "task": "sentence encoding", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We propose a method to jointly combine parsing and interpretation within a single tree-sequence model for representing the meaning of a sentence."}
{"documents": ["300W", "AFLW", "AFLW2000-3D", "BIWI"], "year": 2017, "keyphrase_query": "head pose estimation image", "abstract": "Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models.", "task": "head pose estimation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want to improve head pose estimation from images to match the performance of methods that estimate head pose from depth."}
{"documents": ["MSD", "MovieLens", "Netflix Prize"], "year": 2018, "keyphrase_query": "collaborative filtering text recommender systems", "abstract": "We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research. We introduce a generative model with multinomial likelihood and use Bayesian inference to learn this powerful generative model. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.", "task": "collaborative filtering", "domain": "recommender systems", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "I propose a method for performing collaborative filtering in recommender systems that relies on a VAE based on multinomial likelihood."}
{"documents": ["CUB-200-2011", "FGVC-Aircraft", "ImageNet", "Stanford Cars"], "year": 2018, "keyphrase_query": "object recognition image", "abstract": "Global covariance pooling in Convolutional neural neworks has achieved impressive improvement over the classical first-order pooling. Recent works have shown matrix square root normalization plays a central role in achieving state-of-the-art performance. However, existing methods depending heavily on eigenvalue decomposition (EIG) or singular value decomposition (SVD), suffering from inefficient training due to limited support of EIG and SVD on GPU. Towards addressing this problem, we propose an iterative matrix square root normalization method for end-to-end training of covariance pooling networks. At the core of our method is a meta-layer designed with loop-embedded directed graph structure. The meta-layer consists of three consecutive nonlinear structured layers, which perform pre-normalization, coupled matrix iteration and post-compensation, respectively. Our method is much faster than EIG or SVD based ones, since it involves only matrix multiplications, suitable for parallel implementation on GPU. Moreover, the proposed network with ResNet architecture can converge in much less epochs, further accelerating network training. On large-scale ImageNet, we achieve competitive performance superior to existing covariance pooling networks. By finetuning our models pretrained on ImageNet, we establish state-of-the-art results on three challenging fine-grained benchmarks.", "task": "Object Recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a GPU-efficient way of implementing global covariance pooling in convolutional networks applied to computer vision tasks."}
{"documents": ["BSD", "LIVE1", "Set14", "Set5"], "year": 2016, "keyphrase_query": "image restoration", "abstract": "Image restoration, including image denoising, super resolution, inpainting, and so on, is a well-studied problem in computer vision and image processing, as well as a test bed for low-level image modeling algorithms. In this work, we propose a very deep fully convolutional auto-encoder network for image restoration, which is a encoding-decoding framework with symmetric convolutional-deconvolutional layers. In other words, the network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers capture the abstraction of image contents while eliminating corruptions. Deconvolutional layers have the capability to upsample the feature maps and recover the image details. To deal with the problem that deeper networks tend to be more difficult to train, we propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains better results.", "task": "Image restoration", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel scheme for image restoration which is fully convolutional and relies on shared weights between convolution and deconvolution layers."}
{"documents": ["AFW", "FDDB", "MALF", "WFLW"], "year": 2017, "keyphrase_query": "object detection image", "abstract": "Since convolutional neural network (CNN) lacks an inherent mechanism to handle large scale variations, we always need to compute feature maps multiple times for multi-scale object detection, which has the bottleneck of computational cost in practice. To address this, we devise a recurrent scale approximation (RSA) to compute feature map once only, and only through this map can we approximate the rest maps on other levels. At the core of RSA is the recursive rolling out mechanism: given an initial map at a particular scale, it generates the prediction at a smaller scale that is half the size of input. To further increase efficiency and accuracy, we (a): design a scale-forecast network to globally predict potential scales in the image since there is no need to compute maps on all levels of the pyramid. (b): propose a landmark retracing network (LRN) to trace back locations of the regressed landmarks and generate a confidence score for each landmark; LRN can effectively alleviate false positives caused by the accumulated error in RSA. The whole system can be trained end-to-end in a unified CNN framework. Experiments demonstrate that our proposed algorithm is superior against state-of-the-art methods on face detection benchmarks and achieves comparable results for generic proposal generation. The source code of RSA is available at github.com/sciencefans/RSA-for-object-detection.", "task": "Object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a recurrent application of convolutional networks to handle the multi-scale problem in object detection."}
{"documents": ["CACD", "FG-NET", "LFW", "MORPH"], "year": 2018, "keyphrase_query": "age-invariant face recognition image faces", "abstract": "As facial appearance is subject to significant intra-class variations caused by the aging process over time, age-invariant face recognition (AIFR) remains a major challenge in face recognition community. To reduce the intra-class discrepancy caused by the aging, in this paper we propose a novel approach (namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep face features. Specifically, we decompose deep face features into two orthogonal components to represent age-related and identity-related features. As a result, identity-related features that are robust to aging are then used for AIFR. Besides, for complementing the existing cross-age datasets and advancing the research in this field, we construct a brand-new large-scale Cross-Age Face dataset (CAF). Extensive experiments conducted on the three public domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have shown the effectiveness of the proposed approach and the value of the constructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most popular general face recognition (GFR) dataset LFW additionally demonstrates the comparable generalization performance on GFR.", "task": "age-invariant face recognition", "domain": "faces", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want to build a model for age-invariant facial recognition."}
{"documents": ["TIMIT"], "year": 2015, "keyphrase_query": "speech recognition", "abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1,2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMET phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.", "task": "speech recognition", "domain": "", "modality": "speech", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want to build a model for speech recognition on longer recordings."}
{"documents": ["MultiNLI", "SNLI"], "year": 2017, "keyphrase_query": "multi-domain natural language inference text", "abstract": "We present a simple sequential sentence encoder for multi-domain natural language inference. Our encoder is based on stacked bidirectional LSTM-RNNs with shortcut connections and fine-tuning of word embeddings. The overall supervised model uses the above encoder to encode two input sentences into two vectors, and then uses a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural. Our Shortcut-Stacked sentence encoders achieve strong improvements over existing encoders on matched and mismatched multi-domain natural language inference (top non-ensemble single-model result in the EMNLP RepEval 2017 Shared Task (Nangia et al., 2017)). Moreover, they achieve the new state-of-the-art encoding result on the original SNLI dataset (Bowman et al., 2015).", "task": "multi-domain natural language inference", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "We want to build a stacked Bi-LSTM model for multi-domain natural language inference that performs well across both matched and mismatched evaluation."}
{"documents": ["7-Scenes", "ICL-NUIM", "SUN3D", "Washington RGB-D Scenes"], "year": 2017, "keyphrase_query": "local geometric feature matching rgb-d images", "abstract": "Matching local geometric features on real-world depth images is a challenging task due to the noisy, low-resolution, and incomplete nature of 3D scan data. These difficulties limit the performance of current state-of-art methods, which are typically based on histograms over geometric properties. In this paper, we present 3DMatch, a data-driven model that learns a local volumetric patch descriptor for establishing correspondences between partial 3D data. To amass training data for our model, we propose a self-supervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Experiments show that our descriptor is not only able to match local geometry in new scenes for reconstruction, but also generalize to different tasks and spatial scales (e.g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface correspondence). Results show that 3DMatch consistently outperforms other state-of-the-art approaches by a significant margin. Code, data, benchmarks, and pre-trained models are available online at http://3dmatch.cs.princeton.edu.", "task": "local geometric feature matching", "domain": "", "modality": "RGB-D images", "language": "", "training_style": "Self-supervised", "text_length": "", "query": "We want to build a self-supervised learning model that can match local geometric features on real-world depth images."}
{"documents": ["CIFAR-10", "CIFAR-100", "CUHK03", "DukeMTMC-reID", "Fashion-MNIST", "Market-1501", "PASCAL VOC 2007"], "year": 2017, "keyphrase_query": "image classification, object detection, person re-identification", "abstract": "In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: this https URL", "task": "image classification, object detection, person re-identification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "This paper introduces a new data augmentation technique for images, called Random Erasing, to improve the performance of image models on tasks such as image classification."}
{"documents": ["CHASE_DB1", "DRIVE", "LUNA", "STARE"], "year": 2018, "keyphrase_query": "medical image segmentation", "abstract": "Deep learning (DL) based semantic segmentation methods have been providing state-of-the-art performance in the last few years. More specifically, these techniques have been successfully applied to medical image classification, segmentation, and detection tasks. One deep learning technique, U-Net, has become one of the most popular for these applications. In this paper, we propose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well as a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net models, which are named RU-Net and R2U-Net respectively. The proposed models utilize the power of U-Net, Residual Network, as well as RCNN. There are several advantages of these proposed architectures for segmentation tasks. First, a residual unit helps when training deep architecture. Second, feature accumulation with recurrent residual convolutional layers ensures better feature representation for segmentation tasks. Third, it allows us to design better U-Net architecture with same number of network parameters with better performance for medical image segmentation. The proposed models are tested on three benchmark datasets such as blood vessel segmentation in retina images, skin cancer segmentation, and lung lesion segmentation. The experimental results show superior performance on segmentation tasks compared to equivalent models including U-Net and residual U-Net (ResU-Net).", "task": "medical image segmentation", "domain": "medical", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "U-Net based recurrent network shows improved performance on medical image segmentation"}
{"documents": ["DUC 2004"], "year": 2016, "keyphrase_query": "text summarization", "abstract": "In this work, we cast text summarization as a sequence-to-sequence problem and apply the attentional encoder-decoder RNN that has been shown to be successful for Machine Translation (Bahdanau et al. (2014)). Our experiments show that the proposed architecture significantly outperforms the state-of-the art model of Rush et al. (2015) on the Gigaword dataset without any additional tuning. We also propose additional extensions to the standard architecture, which we show contribute to further improvement in performance.", "task": "text summarization", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "Attentional encoder-decoder network applied to text summarization"}
{"documents": ["IWSLT2015", "Penn Treebank"], "year": 2016, "keyphrase_query": "seq2seq, word ordering, parsing, machine translation text", "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.", "task": "seq2seq, word ordering, parsing, machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "seq2seq augmented with model and beam search to generate global sequence scores"}
{"documents": ["AG News", "Amazon Review", "DBpedia", "Yahoo! Answers"], "year": 2018, "keyphrase_query": "text classification", "abstract": "Text classification is one of the fundamental tasks in natural language processing. Recently, deep neural networks have achieved promising performance in the text classification task compared to shallow models. Despite of the significance of deep models, they ignore the fine-grained (matching signals between words and classes) classification clues since their classifications mainly rely on the text-level representations. To address this problem, we introduce the interaction mechanism to incorporate word-level matching signals into the text classification task. In particular, we design a novel framework, EXplicit interAction Model (dubbed as EXAM), equipped with the interaction mechanism. We justified the proposed approach on several benchmark datasets including both multilabel and multi-class text classification tasks. Extensive experimental results demonstrate the superiority of the proposed method. As a byproduct, we have released the codes and parameter settings to facilitate other researches.", "task": "text classification", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "word-level", "query": "a new approach to text classification that utilizes word-level matching signals"}
{"documents": ["Cityscapes", "NYUv2"], "year": 2017, "keyphrase_query": "photographic synthesis image", "abstract": "We present an approach to synthesizing photographic images conditioned on semantic layouts. Given a semantic label map, our approach produces an image with photographic appearance that conforms to the input layout. The approach thus functions as a rendering engine that takes a two-dimensional semantic specification of the scene and produces a corresponding photographic image. Unlike recent and contemporaneous work, our approach does not rely on adversarial training. We show that photographic images can be synthesized from semantic layouts by a single feedforward network with appropriate structure, trained end-to-end with a direct regression objective. The presented approach scales seamlessly to high resolutions; we demonstrate this by synthesizing photographic images at 2-megapixel resolution, the full resolution of our training data. Extensive perceptual experiments on datasets of outdoor and indoor scenes demonstrate that images synthesized by the presented approach are considerably more realistic than alternative approaches. The results are shown in the supplementary video at this https URL", "task": "photographic synthesis", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce a non-adversarial method that utilizes a semantic map to synthesize photographic images"}
{"documents": ["Penn Treebank", "Universal Dependencies"], "year": 2016, "keyphrase_query": "pos tagging text morphologically complex languages", "abstract": "Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence modeling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.", "task": "POS tagging", "domain": "", "modality": "Text", "language": "morphologically complex languages", "training_style": "Supervised training/finetuning", "text_length": "NA", "query": "I am going to build a supervised model to do part-of-speech tagging on multilingual text."}
{"documents": ["CIFAR-100", "COCO", "DUT-OMRON", "ECSSD", "HKU-IS", "ImageNet", "MSRA-B", "PASCAL VOC", "PASCAL VOC 2007", "PASCAL-S"], "year": 2019, "keyphrase_query": "object detection, class activation mapping, salient image", "abstract": "Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https://mmcheng.net/res2net/.", "task": "object detection, class activation mapping, and salient object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel building block for CNNs for computer vision tasks like object detection, class activation mapping, and salient object detection."}
{"documents": ["COCO"], "year": 2017, "keyphrase_query": "object detection image", "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a fast and accurate feature pyramid network for object detection."}
{"documents": ["CelebA", "IJB-A"], "year": 2017, "keyphrase_query": "video face recognition", "abstract": "This paper aims to learn a compact representation of a video for video face recognition task. We make the following contributions: first, we propose a meta attention-based aggregation scheme which adaptively and fine-grained weighs the feature along each feature dimension among all frames to form a compact and discriminative representation. It makes the best to exploit the valuable or discriminative part of each frame to promote the performance of face recognition, without discarding or despising low quality frames as usual methods do. Second, we build a feature aggregation network comprised of a feature embedding module and a feature aggregation module. The embedding module is a convolutional neural network used to extract a feature vector from a face image, while the aggregation module consists of cascaded two meta attention blocks which adaptively aggregate the feature vectors into a single fixed-length representation. The network can deal with arbitrary number of frames, and is insensitive to frame order. Third, we validate the performance of proposed aggregation scheme. Experiments on publicly available datasets, such as YouTube face dataset and IJB-A dataset, show the effectiveness of our method, and it achieves competitive performances on both the verification and identification protocols.", "task": "video face recognition", "domain": "", "modality": "Video", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to learn a compact representation of video to perform video face recognition."}
{"documents": ["BSD", "CBSD68", "ImageNet", "McMaster", "PASCAL VOC", "Set12"], "year": 2017, "keyphrase_query": "image denoising", "abstract": "Due to the fast inference and good performance, discriminative learning methods have been widely studied in image denoising. However, these methods mostly learn a specific model for each noise level, and require multiple models for denoising images with different noise levels. They also lack flexibility to deal with spatially variant noise, limiting their applications in practical denoising. To address these issues, we present a fast and flexible denoising convolutional neural network, namely FFDNet, with a tunable noise level map as the input. The proposed FFDNet works on downsampled sub-images to speed up the inference, and adopts orthogonal regularization to enhance the generalization ability. In contrast to the existing discriminative denoisers, FFDNet enjoys several desirable properties, including (i) the ability to handle a wide range of noise levels (i.e., [0, 75]) effectively with a single network, (ii) the ability to remove spatially variant noise by specifying a non-uniform noise level map, and (iii) faster speed than benchmark BM3D even on CPU without sacrificing denoising performance. Extensive experiments on synthetic and real noisy images are conducted to evaluate FFDNet in comparison with state-of-the-art denoisers. The results show that FFDNet is effective and efficient, making it highly attractive for practical denoising applications.", "task": "image denoising", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "I propose a method for effective and efficient image denoising."}
{"documents": ["CelebA", "Cityscapes", "MultiMNIST"], "year": 2018, "keyphrase_query": "multi-task learning", "abstract": "In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.", "task": "multi-task learning", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "I propose a method to improve multi-task learning"}
{"documents": [], "year": 2018, "keyphrase_query": "amr alignment, parsing text", "abstract": "In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score.", "task": "AMR alignment, AMR parsing", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "I want to improve the AMR aligner and AMR parsing."}
{"documents": ["CIFAR-10", "COCO", "ImageNet", "PASCAL VOC 2007"], "year": 2016, "keyphrase_query": "image classification, localization, object detection, segmentation", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "task": "Image classification, image localization, object detection, object segmentation ", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I present a framework to ease training for various tasks like Image classification, image localization, object detection, object segmentation."}
{"documents": ["ImageNet", "JFT-300M", "PASCAL VOC", "PASCAL VOC 2007"], "year": 2017, "keyphrase_query": "image classification, object detection, semantic segmentation, human pose estimation", "abstract": "The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.", "task": "image classification, object detection, semantic segmentation, human pose estimation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to study the impact of size of data on pretraining CNN for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation."}
{"documents": ["CNN/Daily Mail"], "year": 2015, "keyphrase_query": "machine reading text", "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "task": "Machine reading", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "I want to train a supervised model for reading comprehension and then answering complex questions."}
{"documents": ["Letter", "MNIST"], "year": 2014, "keyphrase_query": "subset selection", "abstract": "Subset selection from massive data with noised information is increasingly popular for various applications. This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers. To address the above two issues, we propose an accelerated robust subset selection (ARSS) method. Specifically in the subset selection area, this is the first attempt to employ the lp (0 < p ≤ 1)-norm based measure for the representation loss, preventing large errors from dominating our objective. As a result, the robustness against outlier elements is greatly enhanced. Actually, data size is generally much larger than feature length, i.e. N ≫ L. Based on this observation, we propose a speedup solver (via ALM and equivalent derivations) to highly reduce the computational cost, theoretically from O (N4) to O (N2L). Extensive experiments on ten benchmark datasets verify that our method not only outperforms state of the art methods, but also runs 10,000+ times faster than the most related method.", "task": "subset selection", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "We want to build a system for fast and accurate robust subset selection."}
{"documents": ["300W", "AFLW", "COFW", "Helen", "LFPW"], "year": 2018, "keyphrase_query": "facial alignment image", "abstract": "Abstract Facial landmarks are highly correlated with each other since a certain landmark can be estimated by its neighboring landmarks. Most of the existing deep learning methods only use one fully-connected layer called shape prediction layer to estimate the locations of facial landmarks. In this paper, we propose a novel deep learning framework named Multi-Center Learning with multiple shape prediction layers for face alignment. In particular, each shape prediction layer emphasizes on the detection of a certain cluster of semantically relevant landmarks respectively. Challenging landmarks are focused firstly, and each cluster of landmarks is further optimized respectively. Moreover, to reduce the model complexity, we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer. Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real-time performance. The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension .", "task": "facial alignment", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a deep learning framework for facial alignment with real-time performance."}
{"documents": [], "year": 2015, "keyphrase_query": "program verification text", "abstract": "Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.", "task": "program verification", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to feature learning techniques for graph-structured inputs which has applications for graph algorithm learning tasks like program verification."}
{"documents": ["ICDAR 2013", "ICDAR 2015", "MSRA-TD500"], "year": 2017, "keyphrase_query": "text detection image latin chinese", "abstract": "Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decompose text into two locally detectable elements, namely segments and links. A segment is an oriented box covering a part of a word or text line, A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining segments connected by links. Compared with previous methods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512x512 images. Moreover, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.", "task": "text detection", "domain": "", "modality": "Image", "language": "Latin text and Chinese text", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train a text detector from images for both Latin and non Latin text like Chinese."}
{"documents": ["Visual Question Answering"], "year": 2016, "keyphrase_query": "visual question answering image text", "abstract": "Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.", "task": "visual question answering", "domain": "", "modality": "image and text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train a multimodal network to jointly learn visual and language representations from images and text for visual question answering."}
{"documents": ["IMDb Movie Reviews", "IWSLT2015", "Penn Treebank"], "year": 2016, "keyphrase_query": "language modeling, sentiment classification, character-level neural machine translation text", "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.", "task": "language modeling, sentiment classification, and character-level neural machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a model for sequence modeling on text for language modeling, sentiment classification, and character-level neural machine translation."}
{"documents": ["BSDS500", "Manga109", "Set14", "Set5", "Urban100"], "year": 2017, "keyphrase_query": "high-quality reconstruction image", "abstract": "Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.", "task": "high-quality reconstruction", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. "}
{"documents": ["AG News", "Amazon Review", "DBpedia", "Yahoo! Answers"], "year": 2015, "keyphrase_query": "text classification", "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.", "task": "text classification", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "Character-level convolutional networks (ConvNets) show promise for the text classification task. "}
{"documents": ["Arxiv GR-QC", "Citeseer", "Cora", "PROTEINS", "Pubmed"], "year": 2018, "keyphrase_query": "link prediction semi-supervised node classification graphs", "abstract": "We examine two fundamental tasks associated with graph representation learning: link prediction and semi-supervised node classification. We present a novel autoencoder architecture capable of learning a joint representation of both local graph structure and available node features for the multi-task learning of link prediction and node classification. Our autoencoder architecture is efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification, whereas previous related methods require multiple training steps that are difficult to optimize. We provide a comprehensive empirical evaluation of our models on nine benchmark graph-structured datasets and demonstrate significant improvement over related methods for graph representation learning. Reference code and data are available at https://github.com/vuptran/graph-representation-learning", "task": "link prediction and semi-supervised node classification", "domain": "", "modality": "graphs", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We use a novel autoencoder architecture for multi-task learning of link prediction and node classification. "}
{"documents": ["CNN/Daily Mail", "SQuAD"], "year": 2017, "keyphrase_query": "machine comprehension text english", "abstract": "Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.", "task": "machine comprehension", "domain": "", "modality": "Text", "language": "English", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "A neural architecture for machine comprehension tasks."}
{"documents": ["FDDB", "WFLW"], "year": 2017, "keyphrase_query": "face detection image", "abstract": "Face detection has achieved great success using the region-based methods. In this report, we propose a region-based face detector applying deep networks in a fully convolutional fashion, named Face R-FCN. Based on Region-based Fully Convolutional Networks (R-FCN), our face detector is more accurate and computational efficient compared with the previous R-CNN based face detectors. In our approach, we adopt the fully convolutional Residual Network (ResNet) as the backbone network. Particularly, We exploit several new techniques including position-sensitive average pooling, multi-scale training and testing and on-line hard example mining strategy to improve the detection accuracy. Over two most popular and challenging face detection benchmarks, FDDB and WIDER FACE, Face R-FCN achieves superior performance over state-of-the-arts.", "task": "face detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "A neural network for higher accuracy region-based face detection"}
{"documents": ["SK-LARGE"], "year": 2019, "keyphrase_query": "object skeleton detection image", "abstract": "Computing object skeletons in natural images is challenging, owing to large variations in object appearance and scale, and the complexity of handling background clutter. Many recent methods frame object skeleton detection as a binary pixel classification problem, which is similar in spirit to learning-based edge detection, as well as to semantic segmentation methods. In the present article, we depart from this strategy by training a CNN to predict a two-dimensional vector field, which maps each scene point to a candidate skeleton pixel, in the spirit of flux-based skeletonization algorithms. This ``image context flux'' representation has two major advantages over previous approaches. First, it explicitly encodes the relative position of skeletal pixels to semantically meaningful entities, such as the image points in their spatial context, and hence also the implied object boundaries. Second, since the skeleton detection context is a region-based vector field, it is better able to cope with object parts of large width. We evaluate the proposed method on three benchmark datasets for skeleton detection and two for symmetry detection, achieving consistently superior performance over state-of-the-art methods.", "task": "object skeleton detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "This is a method for object skeleton detection using flux-based skeletonization algorithms."}
{"documents": ["COCO", "Cityscapes", "PASCAL VOC"], "year": 2017, "keyphrase_query": "object detection semantic segmentation image", "abstract": "Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.", "task": "object detection and semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "Deformable convolution and deformable RoI pooling to enhance the transformation modeling capability of CNNs for object detection and semantic segmentation."}
{"documents": ["MultiNLI", "SNLI", "SQuAD", "TriviaQA"], "year": 2017, "keyphrase_query": "document question answering, recognizing textual entailment", "abstract": "Common-sense or background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, the requisite background knowledge is indirectly acquired from static corpora. We develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. A new task-agnostic reading module provides refined word representations to a task-specific NLU architecture by processing background knowledge in the form of free-text statements, together with the task-specific inputs. Strong performance on the tasks of document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of our approach. Analysis shows that our models learn to exploit knowledge selectively and in a semantically appropriate way.", "task": "document question answering, recognizing textual entailment", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I'm building a new reading architecture that should be tested on datasets that require explicit background knowledge."}
{"documents": ["Arxiv GR-QC", "Citeseer", "Cora", "Pubmed"], "year": 2018, "keyphrase_query": "link prediction, node classification graph-structured data", "abstract": "We examine two fundamental tasks associated with graph representation learning: link prediction and node classification. We present a new autoencoder architecture capable of learning a joint representation of local graph structure and available node features for the simultaneous multi-task learning of unsupervised link prediction and semi-supervised node classification. Our simple, yet effective and versatile model is efficiently trained end-to-end in a single stage, whereas previous related deep graph embedding methods require multiple training steps that are difficult to optimize. We provide an empirical evaluation of our model on five benchmark relational, graph-structured datasets and demonstrate significant improvement over three strong baselines for graph representation learning. Reference code and data are available at https://github.com/vuptran/graph-representation-learning", "task": "link prediction, node classification", "domain": "", "modality": "graph-structured data", "language": "", "training_style": "unsupervised, semi-supervised", "text_length": "", "query": "We propose a novel method to learn node representations in graph-structured data."}
{"documents": ["CASIA-WebFace", "IJB-A"], "year": 2018, "keyphrase_query": "face recognition image, video", "abstract": "Abstract Face recognition performance evaluation has traditionally focused on one-to-one verification, popularized by the Labeled Faces in the Wild data set [1] for imagery and the YouTubeFaces data set [2] for videos. In contrast, the newly released IJB-A face recognition data set [3] unifies evaluation of one-to-many face identification with one-to-one face verification over templates, or sets of imagery and videos for a subject. In this paper, we study the problem of template adaptation, a form of transfer learning to the set of media in a template. Extensive performance evaluations on IJB-A show a surprising result, that perhaps the simplest method of template adaptation, combining deep convolutional network features with template specific linear SVMs, outperforms the state-of-the-art by a wide margin. We study the effects of template size, negative set construction and classifier fusion on performance, then compare template adaptation to convolutional networks with metric learning, 2D and 3D alignment. Our unexpected conclusion is that these other methods, when combined with template adaptation, all achieve nearly the same top performance on IJB-A for template-based face verification and identification.", "task": "face recognition", "domain": "", "modality": "image, video", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a transfer learning approach for face recognition that require one-to-many face identification with one-to-one face verification over templates."}
{"documents": ["CelebA", "RaFD"], "year": 2017, "keyphrase_query": "image-to-image translation, facial attribute transfer, expression synthesis", "abstract": "Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.", "task": "image-to-image translation, facial attribute transfer, facial expression synthesis", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel and scalable approach to image-to-image translation that uses a single model to handle multiple domains."}
{"documents": ["300W", "AFW", "Helen", "LFW"], "year": 2017, "keyphrase_query": "facial shape reconstruction image", "abstract": "3D Morphable Models (3DMMs) are powerful statistical models of 3D facial shape and texture, and among the state-of-the-art methods for reconstructing facial shape from single images. With the advent of new 3D sensors, many 3D facial datasets have been collected containing both neutral as well as expressive faces. However, all datasets are captured under controlled conditions. Thus, even though powerful 3D facial shape models can be learnt from such data, it is difficult to build statistical texture models that are sufficient to reconstruct faces captured in unconstrained conditions (in-the-wild). In this paper, we propose the first, to the best of our knowledge, in-the-wild 3DMM by combining a powerful statistical model of facial shape, which describes both identity and expression, with an in-the-wild texture model. We show that the employment of such an in-the-wild texture model greatly simplifies the fitting procedure, because there is no need to optimise with regards to the illumination parameters. Furthermore, we propose a new fast algorithm for fitting the 3DMM in arbitrary images. Finally, we have captured the first 3D facial database with relatively unconstrained conditions and report quantitative evaluations with state-of-the-art performance. Complementary qualitative reconstruction results are demonstrated on standard in-the-wild facial databases.", "task": "facial shape reconstruction", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a model capable of reconstructing 3D facial shape and texture in unconstrained conditions."}
{"documents": ["Penn Treebank", "Switchboard-1 Corpus", "WikiText-2"], "year": 2017, "keyphrase_query": "language modeling text", "abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.", "task": "language modeling", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I'm proposing a novel method for language modeling and want typical datasets that use perplexity as the testing score."}
{"documents": ["Human3.6M", "LSP", "MPII Human Pose"], "year": 2017, "keyphrase_query": "3d human pose estimation image", "abstract": "We propose a unified formulation for the problem of 3D human pose estimation from a single raw RGB image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine the search for better 2D locations. The entire process is trained end-to-end, is extremely efficient and obtains state-of-the-art results on Human3.6M outperforming previous approaches both on 2D and 3D errors.", "task": "3D human pose estimation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I present an end-to-end integrated approach for 3D human pose estimation from a single raw RGB image."}
{"documents": ["RotoWire"], "year": 2017, "keyphrase_query": "data-to-text generation", "abstract": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.", "task": "data-to-text generation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to build a model for data-to-text generation that generates documents for a given data record."}
{"documents": ["COCO", "PASCAL VOC", "PASCAL VOC 2007"], "year": 2017, "keyphrase_query": "object detection image", "abstract": "How do we learn an object detector that is invariant to occlusions and deformations? Our current solution is to use a data-driven strategy – collect large-scale datasets which have object instances under different conditions. The hope is that the final classifier can use these examples to learn invariances. But is it really possible to see all the occlusions in a dataset? We argue that like categories, occlusions and object deformations also follow a long-tail. Some occlusions and deformations are so rare that they hardly happen, yet we want to learn a model invariant to such occurrences. In this paper, we propose an alternative solution. We propose to learn an adversarial network that generates examples with occlusions and deformations. The goal of the adversary is to generate examples that are difficult for the object detector to classify. In our framework both the original detector and adversary are learned in a joint manner. Our experimental results indicate a 2.3% mAP boost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge compared to the Fast-RCNN pipeline.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I'm proposing an object detection model that is invariant to occlusions and deformations."}
{"documents": ["COCO-Stuff", "Cityscapes", "ImageNet", "Mapillary Vistas Dataset"], "year": 2017, "keyphrase_query": "image classification, semantic segmentation", "abstract": "In this work we present In-Place Activated Batch Normalization (InPlace-ABN) - a novel approach to drastically reduce the training memory footprint of modern deep neural networks in a computationally efficient way. Our solution substitutes the conventionally used succession of BatchNorm + Activation layers with a single plugin layer, hence avoiding invasive framework surgery while providing straightforward applicability for existing deep learning frameworks. We obtain memory savings of up to 50% by dropping intermediate results and by recovering required information during the backward pass through the inversion of stored forward results, with only minor increase (0.8-2%) in computation time. Also, we demonstrate how frequently used checkpointing approaches can be made computationally as efficient as InPlace-ABN. In our experiments on image classification, we demonstrate on-par results on ImageNet-1k with state-of-the-art approaches. On the memory-demanding task of semantic segmentation, we report results for COCO-Stuff, Cityscapes and Mapillary Vistas, obtaining new state-of-the-art results on the latter without additional training data but in a single-scale and -model scenario. Code can be found at this https URL .", "task": "image classification, semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I'm building a model that helps reduce training memory footprint and want some memory-demanding tasks and datasets "}
{"documents": ["COLLAB", "IMDB-BINARY", "IMDB-MULTI", "NCI1", "PROTEINS", "PTC", "REDDIT-5K", "REDDIT-BINARY"], "year": 2018, "keyphrase_query": "graph classification", "abstract": "Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in \\cite{hinton2011transforming} and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.", "task": "graph classification", "domain": "", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose the framework of Graph Convolutional Neural Networks for graph classification task."}
{"documents": [], "year": 2017, "keyphrase_query": "sudoku puzzles, qa text", "abstract": "Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.", "task": "Sudoku puzzles, QA", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I proposed a recurrent relational networks, which can be used to augment any deep learning model with multi-hop reasoning ability."}
{"documents": ["CIFAR-10", "CelebA", "MNIST"], "year": 2016, "keyphrase_query": "image generation", "abstract": "Generative adversarial networks (GANs) are a framework for producing a generative model by way of a two-player minimax game. In this paper, we propose the \\emph{Generative Multi-Adversarial Network} (GMAN), a framework that extends GANs to multiple discriminators. In previous work, the successful training of GANs requires modifying the minimax objective to accelerate training early on. In contrast, GMAN can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher. Image generation tasks comparing the proposed framework to standard GANs demonstrate GMAN produces higher quality samples in a fraction of the iterations when measured by a pairwise GAM-type metric.", "task": "image generation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I extend the discriminators of generative adversarial network into multiple ones, which can generate better images."}
{"documents": ["DukeMTMC-reID", "Market-1501"], "year": 2018, "keyphrase_query": "person re-identification image", "abstract": "Person Re-Identification is still a challenging task in Computer Vision due to variety of reasons. On the other side, Incremental Learning is still an issue since Deep Learning models tend to face the problem of overcatastrophic forgetting when trained on subsequent tasks. In this paper, we propose a model which can be used for multiple tasks in Person Re-Identification, provide state-of-the-art results on variety of tasks and still achieve considerable accuracy later on. We evaluated our model on two datasets Market 1501 and Duke MTMC. Extensive experiments show that this method can achieve Incremental Learning in Person ReID efficiently as well as for other tasks in computer vision as well.", "task": "person re-identification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to introduce multiple task learning for the task of person re-identification."}
{"documents": ["CUHK03", "MARS", "Market-1501", "VIPeR"], "year": 2017, "keyphrase_query": "person re-identification image", "abstract": "Person Re-identification (ReID) is to identify the same person across different cameras. It is a challenging task due to the large variations in person pose, occlusion, background clutter, etc How to extract powerful features is a fundamental problem in ReID and is still an open problem today. In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn powerful features over full body and body parts, which can well capture the local context knowledge by stacking multi-scale convolutions in each layer. Moreover, instead of using predefined rigid parts, we propose to learn and localize deformable pedestrian parts using Spatial Transformer Networks (STN) with novel spatial constraints. The learned body parts can release some difficulties, eg pose variations and background clutters, in part-based representation. Finally, we integrate the representation learning processes of full body and body parts into a unified framework for person ReID through multi-class person identification tasks. Extensive evaluations on current challenging large-scale person ReID datasets, including the image-based Market1501, CUHK03 and sequence-based MARS datasets, show that the proposed method achieves the state-of-the-art results.", "task": "person re-identification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to use multi-scale context-aware network to learn powerful features for person re-identification task."}
{"documents": ["GLUE", "SNLI", "SciTail"], "year": 2019, "keyphrase_query": "natural language inference text", "abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at this https URL.", "task": "natural language inference", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to build a multi-task learning framework based on BERT for natural language understanding tasks."}
{"documents": ["ImageNet", "RVL-CDIP", "Tobacco-3482"], "year": 2017, "keyphrase_query": "document image classification", "abstract": "We present an exhaustive investigation of recent Deep Learning architectures, algorithms, and strategies for the task of document image classification to finally reduce the error by more than half. Existing approaches, such as the DeepDoc-Classifier, apply standard Convolutional Network architectures with transfer learning from the object recognition domain. The contribution of the paper is threefold: First, it investigates recently introduced very deep neural network architectures (GoogLeNet, VGG, ResNet) using transfer learning (from real images). Second, it proposes transfer learning from a huge set of document images, i.e. 400; 000 documents. Third, it analyzes the impact of the amount of training data (document images) and other parameters to the classification abilities. We use two datasets, the Tobacco-3482 and the large-scale RVL-CDIP dataset. We achieve an accuracy of 91:13% for the Tobacco-3482 dataset while earlier approaches reach only 77:6%. Thus, a relative error reduction of more than 60% is achieved. For the large dataset RVL-CDIP, an accuracy of 90:97% is achieved, corresponding to a relative error reduction of 11:5%.", "task": "document image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "I want to use transfer learning to make better use of existing dataset for document image classification."}
{"documents": ["BSDS500", "PASCAL VOC"], "year": 2018, "keyphrase_query": "instance segmentation, boundary detection image", "abstract": "We introduce a differentiable, end-to-end trainable framework for solving pixel-level grouping problems such as instance segmentation consisting of two novel components. First, we regress pixels into a hyper-spherical embedding space so that pixels from the same group have high cosine similarity while those from different groups have similarity below a specified margin. We analyze the choice of embedding dimension and margin, relating them to theoretical results on the problem of distributing points uniformly on the sphere. Second, to group instances, we utilize a variant of mean-shift clustering, implemented as a recurrent neural network parameterized by kernel bandwidth. This recurrent grouping module is differentiable, enjoys convergent dynamics and probabilistic interpretability. Backpropagating the group-weighted loss through this module allows learning to focus on only correcting embedding errors that won't be resolved during subsequent clustering. Our framework, while conceptually simple and theoretically abundant, is also practically effective and computationally efficient. We demonstrate substantial improvements over state-of-the-art instance segmentation for object proposal generation, as well as demonstrating the benefits of grouping loss for classification tasks such as boundary detection and semantic segmentation.", "task": " instance segmentation, boundary detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce a differentiable, end-to-end trainable framework for solving pixel-level grouping problems."}
{"documents": ["MNIST", "SVHN"], "year": 2015, "keyphrase_query": "semi-supervised classification, image generation", "abstract": "In this paper, we propose the\"adversarial autoencoder\"(AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.", "task": "semi-supervised classification, image generation", "domain": "", "modality": "Image", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "I want to train a generative model using the adversarial autoencoder for semi-supervised classification and image generation."}
{"documents": ["WikiBio"], "year": 2017, "keyphrase_query": "table-to-text generation wikipedia", "abstract": "Table-to-text generation aims to generate a description for a factual table which can be viewed as a set of field-value records. To encode both the content and the structure of a table, we propose a novel structure-aware seq2seq architecture which consists of field-gating encoder and description generator with dual attention. In the encoding phase, we update the cell memory of the LSTM unit by a field gate and its corresponding field value in order to incorporate field information into table representation. In the decoding phase, dual attention mechanism which contains word level attention and field level attention is proposed to model the semantic relevance between the generated description and the table. We conduct experiments on the \\texttt{WIKIBIO} dataset which contains over 700k biographies and corresponding infoboxes from Wikipedia. The attention visualizations and case studies show that our model is capable of generating coherent and informative descriptions based on the comprehensive understanding of both the content and the structure of a table. Automatic evaluations also show our model outperforms the baselines by a great margin. Code for this work is available on this https URL", "task": "table-to-text generation", "domain": "Wikipedia", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train a supervised model for table-to-text generation from Wikipedia"}
{"documents": ["COCO"], "year": 2015, "keyphrase_query": "text generation", "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.", "task": "Text Generation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel sequence level training algorithm for text generation that directly optimizes the metric used at test time"}
{"documents": ["Penn Treebank"], "year": 2016, "keyphrase_query": "language modeling parsing text", "abstract": "Recurrent neural network grammars (RNNG) are a recently proposed probabilistic generative modeling family for natural language. They show state-of-the-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model's latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis.", "task": "Language modeling and parsing", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to investigate what linguistic information the recurrent neural network grammars and how to improve it with explicit attention prior."}
{"documents": ["WFLW"], "year": 2017, "keyphrase_query": "face detection image", "abstract": "The performance of face detection has been largely improved with the development of convolutional neural network. However, the occlusion issue due to mask and sunglasses, is still a challenging problem. The improvement on the recall of these occluded cases usually brings the risk of high false positives. In this paper, we present a novel face detector called Face Attention Network (FAN), which can significantly improve the recall of the face detection problem in the occluded case without compromising the speed. More specifically, we propose a new anchor-level attention, which will highlight the features from the face region. Integrated with our anchor assign strategy and data augmentation techniques, we obtain state-of-art results on public face detection benchmarks like WiderFace and MAFA. The code will be released for reproduction.", "task": "face detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train a face detector for occluded faces"}
{"documents": ["DIRHA", "TIMIT"], "year": 2018, "keyphrase_query": "automatic speech recognition", "abstract": "A field that has directly benefited from the recent advances in deep learning is automatic speech recognition (ASR). Despite the great achievements of the past decades, however, a natural and robust human–machine speech interaction still appears to be out of reach, especially in challenging environments characterized by significant noise and reverberation. To improve robustness, modern speech recognizers often employ acoustic models based on recurrent neural networks (RNNs) that are naturally able to exploit large time contexts and long-term speech modulations. It is thus of great interest to continue the study of proper techniques for improving the effectiveness of RNNs in processing speech signals. In this paper, we revise one of the most popular RNN models, namely, gated recurrent units (GRUs), and propose a simplified architecture that turned out to be very effective for ASR. The contribution of this work is twofold: First, we analyze the role played by the reset gate, showing that a significant redundancy with the update gate occurs. As a result, we propose to remove the former from the GRU design, leading to a more efficient and compact single-gate model. Second, we propose to replace hyperbolic tangent with rectified linear unit activations. This variation couples well with batch normalization and could help the model learn long-term dependencies without numerical issues. Results show that the proposed architecture, called light GRU, not only reduces the per-epoch training time by more than 30% over a standard GRU, but also consistently improves the recognition accuracy across different tasks, input features, noisy conditions, as well as across different ASR paradigms, ranging from standard DNN-HMM speech recognizers to end-to-end connectionist temporal classification models.", "task": "automatic speech recognition", "domain": "", "modality": "Speech", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a light GRU model for automatic speech recognition, which is more efficient and improves accuracy."}
{"documents": ["Amazon Review", "IMDb Movie Reviews", "RCV1"], "year": 2014, "keyphrase_query": "text categorization", "abstract": "Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.", "task": "text categorization", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train a convolutional neural network on text categorization to exploit the 1D structure"}
{"documents": ["CIFAR-10", "MNIST", "STL-10"], "year": 2014, "keyphrase_query": "visual recognition image", "abstract": "An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. ::: ::: Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.", "task": "visual recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce a new type of convolutional neural network that is invariant to transformations for image representation learning."}
{"documents": ["MS MARCO"], "year": 2019, "keyphrase_query": "query-based passage re-ranking text", "abstract": "Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10. The code to reproduce our results is available at this https URL", "task": "query-based passage re-ranking", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "I proposed a simple re-implementation of BERT for query-based passage re-ranking."}
{"documents": ["Caltech Pedestrian Dataset", "ETH", "INRIA Person", "KITTI"], "year": 2018, "keyphrase_query": "pedestrian detection image", "abstract": "In this work, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intra-category variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in sub-networks which detect pedestrians with scales from disjoint ranges. Outputs from all the sub-networks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech, INRIA, and ETH, and obtains competitive results on KITTI.", "task": "pedestrian detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a Scale-Aware Fast R-CNN (SAF R-CNN) framework for pedestrian detection"}
{"documents": ["BigHand2.2M Benchmark", "ICVL Hand Posture", "ITOP", "MSRA Hand", "NYU Hand"], "year": 2018, "keyphrase_query": "3d hand human pose estimation image", "abstract": "Most of the existing deep learning-based methods for 3D hand and human pose estimation from a single depth map are based on a common framework that takes a 2D depth map and directly regresses the 3D coordinates of keypoints, such as hand or human body joints, via 2D convolutional neural networks (CNNs). The first weakness of this approach is the presence of perspective distortion in the 2D depth map. While the depth map is intrinsically 3D data, many previous methods treat depth maps as 2D images that can distort the shape of the actual object through projection from 3D to 2D space. This compels the network to perform perspective distortion-invariant estimation. The second weakness of the conventional approach is that directly regressing 3D coordinates from a 2D image is a highly non-linear mapping, which causes difficulty in the learning procedure. To overcome these weaknesses, we firstly cast the 3D hand and human pose estimation problem from a single depth map into a voxel-to-voxel prediction that uses a 3D voxelized grid and estimates the per-voxel likelihood for each keypoint. We design our model as a 3D CNN that provides accurate estimates while running in real-time. Our system outperforms previous methods in almost all publicly available 3D hand and human pose estimation datasets and placed first in the HANDS 2017 frame-based 3D hand pose estimation challenge. The code is available in this https URL.", "task": "3D hand and human pose estimation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a voxel-to-voxel prediction method for 3D hand and human pose estimation from a single depth map."}
{"documents": ["FB15k", "FB15k-237", "WN18", "WN18RR", "YAGO"], "year": 2018, "keyphrase_query": "link prediction knowledge graph", "abstract": "Knowledge graphs are graphical representations of large databases of facts, which typically suffer from incompleteness. Inferring missing relations (links) between entities (nodes) is the task of link prediction. A recent state-of-the-art approach to link prediction, ConvE, implements a convolutional neural network to extract features from concatenated subject and relation vectors. Whilst results are impressive, the method is unintuitive and poorly understood. We propose a hypernetwork architecture that generates simplified relation-specific convolutional filters that (i) outperforms ConvE and all previous approaches across standard datasets; and (ii) can be framed as tensor factorization and thus set within a well established family of factorization models for link prediction. We thus demonstrate that convolution simply offers a convenient computational means of introducing sparsity and parameter tying to find an effective trade-off between non-linear expressiveness and the number of parameters to learn.", "task": "link prediction", "domain": "", "modality": "Knowledge Graph", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a model for link prediction with hypernetwork."}
{"documents": ["VisDial", "Visual Genome", "Visual Question Answering v2.0"], "year": 2018, "keyphrase_query": "visual question answering image text", "abstract": "This document describes Pythia v0.1, the winning entry from Facebook AI Research (FAIR)'s A-STAR team to the VQA Challenge 2018. ::: Our starting point is a modular re-implementation of the bottom-up top-down (up-down) model. We demonstrate that by making subtle but important changes to the model architecture and the learning rate schedule, fine-tuning image features, and adding data augmentation, we can significantly improve the performance of the up-down model on VQA v2.0 dataset -- from 65.67% to 70.22%. ::: Furthermore, by using a diverse ensemble of models trained with different features and on different datasets, we are able to significantly improve over the 'standard' way of ensembling (i.e. same model with different random seeds) by 1.31%. Overall, we achieve 72.27% on the test-std split of the VQA v2.0 dataset. Our code in its entirety (training, evaluation, data-augmentation, ensembling) and pre-trained models are publicly available at: this https URL", "task": "visual question answering", "domain": "", "modality": "Image and text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "We improve the performance of the up-down model on the for VQA."}
{"documents": ["CoNLL-2000", "Penn Treebank", "SICK"], "year": 2016, "keyphrase_query": "dependency parsing, semantic relatedness, textual entailment, pos tagging", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "task": "dependency parsing, semantic relatedness, textual entailment, POS tagging", "domain": "", "modality": "Text", "language": "", "training_style": "multi-task learning", "text_length": "", "query": "A joint many-task model with a successful strategy for successively growing its depth to solve increasingly complex tasks."}
{"documents": ["CelebA", "NYUv2", "Washington RGB-D"], "year": 2016, "keyphrase_query": "domain adaptation, image transformation", "abstract": "We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.", "task": "domain adaptation, image transformation", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We propose a coupled generative adversarial network for learning a joint distribution of multi-domain images."}
{"documents": ["BraTS 2018"], "year": 2018, "keyphrase_query": "semantic segmentation 3d volumetric biomedical", "abstract": "Automated segmentation of brain tumors from 3D magnetic resonance images (MRIs) is necessary for the diagnosis, monitoring, and treatment planning of the disease. Manual delineation practices require anatomical knowledge, are expensive, time consuming and can be inaccurate due to human error. Here, we describe a semantic segmentation network for tumor subregion segmentation from 3D MRIs based on encoder-decoder architecture. Due to a limited training dataset size, a variational auto-encoder branch is added to reconstruct the input image itself in order to regularize the shared decoder and impose additional constraints on its layers. The current approach won 1st place in the BraTS 2018 challenge.", "task": "semantic segmentation", "domain": "biomedical", "modality": "3D volumetric", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "a semantic segmentation network for tumor subregion segmentation from 3D MRIs based on encoder-decoder architecture"}
{"documents": ["ShapeNet"], "year": 2017, "keyphrase_query": "shape part segmentation,3d keypoint prediction graphs", "abstract": "In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parameterizing kernels in the spectral domain spanned by graph laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strive to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parameterization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested our SyncSpecCNN on various tasks, including 3D shape part segmentation and 3D keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.", "task": "D shape part segmentation,3D keypoint prediction", "domain": "", "modality": "graphs", "language": "", "training_style": "", "text_length": "", "query": "We developed a spectral CNN for semantic annotation on 3D models that are represented as shape graphs."}
{"documents": ["CoNLL-2009", "Penn Treebank", "WMT 2015"], "year": 2016, "keyphrase_query": "dependency parsing text language english, chinese", "abstract": "We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles(Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network.", "task": "dependency parsing", "domain": "language", "modality": "Text", "language": "English, Chinese", "training_style": "", "text_length": "Sentence-level", "query": "We adapt the greedy Stack-LSTM dependency parser to support a training-with-exploration procedure using dynamic oracles."}
{"documents": ["WMT 2014"], "year": 2017, "keyphrase_query": "neural machine translation text language english, german, french", "abstract": "State-of-the-art results on neural machine translation often use attentional sequence-to-sequence models with some form of convolution or recursion. Vaswani et. al. (2017) propose a new architecture that avoids recurrence and convolution completely. Instead, it uses only self-attention and feed-forward layers. While the proposed architecture achieves state-of-the-art results on several machine translation tasks, it requires a large number of parameters and training iterations to converge. We propose Weighted Transformer, a Transformer with modified attention layers, that not only outperforms the baseline network in BLEU score but also converges 15-40% faster. Specifically, we replace the multi-head attention by multiple self-attention branches that the model learns to combine during the training process. Our model improves the state-of-the-art performance by 0.5 BLEU points on the WMT 2014 English-to-German translation task and by 0.4 on the English-to-French translation task.", "task": "neural machine translation", "domain": "language", "modality": "Text", "language": "English, German, French", "training_style": "Supervised training/finetuning", "text_length": "", "query": " We propose a Transformer model with modified attention layers for machine translation."}
{"documents": ["COCO"], "year": 2018, "keyphrase_query": "visual recognition image", "abstract": "We present SNIPER, an algorithm for performing efficient multi-scale training in instance level visual recognition tasks. Instead of processing every pixel in an image pyramid, SNIPER processes context regions around ground-truth instances (referred to as chips) at the appropriate scale. For background sampling, these context-regions are generated using proposals extracted from a region proposal network trained with a short learning schedule. Hence, the number of chips generated per image during training adaptively changes based on the scene complexity. SNIPER only processes 30% more pixels compared to the commonly used single scale training at 800x1333 pixels on the COCO dataset. But, it also observes samples from extreme resolutions of the image pyramid, like 1400x2000 pixels. As SNIPER operates on resampled low resolution chips (512x512 pixels), it can have a batch size as large as 20 on a single GPU even with a ResNet-101 backbone. Therefore it can benefit from batch-normalization during training without the need for synchronizing batch-normalization statistics across GPUs. SNIPER brings training of instance level recognition tasks like object detection closer to the protocol for image classification and suggests that the commonly accepted guideline that it is important to train on high resolution images for instance level visual recognition tasks might not be correct. Our implementation based on Faster-RCNN with a ResNet-101 backbone obtains an mAP of 47.6% on the COCO dataset for bounding box detection and can process 5 images per second during inference with a single GPU. Code is available at this https URL.", "task": "visual recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a multi-scale training algorithm for efficient instance level visual recognition."}
{"documents": ["COCO", "PASCAL Context", "PASCAL VOC", "PASCAL VOC 2011"], "year": 2015, "keyphrase_query": "semantic segmentation image", "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce a new form of neural network that combines the Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs) for semantic segmentation."}
{"documents": ["WMT 2014"], "year": 2014, "keyphrase_query": "translation text english, french", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "task": "translation", "domain": "", "modality": "Text", "language": "English, French", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We present a general end-to-end approach to sequence learning that uses a multilayered Long Short-Term Memory (LSTM)."}
{"documents": ["Caltech Pedestrian Dataset", "CityPersons", "KITTI"], "year": 2018, "keyphrase_query": "pedestrian detection video", "abstract": "A critical issue in pedestrian detection is to detect small-scale objects that will introduce feeble contrast and motion blur in images and videos, which in our opinion should partially resort to deep-rooted annotation bias. Motivated by this, we propose a novel method integrated with somatic topological line localization (TLL) and temporal feature aggregation for detecting multi-scale pedestrians, which works particularly well with small-scale pedestrians that are relatively far from the camera. Moreover, a post-processing scheme based on Markov Random Field (MRF) is introduced to eliminate ambiguities in occlusion cases. Applying with these methodologies comprehensively, we achieve best detection performance on Caltech benchmark and improve performance of small-scale objects significantly (miss rate decreases from 74.53% to 60.79%). Beyond this, we also achieve competitive performance on CityPersons dataset and show the existence of annotation bias in KITTI dataset.", "task": "pedestrian detection", "domain": "", "modality": "Video", "language": "", "training_style": "", "text_length": "", "query": "We propose a novel method integrated with somatic topological line localization (TLL) and temporal feature aggregation for detecting multi-scale pedestrians."}
{"documents": ["COCO"], "year": 2019, "keyphrase_query": "object detection image", "abstract": "Keypoint-based methods are a relatively new paradigm in object detection, eliminating the need for anchor boxes and offering a simplified detection framework. Keypoint-based CornerNet achieves state of the art accuracy among single-stage detectors. However, this accuracy comes at high processing cost. In this work, we tackle the problem of efficient keypoint-based object detection and introduce CornerNet-Lite. CornerNet-Lite is a combination of two efficient variants of CornerNet: CornerNet-Saccade, which uses an attention mechanism to eliminate the need for exhaustively processing all pixels of the image, and CornerNet-Squeeze, which introduces a new compact backbone architecture. Together these two variants address the two critical use cases in efficient object detection: improving efficiency without sacrificing accuracy, and improving accuracy at real-time efficiency. CornerNet-Saccade is suitable for offline processing, improving the efficiency of CornerNet by 6.0x and the AP by 1.0% on COCO. CornerNet-Squeeze is suitable for real-time detection, improving both the efficiency and accuracy of the popular real-time detector YOLOv3 (34.4% AP at 34ms for CornerNet-Squeeze compared to 33.0% AP at 39ms for YOLOv3 on COCO). Together these contributions for the first time reveal the potential of keypoint-based detection to be useful for applications requiring processing efficiency.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We tackle the problem of efficient keypoint-based object detection."}
{"documents": ["IEMOCAP"], "year": 2018, "keyphrase_query": "emotion detection", "abstract": "Emotion detection in conversations is a necessary step for a number of applications, including opinion mining over chat history, social media threads, debates, argumentation mining, understanding consumer feedback in live conversations, and so on. Currently systems do not treat the parties in the conversation individually by adapting to the speaker of each utterance. In this paper, we describe a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification. Our model outperforms the state-of-the-art by a significant margin on two different datasets.", "task": "emotion detection", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "We propose a recurrent neural networks that  keeps track of the individual party states for emotion detection."}
{"documents": ["ModelNet", "PASCAL3D+"], "year": 2018, "keyphrase_query": "3d object retrieval image", "abstract": "We propose the Variational Shape Learner (VSL), a generative model that learns the underlying structure of voxelized 3D shapes in an unsupervised fashion. Through the use of skip-connections, our model can successfully learn and infer a latent, hierarchical representation of objects. Furthermore, realistic 3D objects can be easily generated by sampling the VSL's latent probabilistic manifold. We show that our generative model can be trained end-to-end from 2D images to perform single image 3D model retrieval. Experiments show, both quantitatively and qualitatively, the improved generalization of our proposed model over a range of tasks, performing better or comparable to various state-of-the-art alternatives.", "task": "3D object retrieval", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We propose a generative model that learns the underlying structure of voxelized 3D shapes from 2D images in an unsupervised fashion."}
{"documents": ["COCO", "COCO-QA", "DAQUAR", "Flickr-8k", "Flickr30k", "Visual Question Answering"], "year": 2018, "keyphrase_query": "image captioning, visual question answering", "abstract": "Much of the recent progress in Vision-to-Language problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we first propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We further show that the same mechanism can be used to incorporate external knowledge, which is critically important for answering high level visual questions. Specifically, we design a visual question answering model that combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. It particularly allows questions to be asked where the image alone does not contain the information required to select the appropriate answer. Our final model achieves the best reported results for both image captioning and visual question answering on several of the major benchmark datasets.", "task": "image captioning, visual question answering", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a method of incorporating high-level concepts and knowledge into CNN-RNN for image captioning and visual question answering."}
{"documents": ["MultiNLI", "SNLI"], "year": 2017, "keyphrase_query": "natural language inference text", "abstract": "Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to answer these questions by enriching the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models with external knowledge further improve the state of the art on the Stanford Natural Language Inference (SNLI) dataset.", "task": "natural language inference", "domain": "language", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We propose to enrich the state-of-the-art neural natural language inference models with external knowledge."}
{"documents": ["Arcade Learning Environment"], "year": 2015, "keyphrase_query": "games", "abstract": "The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.", "task": "games", "domain": "", "modality": "", "language": "", "training_style": "Reinforcement learning", "text_length": "", "query": "We propose a specific adaptation to the DQN reinforcement learning algorithm."}
{"documents": ["SNLI"], "year": 2018, "keyphrase_query": "natural language inference text", "abstract": "We present a novel deep learning architecture to address the natural language inference (NLI) task. Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis. Instead, we propose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) to efficiently model the relationship between a premise and a hypothesis during encoding and inference. We also introduce a sophisticated ensemble strategy to combine our proposed models, which noticeably improves final predictions. Finally, we demonstrate how the results can be improved further with an additional preprocessing step. Our evaluation shows that DR-BiLSTM obtains the best single model and ensemble model results achieving the new state-of-the-art scores on the Stanford NLI dataset.", "task": "natural language inference", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We present a a novel dependent reading bidirectional LSTM network to address the natural language inference (NLI) task."}
{"documents": ["SQuAD", "TriviaQA"], "year": 2017, "keyphrase_query": "question answering text", "abstract": "We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.", "task": "question answering", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "We propose method to train document question answering models to produce well calibrated confidence scores for their results on individual paragraphs."}
{"documents": ["300W", "AFLW", "AFLW2000-3D"], "year": 2015, "keyphrase_query": "face alignment image", "abstract": "Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45 degree), lacking the ability to align faces in large poses up to 90 degree. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods.", "task": "face alignment", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a face alignment method that fits a dense 3D face model to the image via convolutional neutral network."}
{"documents": ["CelebA", "MNIST-M", "WikiArt"], "year": 2018, "keyphrase_query": "mage-to-image translation", "abstract": "Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for many applications: (1) the lack of aligned training pairs and (2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for producing diverse outputs without paired training images. To achieve diversity, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Using the disentangled features as inputs greatly reduces mode collapse. To handle unpaired training data, we introduce a novel cross-cycle consistency loss. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks. We validate the effectiveness of our approach through extensive evaluation.", "task": "mage-to-image translation", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We present an Image-to-image translation approach based on disentangled representation, which does not require paired training images."}
{"documents": ["BSD", "Set14", "Set5"], "year": 2017, "keyphrase_query": "image restoration", "abstract": "Traditional works have shown that patches in a natural image tend to redundantly recur many times inside the image, both within the same scale, as well as across different scales. Make full use of these multi-scale information can improve the image restoration performance. However, the current proposed deep learning based restoration methods do not take the multi-scale information into account. In this paper, we propose a dilated convolution based inception module to learn multi-scale information and design a deep network for single image super-resolution. Different dilated convolution learns different scale feature, then the inception module concatenates all these features to fuse multi-scale information. In order to increase the reception field of our network to catch more contextual information, we cascade multiple inception modules to constitute a deep network to conduct single image super-resolution. With the novel dilated convolution based inception module, the proposed end-to-end single image super-resolution network can take advantage of multi-scale information to improve image super-resolution performance. Experimental results show that our proposed method outperforms many state-of-the-art single image super-resolution methods.", "task": "image restoration", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a deep learning-based single image restoration method."}
{"documents": ["COCO", "PASCAL VOC", "PASCAL VOC 2007"], "year": 2015, "keyphrase_query": "object detection image", "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to build a fast CNN model for object detection."}
{"documents": ["MPII Human Pose", "PASCAL-Part"], "year": 2017, "keyphrase_query": "multi-person pose estimation", "abstract": "This paper proposes a new Generative Partition Network (GPN) to address the challenging multi-person pose estimation problem. Different from existing models that are either completely top-down or bottom-up, the proposed GPN introduces a novel strategy--it generates partitions for multiple persons from their global joint candidates and infers instance-specific joint configurations simultaneously. The GPN is favorably featured by low complexity and high accuracy of joint detection and re-organization. In particular, GPN designs a generative model that performs one feed-forward pass to efficiently generate robust person detections with joint partitions, relying on dense regressions from global joint candidates in an embedding space parameterized by centroids of persons. In addition, GPN formulates the inference procedure for joint configurations of human poses as a graph partition problem, and conducts local optimization for each person detection with reliable global affinity cues, leading to complexity reduction and performance improvement. GPN is implemented with the Hourglass architecture as the backbone network to simultaneously learn joint detector and dense regressor. Extensive experiments on benchmarks MPII Human Pose Multi-Person, extended PASCAL-Person-Part, and WAF, show the efficiency of GPN with new state-of-the-art performance.", "task": "multi-person pose estimation", "domain": "", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a method for multi-person pose estimation from global joint candidates."}
{"documents": ["Amazon Review", "CoNLL-2003", "IMDb Movie Reviews", "MR", "Penn Treebank"], "year": 2018, "keyphrase_query": "text classification, sequence labeling", "abstract": "Bi-directional LSTMs are a powerful tool for text representation. On the other hand, they have been shown to suffer various limitations due to their sequential nature. We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word. Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incremental reading of a sequence of words. Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.", "task": "text classification, sequence labeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "I propose an LSTM for representing text simultaneously using local and global information for classification and sequence labelling."}
{"documents": ["ITOP"], "year": 2016, "keyphrase_query": "3d human pose estimation depth image", "abstract": "We propose a viewpoint invariant model for 3D human pose estimation from a single depth image. To achieve this, our discriminative model embeds local regions into a learned viewpoint invariant feature space. Formulated as a multi-task learning problem, our model is able to selectively predict partial poses in the presence of noise and occlusion. Our approach leverages a convolutional and recurrent network architecture with a top-down error feedback mechanism to self-correct previous pose estimates in an end-to-end manner. We evaluate our model on a previously published depth dataset and a newly collected human pose dataset containing 100 K annotated depth images from extreme viewpoints. Experiments show that our model achieves competitive performance on frontal views while achieving state-of-the-art performance on alternate viewpoints.", "task": "3D human pose estimation", "domain": "", "modality": "depth image", "language": "", "training_style": "", "text_length": "", "query": "We propose a viewpoint invariant model for 3D human pose estimation from a single depth image."}
{"documents": ["Dialogue State Tracking Challenge", "Wizard-of-Oz"], "year": 2018, "keyphrase_query": "dialogue state tracking text", "abstract": "Dialogue state tracking, which estimates user goals and requests given the dialogue context, is an essential part of task-oriented dialogue systems. In this paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. Our model uses global modules to share parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. We show that this significantly improves tracking of rare states and achieves state-of-the-art performance on the WoZ and DSTC2 state tracking tasks. GLAD obtains 88.1% joint goal accuracy and 97.1% request accuracy on WoZ, outperforming prior work by 3.7% and 5.5%. On DSTC2, our model obtains 74.5% joint goal accuracy and 97.5% request accuracy, outperforming prior work by 1.1% and 1.0%.", "task": "dialogue state tracking", "domain": "dialogue", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We are building a dialogue state tracking model which learns representations of the user utterance and previous system actions for a task-oriented dialogue system."}
{"documents": ["ImageNet", "LSP", "MPII Human Pose"], "year": 2016, "keyphrase_query": "articulated human pose estimation, detection image", "abstract": "This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation1.", "task": "articulated human pose estimation, detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose an approach that jointly solves the tasks of detection and articulated human pose estimation."}
{"documents": ["CUHK01", "CUHK03", "GRID Dataset", "VIPeR"], "year": 2014, "keyphrase_query": "person re-identification image surveillance videos", "abstract": "Person re-identification is an important technique towards automatic search of a person's presence in a surveillance video. Among various methods developed for person re-identification, the Mahalanobis metric learning approaches have attracted much attention due to their impressive performance. In practice, many previous papers have applied the Principle Component Analysis (PCA) for dimension reduction before metric learning. However, this may not be the optimal way for metric learning in low dimensional space. In this paper, we propose to jointly learn the discriminant low dimensional subspace and the distance metric. This is achieved by learning a projection matrix and a Restricted Quadratic Discriminant Analysis (RQDA) model. We show that the problem can be formulated as a Generalized Rayleigh Quotient, and a closed-form solution can be obtained by the generalized eigenvalue decomposition. We also present a practical computation method for RQDA, as well as its regularization. For the application of person re-identification, we propose a Retinex and maximum occurrence based feature representation method, which is robust to both illumination and viewpoint changes. Experiments on two challenging public databases, VIPeR and QMUL GRID, show that the performance of the proposed method is comparable to the state of the art.", "task": "person re-identification", "domain": "surveillance videos", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We want to build a system for person re-identification that is robust to illumination and viewpoint changes."}
{"documents": ["ACL ARC", "CoNLL-2003", "CoNLL-2012", "OntoNotes 5.0", "SNLI", "SQuAD", "SST"], "year": 2018, "keyphrase_query": "question answering, textual entailment, sentiment analysis", "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "task": "question answering, textual entailment, sentiment analysis", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We want to apply deep contextualized word representations to improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis."}
{"documents": ["300W", "AFLW", "AFLW2000-3D"], "year": 2019, "keyphrase_query": "3d face reconstruction image", "abstract": "3D face reconstruction from a single 2D image is a challenging problem with broad applications. Recent methods typically aim to learn a CNN-based 3D face model that regresses coefficients of 3D Morphable Model (3DMM) from 2D images to render 3D face reconstruction or dense face alignment. However, the shortage of training data with 3D annotations considerably limits performance of those methods. To alleviate this issue, we propose a novel 2D-assisted self-supervised learning (2DASL) method that can effectively use \"in-the-wild\" 2D face images with noisy landmark information to substantially improve 3D face model learning. Specifically, taking the sparse 2D facial landmarks as additional information, 2DSAL introduces four novel self-supervision schemes that view the 2D landmark and 3D landmark prediction as a self-mapping process, including the 2D and 3D landmark self-prediction consistency, cycle-consistency over the 2D landmark prediction and self-critic over the predicted 3DMM coefficients based on landmark predictions. Using these four self-supervision schemes, the 2DASL method significantly relieves demands on the the conventional paired 2D-to-3D annotations and gives much higher-quality 3D face models without requiring any additional 3D annotations. Experiments on multiple challenging datasets show that our method outperforms state-of-the-arts for both 3D face reconstruction and dense face alignment by a large margin.", "task": "3D face reconstruction", "domain": "", "modality": "Image", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We want to use self-supervised learning to leverage \"in-the-wild\" 2D face images with noisy landmark information to substantially improve 3D face model learning."}
{"documents": ["CIFAR-10", "CIFAR-100", "ImageNet", "ImageNet", "MultiNLI", "Penn Treebank", "SNLI", "WikiText-2"], "year": 2018, "keyphrase_query": "language modeling, natural inference, image classification", "abstract": "Optimal parameter initialization remains a crucial problem for neural network training. A poor weight initialization may take longer to train and/or converge to sub-optimal solutions. Here, we propose a method of weight re-initialization by repeated annealing and injection of noise in the training process. We implement this through a cyclical batch size schedule motivated by a Bayesian perspective of neural network training. We evaluate our methods through extensive experiments on tasks in language modeling, natural language inference, and image classification. We demonstrate the ability of our method to improve language modeling performance by up to 7.91 perplexity and reduce training iterations by up to $61\\%$, in addition to its flexibility in enabling snapshot ensembling and use with adversarial training.", "task": "language modeling, natural language inference, image classification", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "We want to see if improving parameter initialization can help deep networks perform on a variety of tasks in language modeling, natural language inference, and image classification."}
{"documents": ["PASCAL VOC 2011"], "year": 2014, "keyphrase_query": "keypoint prediction image", "abstract": "Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3]. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011 [4].", "task": "keypoint prediction", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We study the effectiveness of convnet activation features for tasks requiring correspondence on a finer level than object category, such as keypoint prediction."}
{"documents": ["Cityscapes", "GTA5", "SYNTHIA"], "year": 2018, "keyphrase_query": "semantic segmentation urban scenes", "abstract": "Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a significant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overfits to synthetic images, making the convolutional filters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose a new reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatial-aware adaptation scheme to effectively align the distribution of two domains. These two components can be readily integrated into existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We achieve a new state-of-the-art of 39.4% mean IoU on the Cityscapes dataset by adapting from the GTAV dataset.", "task": "semantic segmentation", "domain": "urban scenes", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want to adapt models trained on synthetic data for urban scene semantic segmentation."}
{"documents": ["KITTI", "SUN RGB-D"], "year": 2018, "keyphrase_query": "3d object detection rgb-d data point clouds", "abstract": "In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.", "task": "3D object detection from RGB-D data", "domain": "", "modality": "point clouds", "language": "", "training_style": "", "text_length": "", "query": "We want to leverage both mature 2D object detectors and advanced 3D deep learning for object localization from RGB-D images."}
{"documents": [], "year": 2017, "keyphrase_query": "abstractive summarization text", "abstract": "Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. Our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. The dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions. Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%. Notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text.", "task": "abstractive summarization", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "I propose a method for abstractive summarization that avoids generating fake facts."}
{"documents": ["CelebA", "LFW"], "year": 2014, "keyphrase_query": "face recognition image", "abstract": "The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset [11], 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result [20] on LFW, the error rate has been significantly reduced by 67%.", "task": "face recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "Proposes a deep learning system for face recognition that utilizes both face identification features and verification signals as supervision."}
{"documents": ["CK+"], "year": 2017, "keyphrase_query": "facial expression recognition image", "abstract": "Facial expression recognition methods use a combination of geometric and appearance-based features. Spatial features are derived from displacements of facial landmarks, and carry geometric information. These features are either selected based on prior knowledge, or dimension-reduced from a large pool. In this study, we produce a large number of potential spatial features using two combinations of facial landmarks. Among these, we search for a descriptive subset of features using sequential forward selection. The chosen feature subset is used to classify facial expressions in the extended Cohn-Kanade dataset (CK+), and delivered 88.7% recognition accuracy without using any appearance-based features.", "task": "facial expression recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We produce a large number of potential spatial features for facial expression recognition, and select a descriptive subset using sequential forward selection."}
{"documents": ["ShapeNet"], "year": 2017, "keyphrase_query": "3d reconstruction single image", "abstract": "Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images, however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output – point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthordox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3D reconstruction benchmarks, but it also shows strong performance for 3D shape completion and promising ability in making multiple plausible predictions.", "task": "3D reconstruction from a single image", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We design architecture, loss function and learning paradigm that are novel and effective for 3D reconstruction from a single image."}
{"documents": ["COCO", "CamVid", "Cityscapes", "KITTI", "PASCAL VOC"], "year": 2015, "keyphrase_query": "semantic segmentation image", "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We develop a new convolutional network module that is specifically designed for dense prediction"}
{"documents": ["COCO", "PASCAL VOC", "PASCAL VOC 2007"], "year": 2017, "keyphrase_query": "object detection image", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals."}
{"documents": ["Penn Treebank", "WikiText-2"], "year": 2016, "keyphrase_query": "language modeling text", "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.", "task": "language modeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We introduce a novel theoretical framework that leads to tying together the input embedding and the output projection matrices for language modeling task."}
{"documents": ["COCO", "PASCAL VOC"], "year": 2016, "keyphrase_query": "semantic segmentation image", "abstract": "Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multitask Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We present Multitask Network Cascades for instance-aware semantic segmentation."}
{"documents": ["WMT 2014", "WMT 2016"], "year": 2019, "keyphrase_query": "machine translation text english, german", "abstract": "While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.", "task": "machine translation", "domain": "", "modality": "Text", "language": "English, German", "training_style": "Unsupervised", "text_length": "", "query": "By exploiting subword information, we develop a theoretically well founded unsupervised tuning method, and incorporate a joint refinement procedure for machine translation."}
{"documents": ["BSD", "DIV2K", "Set14", "Set5", "Urban100"], "year": 2018, "keyphrase_query": "single image super-resolution", "abstract": "Recent years have witnessed the unprecedented success of deep convolutional neural networks (CNNs) in single image super-resolution (SISR). However, existing CNN-based SISR methods mostly assume that a low-resolution (LR) image is bicubicly downsampled from a high-resolution (HR) image, thus inevitably giving rise to poor performance when the true degradation does not follow this assumption. Moreover, they lack scalability in learning a single model to deal with multiple degradations. To address these issues, we propose a dimensionality stretching strategy that enables a single convolutional super-resolution network to take two key factors of the SISR degradation process, i.e., blur kernel and noise level, as input. Consequently, the proposed super-resolver can handle multiple and even spatially variant degradations, which significantly improves the practicability. Extensive experimental results on synthetic and real LR images show that the proposed convolutional super-resolution network not only can produce favorable results on multiple degradations but also is computationally efficient, providing a highly effective and scalable solution to practical SISR applications.", "task": "single image super-resolution", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a dimensionality stretching strategy that enables a single convolutional super-resolution network to take two key factors of the SISR degradation process, i.e., blur kernel and noise level, as input."}
{"documents": ["MNIST", "Omniglot"], "year": 2016, "keyphrase_query": "learning representations datasets", "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes.", "task": "learning representations of datasets", "domain": "", "modality": "", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I would like a dataset for unsupervised learning of dataset representations useful for supervised and unsupervised downstream tasks."}
{"documents": ["WMT 2014", "WMT 2016"], "year": 2017, "keyphrase_query": "english-german translation, english-french text english, german,", "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.", "task": "English-German translation, English-French translation", "domain": "", "modality": "Text", "language": "English, German, French", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I would like a dataset for supervised machine translation from english to german and english to french."}
{"documents": ["CK+", "MMI"], "year": 2015, "keyphrase_query": "facial expression recognition image", "abstract": "We propose a convolutional neural network (CNN) architecture for facial expression recognition. The proposed architecture is independent of any hand-crafted feature extraction and performs better than the earlier proposed convolutional neural network based approaches. We visualize the automatically extracted features which have been learned by the network in order to provide a better understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP) and MMI Facial Expression Databse are used for the quantitative evaluation. On the CKP set the current state of the art approach, using CNNs, achieves an accuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion recognition is 93.33%. The proposed architecture achieves 99.6% for CKP and 98.63% for MMI, therefore performing better than the state of the art using CNNs. Automatic facial expression recognition has a broad spectrum of applications such as human-computer interaction and safety systems. This is due to the fact that non-verbal cues are important forms of communication and play a pivotal role in interpersonal communication. The performance of the proposed architecture endorses the efficacy and reliable usage of the proposed work for real world applications.", "task": "facial expression recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I would like a dataset for supervised facial expression recognition"}
{"documents": [], "year": 2017, "keyphrase_query": "3d infant brain mri segmentation image medical images", "abstract": "Precise 3D segmentation of infant brain tissues is an essential step towards comprehensive volumetric studies and quantitative analysis of early brain developement. However, computing such segmentations is very challenging, especially for 6-month infant brain, due to the poor image quality, among other difficulties inherent to infant brain MRI, e.g., the isointense contrast between white and gray matter and the severe partial volume effect due to small brain sizes. This study investigates the problem with an ensemble of semi-dense fully convolutional neural networks (CNNs), which employs T1-weighted and T2-weighted MR images as input. We demonstrate that the ensemble agreement is highly correlated with the segmentation errors. Therefore, our method provides measures that can guide local user corrections. To the best of our knowledge, this work is the first ensemble of 3D CNNs for suggesting annotations within images. Furthermore, inspired by the very recent success of dense networks, we propose a novel architecture, SemiDenseNet, which connects all convolutional layers directly to the end of the network. Our architecture allows the efficient propagation of gradients during training, while limiting the number of parameters, requiring one order of magnitude less parameters than popular medical image segmentation networks such as 3D U-Net. Another contribution of our work is the study of the impact that early or late fusions of multiple image modalities might have on the performances of deep architectures. We report evaluations of our method on the public data of the MICCAI iSEG-2017 Challenge on 6-month infant brain MRI segmentation, and show very competitive results among 21 teams, ranking first or second in most metrics.", "task": "3D infant brain MRI segmentation", "domain": "medical images", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I would like a dataset for supervised 3D infant brain MRI segmentation"}
{"documents": ["Billion Word Benchmark", "Penn Treebank", "Text8", "WikiText-103"], "year": 2019, "keyphrase_query": "language modeling text", "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.", "task": "language modeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We propose a novel method, called Transformer-XL, for long-range sequence modeling."}
{"documents": ["ICDAR 2015", "ICDAR 2017", "SCUT-CTW1500"], "year": 2019, "keyphrase_query": "scene text detection image", "abstract": "Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape. On the other hand, two text instances which are close to each other may lead to a false detection which covers both instances. Traditionally, the segmentation-based approach can relieve the first problem but usually fail to solve the second challenge. To address these two challenges, in this paper, we propose a novel Progressive Scale Expansion Network (PSENet), which can precisely detect text instances with arbitrary shapes. More specifically, PSENet generates the different scale of kernels for each text instance, and gradually expands the minimal scale kernel to the text instance with the complete shape. Due to the fact that there are large geometrical margins among the minimal scale kernels, our method is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances. Extensive experiments on CTW1500, Total-Text, ICDAR 2015 and ICDAR 2017 MLT validate the effectiveness of PSENet. Notably, on CTW1500, a dataset full of long curve texts, PSENet achieves a F-measure of 74.3% at 27 FPS, and our best F-measure (82.2%) outperforms state-of-art algorithms by 6.6%. The code will be released in the future.", "task": "scene text detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "Dataset for scene text recognition with arbitrary-shaped text instances"}
{"documents": ["Hutter Prize", "WMT 2014", "WMT 2015 News"], "year": 2016, "keyphrase_query": "sequence processing text", "abstract": "We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.", "task": "sequence processing", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We present a novel neural network for sequence-to-sequence modeling."}
{"documents": ["CIFAR-10", "CIFAR-100", "ImageNet"], "year": 2017, "keyphrase_query": "image classification", "abstract": "Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at this https URL}", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want to improve generalization ability for image classification."}
{"documents": ["300W", "AFW", "Helen", "LFPW", "LSP", "MPII Human Pose"], "year": 2018, "keyphrase_query": "visual landmark localization image humans", "abstract": "In this paper, we propose quantized densely connected U-Nets for efficient visual landmark localization. The idea is that features of the same semantic meanings are globally reused across the stacked U-Nets. This dense connectivity largely improves the information flow, yielding improved localization accuracy. However, a vanilla dense design would suffer from critical efficiency issue in both training and testing. To solve this problem, we first propose order-K dense connectivity to trim off long-distance shortcuts; then, we use a memory-efficient implementation to significantly boost the training efficiency and investigate an iterative refinement that may slice the model size in half. Finally, to reduce the memory consumption and high precision operations both in training and testing, we further quantize weights, inputs, and gradients of our localization network to low bit-width numbers. We validate our approach in two tasks: human pose estimation and face alignment. The results show that our approach achieves state-of-the-art localization accuracy, but using ~70% fewer parameters, ~98% less model size and saving ~75% training memory compared with other benchmark localizers. The code is available at https://github.com/zhiqiangdon/CU-Net.", "task": "visual landmark localization", "domain": "humans", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to train a visual landmark localization method for processing images of humans."}
{"documents": ["Visual Question Answering"], "year": 2017, "keyphrase_query": "visual question answering images text", "abstract": "This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.", "task": "visual question answering", "domain": "", "modality": "images and text", "language": "", "training_style": "", "text_length": "", "query": "We want to improve visual question answering (VQA) by leveraging graph representations of both scene contents and questions."}
{"documents": ["BABEL Project", "Switchboard-1 Corpus"], "year": 2016, "keyphrase_query": "speech recognition low-resource", "abstract": "Abstract : Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77 WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub500 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8 after cross-entropy training, a 1.4 WER improvement (10.6 relative) over the best published CNN result so far.", "task": "speech recognition", "domain": "low-resource", "modality": "Speech", "language": "", "training_style": "", "text_length": "", "query": "We propose a system for multilingual speech recognition that can perform in both low-resource and high-resource settings."}
{"documents": ["CIFAR-10", "LSP", "MPII Human Pose"], "year": 2017, "keyphrase_query": "articulated human pose estimation image humans", "abstract": "Articulated human pose estimation is a fundamental yet challenging task in computer vision. The difficulty is particularly pronounced in scale variations of human body parts when camera view changes or severe foreshortening happens. Although pyramid methods are widely used to handle scale changes at inference time, learning feature pyramids in deep convolutional neural networks (DCNNs) is still not well explored. In this work, we design a Pyramid Residual Module (PRMs) to enhance the invariance in scales of DCNNs. Given input features, the PRMs learn convolutional filters on various scales of input features, which are obtained with different subsampling ratios in a multibranch network. Moreover, we observe that it is inappropriate to adopt existing methods to initialize the weights of multi-branch networks, which achieve superior performance than plain networks in many tasks recently. Therefore, we provide theoretic derivation to extend the current weight initialization scheme to multi-branch network structures. We investigate our method on two standard benchmarks for human pose estimation. Our approach obtains state-of-the-art results on both benchmarks. Code is available at https://github.com/bearpaw/PyraNet.", "task": "articulated human pose estimation", "domain": "humans", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We want to improve the robustness of articulated human pose estimation methods to scale variations of human body parts."}
{"documents": ["TIMIT"], "year": 2018, "keyphrase_query": "speech recognition", "abstract": "Recently, the connectionist temporal classification (CTC) model coupled with recurrent (RNN) or convolutional neural networks (CNN), made it easier to train speech recognition systems in an end-to-end fashion. However in real-valued models, time frame components such as mel-filter-bank energies and the cepstral coefficients obtained from them, together with their first and second order derivatives, are processed as individual elements, while a natural alternative is to process such components as composed entities. We propose to group such elements in the form of quaternions and to process these quaternions using the established quaternion algebra. Quaternion numbers and quaternion neural networks have shown their efficiency to process multidimensional inputs as entities, to encode internal dependencies, and to solve many tasks with less learning parameters than real-valued models. This paper proposes to integrate multiple feature views in quaternion-valued convolutional neural network (QCNN), to be used for sequence-to-sequence mapping with the CTC model. Promising results are reported using simple QCNNs in phoneme recognition experiments with the TIMIT corpus. More precisely, QCNNs obtain a lower phoneme error rate (PER) with less learning parameters than a competing model based on real-valued CNNs.", "task": "speech recognition", "domain": "", "modality": "Speech", "language": "", "training_style": "", "text_length": "", "query": "We want to reduce the phoneme error rate of speech recognition models."}
{"documents": ["BraTS 2013"], "year": 2017, "keyphrase_query": "segmentation glioblastoma mri biomedical", "abstract": "Abstract In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we’ve found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.", "task": "segmentation of glioblastoma", "domain": "biomedical", "modality": "MRI", "language": "", "training_style": "", "text_length": "", "query": "A method for segmentation of glioblastoma to handle highly variable tumors in MRI scans."}
{"documents": ["CoNLL-2003"], "year": 2018, "keyphrase_query": "named entity recognition text", "abstract": "Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora. In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component. We evaluate on the task of named entity recognition (NER), where we show that including manual features for part-of-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a $F_1$ of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of auto-encoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60\\%, while retaining the same predictive accuracy.", "task": "named entity recognition", "domain": "", "modality": "Text", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We want to investigate the effectiveness of handcrafted features like part-of-speech, word shapes, and gazetteers for the task of named entity recognition (NER)."}
{"documents": ["AFLW", "ChaLearn Pose", "MORPH", "PASCAL VOC", "PASCAL VOC 2007", "PASCAL VOC 2011", "SBD"], "year": 2016, "keyphrase_query": "age estimation, head pose image recognition, semantic segmentation", "abstract": "Convolutional neural networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient training images with precise labels in some domains, such as apparent age estimation, head pose estimation, multilabel classification, and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete label distribution, and learn the label distribution by minimizing a Kullback–Leibler divergence between the predicted and ground-truth label distributions using deep ConvNets. The proposed deep label distribution learning (DLDL) method effectively utilizes the label ambiguity in both feature learning and classifier learning, which help prevent the network from overfitting even when the training set is small. Experimental results show that the proposed approach produces significantly better results than the state-of-the-art methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label classification and semantic segmentation tasks.", "task": "age estimation, head pose estimation, image recognition, semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to create a method that can perform image processing while being robust to label ambiguity."}
{"documents": ["Billion Word Benchmark"], "year": 2016, "keyphrase_query": "language modeling text", "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.", "task": "language modeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "An language modeling approach based on character-level neural networks."}
{"documents": ["PASCAL VOC 2007"], "year": 2017, "keyphrase_query": "weakly-supervised object detection image", "abstract": "Weakly-supervised object detection (WOD) is a challenging problems in computer vision. The key problem is to simultaneously infer the exact object locations in the training images and train the object detectors, given only the training images with weak image-level labels. Intuitively, by simulating the selective attention mechanism of human visual system, saliency detection technique can select attractive objects in scenes and thus is a potential way to provide useful priors for WOD. However, the way to adopt saliency detection in WOD is not trivial since the detected saliency region might be possibly highly ambiguous in complex cases. To this end, this paper first comprehensively analyzes the challenges in applying saliency detection to WOD. Then, we make one of the earliest efforts to bridge saliency detection to WOD via the self-paced curriculum learning, which can guide the learning procedure to gradually achieve faithful knowledge of multi-class objects from easy to hard. The experimental results demonstrate that the proposed approach can successfully bridge saliency detection and WOD tasks and achieve the state-of-the-art object detection results under the weak supervision.", "task": "weakly-supervised object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Weakly supervised", "text_length": "", "query": "I want to train an object detector under weak supervision of image-level labels."}
{"documents": ["2000 HUB5 English", "Switchboard-1 Corpus"], "year": 2016, "keyphrase_query": "automatic speech recognition text conversational data english", "abstract": "We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6% on the Switchboard subset of the Hub5 2000 evaluation testset. On the acoustic side, we use a score fusion of three strong models: recurrent nets with maxout activations, very deep convolutional nets with 3x3 kernels, and bidirectional long short-term memory nets which operate on FMLLR and i-vector features. On the language modeling side, we use an updated model \"M\" and hierarchical neural network LMs.", "task": "automatic speech recognition", "domain": "conversational data", "modality": "speech and text", "language": "English", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train an automatic speech recognizer system for conversational data in English."}
{"documents": ["ImageNet"], "year": 2016, "keyphrase_query": "image classification", "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We explore ways to efficiently scale a convolutional neural network model for image classification"}
{"documents": ["COCO", "PASCAL VOC"], "year": 2016, "keyphrase_query": "object classification, point-based localization image", "abstract": "This paper aims to classify and locate objects accurately and efficiently, without using bounding box annotations. It is challenging as objects in the wild could appear at arbitrary locations and in different scales. In this paper, we propose a novel classification architecture ProNet based on convolutional neural networks. It uses computationally efficient neural networks to propose image regions that are likely to contain objects, and applies more powerful but slower networks on the proposed regions. The basic building block is a multi-scale fully-convolutional network which assigns object confidence scores to boxes at different locations and scales. We show that such networks can be trained effectively using image-level annotations, and can be connected into cascades or trees for efficient object classification. ProNet outperforms previous state-of-the-art significantly on PASCAL VOC 2012 and MS COCO datasets for object classification and point-based localization.", "task": "object classification, point-based localization", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to build a model for object classification and point-based localization using image-level annotations"}
{"documents": [], "year": 2017, "keyphrase_query": "video super-resolution", "abstract": "Convolutional neural networks have enabled accurate image super-resolution in real-time. However, recent attempts to benefit from temporal correlations in video super-resolution have been limited to naive or inefficient architectures. In this paper, we introduce spatio-temporal sub-pixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real-time speed. Specifically, we discuss the use of early fusion, slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames. We also propose a novel joint motion compensation and video super-resolution algorithm that is orders of magnitude more efficient than competing methods, relying on a fast multi-resolution spatial transformer module that is end-to-end trainable. These contributions provide both higher accuracy and temporally more consistent videos, which we confirm qualitatively and quantitatively. Relative to single-frame models, spatio-temporal networks can either reduce the computational cost by 30% whilst maintaining the same quality or provide a 0.2dB gain for a similar computational cost. Results on publicly available datasets demonstrate that the proposed algorithms surpass current state-of-the-art performance in both accuracy and efficiency.", "task": "video super-resolution", "domain": "", "modality": "Video", "language": "", "training_style": "", "text_length": "", "query": "A novel joint motion compensation and video super-resolution algorithm using a fast multi-resolution spatial transformer module that is end-to-end trainable"}
{"documents": ["DBLP"], "year": 2015, "keyphrase_query": "network embedding information networks", "abstract": "This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\\footnote{\\url{https://github.com/tangjianpku/LINE}}.", "task": "network embedding", "domain": "", "modality": "information networks", "language": "", "training_style": "", "text_length": "", "query": "Efficient algorithm to embed a large information network into low-dimensional vectors"}
{"documents": ["CoNLL-2014 Shared Task: Grammatical Error Correction", "FCE"], "year": 2016, "keyphrase_query": "grammatical error detection text", "abstract": "In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs. Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment system, leading to performance comparable to human annotators.", "task": "grammatical error detection", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose an LSTM-based model for grammatical error detection in learner writing."}
{"documents": ["Caltech Pedestrian Dataset", "KITTI"], "year": 2015, "keyphrase_query": "pedestrian detection image", "abstract": "This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low-level features in combination with a boosted decision forest. Based on this observation we propose a unifying framework and experimentally explore different filter families. We report extensive results enabling a systematic analysis. Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets, while using only HOG+LUV as low-level features. When adding optical flow features we further improve detection quality and report the best known results on the Caltech dataset, reaching 93% recall at 1 FPPI.", "task": "pedestrian detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a unified framework for pedestrian detection based on feature filtering and random forests."}
{"documents": ["CCGbank", "CoNLL-2003", "CoNLL-2003", "IWSLT2015", "OntoNotes 5.0", "Penn Treebank"], "year": 2018, "keyphrase_query": "representation learning, sequence tagging, machine translation, dependency parsing image", "abstract": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.", "task": "Representation learning, sequence tagging, machine translation, dependency parsing", "domain": "", "modality": "Image", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We propose a semi-supervised learning algorithm, Cross-View Training, that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data."}
{"documents": ["Billion Word Benchmark"], "year": 2014, "keyphrase_query": "language model estimation text", "abstract": "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the On e Billion Word Benchmark [Chelba et al., 2013] shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary feature s effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do.", "task": "Language model estimation", "domain": "", "modality": "Text", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I want to develop a self-supervised language model estimation method"}
{"documents": ["Amazon Review", "CUHK03", "Office-31", "PRID2011", "VIPeR"], "year": 2015, "keyphrase_query": "document sentiment analysis, image classification", "abstract": "We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. ::: ::: The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. ::: ::: We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.", "task": "document sentiment analysis, image classification", "domain": "", "modality": "", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We introduce a domain adaptation approach that uses labeled data from the source domain and unlabeled data from the target domain during training. "}
{"documents": ["MNIST", "SVHN", "smallNORB"], "year": 2017, "keyphrase_query": "image classification, generation", "abstract": "In this paper, we describe the \"PixelGAN autoencoder\", a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels (PixelCNN) that is conditioned on a latent code, and the recognition path uses a generative adversarial network (GAN) to impose a prior distribution on the latent code. We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder. For example, by imposing a Gaussian distribution as the prior, we can achieve a global vs. local decomposition, or by imposing a categorical distribution as the prior, we can disentangle the style and content information of images in an unsupervised fashion. We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi-supervised settings and achieve competitive semi-supervised classification results on the MNIST, SVHN and NORB datasets.", "task": "image classification, image generation", "domain": "", "modality": "Image", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We devise a model for unsupervised image style and content disentanglement and semi-supervised image classification."}
{"documents": ["SNLI"], "year": 2015, "keyphrase_query": "natural language inference (nli) text", "abstract": "In this paper, we propose the TBCNN-pair model to recognize entailment and contradiction between two sentences. In our model, a tree-based convolutional neural network (TBCNN) captures sentence-level semantics; then heuristic matching layers like concatenation, element-wise product/difference combine the information in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin.", "task": "natural language inference (NLI)", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level", "query": "We propose a model for entailment recognition between two sentences."}
{"documents": ["Flickr30k", "SNLI", "Visual Genome"], "year": 2018, "keyphrase_query": "visually-grounded textual entailment image,", "abstract": "Capturing semantic relations between sentences, such as entailment, is a long-standing challenge for computational semantics. Logic-based models analyse entailment in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant\"world\"or\"situation\"). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare\"blind\"and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing\"grounding\"in an optimal fashion.", "task": "visually-grounded textual entailment", "domain": "", "modality": "image, text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "In-depth error analysis of grounding in current multimodal models through visually-grounded textual entailment."}
{"documents": ["ImageNet"], "year": 2015, "keyphrase_query": "object detection, image classification", "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.", "task": "object detection, image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "A deep convolutional neural network architecture for object detection and image classification."}
{"documents": ["BSD100", "Set14", "Set5"], "year": 2017, "keyphrase_query": "image super-resolution", "abstract": "Recent random-forest (RF)-based image super-resolution approaches inherit some properties from dictionary-learning-based algorithms, but the effectiveness of the properties in RF is overlooked in the literature. In this paper, we present a novel feature-augmented random forest (FARF) for image super-resolution, where the conventional gradient-based features are augmented with gradient magnitudes and different feature recipes are formulated on different stages in an RF. The advantages of our method are that, firstly, the dictionary-learning-based features are enhanced by adding gradient magnitudes, based on the observation that the non-linear gradient magnitude are with highly discriminative property. Secondly, generalized locality-sensitive hashing (LSH) is used to replace principal component analysis (PCA) for feature dimensionality reduction and original high-dimensional features are employed, instead of the compressed ones, for the leaf-nodes' regressors, since regressors can benefit from higher dimensional features. This original-compressed coupled feature sets scheme unifies the unsupervised LSH evaluation on both image super-resolution and content-based image retrieval (CBIR). Finally, we present a generalized weighted ridge regression (GWRR) model for the leaf-nodes' regressors. Experiment results on several public benchmark datasets show that our FARF method can achieve an average gain of about 0.3 dB, compared to traditional RF-based methods. Furthermore, a fine-tuned FARF model can compare to or (in many cases) outperform some recent stateof-the-art deep-learning-based algorithms.", "task": "image super-resolution", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "-I proposed a feature augmented random forest model for image super-resolution that is competitive with some recent stateof-the-art deep-learning-based algorithms.\n\n-I proposed a feature augmented random forest model for image super-resolution where the dictionary-learning-based features are enhanced by adding gradient magnitudes.\n\n"}
{"documents": ["BUCC", "MLDoc", "Tatoeba", "XNLI"], "year": 2019, "keyphrase_query": "multilingual sentence embedding text", "abstract": "We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses...", "task": "multilingual sentence embedding", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "I introduce an architecture to learn joint multilingual sentence representations."}
{"documents": ["BSD100", "DIV2K", "Set14", "Set5", "Urban100"], "year": 2018, "keyphrase_query": "image super-resolution", "abstract": "Advances in image super-resolution (SR) have recently benefited significantly from rapid developments in deep neural networks. Inspired by these recent discoveries, we note that many state-of-the-art deep SR architectures can be reformulated as a single-state recurrent neural network (RNN) with finite unfoldings. In this paper, we explore new structures for SR based on this compact RNN view, leading us to a dual-state design, the Dual-State Recurrent Network (DSRN). Compared to its single state counterparts that operate at a fixed spatial resolution, DSRN exploits both low-resolution (LR) and high-resolution (HR) signals jointly. Recurrent signals are exchanged between these states in both directions (both LR to HR and HR to LR) via delayed feedback. Extensive quantitative and qualitative evaluations on benchmark datasets and on a recent challenge demonstrate that the proposed DSRN performs favorably against state-of-the-art algorithms in terms of both memory consumption and predictive accuracy.", "task": "image super-resolution", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want an efficient deep recurrent approach that performs favourably with the state-of-the-art on the image super-resolution task."}
{"documents": ["MultiNLI", "Quora Question Pairs", "SNLI"], "year": 2017, "keyphrase_query": "natural language inference text", "abstract": "Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.", "task": "natural language inference", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "We want to improve on the state-of-the-art on natural language inference tasks."}
{"documents": ["TIMIT"], "year": 2013, "keyphrase_query": "speech recognition", "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.", "task": "speech recognition", "domain": "", "modality": "speech", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to build an end-to-end speech recognition model."}
{"documents": ["BSD", "BSD100", "Set14", "Set5"], "year": 2016, "keyphrase_query": "image restoration, super-resolution, denoising", "abstract": "In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. De-convolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and de-convolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, The skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to de-convolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than all previously reported state-of-the-art methods.", "task": "image restoration, image super-resolution, image denoising", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "I want to build a very deep network to improve on the state-of-the-art of the image restoration task."}
{"documents": ["CUHK03", "Market-1501", "Oxford5k"], "year": 2017, "keyphrase_query": "person re-identification image", "abstract": "In this article, we revisit two popular convolutional neural networks in person re-identification (re-ID): verification and identification models. The two models have their respective advantages and limitations due to different loss functions. Here, we shed light on how to combine the two models to learn more discriminative pedestrian descriptors. Specifically, we propose a Siamese network that simultaneously computes the identification loss and verification loss. Given a pair of training images, the network predicts the identities of the two input images and whether they belong to the same identity. Our network learns a discriminative embedding and a similarity measurement at the same time, thus taking full usage of the re-ID annotations. Our method can be easily applied on different pretrained networks. Albeit simple, the learned embedding improves the state-of-the-art performance on two public person re-ID benchmarks. Further, we show that our architecture can also be applied to image retrieval. The code is available at https://github.com/layumi/2016_person_re-ID.", "task": "person re-identification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We want to build a system to perform better pedestrian re-identification by learning better pedestrian descriptors."}
{"documents": ["Cityscapes", "GTA5", "SYNTHIA"], "year": 2018, "keyphrase_query": "semantic segmentation image", "abstract": "Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to use of adversarial learning to perform domain adaptation for semantic segmentation of images."}
{"documents": ["CUHK03", "LSP", "MPII Human Pose", "Market-1501", "VIPeR"], "year": 2017, "keyphrase_query": "person re-identification image humans", "abstract": "Feature extraction and matching are two crucial components in person Re-Identification (ReID). The large pose deformations and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images. To overcome these difficulties, in this work we propose a Pose-driven Deep Convolutional (PDC) model to learn improved feature extraction and matching models from end to end. Our deep architecture explicitly leverages the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts. To match the features from global human body and local body parts, a pose driven feature weighting sub-network is further designed to learn adaptive feature fusions. Extensive experimental analyses and results on three popular datasets demonstrate significant performance improvements of our model over all published state-of-the-art methods.", "task": "person re-identification", "domain": "humans", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to improve person re-identification by explicitly leveraging human part cues to better deal with pose variations."}
{"documents": ["Arcade Learning Environment"], "year": 2018, "keyphrase_query": "deep rl", "abstract": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.", "task": "deep RL", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "I want to develop a model for reinforcement learning system at scale using simulations of atari games. "}
{"documents": ["WikiSQL"], "year": 2018, "keyphrase_query": "graph-to-sequence generation text, graphs", "abstract": "The celebrated Sequence to Sequence learning (Seq2Seq) technique and its numerous variants achieve excellent performance on many tasks. However, many machine learning tasks have inputs naturally represented as graphs; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence. To address this challenge, we introduce a novel general end-to-end graph-to-sequence neural encoder-decoder model that maps an input graph to a sequence of vectors and uses an attention-based LSTM method to decode the target sequence from these vectors. Our method first generates the node and graph embeddings using an improved graph-based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings. We further introduce an attention mechanism that aligns node embeddings and the decoding sequence to better cope with large graphs. Experimental results on bAbI, Shortest Path, and Natural Language Generation tasks demonstrate that our model achieves state-of-the-art performance and significantly outperforms existing graph neural networks, Seq2Seq, and Tree2Seq models; using the proposed bi-directional node embedding aggregation strategy, the model can converge rapidly to the optimal performance.", "task": "graph-to-sequence generation", "domain": "", "modality": "Text, Graphs", "language": "", "training_style": "", "text_length": "", "query": "We propose a graph-to-sequence neural encoder that can be applied to tasks like natural language generation."}
{"documents": ["ImageNet", "PASCAL VOC", "PASCAL VOC 2007", "PeopleArt"], "year": 2016, "keyphrase_query": "object detection image natural images, artwork", "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. ::: Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.", "task": "object detection", "domain": "natural images, artwork", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to train an object detection model that can run in real time, and can also generalize to new domains like artwork."}
{"documents": ["Arcade Learning Environment"], "year": 2018, "keyphrase_query": "reinforcement learning games", "abstract": "In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.", "task": "reinforcement learning", "domain": "", "modality": "games", "language": "", "training_style": "Reinforcement Learning", "text_length": "", "query": "We improve the state-of-the-art in reinforcement learning by using a variant of DQN."}
{"documents": ["Nottingham"], "year": 2016, "keyphrase_query": "sequence generation discrete tokens", "abstract": "As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.", "task": "sequence generation", "domain": "", "modality": "discrete tokens", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We modify the GAN architecture to allow us to generate sequences of discrete tokens."}
{"documents": ["BSD100", "General-100", "Set14", "Set5"], "year": 2018, "keyphrase_query": "image super-resolution", "abstract": "In this paper, we propose a novel random-forest scheme, namely Joint Maximum Purity Forest (JMPF), for classification, clustering, and regression tasks. In the JMPF scheme, the original feature space is transformed into a compactly pre-clustered feature space, via a trained rotation matrix. The rotation matrix is obtained through an iterative quantization process, where the input data belonging to different classes are clustered to the respective vertices of the new feature space with maximum purity. In the new feature space, orthogonal hyperplanes, which are employed at the split-nodes of decision trees in random forests, can tackle the clustering problems effectively. We evaluated our proposed method on public benchmark datasets for regression and classification tasks, and experiments showed that JMPF remarkably outperforms other state-of-the-art random-forest-based approaches. Furthermore, we applied JMPF to image super-resolution, because the transformed, compact features are more discriminative to the clustering-regression scheme. Experiment results on several public benchmark datasets also showed that the JMPF-based image super-resolution scheme is consistently superior to recent state-of-the-art image super-resolution algorithms.", "task": "image super-resolution", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel scheme for image super-resolution, which uses compactly pre-clustered features."}
{"documents": ["IWSLT2015", "MultiNLI", "Quora Question Pairs", "SNLI"], "year": 2018, "keyphrase_query": "natural language inference, paraphrase detection, sentiment classification, machine translation text", "abstract": "We propose a method of stacking multiple long short-term memory (LSTM) layers for modeling sentences. In contrast to the conventional stacked LSTMs where only hidden states are fed as input to the next layer, the suggested architecture accepts both hidden and memory cell states of the preceding layer and fuses information from the left and the lower context using the soft gating mechanism of LSTMs. Thus the architecture modulates the amount of information to be delivered not only in horizontal recurrence but also in vertical connections, from which useful features extracted from lower layers are effectively conveyed to upper layers. We dub this architecture Cell-aware Stacked LSTM (CAS-LSTM) and show from experiments that our models bring significant performance gain over the standard LSTMs on benchmark datasets for natural language inference, paraphrase detection, sentiment classification, and machine translation. We also conduct extensive qualitative analysis to understand the internal behavior of the suggested approach.", "task": "natural language inference, paraphrase detection, sentiment classification, and machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel method architecture, passing information from memory cells and hidden states forward during LSTM inference."}
{"documents": ["AFW", "FDDB", "PASCAL Face", "WFLW"], "year": 2018, "keyphrase_query": "face detection image", "abstract": "Recent anchor-based deep face detectors have achieved promising performance, but they are still struggling to detect hard faces, such as small, blurred and partially occluded faces. A reason is that they treat all images and faces equally, without putting more effort on hard ones; however, many training images only contain easy faces, which are less helpful to achieve better performance on hard images. In this paper, we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images. Our intuitions are (1) hard images are the images which contain at least one hard face, thus they facilitate training robust face detectors; (2) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking. We build an anchor-based deep face detector, which only output a single feature map with small anchors, to specifically learn small faces and train it by a novel hard image mining strategy. Extensive experiments have been conducted on WIDER FACE, FDDB, Pascal Faces, and AFW datasets to show the effectiveness of our method. Our method achieves APs of 95.7, 94.9 and 89.7 on easy, medium and hard WIDER FACE val dataset respectively, which surpass the previous state-of-the-arts, especially on the hard subset. Code and model are available at https://github.com/bairdzhang/smallhardface.", "task": "Face detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel method for learning hard faces – small faces on images."}
{"documents": ["FLIC", "LSP", "MPII Human Pose"], "year": 2016, "keyphrase_query": "pose estimation image", "abstract": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.", "task": "pose estimation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to build a model for articulated human pose estimation from labeled images."}
{"documents": ["Dialogue State Tracking Challenge"], "year": 2017, "keyphrase_query": "question answering text", "abstract": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.", "task": "question answering", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "We want to build an RNN model for question-answering whose answers can reason over multiple facts across a sequence of input sentences."}
{"documents": ["SNLI", "SST", "WikiQA"], "year": 2016, "keyphrase_query": "natural language inference, answer sentence selection, classification text", "abstract": "Neural networks with recurrent or recursive architecture have shown promising results on various natural language processing (NLP) tasks. The recurrent and recursive architectures have their own strength and limitations. The recurrent networks process input text sequentially and model the conditional transition between word tokens. In contrast, the recursive networks explicitly model the compositionality and the recursive structure of natural language. Current recursive architecture is based on syntactic tree, thus limiting its practical applicability in different NLP applications. In this paper, we introduce a class of tree structured model, Neural Tree Indexers (NTI) that provides a middle ground between the sequential RNNs and the syntactic tree-based recursive models. NTI constructs a full n-ary tree by processing the input text with its node function in a bottom-up fashion. Attention mechanism can then be applied to both structure and different forms of node function. We demonstrated the effectiveness and the flexibility of a binary-tree model of NTI, showing the model achieved the state-of-the-art performance on three different NLP tasks: natural language inference, answer sentence selection, and sentence classification.", "task": "natural language inference, answer sentence selection, sentence classification", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Sentence-level", "query": "We want to build a \"neural tree\" model, halfway between an RNN and a syntactic tree-based recursive model, that performs well on three different NLP tasks: natural language inference, answer sentence selection, and sentence classification."}
{"documents": ["MHP"], "year": 2018, "keyphrase_query": "multi-human parsing, person re-identification, dataset generation image object recognition, facial", "abstract": "Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database \"Multi-Human Parsing (MHP)\" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.", "task": "multi-human parsing, person re-identification, dataset generation", "domain": "object recognition, facial recognition", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We created a new large-scale dataset for a recently defined task, called \"multi-human parsing\" and introduce a nested adversarial network (NAN) based on GANs, to provide a strong baseline for performance on the new dataset."}
{"documents": ["ImageNet", "MNIST"], "year": 2015, "keyphrase_query": "image recognition", "abstract": "The fully-connected layers of deep convolutional neural networks typically contain over 90% of the network parameters. Reducing the number of parameters while preserving predictive performance is critically important for training big models in distributed systems and for deployment in embedded devices. In this paper, we introduce a novel Adaptive Fastfood transform to reparameterize the matrix-vector multiplication of fully connected layers. Reparameterizing a fully connected layer with d inputs and n outputs with the Adaptive Fastfood transform reduces the storage and computational costs costs from O(nd) to O(n) and O(n log d) respectively. Using the Adaptive Fastfood transform in convolutional networks results in what we call a deep fried convnet. These convnets are end-to-end trainable, and enable us to attain substantial reductions in the number of parameters without affecting prediction accuracy on the MNIST and ImageNet datasets.", "task": "image recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "Novel image architecture drastically reduces model size"}
{"documents": ["Amazon Review"], "year": 2018, "keyphrase_query": "sentiment analysis", "abstract": "Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline.", "task": "sentiment analysis", "domain": "", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "Model benchmarking baselines for evaluating models under domain shift"}
{"documents": ["COCO", "ImageNet", "PASCAL VOC", "PASCAL VOC 2007"], "year": 2020, "keyphrase_query": "object detection image", "abstract": "Weakly Supervised Object Detection (WSOD), using only image-level annotations to train object detectors, is of growing importance in object recognition. In this paper, we propose a novel deep network for WSOD. Unlike previous networks that transfer the object detection problem to an image classification problem using Multiple Instance Learning (MIL), our strategy generates proposal clusters to learn refined instance classifiers by an iterative process. The proposals in the same cluster are spatially adjacent and associated with the same object. This prevents the network from concentrating too much on parts of objects instead of whole objects. We first show that instances can be assigned object or background labels directly based on proposal clusters for instance classifier refinement, and then show that treating each cluster as a small new bag yields fewer ambiguities than the directly assigning label method. The iterative instance classifier refinement is implemented online using multiple streams in convolutional neural networks, where the first is an MIL network and the others are for instance classifier refinement supervised by the preceding one. Experiments are conducted on the PASCAL VOC, ImageNet detection, and MS-COCO benchmarks for WSOD. Results show that our method outperforms the previous state of the art significantly.", "task": "Object Detection", "domain": "", "modality": "Image", "language": "", "training_style": "Weakly supervised", "text_length": "", "query": "We propose a new network to detect objects using only image-level labels."}
{"documents": ["AFW", "FDDB"], "year": 2014, "keyphrase_query": "face detection image", "abstract": "Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones. While many subsequences have improved the work with more powerful learning algorithms, the feature representation used for face detection still can’t meet the demand for effectively and efficiently handling faces with large appearance variance in the wild. To solve this bottleneck, we borrow the concept of channel features to the face detection domain, which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form. We adopt a novel variant called aggregate channel features, make a full exploration of feature design, and discover a multiscale version of features with better performance. To deal with poses of faces in the wild, we propose a multi-view detection approach featuring score re-ranking and detection adjustment. Following the learning pipelines in ViolaJones framework, the multi-view face detector using aggregate channel features surpasses current state-of-the-art detectors on AFW and FDDB testsets, while runs at 42 FPS", "task": "face detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "Aggregates concept of channel features to encode and have a richer representation for face detection."}
{"documents": ["COCO", "PASCAL VOC", "PASCAL VOC 2007"], "year": 2016, "keyphrase_query": "object detection image", "abstract": "The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been – detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "An online hard example mining (OHEM) algorithm for training region-based ConvNet detectors."}
{"documents": ["2000 HUB5 English", "Switchboard-1 Corpus"], "year": 2016, "keyphrase_query": "conversational speech recognition audio", "abstract": "We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.2%, representing an improvement over previously reported results on this benchmark task.", "task": "conversational speech recognition", "domain": "", "modality": "audio", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We build a highly accurate speech recognition system."}
{"documents": ["ADE20K", "Cityscapes", "PASCAL VOC"], "year": 2017, "keyphrase_query": "scene parsing image", "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.", "task": "scene parsing", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I'm building a scene parsing model and want to test for unrestricted open vocabulary and diverse scenes."}
{"documents": ["MPII Human Pose"], "year": 2017, "keyphrase_query": "articulated tracking multiple people video", "abstract": "In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by offloading a substantial share of computation onto a feed-forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public MPII Human Pose benchmark and on a new MPII Video Pose dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes.", "task": "articulated tracking of multiple people", "domain": "", "modality": "Video", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I'm testing an approach for articulated tracking of multiple people in unconstrained videos."}
{"documents": ["WebQuestions"], "year": 2014, "keyphrase_query": "knowledge base question answering text", "abstract": "This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a recent benchmark of the literature.", "task": "knowledge base question answering", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "I need to train my model with using pairs of questions and structured representations of their answers (from a knowledge base)."}
{"documents": ["Criteo"], "year": 2018, "keyphrase_query": "factorization models", "abstract": "Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at https://github.com/Leavingseason/xDeepFM.", "task": "factorization models", "domain": "", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level."}
{"documents": ["CIFAR-10", "ImageNet", "MNIST"], "year": 2014, "keyphrase_query": "adversarial example generation image", "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.", "task": "adversarial example generation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "A new framework to generate adversarial examples for deep neural networks"}
{"documents": ["TrecQA"], "year": 2016, "keyphrase_query": "question answering text", "abstract": "As an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs) have recently been proposed for semantic matching of questions and answers. To achieve good results, however, these models have been combined with additional features such as word overlap or BM25 scores. Without this combination, these models perform significantly worse than methods based on linguistic feature engineering. In this paper, we propose an attention based neural matching model for ranking short answer text. We adopt value-shared weighting scheme instead of position-shared weighting scheme for combining different matching signals and incorporate question term importance learning using question attention network. Using the popular benchmark TREC QA data, we show that the relatively simple aNMM model can significantly outperform other neural network models that have been used for the question answering task, and is competitive with models that are combined with additional features. When aNMM is combined with additional features, it outperforms all baselines.", "task": "question answering", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a simple, attention-based neural matching model for question answering."}
{"documents": ["NYUv2", "SVHN", "USPS"], "year": 2017, "keyphrase_query": "cross-modality object classification", "abstract": "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.", "task": "cross-modality object classification", "domain": "", "modality": "", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We propose a generalized framework for adversarial domain adaptation."}
{"documents": ["Human3.6M"], "year": 2017, "keyphrase_query": "3d human pose estimation", "abstract": "In this work, we address the problem of 3D human pose estimation from a sequence of 2D human poses. Although the recent success of deep networks has led many state-of-the-art methods for 3D pose estimation to train deep networks end-to-end to predict from images directly, the top-performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space. They also showed that a low-dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy. However, estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter. Therefore, in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses. We designed a sequence-to-sequence network composed of layer-normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training. We found that the knowledge of temporal consistency improves the best reported result on Human3.6M dataset by approximately \\(12.2\\%\\) and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails.", "task": "3D human pose estimation", "domain": "", "modality": "", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We designed a sequence-to-sequence network based on LSTM for 3D human pose estimation from a sequence of 2D human poses."}
{"documents": ["CelebA", "mini-Imagenet"], "year": 2018, "keyphrase_query": "few-shot learning, meta-learning", "abstract": "Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems.", "task": "few-shot learning, meta-learning", "domain": "", "modality": "", "language": "", "training_style": "Few-shot", "text_length": "", "query": "We propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution"}
{"documents": ["ShapeNet", "Stanford Online Products"], "year": 2018, "keyphrase_query": "3d shape generation", "abstract": "We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.", "task": "3D shape generation", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image."}
{"documents": ["Cityscapes", "GTA5", "SYNTHIA"], "year": 2017, "keyphrase_query": "semantic segmentation image", "abstract": "During the last half decade, convolutional neural networks (CNNs) have triumphed over semantic segmentation, which is a core task of various emerging industrial applications such as autonomous driving and medical imaging. However, to train CNNs requires a huge amount of data, which is difficult to collect and laborious to annotate. Recent advances in computer graphics make it possible to train CNN models on photo-realistic synthetic data with computer-generated annotations. Despite this, the domain mismatch between the real images and the synthetic data significantly decreases the models’ performance. Hence we propose a curriculum-style learning approach to minimize the domain gap in semantic segmentation. The curriculum domain adaptation solves easy tasks first in order to infer some necessary properties about the target domain; in particular, the first task is to learn global label distributions over images and local distributions over landmark superpixels. These are easy to estimate because images of urban traffic scenes have strong idiosyncrasies (e.g., the size and spatial relations of buildings, streets, cars, etc.). We then train the segmentation network in such a way that the network predictions in the target domain follow those inferred properties. In experiments, our method significantly outperforms the baselines as well as the only known existing approach to the same problem.", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a curriculum-style learning approach to reduce the domain mismatch in semantic segmentation."}
{"documents": ["T-LESS"], "year": 2017, "keyphrase_query": "3d object detection, pose estimation image", "abstract": "We introduce a novel method for 3D object detection and pose estimation from color images only. We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a \"holistic\" approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes. This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this problem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional additional step that refines the predicted poses. We improve the state-of-the-art on the LINEMOD dataset from 73.7% to 89.3% of correctly registered RGB frames. We are also the first to report results on the Occlusion dataset using color images only. We obtain 54% of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, compared to the 67% of the state-of-the-art on the same sequences which uses both color and depth. The full approach is also scalable, as a single network can be trained for multiple objects simultaneously.", "task": "3D object detection, pose estimation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We introduce a novel method for 3D object detection and pose estimation from color images only."}
{"documents": ["Chinese Treebank", "Penn Treebank"], "year": 2016, "keyphrase_query": "dependency parsing text english, chinese", "abstract": "We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.", "task": "dependency parsing", "domain": "", "modality": "Text", "language": "English, Chinese", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a bidirectional-LSTMs based method for dependency parsing."}
{"documents": ["PASCAL-Part"], "year": 2017, "keyphrase_query": "object parsing image", "abstract": "Object parsing -- the task of decomposing an object into its semantic parts -- has traditionally been formulated as a category-level segmentation problem. Consequently, when there are multiple objects in an image, current methods cannot count the number of objects in the scene, nor can they determine which part belongs to which object. We address this problem by segmenting the parts of objects at an instance-level, such that each pixel in the image is assigned a part label, as well as the identity of the object it belongs to. Moreover, we show how this approach benefits us in obtaining segmentations at coarser granularities as well. Our proposed network is trained end-to-end given detections, and begins with a category-level segmentation module. Thereafter, a differentiable Conditional Random Field, defined over a variable number of instances for every input image, reasons about the identity of each part by associating it with a human detection. In contrast to other approaches, our method can handle the varying number of people in each image and our holistic network produces state-of-the-art results in instance-level part and human segmentation, together with competitive results in category-level part segmentation, all achieved by a single forward-pass through our neural network.", "task": "object parsing", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a method for object parsing given human object detections."}
{"documents": ["CIFAR-10", "CIFAR-100", "Caltech-101", "Caltech-256", "STL-10"], "year": 2016, "keyphrase_query": "unsupervised image classification", "abstract": "Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled ‘seed’ image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While features learned with our approach cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor.", "task": "unsupervised image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We want to improve fully-unsupervised feature learning for image classification."}
{"documents": ["COCO", "PASCAL VOC", "PASCAL VOC 2007"], "year": 2017, "keyphrase_query": "object detection image", "abstract": "The region-based Convolutional Neural Network (CNN) detectors such as Faster R-CNN or R-FCN have already shown promising results for object detection by combining the region proposal subnetwork and the classification subnetwork together. Although R-FCN has achieved higher detection speed while keeping the detection performance, the global structure information is ignored by the position-sensitive score maps. To fully explore the local and global properties, in this paper, we propose a novel fully convolutional network, named as CoupleNet, to couple the global structure with local parts for object detection. Specifically, the object proposals obtained by the Region Proposal Network (RPN) are fed into the the coupling module which consists of two branches. One branch adopts the position-sensitive RoI (PSRoI) pooling to capture the local part information of the object, while the other employs the RoI pooling to encode the global and context information. Next, we design different coupling strategies and normalization ways to make full use of the complementary advantages between the global and local branches. Extensive experiments demonstrate the effectiveness of our approach. We achieve state-of-the-art results on all three challenging datasets, i.e. a mAP of 82.7% on VOC07, 80.4% on VOC12, and 34.4% on COCO. Codes will be made publicly available.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I would like a dataset for supervised object detection"}
{"documents": ["Billion Word Benchmark", "WMT 2014"], "year": 2017, "keyphrase_query": "language modeling, machine translation text", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "task": "language modeling, machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "This work provides an improved technique for increasing model capacity with minor losses in computational efficiency, applied to language modeling and machine translation tasks"}
{"documents": ["IJB-A", "LFW"], "year": 2016, "keyphrase_query": "face verification image", "abstract": "Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding step, learned using triplet probability constraints to address the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs close to the state of the art methods in verification and identification metrics, while requiring much less training data and training/test time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to large pose variation. Furthermore, we demonstrate the robustness of deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets.", "task": "face verification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I would like a dataset for face verification with subject variations in age, pose, blur and clutter."}
{"documents": ["ADE20K", "Cityscapes", "Helen", "NYUv2"], "year": 2018, "keyphrase_query": "photo-realistic image synthesis high-resolution images", "abstract": "We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.", "task": "photo-realistic image synthesis", "domain": "high-resolution images", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We want to synthesize photo-realistic, high-resolution, 2048x1024 images."}
{"documents": ["Caltech-101", "ImageNet", "PASCAL VOC 2007"], "year": 2014, "keyphrase_query": "image classification", "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224×224) input image. This requirement is “artificial” and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We want to perform image classification directly on images of variable size, without requiring a fixed-sized image input."}
{"documents": ["MultiNLI", "Quora Question Pairs", "SNLI"], "year": 2018, "keyphrase_query": "natural language inference text", "abstract": "Attention mechanism has been proven effective on natural language processing. This paper proposes an attention boosted natural language inference model named aESIM by adding word attention and adaptive direction-oriented attention mechanisms to the traditional Bi-LSTM layer of natural language inference models, e.g. ESIM. This makes the inference model aESIM has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis. The empirical studies on the SNLI, MultiNLI and Quora benchmarks manifest that aESIM is superior to the original ESIM model.", "task": "natural language inference", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "I want to build a model that can identify what hypothesis can be inferred from a text premise."}
{"documents": ["CIFAR-10", "CIFAR-100", "ImageNet", "MNIST"], "year": 2015, "keyphrase_query": "image classification", "abstract": "Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to create a method that allows for robust learning of deep neural networks for image classification."}
{"documents": ["SQuAD"], "year": 2016, "keyphrase_query": "reading comprehension text", "abstract": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset.", "task": "reading comprehension", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "Paragraph-level", "query": "I want to build a neural reading comprehension model that can select spans of various lengths as the answer"}
{"documents": ["PASCAL VOC"], "year": 2014, "keyphrase_query": "semantic image segmentation", "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.", "task": "semantic image segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I propose a model for semantic image segmentation that combines neural networks with probabilistic graphical models"}
{"documents": ["SNLI"], "year": 2016, "keyphrase_query": "natural language inference text", "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "task": "natural language inference", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "A novel neural architecture for natural language inference which uses attention to decompose the problem into subproblems that can be solved separately."}
{"documents": ["NYUv2", "PASCAL Context", "PASCAL VOC 2011"], "year": 2017, "keyphrase_query": "semantic segmentation image", "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "A fully convolutional neural network finetuned from contemporary classification networks for semantic segmentation task"}
{"documents": ["Penn Treebank", "SNLI", "SST"], "year": 2016, "keyphrase_query": "machine reading text", "abstract": "Machine reading, the automatic understanding of text, remains a challenging task of great value for NLP applications. We propose a machine reader which processes text incrementally from left to right, while linking the current word to previous words stored in memory and implicitly discovering lexical dependencies facilitating understanding. The reader is equipped with a Long Short-Term Memory architecture, which differs from previous work in that it has a memory tape (instead of a memory cell) for adaptively storing past information without severe information compression. We also integrate our reader with a new attention mechanism in encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.", "task": "machine reading", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "A machine reader equipped with long short-term memory architecture and a new attention mechanism in encoder-decoder architecture"}
{"documents": ["COCO-QA", "Visual Question Answering"], "year": 2016, "keyphrase_query": "visual question answering image text", "abstract": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.", "task": "Visual Question Answering", "domain": "", "modality": "image and text", "language": "", "training_style": "", "text_length": "", "query": "A novel co-attention model for VQA that jointly reasons about image and question attention"}
{"documents": ["STL-10"], "year": 2014, "keyphrase_query": "unsupervised feature learning image", "abstract": "We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on STL-10 show that the method presents state-of-the-art performance and provides discriminative features that generalize well.", "task": "unsupervised feature learning", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We propose a new unsupervised feature learning algorithm that exploits a new way of optimizing for sparsity."}
{"documents": [], "year": 2015, "keyphrase_query": "target-dependent sentiment classification text social media, twitter", "abstract": "Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.", "task": "target-dependent sentiment classification", "domain": "social media, twitter", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose a target-aware sentiment classification models using LSTMs that does not rely on syntactic parsers or external sentiment lexicons. "}
{"documents": ["CIFAR-10", "CIFAR-100", "SVHN", "Tiny Images"], "year": 2016, "keyphrase_query": "image classification", "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.", "task": "Image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We propose a method for semi-supervised training of neural networks using the in-training snapshots of the network under different regularization conditions as the components of an ensemble. "}
{"documents": ["IWSLT2015", "WMT 2015"], "year": 2015, "keyphrase_query": "machine translation text", "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.", "task": "machine translation", "domain": "", "modality": "Text", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "I investigate the use of monolingual data for neural machine translation."}
{"documents": ["Citeseer", "Cora", "NELL", "Pubmed"], "year": 2016, "keyphrase_query": "semi-supervised graph learning text, graphs", "abstract": "We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.", "task": "semi-supervised graph learning", "domain": "", "modality": "Text, Graphs", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "We present a semi-supervised graph learning framework based on graph embeddings for several benchmark tasks in natural language processing."}
{"documents": ["CIFAR-10", "CUB-200-2011", "Cityscapes", "cats_vs_dogs"], "year": 2019, "keyphrase_query": "categorical generation, image-to-image translation, text-to-image synthesis images,", "abstract": "Most conditional generation tasks expect diverse outputs given a single conditional context. However, conditional generative adversarial networks (cGANs) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. Recent attempts to resolve the mode collapse issue for cGANs are usually task-specific and computationally expensive. In this work, we propose a simple yet effective regularization term to address the mode collapse issue for cGANs. The proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. This mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. We validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-to-image translation, and text-to-image synthesis with different baseline models. Both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality.", "task": "categorical generation, image-to-image translation, text-to-image synthesis", "domain": "", "modality": "images, text", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "Simple, effective and task independent regularization to address mode collapse issues for cGAN."}
{"documents": ["CIFAR-100", "ImageNet"], "year": 2015, "keyphrase_query": "image classification", "abstract": "In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train hierarchical deep CNNs for category hierarchy-aware image classification."}
{"documents": [], "year": 2015, "keyphrase_query": "lane detection, vehicle image autonomous driving", "abstract": "Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.", "task": "lane detection, vehicle detection", "domain": "autonomous driving", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "Creation of large scale highway dataset for developing real-time deep learning solutions for lane and highway detection"}
{"documents": ["CelebA", "MNIST", "SVHN"], "year": 2018, "keyphrase_query": "image representation learning", "abstract": "Combining Generative Adversarial Networks (GANs) with encoders that learn to encode data points has shown promising results in learning data representations in an unsupervised way. We propose a framework that combines an encoder and a generator to learn disentangled representations which encode meaningful information about the data distribution without the need for any labels. While current approaches focus mostly on the generative aspects of GANs, our framework can be used to perform inference on both real and generated data points. Experiments on several data sets show that the encoder learns interpretable, disentangled representations which encode descriptive properties and can be used to sample images that exhibit specific characteristics.", "task": "image representation learning", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "Combining generative adversarial networks and encoders for disentangling unsupervised image representations"}
{"documents": ["Caltech Pedestrian Dataset", "ImageNet", "KITTI", "Places"], "year": 2015, "keyphrase_query": "pedestrian detection image autonomous driving", "abstract": "In this paper we study the use of convolutional neural networks (convnets) for the task of pedestrian detection. Despite their recent diverse successes, convnets historically underperform compared to other pedestrian detectors. We deliberately omit explicitly modelling the problem into the network (e.g. parts or occlusion modelling) and show that we can reach competitive performance without bells and whistles. In a wide range of experiments we analyse small and big convnets, their architectural choices, parameters, and the influence of different training data, including pretraining on surrogate tasks. We present the best convnet detectors on the Caltech and KITTI dataset. On Caltech our convnets reach top performance both for the Caltech1x and Caltech10x training setup. Using additional data at training time our strongest convnet model is competitive even to detectors that use additional data (optical flow) at test time.", "task": "pedestrian detection", "domain": "autonomous driving", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to analyze deep convolutional neural networks for pedestrian detection without explicit modeling or optical flow."}
{"documents": ["LibriSpeech", "SNIPS"], "year": 2018, "keyphrase_query": "spoken language understanding audio", "abstract": "This paper presents the machine learning architecture of the Snips Voice Platform, a software solution to perform Spoken Language Understanding on microprocessors typical of IoT devices. The embedded inference is fast and accurate while enforcing privacy by design, as no personal user data is ever collected. Focusing on Automatic Speech Recognition and Natural Language Understanding, we detail our approach to training high-performance Machine Learning models that are small enough to run in real-time on small devices. Additionally, we describe a data generation procedure that provides sufficient, high-quality training data without compromising user privacy.", "task": "Spoken Language Understanding", "domain": "", "modality": "audio", "language": "", "training_style": "", "text_length": "", "query": "We propose a fast and secure machine learning architecture for Spoken Language Understanding on microprocessors typical of IoT devices."}
{"documents": ["COCO"], "year": 2018, "keyphrase_query": "object detection image", "abstract": "We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.", "task": "object detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "- I propose CorNet, a new model for object detection that identifies objects as paired keypoints and out performs all existing one-stage detectors.\n\n- I want to perform object detection by detecting objects as paired keypoints."}
{"documents": ["Arcade Learning Environment"], "year": 2016, "keyphrase_query": "exploration complex environments atari games", "abstract": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.", "task": "exploration in complex environments", "domain": "", "modality": "Atari games", "language": "", "training_style": "", "text_length": "reinforcement learning", "query": "I propose bootstrapped DQN for efficient exploration in complex environments."}
{"documents": ["Citeseer", "Cora", "NELL", "Pubmed"], "year": 2016, "keyphrase_query": "citation networks, knowledge graphs", "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.", "task": "citation networks, knowledge graphs", "domain": "", "modality": "graphs", "language": "", "training_style": "Semi-supervised", "text_length": "", "query": "I want a scalable approach for semi-supervised learning on graph-structured data."}
{"documents": ["ImageNet", "MNIST"], "year": 2015, "keyphrase_query": "image classification", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "i want to improve the efficiency of training of deep neural networks and reduce sensitivity to selecting learning rates and other hyperparameters."}
{"documents": ["Arcade Learning Environment"], "year": 2016, "keyphrase_query": "playing atari 2600 games", "abstract": "We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use sequential density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary sequential density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.", "task": "playing atari 2600 games", "domain": "", "modality": "", "language": "", "training_style": "reinforcement learning", "text_length": "", "query": "I want to improve exploration in non-tabular reinforcement learning tasks using intrinsic rewards."}
{"documents": ["CoNLL-2000", "CoNLL-2003", "GENIA", "Penn Treebank", "Ritter PoS"], "year": 2017, "keyphrase_query": "pos tagging text", "abstract": "Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering. However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (e.g., POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained. These improvements lead to improvements over the current state-of-the-art on several well-studied tasks.", "task": "POS tagging", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to use deep neural networks to improve performance on sequence tagging tasks with less training data using similar sequence tagging tasks which have a lot of training data."}
{"documents": ["LFW"], "year": 2015, "keyphrase_query": "face recognition image", "abstract": "Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.", "task": "face recognition", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I need to build a scalable system for face recognition and face verification. "}
{"documents": ["PPI"], "year": 2016, "keyphrase_query": "network representation learning nodes edges", "abstract": "Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.", "task": "network representation learning", "domain": "", "modality": "Network nodes and edges", "language": "", "training_style": "", "text_length": "", "query": "We propose node2vec, a novel method for learning continuous feature representations for nodes in networks"}
{"documents": ["KITTI", "PASCAL VOC 2007", "PASCAL3D+"], "year": 2017, "keyphrase_query": "object detection, subcategory classification image", "abstract": "In Convolutional Neural Network (CNN)-based object detection methods, region proposal becomes a bottleneck when objects exhibit significant scale variation, occlusion or truncation. In addition, these methods mainly focus on 2D object detection and cannot estimate detailed properties of objects. In this paper, we propose subcategory-aware CNNs for object detection. We introduce a novel region proposal network that uses subcategory information to guide the proposal generating process, and a new detection network for joint detection and subcategory classification. By using subcategories related to object pose, we achieve state of-the-art performance on both detection and pose estimation on commonly used benchmarks.", "task": "object detection, subcategory classification", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "I want to improve object detection by performing joint detection and subcategory classification."}
{"documents": ["Douban", "MovieLens", "Netflix Prize"], "year": 2017, "keyphrase_query": "recommender system graphs", "abstract": "Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationarity structures of user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines graph convolutional neural networks and recurrent neural networks to learn meaningful statistical graph-structured patterns and the non-linear diffusion process that generates the known ratings. This neural network system requires a constant number of parameters independent of the matrix size. We apply our method on both synthetic and real datasets, showing that it outperforms state-of-the-art techniques.", "task": "recommender system", "domain": "", "modality": "Graphs", "language": "", "training_style": "", "text_length": "", "query": "We want to improve recommendation systems by modelling pairwise relationships between users and items in the form of graphs"}
{"documents": ["COCO", "Cityscapes", "PASCAL VOC"], "year": 2017, "keyphrase_query": "semantic segmentation image", "abstract": "Semantic instance segmentation remains a challenging task. In this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a representation of the image that can easily be clustered into instances with a simple post-processing step. The loss function encourages the network to map each pixel to a point in feature space so that pixels belonging to the same instance lie close together while different instances are separated by a wide margin. Our approach of combining an off-the-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and distinct from recent efforts in instance segmentation. In contrast to previous works, our method does not rely on object proposals or recurrent mechanisms. A key contribution of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform on par with more complex methods. Moreover, we show that it does not suffer from some of the limitations of the popular detect-and-segment approaches. We achieve competitive performance on the Cityscapes and CVPPP leaf segmentation benchmarks.", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to develop a supervised model for semantic segmentation"}
{"documents": ["KITTI", "MPI Sintel", "Middlebury"], "year": 2016, "keyphrase_query": "image matching, optical flow estimation", "abstract": "We introduce a novel matching algorithm, called DeepMatching, to compute dense correspondences between images. DeepMatching relies on a hierarchical, multi-layer, correlational architecture designed for matching images and was inspired by deep convolutional approaches. The proposed matching algorithm can handle non-rigid deformations and repetitive textures and efficiently determines dense correspondences in the presence of significant changes between images. We evaluate the performance of DeepMatching, in comparison with state-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al 2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013) datasets. DeepMatching outperforms the state-of-the-art algorithms and shows excellent results in particular for repetitive textures.We also propose a method for estimating optical flow, called DeepFlow, by integrating DeepMatching in the large displacement optical flow (LDOF) approach of Brox and Malik (2011). Compared to existing matching algorithms, additional robustness to large displacements and complex motion is obtained thanks to our matching approach. DeepFlow obtains competitive performance on public benchmarks for optical flow estimation.", "task": "image matching, optical flow estimation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We are introducing a novel matching algorithm to compute dense correspondences between images, which we want to apply to optical flow estimation if possible."}
{"documents": ["CIFAR-10", "ImageNet"], "year": 2017, "keyphrase_query": "image classification", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", "task": "image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We trained a deep convolutional neural network for large-scale image classification."}
{"documents": ["BC2GM", "BC4CHEMD", "CoNLL-2000", "CoNLL-2003", "FCE", "GENIA", "JNLPBA", "Penn Treebank"], "year": 2016, "keyphrase_query": "sequence labeling text", "abstract": "Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.", "task": "sequence labeling", "domain": "", "modality": "Text", "language": "", "training_style": "", "text_length": "", "query": "I want to study the effect of character-level extensions to word embedding models for sequence labeling."}
{"documents": ["ICDAR 2015", "MSRA-TD500", "Total-Text"], "year": 2018, "keyphrase_query": "scene text detection text,image", "abstract": "In this paper, we introduce a novel end-end framework for multi-oriented scene text detection from an instance-aware semantic segmentation perspective. We present Fused Text Segmentation Networks, which combine multi-level features during the feature extracting as text instance may rely on finer feature expression compared to general objects. It detects and segments the text instance jointly and simultaneously, leveraging merits from both semantic segmentation task and region proposal based object detection task. Not involving any extra pipelines, our approach surpasses the current state of the art on multi-oriented scene text detection benchmarks: ICDAR2015 Incidental Scene Text and MSRA-TD500 reaching Hmean 84.1% and 82.0% respectively. Morever, we report a baseline on total-text containing curved text which suggests effectiveness of the proposed approach.", "task": "scene text detection", "domain": "", "modality": "Text,Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a novel method for scene text detection using instance-aware semantic segmentation."}
{"documents": ["CIFAR-10", "MNIST", "SVHN"], "year": 2015, "keyphrase_query": "image generation", "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.", "task": "image generation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We want to build a model that generates images that are indistinguishable from real images."}
{"documents": ["SQuAD"], "year": 2018, "keyphrase_query": "machine reading comprehension, question answerability text", "abstract": "Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U-Net. Different from the state-of-art pipeline models, U-Net can be learned in an end-to-end fashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0.", "task": "machine reading comprehension, question answerability", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Paragraph-level", "query": "Novel NLP architecture for determining question answerability"}
{"documents": ["English Web Treebank", "OntoNotes 5.0", "Penn Treebank"], "year": 2014, "keyphrase_query": "syntactic parsing text", "abstract": "Syntactic parsing is a fundamental problem in computational linguistics and Natural Language Processing. Traditional approaches to parsing are highly complex and problem specific. Recently, Sutskever et al. (2014) presented a domain-independent method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem. In this work, we show that precisely the same sequence-to-sequence method achieves results that are close to state-of-the-art on syntactic constituency parsing, whilst making almost no assumptions about the structure of the problem. To achieve these results we need to mitigate the lack of domain knowledge in the model by providing it with a large amount of automatically parsed data.", "task": "syntactic parsing", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "Larger training corpus allows seq2seq architecture to achieve state of the art without explicit assumptions about the problem domain"}
{"documents": ["Caltech Pedestrian Dataset", "CityPersons", "ETH", "INRIA Person"], "year": 2018, "keyphrase_query": "pedestrian detection image", "abstract": "Pedestrian detection in crowded scenes is a challenging problem since the pedestrians often gather together and occlude each other. In this paper, we propose a new occlusion-aware R-CNN (OR-CNN) to improve the detection accuracy in the crowd. Specifically, we design a new aggregation loss to enforce proposals to be close and locate compactly to the corresponding objects. Meanwhile, we use a new part occlusion-aware region of interest (PORoI) pooling unit to replace the RoI pooling layer in order to integrate the prior structure information of human body with visibility prediction into the network to handle occlusion. Our detector is trained in an end-to-end fashion, which achieves state-of-the-art results on three pedestrian detection datasets, i.e., CityPersons, ETH, and INRIA, and performs on-pair with the state-of-the-arts on Caltech.", "task": "pedestrian detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "Novel occlusion-aware RNN architecture achieves state of the art on pedestrian detection task"}
{"documents": [], "year": 2018, "keyphrase_query": "story-based question answering, dialog text", "abstract": "Memory Network based models have shown a remarkable progress on the task of relational reasoning. Recently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. Despite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory. We introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. We follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. As a result, our model is as simple as RN but the computational complexity is reduced to linear time. It achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and bAbI dialog dataset.", "task": "story-based question answering, dialog", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We want to build a fast and accurate system for question answering and dialog."}
{"documents": ["CelebA"], "year": 2016, "keyphrase_query": "identity-aware facial attribute transfer,expression accessory removal, age progression, gender face enhancement tasks, hallucination. image", "abstract": "This paper presents a Deep convolutional network model for Identity-Aware Transfer (DIAT) of facial attributes. Given the source input image and the reference attribute, DIAT aims to generate a facial image that owns the reference attribute as well as keeps the same or similar identity to the input image. In general, our model consists of a mask network and an attribute transform network which work in synergy to generate a photo-realistic facial image with the reference attribute. Considering that the reference attribute may be only related to some parts of the image, the mask network is introduced to avoid the incorrect editing on attribute irrelevant region. Then the estimated mask is adopted to combine the input and transformed image for producing the transfer result. For joint training of transform network and mask network, we incorporate the adversarial attribute loss, identity-aware adaptive perceptual loss, and VGG-FACE based identity loss. Furthermore, a denoising network is presented to serve for perceptual regularization to suppress the artifacts in transfer result, while an attribute ratio regularization is introduced to constrain the size of attribute relevant region. Our DIAT can provide a unified solution for several representative facial attribute transfer tasks, e.g., expression transfer, accessory removal, age progression, and gender transfer, and can be extended for other face enhancement tasks such as face hallucination. The experimental results validate the effectiveness of the proposed method. Even for the identity-related attribute (e.g., gender), our DIAT can obtain visually impressive results by changing the attribute while retaining most identity-aware features.", "task": "identity-aware facial attribute transfer,expression transfer, accessory removal, age progression, and gender transfer, face enhancement tasks, face hallucination.", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I propose a method for identity-aware transfer of facial attribute to improve expression transfer, accessory removal, age progression, and gender transfer, and other face enhancement tasks such as face hallucination. "}
{"documents": ["AG News"], "year": 2019, "keyphrase_query": "text classification embedded systems mobile devices", "abstract": "Embedding artificial intelligence on constrained platforms has become a trend since the growth of embedded systems and mobile devices, experimented in recent years. Although constrained platforms do not have enough processing capabilities to train a sophisticated deep learning model, like convolutional neural networks (CNN), they are already capable of performing inference locally by using a previously trained embedded model. This approach enables numerous advantages such as privacy, response latency, and no real time network dependence. Still, the use of a local CNN model on constrained platforms is restricted by its storage size. Most of the research in CNNs has focused on increasing network depth to improve accuracy. In the text classification area, deep models were proposed with excellent performance but relying on large architectures with thousands of parameters, and consequently, high storage size. We propose to modify the structure of the Very Deep Convolutional Neural Networks (VDCNN) model to reduce its storage size while keeping the model performance. In this paper, we evaluate the impact of Temporal Depthwise Separable Convolutions and Global Average Pooling in the network parameters, storage size, dedicated hardware dependence, and accuracy. The proposed squeezed model (SVDCNN) is between 10x and 20x smaller than the original version, depending on the network depth, maintaining a maximum disk size of 6MB. Regarding accuracy, the network experiences a loss between 0.4% and 1.3% in the accuracy performance while obtains lower latency over non-dedicated hardware and higher inference time ratio compared to the baseline model.", "task": "text classification", "domain": "embedded systems and mobile devices", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "A very deep convolutional neural network to reduce its storage size while keeping the model performance for text classification task."}
{"documents": ["ADE20K", "Cityscapes", "ImageNet", "PASCAL Context", "PASCAL VOC"], "year": 2016, "keyphrase_query": "semantic image segmentation", "abstract": "Abstract The community has been going deeper and deeper in designing one cutting edge network after another, yet some works are there suggesting that we may have gone too far in this dimension. Some researchers unravelled a residual network into an exponentially wider one, and assorted the success of residual networks to fusing a large amount of relatively shallow models. Since some of their early claims are still not settled, we in this paper dig more on this topic, i.e., the unravelled view of residual networks. Based on that, we try to find a good compromise between the depth and width. Afterwards, we walk through a typical pipeline of developing a deep-learning-based algorithm. We start from a group of relatively shallow networks, which perform as well or even better than the current (much deeper) state-of-the-art models on the ImageNet classification dataset. Then, we initialize fully convolutional networks (FCNs) using our pre-trained models, and tune them for semantic image segmentation. Results show that the proposed networks, as pre-trained features, can boost existing methods a lot. Even without exhausting the sophistical techniques to improve the classic FCN model, we achieve comparable results with the best performers on four widely-used datasets, i.e., Cityscapes, PASCAL VOC, ADE20k and PASCAL-Context. The code and pre-trained models are released for public access 1 .", "task": "semantic image segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I analyze the effect of neural network depth on semantic image segmentation datasets."}
{"documents": ["Indian Pines", "Pavia University"], "year": 2018, "keyphrase_query": "hyperspectral sensing image classification", "abstract": "Convolutional neural networks (CNNs) attained a good performance in hyperspectral sensing image (HSI) classification, but CNNs consider spectra as orderless vectors. Therefore, considering the spectra as sequences, recurrent neural networks (RNNs) have been applied in HSI classification, for RNNs is skilled at dealing with sequential data. However, for a long-sequence task, RNNs is difficult for training and not as effective as we expected. Besides, spatial contextual features are not considered in RNNs. In this study, we propose a Shorten Spatial-spectral RNN with Parallel-GRU (St-SS-pGRU) for HSI classification. A shorten RNN is more efficient and easier for training than band-by-band RNN. By combining converlusion layer, the St-SSpGRU model considers not only spectral but also spatial feature, which results in a better performance. An architecture named parallel-GRU is also proposed and applied in St-SS-pGRU. With this architecture, the model gets a better performance and is more robust.", "task": "hyperspectral sensing image classification", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want to train a convolutional neural network to classify hyperspectral sensing images."}
{"documents": ["20NewsGroups", "MR", "SST", "SUBJ", "TREC-10"], "year": 2016, "keyphrase_query": "text classification sentiment analysis, question classification, subjectivity newsgroup", "abstract": "Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variablelength text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.", "task": "text classification", "domain": "sentiment analysis, question classification, subjectivity classification and newsgroup classification", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I want classification datasets that can help me test a text representation computation method."}
{"documents": ["COCO", "SNLI"], "year": 2015, "keyphrase_query": "hypernym prediction, image-caption retrieval images, text", "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.", "task": "hypernym prediction, image-caption retrieval", "domain": "", "modality": "Images, text", "language": "", "training_style": "", "text_length": "", "query": "I propose a model for learning a visual-semantic hierarchy over images and text"}
{"documents": ["ICDAR 2015", "SCUT-CTW1500"], "year": 2018, "keyphrase_query": "text detection image", "abstract": "Traditional text detection methods mostly focus on quadrangle text. In this study we propose a novel method named sliding line point regression (SLPR) in order to detect arbitrary-shape text in natural scene. SLPR regresses multiple points on the edge of text line and then utilizes these points to sketch the outlines of the text. The proposed SLPR can be adapted to many object detection architectures such as Faster R-CNN and R-FCN. Specifically, we first generate the smallest rectangular box including the text with region proposal network (RPN), then isometrically regress the points on the edge of text by using the vertically and horizontally sliding lines. To make full use of information and reduce redundancy, we calculate x-coordinate or y-coordinate of target point by the rectangular box position, and just regress the remaining y-coordinate or x-coordinate. Accordingly we can not only reduce the parameters of system, but also restrain the points which will generate more regular polygon. Our approach achieved competitive results on traditional ICDAR2015 Incidental Scene Text benchmark and curve text detection dataset CTW1500.", "task": "text detection", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "I aim to propose a novel method for arbitrary-shape text detection in natural scene."}
{"documents": ["CIFAR-10", "ImageNet", "LSUN", "SVHN"], "year": 2015, "keyphrase_query": "image representation learning", "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.", "task": "Image representation learning", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We introduce a novel unsupervised deep convolutional generative adversarial network model for representation learning of the image."}
{"documents": ["DISFA"], "year": 2016, "keyphrase_query": "facial expression recognition image", "abstract": "Inspired by recent successes of deep learning in computer vision, we propose a novel application of deep convolutional neural networks to facial expression recognition, in particular smile recognition. A smile recognition test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches based on hand-crafted features with accuracies ranging from 65.55% to 79.67%. The novelty of this approach includes a comprehensive model selection of the architecture parameters, allowing to find an appropriate architecture for each expression such as smile. This is feasible because all experiments were run on a Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations on a CPU.", "task": "facial expression recognition", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a novel application of deep convolutional neural networks to facial expression recognition."}
{"documents": ["CASIA-HWDB", "CIFAR-10", "GTSRB", "MNIST", "NIST SD 19", "smallNORB"], "year": 2012, "keyphrase_query": "handwriting recognition, traffic sign image", "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.", "task": "handwriting recognition, traffic sign recognition", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We propose convolutional wide and deep artificial neural networks for handwriting and traffic sign recognition."}
{"documents": ["CIFAR-10", "ImageNet", "Omniglot"], "year": 2016, "keyphrase_query": "image modeling", "abstract": "We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.", "task": "image modeling", "domain": "", "modality": "Image", "language": "", "training_style": "Unsupervised", "text_length": "", "query": "We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling."}
{"documents": ["CamVid", "NYUv2", "SUN RGB-D"], "year": 2017, "keyphrase_query": "semantic segmentation image", "abstract": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet.", "task": "semantic segmentation", "domain": "", "modality": "Image", "language": "", "training_style": "Supervised training/finetuning", "text_length": "", "query": "We present a novel and practical deep fully convolutional neural network architecture for image semantic segmentation."}
{"documents": ["CIFAR-10", "CelebA", "ImageNet"], "year": 2018, "keyphrase_query": "image generation", "abstract": "We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on $160 \\times 160$ CelebA and $64 \\times 64$ unconditional ImageNet.", "task": "image generation", "domain": "", "modality": "Image", "language": "", "training_style": "", "text_length": "", "query": "We propose a principled method for gradient-based regularization of the critic of GAN-like image generation models."}
{"documents": ["COCO", "Flickr-8k", "MPQA Opinion Corpus", "MR", "MRPC", "MultiNLI", "SICK", "SNLI", "SST", "STS 2014", "SUBJ", "TREC-10"], "year": 2017, "keyphrase_query": "natural language inference, nlp text", "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.", "task": "natural language inference, NLP", "domain": "", "modality": "Text", "language": "", "training_style": "Supervised training/finetuning", "text_length": "Sentence-level", "query": "We want to show that sentence representations learned from NLI datasets can be transferred to other NLP tasks."}
{"documents": ["IWSLT2015", "WMT 2014"], "year": 2017, "keyphrase_query": "sequence modelling", "abstract": "There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT 2014 German-English translation as well as Gigaword abstractive summarization.", "task": "sequence modelling", "domain": "", "modality": "", "language": "", "training_style": "", "text_length": "", "query": "We want to apply classical objective functions to sequence-to-sequence tasks."}
