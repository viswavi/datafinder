{"id": "MuMu", "contents": "MuMu is a new dataset of more than 31k albums classified into 250 genre classes.\r\n\r\nSource: [Multi-label Music Genre Classification from Audio, Text, and Images Using Deep Features](https://arxiv.org/pdf/1707.04916)", "variants": ["MuMu"], "title": "Multi-label Music Genre Classification from Audio, Text, and Images Using Deep Features"}
{"id": "PTL", "contents": "A dataset of pedestrian traffic lights containing over 5000 photos taken at hundreds of intersections in Shanghai.\r\n\r\nSource: [LYTNet: A Convolutional Neural Network for Real-Time Pedestrian Traffic Lights and Zebra Crossing Recognition for the Visually Impaired](/paper/lytnet-a-convolutional-neural-network-for)", "variants": ["PTL"], "title": "LYTNet: A Convolutional Neural Network for Real-Time Pedestrian Traffic Lights and Zebra Crossing Recognition for the Visually Impaired"}
{"id": "DIV2K", "contents": "**DIV2K** is a popular single-image super-resolution dataset which contains 1,000 images with different scenes and is splitted to 800 for training, 100 for validation and 100 for testing. It was collected for NTIRE2017 and NTIRE2018 Super-Resolution Challenges in order to encourage research on image super-resolution with more realistic degradation. This dataset contains low resolution images with different types of degradations. Apart from the standard bicubic downsampling, several types of degradations are considered in synthesizing low resolution images for different tracks of the challenges. Track 2 of NTIRE 2017 contains low resolution images with unknown x4 downscaling. Track 2 and track 4 of NTIRE 2018 correspond to realistic mild ×4 and realistic wild ×4 adverse conditions, respectively. Low-resolution images under realistic mild x4 setting suffer from motion blur, Poisson noise and pixel shifting. Degradations under realistic wild x4 setting are further extended to be of different levels from image to image.\r\n\r\nSource: [Unsupervised Image Super-Resolution with an Indirect Supervised Path](https://arxiv.org/abs/1910.02593)", "variants": ["DIV2K", "DIV2K val - 16x upscaling", "DIV2K val - 2x upscaling", "DIV2K val - 4x upscaling"], "title": "NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study"}
{"id": "ATIS", "contents": "The **ATIS** (**Airline Travel Information Systems**) is a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories. The original split contains 4478, 500 and 893 intent-labeled reference utterances in train, development and test set respectively.\r\n\r\nSource: [Spoken Language Intent Detection using Confusion2Vec](https://arxiv.org/abs/1904.03576)", "variants": ["ATIS"], "title": "The ATIS Spoken Language Systems Pilot Corpus"}
{"id": "JFT-300M", "contents": "**JFT-300M** is an internal Google dataset used for training image classification models. Images are labeled using an algorithm that uses complex mixture of raw web signals, connections between web-pages and user feedback. This results in over one billion labels for the 300M images (a single image can have multiple labels). Of the billion image labels, approximately 375M are selected via an algorithm that aims to maximize label precision of selected images.", "variants": ["JFT-300M"], "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}
{"id": "AISHELL-2", "contents": "AISHELL-2 contains 1000 hours of clean read-speech data from iOS is free for academic usage. \r\n\r\nSource: [AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale](/paper/aishell-2-transforming-mandarin-asr-research)\r\nImage Source: [https://arxiv.org/pdf/1808.10583v2.pdf](https://arxiv.org/pdf/1808.10583v2.pdf)", "variants": ["AISHELL-2"], "title": "AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale"}
{"id": "LKS", "contents": "**LKS** is a dataset of 684 Liver-Kidney-Stomach immunofluorescence whole slide images (WSIs) used in the investigation of autoimmune liver disease.\n\nSource: [https://arxiv.org/abs/2003.05080](https://arxiv.org/abs/2003.05080)\nImage Source: [https://github.com/cradleai/LKS-Dataset](https://github.com/cradleai/LKS-Dataset)", "variants": ["LKS"], "title": "SOS: Selective Objective Switch for Rapid Immunofluorescence Whole Slide Image Classification"}
{"id": "SALSA", "contents": "A novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis.\r\n\r\nSource: [SALSA: A Novel Dataset for Multimodal Group Behavior Analysis](/paper/salsa-a-novel-dataset-for-multimodal-group)", "variants": ["SALSA"], "title": "SALSA: A Novel Dataset for Multimodal Group Behavior Analysis"}
{"id": "Fashion-MNIST", "contents": "**Fashion-MNIST** is a dataset comprising of 28×28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST.\r\n\r\nSource: [Generative Probabilistic Novelty Detection with Adversarial Autoencoders](https://arxiv.org/abs/1807.02588)\r\nImage Source: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)", "variants": ["Fashion-MNIST", "Rotated Fashion-MNIST"], "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms"}
{"id": "SEND", "contents": "SEND (Stanford Emotional Narratives Dataset) is a set of rich, multimodal videos of self-paced, unscripted emotional narratives, annotated for emotional valence over time. The complex narratives and naturalistic expressions in this dataset provide a challenging test for contemporary time-series emotion recognition models.\r\n\r\nSource: [Modeling emotion in complex stories: the Stanford Emotional Narratives Dataset](/paper/modeling-emotion-in-complex-stories-the)", "variants": ["SEND"], "title": "Modeling emotion in complex stories: the Stanford Emotional Narratives Dataset"}
{"id": "AIDA CoNLL-YAGO", "contents": "**AIDA CoNLL-YAGO** contains assignments of entities to the mentions of named entities annotated for the original [CoNLL 2003 entity recognition task](https://www.clips.uantwerpen.be/conll2003/ner/). The entities are identified by YAGO2 entity name, by Wikipedia URL, or by Freebase mid. \r\n\r\nSource: [AIDA CoNLL-YAGO](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads)", "variants": ["AIDA-CoNLL", "AIDA CoNLL-YAGO"], "title": "Robust Disambiguation of Named Entities in Text"}
{"id": "CSS10", "contents": "A collection of single speaker speech datasets for ten languages. It is composed of short audio clips from LibriVox audiobooks and their aligned texts.\r\n\r\nSource: [CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages](/paper/css10-a-collection-of-single-speaker-speech)", "variants": ["CSS10"], "title": "CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages"}
{"id": "MovieFIB", "contents": "A quantitative benchmark for developing and understanding video of fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired. \r\n\r\nSource: [A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering](/paper/a-dataset-and-exploration-of-models-for)", "variants": ["MovieFIB"], "title": "A Dataset and Exploration of Models for Understanding Video Data through Fill-in-the-Blank Question-Answering"}
{"id": "YouCook2", "contents": "**YouCook2** is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.\r\n\r\nSource: [http://youcook2.eecs.umich.edu/](http://youcook2.eecs.umich.edu/)\r\nImage Source: [https://competitions.codalab.org/competitions/20594](https://competitions.codalab.org/competitions/20594)", "variants": ["YouCook2"], "title": "Towards Automatic Learning of Procedures from Web Instructional Videos"}
{"id": "QuaRTz", "contents": "QuaRTz is a crowdsourced dataset of 3864 multiple-choice questions about open domain qualitative relationships. Each question is paired with one of 405 different background sentences (sometimes short paragraphs).\r\n\r\nThe QuaRTz dataset V1 contains 3864 questions about open domain qualitative relationships. Each question is paired with one of 405 different background sentences (sometimes short paragraphs).\r\n\r\nThe dataset is split into train (2696), dev (384) and test (784). A background sentence will only appear in a single split.\r\n\r\nEach line in a dataset file is a question specified as a json object, e.g., (with extra whitespace for readability).\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/quartz)", "variants": ["QuaRTz"], "title": "QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions"}
{"id": "Habitat Platform", "contents": "A platform for research in embodied artificial intelligence (AI).\r\n\r\nSource: [Habitat: A Platform for Embodied AI Research](/paper/habitat-a-platform-for-embodied-ai-research)", "variants": ["Habitat Platform"], "title": "Habitat: A Platform for Embodied AI Research"}
{"id": "Watch-n-Patch", "contents": "The **Watch-n-Patch** dataset was created with the focus on modeling human activities, comprising multiple actions in a completely unsupervised setting. It is collected with Microsoft Kinect One sensor for a total length of about 230 minutes, divided in 458 videos. 7 subjects perform human daily activities in 8 offices and 5 kitchens with complex backgrounds. Moreover, skeleton data are provided as ground truth annotations.\r\n\r\nSource: [Head Detection with Depth Images in the Wild](https://arxiv.org/abs/1707.06786)\r\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2015/papers/Wu_Watch-n-Patch_Unsupervised_Understanding_2015_CVPR_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2015/papers/Wu_Watch-n-Patch_Unsupervised_Understanding_2015_CVPR_paper.pdf)", "variants": ["Watch-n-Patch"], "title": "Watch-n-patch: Unsupervised understanding of actions and relations"}
{"id": "TArC", "contents": "A morpho-syntactically annotated Tunisian Arabish Corpus (TArC).\r\n\r\nSource: [TArC: Incrementally and Semi-Automatically Collecting a Tunisian Arabish Corpus](/paper/tarc-incrementally-and-semi-automatically)", "variants": ["TArC"], "title": "TArC: Incrementally and Semi-Automatically Collecting a Tunisian Arabish Corpus"}
{"id": "NewsQA", "contents": "The **NewsQA** dataset is a crowd-sourced machine reading comprehension dataset of 120,000 question-answer pairs.\r\n\r\n* Documents are CNN news articles.\r\n* Questions are written by human users in natural language.\r\n* Answers may be multiword passages of the source text.\r\n* Questions may be unanswerable.\r\n* NewsQA is collected using a 3-stage, siloed process.\r\n* Questioners see only an article’s headline and highlights.\r\n* Answerers see the question and the full article, then select an answer passage.\r\n* Validators see the article, the question, and a set of answers that they rank.\r\n* NewsQA is more natural and more challenging than previous datasets.\r\n\r\nSource: [https://www.microsoft.com/en-us/research/project/newsqa-dataset/](https://www.microsoft.com/en-us/research/project/newsqa-dataset/)\r\nImage Source: [Trischler et al](https://arxiv.org/pdf/1611.09830v3.pdf)", "variants": ["NewsQA"], "title": "NewsQA: A Machine Comprehension Dataset"}
{"id": "CFQ", "contents": "A large and realistic natural language question answering dataset.\r\n\r\nSource: [Measuring Compositional Generalization: A Comprehensive Method on Realistic Data](/paper/measuring-compositional-generalization-a-1)", "variants": ["CFQ"], "title": "Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"}
{"id": "Fine-Grained R2R", "contents": "This dataset enriches the benchmark Room-to-Room (R2R) dataset by dividing the instructions into sub-instructions and pairing each of those with their corresponding viewpoints in the path. The overall instruction and trajectory of each sample remains the same.\n\nSource: [https://github.com/YicongHong/Fine-Grained-R2R](https://github.com/YicongHong/Fine-Grained-R2R)", "variants": ["Fine-Grained R2R"], "title": "Sub-Instruction Aware Vision-and-Language Navigation"}
{"id": "CO-SKEL dataset", "contents": "A benchmark dataset for the co-skeletonization task.\r\n\r\nSource: [Object Co-Skeletonization With Co-Segmentation](/paper/object-co-skeletonization-with-co)", "variants": ["CO-SKEL dataset"], "title": "Object Co-skeletonization with Co-segmentation"}
{"id": "3D Hand Pose", "contents": "**3D Hand Pose** is a multi-view hand pose dataset consisting of color images of hands and different kind of annotations for each: the bounding box and the 2D and 3D location on the joints in the hand. \r\n\r\nSource: [Large-scale Multiview 3D Hand Pose Dataset](/paper/large-scale-multiview-3d-hand-pose-dataset)", "variants": ["3D Hand Pose"], "title": "Large-scale Multiview 3D Hand Pose Dataset"}
{"id": "WikiMatrix", "contents": "**WikiMatrix** is a dataset of parallel sentences in the textual content of Wikipedia for all possible language pairs. The mined data consists of:\r\n\r\n- 85 different languages, 1620 language pairs\r\n- 134M parallel sentences, out of which 34M are aligned with English\r\n\r\nSource: [WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia](/paper/wikimatrix-mining-135m-parallel-sentences-in)", "variants": ["WikiMatrix"], "title": "WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia"}
{"id": "ViGGO", "contents": "The ViGGO corpus is a set of 6,900 meaning representation to natural language utterance pairs in the video game domain. The meaning representations are of 9 different dialogue acts.\r\n\r\nSource: [VIGGO](https://nlds.soe.ucsc.edu/viggo)", "variants": ["ViGGO"], "title": "ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation"}
{"id": "Topical-Chat", "contents": "A knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don’t have explicitly defined roles.\r\n\r\nSource: [Topical-Chat](https://github.com/alexa/alexa-prize-topical-chat-dataset)", "variants": ["Topical-Chat"], "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations"}
{"id": "EPIC-KITCHENS-55", "contents": "The EPIC-KITCHENS-55 dataset comprises a set of 432 egocentric videos recorded by 32 participants in their kitchens at 60fps with a head mounted camera. There is no guiding script for the participants who freely perform activities in kitchens related to cooking, food preparation or washing up among others. Each video is split into short action segments (mean duration is 3.7s) with specific start and end times and a verb and noun annotation describing the action (e.g. ‘open fridge‘). The verb classes are 125 and the noun classes 331. The dataset is divided into one train and two test splits.\r\n\r\nSource: [Egocentric Hand Track and Object-based Human Action Recognition](https://arxiv.org/abs/1905.00742)\r\nImage Source: [https://epic-kitchens.github.io/2020-100](https://epic-kitchens.github.io/2020-100)", "variants": ["EPIC-KITCHENS-55"], "title": "Scaling Egocentric Vision: The EPIC-KITCHENS Dataset"}
{"id": "FUNSD", "contents": "Form Understanding in Noisy Scanned Documents (FUNSD) comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking.\r\n\r\nSource: [FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents](/paper/190513538)", "variants": ["FUNSD"], "title": "FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents"}
{"id": "Japanese Word Similarity", "contents": "This dataset contains information about Japanese word similarity including rare words. The dataset is constructed following the Stanford Rare Word Similarity Dataset. 10 annotators annotated word pairs with 11 levels of similarity.\n\nSource: [https://github.com/tmu-nlp/JapaneseWordSimilarityDataset](https://github.com/tmu-nlp/JapaneseWordSimilarityDataset)", "variants": ["Japanese Word Similarity"], "title": "Construction of a Japanese Word Similarity Dataset"}
{"id": "DVS128 Gesture Dataset", "contents": "Comprises 11 hand gesture categories from 29 subjects under 3 illumination conditions.\r\n\r\nSource: [A Low Power, Fully Event-Based Gesture Recognition System](/paper/a-low-power-fully-event-based-gesture)", "variants": ["DVS128 Gesture Dataset"], "title": "A Low Power, Fully Event-Based Gesture Recognition System"}
{"id": "IMDb Movie Reviews", "contents": "The **IMDb Movie Reviews** dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.\r\n\r\nSource: [http://nlpprogress.com/english/sentiment_analysis.html](http://nlpprogress.com/english/sentiment_analysis.html)\r\nImage Source: [Maas et al](https://www.aclweb.org/anthology/P11-1015/)", "variants": ["IMDb", "IMDb Movie Reviews", "User and product information"], "title": "Learning Word Vectors for Sentiment Analysis"}
{"id": "Partial-REID", "contents": "Partial REID is a specially designed partial person reidentification dataset that includes 600 images from 60 people, with 5 full-body images and 5 occluded images per person. These images were collected on a university campus by 6 cameras from different viewpoints, backgrounds and different types of occlusion. The examples of partial persons in the Partial REID dataset are shown in the Figure.\r\n\r\nSource: [Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification](https://arxiv.org/abs/1904.04975)\r\nImage Source: [https://arxiv.org/abs/1810.07399](https://arxiv.org/abs/1810.07399)", "variants": ["Partial-REID"], "title": "Partial Person Re-Identification"}
{"id": "NTU RGB+D", "contents": "**NTU RGB+D** is a large-scale dataset for RGB-D human action recognition. It involves 56,880 samples of 60 action classes collected from 40 subjects. The actions can be generally divided into three categories: 40 daily actions (e.g., drinking, eating, reading), nine health-related actions (e.g., sneezing, staggering, falling down), and 11 mutual actions (e.g., punching, kicking, hugging). These actions take place under 17 different scene conditions corresponding to 17 video sequences (i.e., S001–S017). The actions were captured using three cameras with different horizontal imaging viewpoints, namely, −45∘,0∘, and +45∘. Multi-modality information is provided for action characterization, including depth maps, 3D skeleton joint position, RGB frames, and infrared sequences. The performance evaluation is performed by a cross-subject test that split the 40 subjects into training and test groups, and by a cross-view test that employed one camera (+45∘) for testing, and the other two cameras for training.\r\n\r\nSource: [Action Recognition for Depth Video using Multi-view Dynamic Images](https://arxiv.org/abs/1806.11269)", "variants": ["NTU RGB+D", "NTU RGB+D 120", "Filtered NTU RGB+D"], "title": "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis"}
{"id": "MELD", "contents": "**Multimodal EmotionLines Dataset** (**MELD**) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been labeled by any of these seven emotions -- Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. MELD also has sentiment (positive, negative and neutral) annotation for each utterance.\r\n\r\nSource: [https://affective-meld.github.io/](https://affective-meld.github.io/)\r\nImage Source: [https://affective-meld.github.io/](https://affective-meld.github.io/)", "variants": ["MELD"], "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations"}
{"id": "Hanabi Learning Environment", "contents": "A new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information.\r\n\r\nSource: [The Hanabi Challenge: A New Frontier for AI Research](/paper/the-hanabi-challenge-a-new-frontier-for-ai)", "variants": ["Hanabi Learning Environment"], "title": "The Hanabi Challenge: A New Frontier for AI Research"}
{"id": "LSHTC", "contents": "LSHTC is a dataset for large-scale text classification. The data used in the LSHTC challenges originates from two popular sources: the DBpedia and the ODP (Open Directory Project) directory, also known as DMOZ. DBpedia instances were selected from the english, non-regional Extended Abstracts provided by the DBpedia site. The DMOZ instances consist\r\nof either Content vectors, Description vectors or both. A Content vectors is obtained by directly indexing the web page using standard indexing chain (preprocessing, stemming/lemmatization, stop-word removal). \r\n\r\nSource: [LSHTC: A Benchmark for Large-Scale Text Classification](/paper/lshtc-a-benchmark-for-large-scale-text)", "variants": ["LSHTC"], "title": "LSHTC: A Benchmark for Large-Scale Text Classification"}
{"id": "DeepWriting", "contents": "A new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation.\r\n\r\nSource: [DeepWriting: Making Digital Ink Editable via Deep Generative Modeling](/paper/deepwriting-making-digital-ink-editable-via)", "variants": ["DeepWriting"], "title": "DeepWriting: Making Digital Ink Editable via Deep Generative Modeling"}
{"id": "GitHub Typo Corpus", "contents": "Are you the kind of person who makes a lot of typos when writing code? Or are you the one who fixes them by making \"fix typo\" commits? Either way, thank you—you contributed to the state-of-the-art in the NLP field.\r\n\r\nGitHub Typo Corpus is a large-scale dataset of misspellings and grammatical errors along with their corrections harvested from GitHub. It contains more than 350k edits and 65M characters in more than 15 languages, making it the largest dataset of misspellings to date.\r\n\r\nSource: [GitHub](https://github.com/mhagiwara/github-typo-corpus)", "variants": ["GitHub Typo Corpus"], "title": "GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors"}
{"id": "iLur News Texts", "contents": "iLur News Texts is a dataset of over 12000 news articles from iLur.am, categorized into 7 classes: sport, politics, weather, economy, accidents, art, society. The articles are split into train (2242k tokens) and test sets (425k tokens).\r\n\r\nSource: [iLur News Texts](https://github.com/ispras-texterra/word-embeddings-eval-hy)", "variants": ["iLur News Texts"], "title": "Word Embeddings for the Armenian Language: Intrinsic and Extrinsic Evaluation"}
{"id": "VeRi-Wild", "contents": "Veri-Wild is the largest vehicle re-identification dataset (as of CVPR 2019). The dataset is captured from a large CCTV surveillance system consisting of 174 cameras across one month (30× 24h) under unconstrained scenarios. This dataset comprises 416,314 vehicle images of 40,671 identities. Evaluation on this dataset is split across three subsets: small, medium and large; comprising 3000, 5000 and 10,000 identities respectively (in probe and gallery sets).\r\n\r\nSource: [Vehicle Re-Identification: an Efficient Baseline Using Triplet Embedding](https://arxiv.org/abs/1901.01015)\r\nImage Source: [https://github.com/PKU-IMRE/VERI-Wild](https://github.com/PKU-IMRE/VERI-Wild)", "variants": ["VeRi-Wild"], "title": "VERI-Wild: A Large Dataset and a New Method for Vehicle Re-Identification in the Wild"}
{"id": "AudioSet", "contents": "Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.\r\n\r\nSource: [Curriculum Audiovisual Learning](https://arxiv.org/abs/2001.09414)", "variants": ["AudioSet"], "title": "Audio Set: An ontology and human-labeled dataset for audio events"}
{"id": "JRDB", "contents": "A novel egocentric dataset collected from social mobile manipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal sensor data including stereo cylindrical 360 degrees RGB video at 15 fps, 3D point clouds from two Velodyne 16 Lidars, line 3D point clouds from two Sick Lidars, audio signal, RGB-D video at 30 fps, 360 degrees spherical image from a fisheye camera and encoder values from the robot's wheels.\r\n\r\nSource: [JRDB: A Dataset and Benchmark of Egocentric Visual Perception for Navigation in Human Environments](/paper/jrdb-a-dataset-and-benchmark-for-visual)", "variants": ["JRDB"], "title": "JRDB: A Dataset and Benchmark for Visual Perception for Navigation in Human Environments"}
{"id": "SEN12MS", "contents": "A dataset consisting of 180,662 triplets of dual-pol synthetic aperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches, and MODIS land cover maps.\r\n\r\nSource: [SEN12MS -- A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion](/paper/sen12ms-a-curated-dataset-of-georeferenced)", "variants": ["SEN12MS"], "title": "SEN12MS -- A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion"}
{"id": "Diabetic Foot Ulcers Classification Datasets", "contents": "Contains Diabetic Foot Ulcers (DFU) from different patients.\r\n\r\nSource: [DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer Classification](/paper/dfunet-convolutional-neural-networks-for)", "variants": ["Diabetic Foot Ulcers Classification Datasets"], "title": "DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer Classification"}
{"id": "VIENA2", "contents": "Covers 5 generic driving scenarios, with a total of 25 distinct action classes. It contains more than 15K full HD, 5s long videos acquired in various driving conditions, weathers, daytimes and environments, complemented with a common and realistic set of sensor measurements. This amounts to more than 2.25M frames, each annotated with an action label, corresponding to 600 samples per action class. \r\n\r\nSource: [VIENA2: A Driving Anticipation Dataset](/paper/viena2-a-driving-anticipation-dataset)", "variants": ["VIENA2"], "title": "VIENA2: A Driving Anticipation Dataset"}
{"id": "CSQA", "contents": "Contains around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in the dialogs require a larger subgraph of the KG. \r\n\r\nSource: [Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph](/paper/complex-sequential-question-answering-towards)", "variants": ["CSQA"], "title": "Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph"}
{"id": "Washington RGB-D", "contents": "**Washington RGB-D** is a widely used testbed in the robotic community, consisting of 41,877 RGB-D images organized into 300 instances divided in 51 classes of common indoor objects (e.g. scissors, cereal box, keyboard etc). Each object instance was positioned on a turntable and captured from three different viewpoints while rotating.\r\n\r\nSource: [Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work](https://arxiv.org/abs/1702.08513)\r\nImage Source: [https://rgbd-dataset.cs.washington.edu/](https://rgbd-dataset.cs.washington.edu/)", "variants": ["Washington RGB-D"], "title": "A large-scale hierarchical multi-view RGB-D object dataset"}
{"id": "SemEval-2018 Task 9: Hypernym Discovery", "contents": "The SemEval-2018 hypernym discovery evaluation benchmark (Camacho-Collados et al. 2018) contains three domains (general, medical and music) and is also available in Italian and Spanish (not in this repository). For each domain a target corpus and vocabulary (i.e. hypernym search space) are provided. The dataset contains both concepts (e.g. dog) and entities (e.g. Manchester United) up to trigrams.\r\n\r\nSource: [NLP Progress](http://nlpprogress.com/english/taxonomy_learning.html)\r\n\r\nImage source: [SemEval-2018 Task 9: Hypernym Discovery](https://competitions.codalab.org/competitions/17119#learn_the_details-overview)", "variants": ["General", "Medical domain", "Music domain", "SemEval-2018 Task 9: Hypernym Discovery"], "title": "SemEval-2018 Task 9: Hypernym Discovery"}
{"id": "SST", "contents": "The **Stanford Sentiment Treebank** is a corpus with fully labeled parse trees that allows for a\r\ncomplete analysis of the compositional effects of\r\nsentiment in language. The corpus is based on\r\nthe dataset introduced by Pang and Lee (2005) and\r\nconsists of 11,855 single sentences extracted from\r\nmovie reviews. It was parsed with the Stanford\r\nparser and includes a total of 215,154 unique phrases\r\nfrom those parse trees, each annotated by 3 human judges.\r\n\r\nEach phrase is labelled as either *negative*, *somewhat negative*, *neutral*, *somewhat positive* or *positive*.\r\nThe corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (*negative* or *somewhat negative* vs *somewhat positive* or *positive* with *neutral* sentences discarded) refer to the dataset as SST-2 or SST binary.", "variants": ["SST", "SST-5 Fine-grained classification", "SST-2 Binary classification"], "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}
{"id": "The Annotated Gumar Corpus", "contents": "", "variants": ["The Annotated Gumar Corpus"], "title": "A morphologically annotated corpus of Emirati Arabic"}
{"id": "MeGlass", "contents": "**MeGlass** is an eyeglass dataset originally designed for eyeglass face recognition evaluation. All the face images are selected and cleaned from MegaFace. Each identity has at least two face images with eyeglass and two face images without eyeglass. It contains 47,817 images from 1,710 different identities.\n\nSource: [https://github.com/cleardusk/MeGlass](https://github.com/cleardusk/MeGlass)\nImage Source: [https://github.com/cleardusk/MeGlass](https://github.com/cleardusk/MeGlass)", "variants": ["MeGlass"], "title": "Face Synthesis for Eyeglass-Robust Face Recognition"}
{"id": "AwA", "contents": "**Animals with Attributes** (**AwA**) was a dataset for benchmarking transfer-learning algorithms, in particular attribute base classification. It consisted of 30475 images of 50 animals classes with six pre-extracted feature representations for each image. The animals classes are aligned with Osherson's classical class/attribute matrix, thereby providing 85 numeric attribute values for each class. Using the shared attributes, it is possible to transfer information between different classes.\r\nThe Animals with Attributes dataset was suspended. Its images are not available anymore because of copyright restrictions. A drop-in replacement, Animals with Attributes 2, is available instead.\r\n\r\nSource: [Transductive Multi-view Zero-Shot Learning](https://arxiv.org/abs/1501.04560)\r\nImage Source: [https://cvml.ist.ac.at/AwA/](https://cvml.ist.ac.at/AwA/)", "variants": ["AWA-LT", "AWA2", "AwA", "AWA - 0-Shot", "AWA1 - 0-Shot"], "title": "Learning to detect unseen object classes by between-class attribute transfer"}
{"id": "ExtremeWeather", "contents": "Encourages machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change.\r\n\r\nSource: [ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events](https://arxiv.org/pdf/1612.02095v2.pdf)", "variants": ["ExtremeWeather"], "title": "ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events"}
{"id": "AVD", "contents": "AVD focuses on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes.\r\n\r\nSource: [A Dataset for Developing and Benchmarking Active Vision](/paper/a-dataset-for-developing-and-benchmarking)", "variants": ["AVD"], "title": "A dataset for developing and benchmarking active vision"}
{"id": "ScanObjectNN", "contents": "**ScanObjectNN** is a newly published real-world dataset comprising of 2902 3D objects in 15 categories. It is a challenging point cloud classification datasets due to the background, missing parts and deformations.\r\n\r\nSource: [A Self Contour-based Rotation and Translation-Invariant Transformation for Point Clouds Recognition](https://arxiv.org/abs/2009.06903)\r\nImage Source: [https://hkust-vgd.github.io/scanobjectnn/](https://hkust-vgd.github.io/scanobjectnn/)", "variants": ["ScanObjectNN"], "title": "Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data"}
{"id": "Headcam", "contents": "This dataset contains panoramic video captured from a helmet-mounted camera while riding a bike through suburban Northern Virginia. \r\n\r\nSource: [Headcam](https://github.com/jonathanventura/cylindricalsfmlearner)", "variants": ["Headcam"], "title": "Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video"}
{"id": "Perspectrum", "contents": "Perspectrum is a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify the dataset. Crowd-sourcing was used to filter out noise and ensure high-quality data. The dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively.\r\n\r\nSource: [Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims](/paper/seeing-things-from-a-different-angle)", "variants": ["Perspectrum"], "title": "Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims"}
{"id": "SPEECH-COCO", "contents": "SPEECH-COCO contains speech captions that are generated using text-to-speech (TTS) synthesis resulting in 616,767 spoken captions (more than 600h) paired with images. \r\n\r\nSource: [SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set](/paper/speech-coco-600k-visually-grounded-spoken)", "variants": ["SPEECH-COCO"], "title": "SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set"}
{"id": "Spot-the-diff", "contents": "Spot-the-diff is a dataset consisting of 13,192 image pairs along with corresponding human provided text annotations stating the differences between the two images.\r\n\r\nSource: [Learning to Describe Differences Between Pairs of Similar Images](https://arxiv.org/pdf/1808.10584.pdf)", "variants": ["Spot-the-diff"], "title": "Learning to Describe Differences Between Pairs of Similar Images"}
{"id": "OK-VQA", "contents": "Outside Knowledge Visual Question Answering (OK-VQA) includes more than 14,000 questions that require external knowledge to answer. \r\n\r\nSource: [OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge](https://arxiv.org/pdf/1906.00067v2.pdf)\r\nImage Source: [https://okvqa.allenai.org/](https://okvqa.allenai.org/)", "variants": ["OK-VQA"], "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}
{"id": "TWEETQA", "contents": "Large-scale dataset for QA over social media data.\r\n\r\nSource: [TWEETQA: A Social Media Focused Question Answering Dataset](/paper/tweetqa-a-social-media-focused-question)", "variants": ["TWEETQA"], "title": "TWEETQA: A Social Media Focused Question Answering Dataset"}
{"id": "Serial Speakers", "contents": "An annotated dataset of 161 episodes from three popular American TV serials: Breaking Bad, Game of Thrones and House of Cards. Serial Speakers is suitable both for investigating multimedia retrieval in realistic use case scenarios, and for addressing lower level speech related tasks in especially challenging conditions.\r\n\r\nSource: [Serial Speakers: a Dataset of TV Series](/paper/serial-speakers-a-dataset-of-tv-series)", "variants": ["Serial Speakers"], "title": "Serial Speakers: a Dataset of TV Series"}
{"id": "PMIndia", "contents": "Consists of parallel sentences which pair 13 major languages of India with English. The corpus includes up to 56000 sentences for each language pair.\r\n\r\nSource: [PMIndia -- A Collection of Parallel Corpora of Languages of India](/paper/pmindia-a-collection-of-parallel-corpora-of)", "variants": ["PMIndia"], "title": "PMIndia -- A Collection of Parallel Corpora of Languages of India"}
{"id": "N-Digit MNIST", "contents": "N-Digit MNIST is a multi-digit MNIST-like dataset.", "variants": ["N-Digit MNIST"], "title": "Modeling Uncertainty with Hedged Instance Embedding"}
{"id": "SQUID", "contents": "A dataset of images taken in different locations with varying water properties, showing color charts in the scenes. Moreover, to obtain ground truth, the 3D structure of the scene was calculated based on stereo imaging. This dataset enables a quantitative evaluation of restoration algorithms on natural images.\r\n\r\nSource: [Underwater Single Image Color Restoration Using Haze-Lines and a New Quantitative Dataset](/paper/underwater-single-image-color-restoration)", "variants": ["SQUID"], "title": "Underwater Single Image Color Restoration Using Haze-Lines and a New Quantitative Dataset"}
{"id": "WIDER FACE", "contents": "The **WIDER FACE** dataset contains 32,203 images and labels 393,703 faces with a high degree of variability in scale, pose and occlusion. The database is split into training (40%), validation (10%) and testing (50%) set. Besides, the images are divided into three levels (Easy ⊆ Medium ⊆ Hard) according to the difficulties of the detection. The images and annotations of training and validation set are available online, while the annotations of testing set are not released and the results are sent to the database server for receiving the precision-recall curves.\r\n\r\nSource: [S{}^{3}FD: Single Shot Scale-invariant Face Detector](https://arxiv.org/abs/1708.05237)", "variants": ["WIDER Face (Easy)", "WIDER Face (Medium)", "WIDER Face (Hard)", "WIDER FACE"], "title": "WIDER FACE: A Face Detection Benchmark"}
{"id": "CBT", "contents": "Children’s Book Test (CBT) is designed to measure directly how well language models can exploit wider linguistic context. The CBT is built from books that are freely available thanks to Project Gutenberg. \r\n\r\nSource: [CBT](https://research.fb.com/downloads/babi/)\r\nImage Source: [https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/)", "variants": ["Children's Book Test", "Children's Book Test Common noun", "CBT"], "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations"}
{"id": "MannequinChallenge", "contents": "The **MannequinChallenge** Dataset (MQC) provides in-the-wild videos of people in static poses while a hand-held camera pans around the scene. The dataset consists of three splits for training, validation and testing.\r\n\r\nSource: [Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild](https://arxiv.org/abs/2003.07581)\r\nImage Source: [https://google.github.io/mannequinchallenge/www/index.html](https://google.github.io/mannequinchallenge/www/index.html)", "variants": ["MannequinChallenge"], "title": "Learning the Depths of Moving People by Watching Frozen People"}
{"id": "HPatches", "contents": "The **HPatches** is a recent dataset for local patch descriptor evaluation that consists of 116 sequences of 6 images with known homography. The dataset is split into two parts: viewpoint - 59 sequences with significant viewpoint change and illumination - 57 sequences with significant illumination change, both natural and artificial.\r\n\r\nSource: [RF-Net: An End-to-End Image Matching Network based on Receptive Field](https://arxiv.org/abs/1906.00604)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/publications/2017/Balntas17/balntas17.pdf](https://www.robots.ox.ac.uk/~vgg/publications/2017/Balntas17/balntas17.pdf)", "variants": ["HPatches"], "title": "HPatches: A benchmark and evaluation of handcrafted and learned local descriptors"}
{"id": "COVIDx", "contents": "An open access benchmark dataset comprising of 13,975 CXR images across 13,870 patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge.\r\n\r\nSource: [COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images](/paper/covid-net-a-tailored-deep-convolutional)", "variants": ["COVIDx"], "title": "COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest Radiography Images"}
{"id": "AFLW", "contents": "The **Annotated Facial Landmarks in the Wild** (**AFLW**) is a large-scale collection of annotated face images gathered from Flickr, exhibiting a large variety in appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total about 25K faces are annotated with up to 21 landmarks per image.\r\n\r\nSource: [Nose, Eyes and Ears: Head Pose Estimation by Locating Facial Keypoints](https://arxiv.org/abs/1812.00739)", "variants": ["AFLW", "AFLW (Zhang CVPR 2018 crops)", "AFLW-Front", "AFLW-Full", "AFLW-LFPA", "AFLW-MTFL", "AFLW-PIFA (21 points)", "AFLW-PIFA (34 points)"], "title": "Annotated Facial Landmarks in the Wild: A large-scale, real-world database for facial landmark localization"}
{"id": "UCF-Crime", "contents": "The UCF-Crime dataset is a large-scale dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies including Abuse, Arrest, Arson, Assault, Road Accident, Burglary, Explosion, Fighting, Robbery, Shooting, Stealing, Shoplifting, and Vandalism. These anomalies are selected because they have a significant impact on public safety. \r\n\r\nThis dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities.\r\n\r\nSource: [Video Anomaly Dection Dataset](https://webpages.uncc.edu/cchen62/dataset.html)", "variants": ["UCF-Crime"], "title": "Real-World Anomaly Detection in Surveillance Videos"}
{"id": "e-SNLI", "contents": "e-SNLI is used for various goals, such as obtaining full sentence justifications of a model's decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets.\r\n\r\nSource: [e-SNLI: Natural Language Inference with Natural Language Explanations](/paper/e-snli-natural-language-inference-with)\r\nImage Source: [https://arxiv.org/pdf/1812.01193v2.pdf](https://arxiv.org/pdf/1812.01193v2.pdf)", "variants": ["e-SNLI"], "title": "e-SNLI: Natural Language Inference with Natural Language Explanations"}
{"id": "SMHD", "contents": "A novel large dataset of social media posts from users with one or multiple mental health conditions along with matched control users.\r\n\r\nSource: [SMHD: A Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions](/paper/smhd-a-large-scale-resource-for-exploring)", "variants": ["SMHD"], "title": "SMHD: A Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions"}
{"id": "RESISC45", "contents": "RESISC45 dataset is a dataset for Remote Sensing Image Scene Classification (RESISC). It contains 31,500 images with 45 scene classes and 700 images in each class.", "variants": ["RESISC45"], "title": "Remote Sensing Image Scene Classification: Benchmark and State of the Art"}
{"id": "2-PM Vessel Dataset", "contents": "2-PM Vessel is an open-source volumetric brain vasculature dataset obtained with two-photon microscopy at Focused Ultrasound Lab, at Sunnybrook Research Institute (affiliated with University of Toronto by Dr. Alison Burgess, Charissa Poon and Marc Santos. The dataset contains a total of 12 volumetric stacks consisting of images of mouse brain vasculature and tumour vasculature.\r\n\r\nSource: [https://github.com/petteriTeikari/vesselNN_dataset](https://github.com/petteriTeikari/vesselNN_dataset)\r\nImage Source: [Teikari et al](https://arxiv.org/pdf/1606.02382v1.pdf)", "variants": ["2-PM Vessel Dataset"], "title": "Deep Learning Convolutional Networks for Multiphoton Microscopy Vasculature Segmentation"}
{"id": "PanNuke", "contents": "PanNuke is a semi automatically generated nuclei instance segmentation and classification dataset with exhaustive nuclei labels across 19 different tissue types. The dataset consists of 481 visual fields, of which 312 are randomly sampled from more than 20K whole slide images at different magnifications, from multiple data sources. In total the dataset contains 205,343 labeled nuclei, each with an instance segmentation mask. \r\n\r\nSource: [PanNuke Dataset Extension, Insights and Baselines](/paper/pannuke-dataset-extension-insights-and)\r\nImage Source: [https://jgamper.github.io/PanNukeDataset/](https://jgamper.github.io/PanNukeDataset/)", "variants": ["PanNuke"], "title": "PanNuke Dataset Extension, Insights and Baselines"}
{"id": "smallNORB", "contents": "The **smallNORB** dataset is a datset for 3D object recognition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees).\r\nThe training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).\r\n\r\nSource: [https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/](https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/)\r\nImage Source: [https://www.kaggle.com/nepuerto/the-small-norb-dataset-v10](https://www.kaggle.com/nepuerto/the-small-norb-dataset-v10)", "variants": ["smallNORB"], "title": "Dense-Captioning Events in Videos"}
{"id": "CC100", "contents": "This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages. This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Each file comprises of documents separated by double-newlines and paragraphs within the same document separated by a newline. The data is generated using the open source CC-Net repository.\r\n\r\nSource: [Unsupervised Cross-lingual Representation Learning at Scale](/paper/unsupervised-cross-lingual-representation-1)", "variants": ["CC100"], "title": "Unsupervised Cross-lingual Representation Learning at Scale"}
{"id": "COPA", "contents": "The Choice Of Plausible Alternatives (**COPA**) evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning. COPA consists of 1000 questions, split equally into development and test sets of 500 questions each. Each question is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The correct alternative is randomized so that the expected performance of randomly guessing is 50%.\r\n\r\nSource: [Choice of Plausible Alternatives (COPA)](https://people.ict.usc.edu/~gordon/copa.html)\r\nImage Source: [https://people.ict.usc.edu/~gordon/copa.html](https://people.ict.usc.edu/~gordon/copa.html)", "variants": ["COPA"], "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}
{"id": "Funcom", "contents": "Funcom is a collection of ~2.1 million Java methods and their associated Javadoc comments. This data set was derived from a set of 51 million Java methods and only includes methods that have an associated comment, comments that are in the English language, and has had auto-generated files removed. Each method/comment pair also has an associated method_uid and project_uid so that it is easy to group methods by their parent project.\r\n\r\nThis dataset of function pairs is used for source code summarisation.\r\n\r\nSource: [Funcom](http://leclair.tech/data/funcom/)", "variants": ["Funcom"], "title": "Recommendations for Datasets for Source Code Summarization"}
{"id": "Chinese Traditional Painting dataset", "contents": "The **Chinese Traditional Painting dataset** for style transfer contains 1000 content images and 100 style images.\nThe content images are mostly the photorealistic scenes of mountain, lake, river, bridge, and buildings in regions south of the Yangtze River. It includes not only the scenes of China, but also beautiful pictures of Rhine, Alps, Yellow Stone, Grand Canyon, etc. The content images include diverse types of Chinese traditional paintings.\n\nSource: [https://github.com/lbsswu/Chinese_style_transfer](https://github.com/lbsswu/Chinese_style_transfer)\nImage Source: [https://github.com/lbsswu/Chinese_style_transfer](https://github.com/lbsswu/Chinese_style_transfer)", "variants": ["Chinese Traditional Painting dataset"], "title": "Neural Abstract Style Transfer for Chinese Traditional Painting"}
{"id": "WikiHow", "contents": "**WikiHow** is a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and represent high diversity styles.\r\n\r\nSource: [WikiHow: A Large Scale Text Summarization Dataset](https://paperswithcode.com/paper/wikihow-a-large-scale-text-summarization/)\r\nImage Source: [WikiHow: A Large Scale Text Summarization Dataset](https://paperswithcode.com/paper/wikihow-a-large-scale-text-summarization/)", "variants": ["WikiHow"], "title": "WikiHow: A Large Scale Text Summarization Dataset"}
{"id": "WikiConv", "contents": "A corpus that encompasses the complete history of conversations between contributors to Wikipedia, one of the largest online collaborative communities. By recording the intermediate states of conversations---including not only comments and replies, but also their modifications, deletions and restorations---this data offers an unprecedented view of online conversation.\r\n\r\nSource: [WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community](/paper/wikiconv-a-corpus-of-the-complete)", "variants": ["WikiConv"], "title": "WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community"}
{"id": "UAVid", "contents": "UAVid is a high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. The UAV dataset consists of 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. \r\n\r\nSource: [UAVid: A Semantic Segmentation Dataset for UAV Imagery](/paper/the-uavid-dataset-for-video-semantic)\r\nImage Source: [https://uavid.nl/](https://uavid.nl/)", "variants": ["UAVid"], "title": "The UAVid Dataset for Video Semantic Segmentation"}
{"id": "StreetHazards", "contents": "StreetHazards is a synthetic dataset for anomaly detection, created by inserting a diverse array of foreign objects into driving scenes and re-render the scenes with these novel objects.\r\n\r\nSource: [Scaling Out-of-Distribution Detection for Real-World Settings](/paper/a-benchmark-for-anomaly-segmentation)", "variants": ["StreetHazards"], "title": "A Benchmark for Anomaly Segmentation"}
{"id": "SPOT", "contents": "The SPOT dataset contains 197 reviews originating from the Yelp'13 and IMDB collections ([1][2]), annotated with segment-level polarity labels (positive/neutral/negative). Annotations have been gathered on 2 levels of granulatiry:\r\n\r\n- Sentences\r\n- Elementary Discourse Units (EDUs), i.e. sub-sentence clauses produced by a state-of-the-art RST parser\r\n\r\nThis dataset is intended to aid sentiment analysis research and, in particular, the evaluation of methods that attempt to predict sentiment on a fine-grained, segment-level basis.\r\n\r\nSource: [SPOT](https://github.com/EdinburghNLP/spot-data)", "variants": ["SPOT", "Yelp Fine-grained classification"], "title": "Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis"}
{"id": "WeatherBench", "contents": "A benchmark dataset for data-driven medium-range weather forecasting, a topic of high scientific interest for atmospheric and computer scientists alike. \r\n\r\nSource: [WeatherBench: A benchmark dataset for data-driven weather forecasting](/paper/weatherbench-a-benchmark-dataset-for-data)", "variants": ["WeatherBench"], "title": "WeatherBench: A benchmark dataset for data-driven weather forecasting"}
{"id": "Kinect-WSJ", "contents": "Kinect-WSJ is a multichannel, multispeaker, reverberated, noisy dataset which extends the [WSJ0-2mix](/dataset/wsj0-2mix-1) singlechannel, non-reverberated, noiseless dataset to the strong reverberation and noise conditions and the Kinect-like microphone array geometry used in [CHiME-5](/dataset/chime-5).", "variants": ["Kinect-WSJ"], "title": "Analyzing the impact of speaker localization errors on speech separation for automatic speech recognition"}
{"id": "CNN/Daily Mail", "contents": "**CNN/Daily Mail** is a dataset for text summarization. Human generated abstractive summary bullets were generated from news stories in CNN and Daily Mail websites as questions (with one of the entities hidden), and stories as the corresponding passages from which the system is expected to answer the fill-in the-blank question. The authors released the scripts that crawl, extract and generate pairs of passages and questions from these websites.\r\n\r\nIn all, the corpus has 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs, as defined by their scripts. The source documents in the training set have 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences. \r\n\r\nSource: [Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](https://arxiv.org/pdf/1602.06023v5.pdf)", "variants": ["CNN-DM", "CNN / Daily Mail", "CNNDailyMail", "CNN/Daily Mail", "CNN / Daily Mail (Anonymized)"], "title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond"}
{"id": "CommonGen", "contents": "CommonGen is constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. \r\n\r\nSource: [CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning](/paper/commongen-a-constrained-text-generation)", "variants": ["CommonGen"], "title": "CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning"}
{"id": "Breakfast", "contents": "The **Breakfast** Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded “in the wild” as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.\r\n\r\nSource: [https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)\r\nImage Source: [https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)", "variants": ["Breakfast"], "title": "The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities"}
{"id": "NAS-Bench-101", "contents": "**NAS-Bench-101** is the first public architecture dataset for NAS research. To build NASBench-101, the authors carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional\r\narchitectures. The authors trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the precomputed dataset. \r\n\r\nSource: [NAS-Bench-101: Towards Reproducible Neural Architecture Search](/paper/nas-bench-101-towards-reproducible-neural)", "variants": ["NAS-Bench-101"], "title": "NAS-Bench-101: Towards Reproducible Neural Architecture Search"}
{"id": "DemCare", "contents": "Dem@Care is providing the following datasets, which are collected during lab and home experiments. The data collection took place in the Greek Alzheimer’s Association for Dementia and Related Disorders in Thessaloniki, Greece and in participants’ homes. The datasets include video and audio recordings as well as data from physiological sensors. Moreover, they include data from sleep, motion and plug sensors.\r\n\r\nSource: [DemCare](http://www.demcare.eu/results/datasets)", "variants": ["DemCare"], "title": "The Dem@Care Experiments and Datasets: a Technical Report"}
{"id": "m2cai16-tool-locations", "contents": "The m2cai16-tool-locations dataset contains spatial tool annotations for 2,532 frames across the first 10 videos in the m2cai16-tool dataset, which includes 15 videos in total. The dataset consists of 3,141 annotations of 7 surgical instrument classes, with an average of 1.2 labels per frame and 7 instrument classes per video.\r\n\r\nSource: [http://ai.stanford.edu/~syyeung/tooldetection.html](http://ai.stanford.edu/~syyeung/tooldetection.html)", "variants": ["m2cai16-tool-locations"], "title": "Tool Detection and Operative Skill Assessment in Surgical Videos Using Region-Based Convolutional Neural Networks"}
{"id": "LSMDC", "contents": "This dataset contains 118,081 short video clips extracted from 202 movies. Each video has a caption, either extracted from the movie script or from transcribed DVS (descriptive video services) for the visually impaired. The validation set contains 7408 clips and evaluation is performed on a test set of 1000 videos from movies disjoint from the training and val sets.\r\n\r\nSource: [Use What You Have: Video Retrieval Using Representations From Collaborative Experts](https://arxiv.org/abs/1907.13487)\r\nImage Source: [https://sites.google.com/site/describingmovies/](https://sites.google.com/site/describingmovies/)", "variants": ["LSMDC"], "title": "A dataset for Movie Description"}
{"id": "A Large Dataset of Object Scans", "contents": "**A Large Dataset of Object Scans** is a dataset of more than ten thousand 3D scans of real objects. To create the dataset, the authors recruited 70 operators, equipped them with consumer-grade mobile 3D scanning setups, and paid them to scan objects in their environments. The operators scanned objects of their choosing, outside the laboratory and without direct supervision by computer vision professionals. The result is a large and diverse collection of object scans: from shoes, mugs, and toys to grand pianos, construction vehicles, and large outdoor sculptures. The authors worked with an attorney to ensure that data acquisition did not violate privacy constraints. The acquired data was placed in the public domain and is available freely.", "variants": ["A Large Dataset of Object Scans"], "title": "A Large Dataset of Object Scans"}
{"id": "RICE", "contents": "**RICE** is a remote sensing image dataset for cloud removal. The proposed dataset consists of two parts: RICE1 contains 500 pairs of images, each pair has images with cloud and cloudless size of 512*512; RICE2 contains 450 sets of images, each set contains three 512*512 size images, respectively, the reference picture without clouds, the picture of the cloud and the mask of its cloud.\n\nSource: [https://github.com/BUPTLdy/RICE_DATASET](https://github.com/BUPTLdy/RICE_DATASET)", "variants": ["Rice Grain Disease Dataset", "RICE"], "title": "A Remote Sensing Image Dataset for Cloud Removal"}
{"id": "Contour Drawing Dataset", "contents": "A new dataset of contour drawings.\r\n\r\nSource: [Photo-Sketching: Inferring Contour Drawings from Images](/paper/photo-sketching-inferring-contour-drawings)", "variants": ["Contour Drawing Dataset"], "title": "Photo-Sketching: Inferring Contour Drawings From Images"}
{"id": "Set11", "contents": "**Set11** is a dataset of 11 grayscale images. It is a dataset used for image reconstruction and image compression.\n\nSource: [ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing](https://arxiv.org/abs/1706.07929)\nImage Source: [https://arxiv.org/pdf/1706.07929.pdf](https://arxiv.org/pdf/1706.07929.pdf)", "variants": ["Set11 cs=50%", "Set11"], "title": "ReconNet: Non-Iterative Reconstruction of Images from Compressively Sensed Measurements"}
{"id": "KITTI Road", "contents": "KITTI Road is road and lane estimation benchmark that consists of 289 training and 290 test images. It contains three different categories of road scenes:\r\n* uu - urban unmarked (98/100)\r\n* um - urban marked (95/96)\r\n* umm - urban multiple marked lanes (96/94)\r\n* urban - combination of the three above\r\nGround truth has been generated by manual annotation of the images and is available for two different road terrain types: road - the road area, i.e, the composition of all lanes, and lane - the ego-lane, i.e., the lane the vehicle is currently driving on (only available for category \"um\"). Ground truth is provided for training images only.\r\n\r\nSource: [http://www.cvlibs.net/datasets/kitti/eval_road.php](http://www.cvlibs.net/datasets/kitti/eval_road.php)\r\nImage Source: [http://www.cvlibs.net/datasets/kitti/eval_road.php](http://www.cvlibs.net/datasets/kitti/eval_road.php)", "variants": ["KITTI Road"], "title": "A new performance measure and evaluation benchmark for road detection algorithms"}
{"id": "SVD", "contents": "SVD is a large-scale short video dataset, which contains over 500,000 short videos collected from http://www.douyin.com and over 30,000 labeled pairs of near-duplicate videos.\r\n\r\nSource: [SVD: A Large-Scale Short Video Dataset for Near-Duplicate Video Retrieval](/paper/svd-a-large-scale-short-video-dataset-for)", "variants": ["SVD"], "title": "SVD: A Large-Scale Short Video Dataset for Near-Duplicate Video Retrieval"}
{"id": "DIPS", "contents": "Contains biases but is two orders of magnitude larger than those used previously. \r\n\r\nSource: [End-to-End Learning on 3D Protein Structure for Interface Prediction](/paper/transferrable-end-to-end-learning-for-protein)", "variants": ["DIPS"], "title": "Generalizable Protein Interface Prediction with End-to-End Learning"}
{"id": "EGO-CH", "contents": "EGO-CH is a dataset of egocentric videos for visitors’ behavior understanding. The dataset has been collected in two different cultural sites and includes more than 27 hours of video acquired by 70 subjects, including volunteers and 60 real visitors. The overall dataset includes labels for 26 environments and over 200 Points of Interest (POIs). Specifically, each video of EGO-CH has been annotated with 1) temporal labels specifying the current location of the visitor and the observed POI, 2) bounding box annotations around POIs. A large subset of the dataset, consisting of 60 videos, is also associated with surveys filled out by the visitors at the end of each visit.\r\n\r\nSource: [EGO-CH](https://iplab.dmi.unict.it/EGO-CH/)", "variants": ["EGO-CH"], "title": "EGO-CH: Dataset and Fundamental Tasks for Visitors BehavioralUnderstanding using Egocentric Vision"}
{"id": "OCHuman", "contents": "This dataset focuses on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13,360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, **OCHuman** is the most complex and challenging dataset related to human.\n\nSource: [https://github.com/liruilong940607/OCHumanApi](https://github.com/liruilong940607/OCHumanApi)\nImage Source: [https://github.com/liruilong940607/OCHumanApi](https://github.com/liruilong940607/OCHumanApi)", "variants": ["OCHuman"], "title": "Pose2Seg: Detection Free Human Instance Segmentation"}
{"id": "Modern Hebrew Sentiment Dataset", "contents": "Modern Hebrew Sentiment Dataset is a sentiment analysis benchmark for Hebrew, based on 12K social media comments, and provide two instances of these data: in token-based and morpheme-based settings.\r\n\r\nSource: [Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from Modern Hebrew](/paper/representations-and-architectures-in-neural)", "variants": ["Modern Hebrew Sentiment Dataset"], "title": "Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from Modern Hebrew"}
{"id": "SAVOIAS", "contents": "A visual complexity dataset that compromises of more than 1,400 images from seven image categories relevant to the above research areas, namely Scenes, Advertisements, Visualization and infographics, Objects, Interior design, Art, and Suprematism. The images in each category portray diverse characteristics including various low-level and high-level features, objects, backgrounds, textures and patterns, text, and graphics. \r\n\r\nSource: [SAVOIAS: A Diverse, Multi-Category Visual Complexity Dataset](/paper/savoias-a-diverse-multi-category-visual)", "variants": ["SAVOIAS"], "title": "SAVOIAS: A Diverse, Multi-Category Visual Complexity Dataset"}
{"id": "ComplexWebQuestions", "contents": "ComplexWebQuestions is a dataset for answering complex questions that require reasoning over multiple web snippets. It contains a large set of complex questions in natural language, and can be used in multiple ways:\r\n\r\n1. By interacting with a search engine;\r\n2. As a reading comprehension task: the authors release 12,725,989 web snippets that are relevant for the questions, and were collected during the development of their model;\r\n3. As a semantic parsing task: each question is paired with a SPARQL query that can be executed against Freebase to retrieve the answer.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/complexwebquestions)\r\nImage Source: [Talmor et al](https://arxiv.org/pdf/1803.06643v1.pdf)", "variants": ["complexWebQuestions-V1.0", "ComplexWebQuestions"], "title": "The Web as a Knowledge-base for Answering Complex Questions"}
{"id": "WebQuestions", "contents": "The **WebQuestions** dataset is a question answering dataset using Freebase as the knowledge base and contains 6,642 question-answer pairs. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. The original split uses 3,778 examples for training and 2,032 for testing. All answers are defined as Freebase entities.\r\n\r\nExample questions (answers) in the dataset include “Where did Edgar Allan Poe died?” (baltimore) or “What degrees did Barack Obama get?” (bachelor_of_arts, juris_doctor).\r\n\r\nSource: [Question Answering with Subgraph Embeddings](https://arxiv.org/abs/1406.3676)\r\nImage Source: [Berant et al](https://www.aclweb.org/anthology/D13-1160)", "variants": ["WebQuestions"], "title": "Semantic Parsing on Freebase from Question-Answer Pairs"}
{"id": "Mouse Reach", "contents": "A large, annotated video dataset of mice performing a sequence of actions. The dataset was collected and labeled by experts for the purpose of neuroscience research. \r\n\r\nSource: [Detecting the Starting Frame of Actions in Video](/paper/detecting-the-starting-frame-of-actions-in)", "variants": ["Mouse Reach"], "title": "Detecting the Starting Frame of Actions in Video"}
{"id": "HIDE", "contents": "Consists of 8,422 blurry and sharp image pairs with 65,784 densely annotated FG human bounding boxes. \r\n\r\nSource: [Human-Aware Motion Deblurring](/paper/human-aware-motion-deblurring-1)", "variants": ["HIDE", "HIDE (trained on GOPRO)"], "title": "Human-Aware Motion Deblurring"}
{"id": "VIST-Edit", "contents": "The dataset, VIST-Edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions.\r\n\r\nSource: [Visual Story Post-Editing](https://arxiv.org/pdf/1906.01764v1.pdf)", "variants": ["VIST-Edit"], "title": "Visual Story Post-Editing"}
{"id": "Google Landmarks Dataset v2", "contents": "This is the second version of the Google Landmarks dataset (GLDv2), which contains images annotated with labels representing human-made and natural landmarks. The dataset can be used for landmark recognition and retrieval experiments. This version of the dataset contains approximately 5 million images, split into 3 sets of images: train, index and test\n\nSource: [https://github.com/cvdfoundation/google-landmark](https://github.com/cvdfoundation/google-landmark)\nImage Source: [https://github.com/cvdfoundation/google-landmark](https://github.com/cvdfoundation/google-landmark)", "variants": ["Google Landmarks Dataset v2", "Google Landmarks Dataset v2 (recognition, testing)", "Google Landmarks Dataset v2 (recognition, validation)", "Google Landmarks Dataset v2 (retrieval, testing)", "Google Landmarks Dataset v2 (retrieval, validation)"], "title": "Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval"}
{"id": "VGG Cell", "contents": "The **VGG Cell** dataset (made up entirely of synthetic images) is the main public benchmark used to compare cell counting techniques.\n\nSource: [People, Penguins and Petri Dishes: Adapting Object Counting Models To New Visual Domains And Object Types Without Forgetting](https://arxiv.org/abs/1711.05586)\nImage Source: [https://www.robots.ox.ac.uk/~vgg/research/counting/index_org.html](https://www.robots.ox.ac.uk/~vgg/research/counting/index_org.html)", "variants": ["VGG Cell"], "title": "Microscopy cell counting and detection with fully convolutional regression networks"}
{"id": "Hyperspectral City", "contents": "Propose a dataset which adopts multi-channel visual input.\r\n\r\nSource: [Hyperspectral City](https://pbdl2019.github.io/challenge/index.html)", "variants": ["Hyperspectral City"], "title": "Hyperspectral City V1.0 Dataset and Benchmark"}
{"id": "UTA-RLDD", "contents": "Consists of around 30 hours of video, with contents ranging from subtle signs of drowsiness to more obvious ones.\r\n\r\nSource: [A Realistic Dataset and Baseline Temporal Model for Early Drowsiness Detection](/paper/a-realistic-dataset-and-baseline-temporal)", "variants": ["UTA-RLDD"], "title": "A Realistic Dataset and Baseline Temporal Model for Early Drowsiness Detection"}
{"id": "SoccerNet", "contents": "A benchmark for action spotting in soccer videos. The dataset is composed of 500 complete soccer games from six main European leagues, covering three seasons from 2014 to 2017 and a total duration of 764 hours. A total of 6,637 temporal annotations are automatically parsed from online match reports at a one minute resolution for three main classes of events (Goal, Yellow/Red Card, and Substitution). \r\n\r\nSource: [SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos](/paper/soccernet-a-scalable-dataset-for-action)", "variants": ["SoccerNet"], "title": "SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos"}
{"id": "AnimalWeb", "contents": "A large-scale, hierarchical annotated dataset of animal faces, featuring 21.9K faces from 334 diverse species and 21 animal orders across biological taxonomy. These faces are captured `in-the-wild' conditions and are consistently annotated with 9 landmarks on key facial features. The proposed dataset is structured and scalable by design; its development underwent four systematic stages involving rigorous, manual annotation effort of over 6K man-hours.\r\n\r\nSource: [AnimalWeb: A Large-Scale Hierarchical Dataset of Annotated Animal Faces](/paper/animalweb-a-large-scale-hierarchical-dataset)", "variants": ["AnimalWeb"], "title": "AnimalWeb: A Large-Scale Hierarchical Dataset of Annotated Animal Faces"}
{"id": "TyDiQA-GoldP", "contents": "**TyDiQA** is the gold passage version of the Typologically Diverse Question Answering (TyDiWA) dataset, a benchmark for information-seeking question answering, which covers nine languages. The gold passage version is a simplified version of the primary task, which uses only the gold passage as context and excludes unanswerable questions. It is thus similar to XQuAD and MLQA, while being more challenging as questions have been written without seeing the answers, leading to 3× and 2× less lexical overlap compared to XQuAD and MLQA respectively.\r\n\r\nSource: [XTREME](https://arxiv.org/pdf/2003.11080.pdf)", "variants": ["TyDiQA-GoldP"], "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"}
{"id": "Django", "contents": "The **Django** dataset is a dataset for code generation comprising of 16000 training, 1000 development and 1805 test annotations. Each data point consists of a line of Python code together with a manually created natural language description.\r\n\r\nSource: [Latent Predictor Networks for Code Generation](https://arxiv.org/abs/1603.06744)\r\nImage Source: [https://github.com/microsoft/vscode-docs/issues/2696](https://github.com/microsoft/vscode-docs/issues/2696)", "variants": ["Django"], "title": "Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"}
{"id": "PST900", "contents": "**PST900** is a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge.\n\nSource: [https://arxiv.org/abs/1909.10980](https://arxiv.org/abs/1909.10980)\nImage Source: [https://github.com/ShreyasSkandanS/pst900_thermal_rgb](https://github.com/ShreyasSkandanS/pst900_thermal_rgb)", "variants": ["PST900"], "title": "PST900: RGB-Thermal Calibration, Dataset and Segmentation Network"}
{"id": "HAKE-Large", "contents": "HAKE is built upon existing activity datasets and provides human body part level atomic action labels (Part States).", "variants": ["HAKE-Large"], "title": "PaStaNet: Toward Human Activity Knowledge Engine"}
{"id": "hls4ml LHC Jet dataset", "contents": "Dataset of high-pT jets from simulations of LHC proton-proton collisions\r\n\r\nPrepared for FastML/HLS4ML studies: https://fastmachinelearning.org\r\n\r\nIncludes: High level features (see https://arxiv.org/abs/1804.06913)\r\n\r\nImages: jet images with up to 100 particles/jet (see https://arxiv.org/abs/1908.05318)\r\n\r\nList: list of jet features with up to 100 particles/jet (see https://arxiv.org/abs/1908.05318)", "variants": ["hls4ml LHC Jet dataset"], "title": "Fast inference of deep neural networks in FPGAs for particle physics"}
{"id": "RSDD-Time", "contents": "RSDD-Time is a dataset of 598 manually annotated self-reported depression diagnosis posts from Reddit that include temporal information about the diagnosis. Annotations include whether a mental health condition is present and how recently the diagnosis happened. Additionally, the dataset includes exact temporal spans that relate to the date of diagnosis. \r\n\r\nSource: [RSDD-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses](/paper/rsdd-time-temporal-annotation-of-self)", "variants": ["RSDD-Time"], "title": "RSDD-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses"}
{"id": "FAIR-Play", "contents": "**FAIR-Play** is a video-audio dataset consisting of 1,871 video clips and their corresponding binaural audio clips recording in a music room. The video clip and binaural clip of the same index are roughly aligned.\n\nSource: [https://github.com/facebookresearch/FAIR-Play](https://github.com/facebookresearch/FAIR-Play)\nImage Source: [https://github.com/facebookresearch/FAIR-Play](https://github.com/facebookresearch/FAIR-Play)", "variants": ["FAIR-Play"], "title": "2.5D Visual Sound"}
{"id": "PieAPP dataset", "contents": "# PieAPP dataset \r\nThe dataset associated with the paper PieAPP: Perceptual Image Error Assessment through Pairwise Preference [[arXiv link](https://arxiv.org/abs/1806.02067)] can be downloaded from:\r\n\r\n- [server containing a zip file with all data](https://web.ece.ucsb.edu/~ekta/projects/PieAPPv0.1/all_data_PieAPP_dataset_CVPR_2018.zip) (2.2GB),\r\n- [Google Drive](https://drive.google.com/drive/folders/10RmBhfZFHESCXhhWq0b3BkO5z8ryw85p?usp=sharing) (ideal for quick browsing). \r\n\r\nThe dataset contains undistorted high-quality reference images and several distorted versions of these reference images. Pairs of distorted images corresponding to a reference image are labeled with probability of preference labels. These labels that indicate the fraction of human population that considers one image to be visually closer to the reference over another in the pair. To ensure reliable pairwise probability of preference labels, we query 40 human subjects via Amazon Mechanical Turk for each image pair. Furthermore, we find the strategy of pairwise-preference labeling to be more robust to errors compared to traditional image-quality-labeling scheme based on mean opinion scores (MOS) - details about this and additional statistical analysis around reliable data collection can be found in the main paper and supplementary material. \r\n\r\nWe make this dataset available for non-commercial and educational purposes only. \r\nThe dataset contains a total of 200 undistorted reference images, divided into train / validation / test split.\r\nThese reference images are derived from the [Waterloo Exploration Dataset](https://ece.uwaterloo.ca/~k29ma/exploration/). We release the subset of 200 reference images used in PieAPP from the Waterloo Exploration Dataset with permissions for non-commercial, educational, use from the authors.\r\nThe users of the PieAPP dataset are requested to cite the Waterloo Exploration Dataset for the reference images, along with PieAPP dataset, as mentioned below.\r\n\r\n## Dataset statistics\r\nThe training + validation set contain a total of 160 reference images and test set contains 40 reference images.\r\nA total of 19,680 distorted images are generated for the train/val set and pairwise probability of preference labels for 77,280 image pairs are made available (derived from querying 40 human subjects for every pairwise comparison + ML estimation).\r\n\r\nFor test set, 15 distorted images per reference (total 600 distorted images) are created and dense pariwise comparisons (total 4200) are performed to label each image pair with a probability of preference, again derived from 40 human subjects' votes.\r\n\r\nOverall, the PieAPP dataset provides a total of 20,280 distorted images derived from 200 reference images, and 81,480 pairwise probability-of-preference labels.\r\n\r\nMore details of dataset collection can be found in Sec.4 of the paper and supplementary document.\r\n\r\n## Folder organization\r\n\r\n- **reference_images** contains the undistorted referene images derived from the Waterloo Exploration dataset. \r\n\r\n- **distorted_images** contains all the distorted versions for each reference image (organized such that one folder contains distorted versions for one reference image), within the train / val / test sub-folders.\r\n\r\n- **labels** contains csv files containing pairwise preference labels for distorted images (see main paper and supplementary material for details on how data is captured). There is one (two in case of test set) csv file for each reference image, in train / val / test subfolders.\r\n\r\n## Interpreting the csv files contained in the labels folder\r\n\r\n- For train and validation (val) set: there is one csv file for to each reference image (`ref_<image number>_pairwise_labels.csv`) containing pairwise labels:\r\n        \r\n        column 1: reference image\r\n        column 2: distorted image A\r\n        column 3: distorted image B\r\n        column 4: raw probability of preference for image A, as obtained by MTurk data - not all pairs for a given reference are labeled; the ones that are not labeled are left blank in this column\r\n        column 5: processed probability of preference for image A - we use the MTurk-labeled pairs to do an ML estimation of probability of preference for all pairs in a given inter set of distorted images for a reference (see section 4.3 in supplementary document for details)\r\n\r\n- For the test set, each reference image has two csv files: one containing preference labels obtained through exhaustive per-reference pairwise comparisons using Amazon Mechanical Turk (naming convention: `ref_<image number>_pairwise_labels.csv`) \r\nand other containing per-image MAP-estimated scores for all distorted images (`ref_<image number>_per_image_score.csv`). \r\n\r\n        for ref_<image number>_pairwise_labels.csv in labels/test/ folder:\r\n        column 1: reference image\r\n        column 2: distorted image A\r\n        column 3: distorted image B\r\n        column 4: probability of preference for image A, as obtained by MTurk data - all pairs are labeled using human data, therefore no additional processing done to estimate missing pairs is needed\r\n\r\n        for ref_<image number>_per_image_score.csv in labels/test/ folder:\r\n        column 1: reference image\r\n        column 2: distorted image A\r\n        column 3: score for image A\r\n\r\nComputing per-image scores enables evaluating the performance of image error/quality metrics using Pearson's Linear Correlation Coefficient and Spearman rank correlation coefficient. \r\nThe per-image score indicates the **level of dissimilarity** of a given distorted image as compared to the reference image. That is, an image considered very different from the reference by humans would get a higher score.\r\n\r\nNote that for the pairwise comparisons on test images, the reference image is also considered a \"distorted\" image and human pairwise preference between a distorted image and its reference image is also collected (again with 40 subjects). Since the ML-estimated scores using the Bradley-Terry model are correct up to an additive constant, this strategy allows for computing an MAP-estimated score for the reference image as well and serves as the constant which is then subtracted from the scores estimated for all the distorted versions of that reference image. As a result, the final reference-image score gets set to 0.\r\n\r\n## Naming convention for images\r\n\r\nReference image: `ref_<image number>.png`\r\n\r\nDistorted image: `distort_<image number>_<distortion type>_<inter or intra>_<identifier>.png`\r\n\r\nFor each reference image, several distorted versions are generated.\r\nThe name of any distorted image contains the following parts:\r\n1. an image number that indicates its corresponding reference\r\n2. the name of the distortion type\r\n3. whether this distorted image is used during inter or intra type comparison (paper section 4.1) \r\n4. a unique identifier for a given distortion type: several realizations of any given distortion type are generated to choose from (for both inter-type and intra-type comparisons), this identifier helps distinguish those realizations\r\n\r\n## Terms of Usage and how to cite this dataset\r\nThis dataset is made available only for non-commercial, educational purposes. The TERMS_OF_USE.pdf in the dataset directory highlights the details of the terms of usage.\r\n\r\nIf you find this dataset useful, please cite the PieAPP dataset:\r\n        \r\n        @InProceedings{Prashnani_2018_CVPR,\r\n        author    = {Prashnani, Ekta and Cai, Hong and Mostofi, Yasamin and Sen, Pradeep},\r\n        title     = {PieAPP: Perceptual Image-Error Assessment Through Pairwise Preference},\r\n        booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\r\n        month     = {June},\r\n        year      = {2018}\r\n        }\r\n\r\n\r\nAlso, for the undistorted reference images, please cite the Waterloo Exploration dataset:\r\n        \r\n        @article{ma2017waterloo,\r\n        author    = {Ma, Kede and Duanmu, Zhengfang and Wu, Qingbo and Wang, Zhou and Yong, Hongwei and Li, Hongliang and Zhang, Lei}, \r\n        title     = {{Waterloo Exploration Database}: New Challenges for Image Quality Assessment Models}, \r\n        journal   = {IEEE Transactions on Image Processing},\r\n        volume    = {26},\r\n        number    = {2},\r\n        pages     = {1004--1016},\r\n        month     = {Feb.},\r\n        year      = {2017}\r\n        }\r\n\r\n\r\nFor comments on improving this dataset release or questions or for reporting errors, please contact Ekta Prashnani or raise an issue on GitHub.\r\n\r\n## Acknowledgements\r\nThis project was supported in part by NSF grants IIS-1321168 and IIS-1619376, as well as a Fall 2017 AI Grant (awarded to Ekta Prashnani).", "variants": ["PieAPP dataset"], "title": "PieAPP: Perceptual Image-Error Assessment through Pairwise Preference"}
{"id": "SynthHands", "contents": "The **SynthHands** dataset is a dataset for hand pose estimation which consists of real captured hand motion retargeted to a virtual hand with natural backgrounds and interactions with different objects. The dataset contains data for male and female hands, both with and without interaction with objects. While the hand and foreground object are synthtically generated using Unity, the motion was obtained from real performances as described in the accompanying paper. In addition, real object textures and background images (depth and color) were used. Ground truth 3D positions are provided for 21 keypoints of the hand.\n\nSource: [Egocentric 6-DoF Tracking of Small Handheld Objects](https://arxiv.org/abs/1804.05870)\nImage Source: [https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm](https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm)", "variants": ["SynthHands"], "title": "Real-Time Hand Tracking Under Occlusion from an Egocentric RGB-D Sensor"}
{"id": "UCI Machine Learning Repository", "contents": "**UCI Machine Learning Repository** is a collection of over 550 datasets.", "variants": ["UCI Machine Learning Repository", "UCI Epileptic Seizure Recognition", "UCI GAS", "UCI HEPMASS", "UCI MINIBOONE", "UCI POWER", "UCI localization data"], "title": "Endgame Analysis of Dou Shou Qi"}
{"id": "Syn2Real", "contents": "**Syn2Real**, a synthetic-to-real visual domain adaptation benchmark meant to encourage further development of robust domain transfer methods. The goal is to train a model on a synthetic \"source\" domain and then update it so that its performance improves on a real \"target\" domain, without using any target annotations. It includes three tasks, illustrated in figures above: the more traditional closed-set classification task with a known set of categories; the less studied open-set classification task with unknown object categories in the target domain; and the object detection task, which involves localizing instances of objects by predicting their bounding boxes and corresponding class labels.\r\n\r\nSource: [Syn2Real](https://ai.bu.edu/syn2real/)\r\nImage Source: [https://ai.bu.edu/syn2real/](https://ai.bu.edu/syn2real/)", "variants": ["Syn2Real-C", "Syn2Real"], "title": "Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation"}
{"id": "NAF", "contents": "This dataset was created with images provided by the United States National Archive and FamilySearch.\r\n\r\nThe goal of this data is to capture relationships between text/handwriting entities on form images. It will include transcriptions in the future, but doesn't currently.\r\n\r\nThe form images are organized into \"groups\", each group containing images of the same form type.\r\n\r\nSource: [NAF Dataset](https://github.com/herobd/NAF_dataset)", "variants": ["NAF"], "title": "Deep Visual Template-Free Form Parsing"}
{"id": "pioNER", "contents": "The **pioNER** corpus provides gold-standard and automatically generated named-entity datasets for the Armenian language.\nThe automatically generated corpus is generated from Wikipedia. The gold-standard set is a collection of over 250 news articles from iLur.am with manual named-entity annotation. It includes sentences from political, sports, local and world news, and is comparable in size with the test sets of other languages.\n\nSource: [https://github.com/ispras-texterra/pioner](https://github.com/ispras-texterra/pioner)", "variants": ["pioNER"], "title": "pioNER: Datasets and Baselines for Armenian Named Entity Recognition"}
{"id": "LAOFIW Dataset", "contents": "An ancestral origin database of 14,000 images of individuals from East Asia, the Indian subcontinent, sub-Saharan Africa, and Western Europe.\r\n\r\nSource: [Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings](/paper/turning-a-blind-eye-explicit-removal-of)", "variants": ["LAOFIW Dataset"], "title": "Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings"}
{"id": "Rotowire-Modified", "contents": "The RotoWire-Modified dataset is a cleaned extension of the RotoWire dataset, with writer information about each document. It contains 2705 samples for training, 532 for validation and 497 for testing.\n\nSource: [https://github.com/aistairc/rotowire-modified](https://github.com/aistairc/rotowire-modified)", "variants": ["Rotowire-Modified"], "title": "Learning to Select, Track, and Generate for Data-to-Text"}
{"id": "NCLT", "contents": "The **NCLT** dataset is a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan’s North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is comprised of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g., pedestrians, bicyclists, and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g., falling leaves and snow), and long-term structural changes caused by construction projects.\r\n\r\nSource: [http://robots.engin.umich.edu/nclt/nclt.pdf](http://robots.engin.umich.edu/nclt/nclt.pdf)\r\nImage Source: [http://robots.engin.umich.edu/nclt/](http://robots.engin.umich.edu/nclt/)", "variants": ["NCLT"], "title": "University of Michigan North Campus long-term vision and lidar dataset"}
{"id": "KaoKore", "contents": "Consists of faces extracted from pre-modern Japanese artwork.\r\n\r\nSource: [KaoKore: A Pre-modern Japanese Art Facial Expression Dataset](/paper/kaokore-a-pre-modern-japanese-art-facial)", "variants": ["KaoKore"], "title": "KaoKore: A Pre-modern Japanese Art Facial Expression Dataset"}
{"id": "SherLIiC", "contents": "SherLIiC is a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types.\r\n\r\nSource: [SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference](https://arxiv.org/pdf/1906.01393v1.pdf)", "variants": ["SherLIiC"], "title": "SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference"}
{"id": "Relational Pattern Similarity Dataset", "contents": "The relational pattern similarity dataset is a new dataset upon the work of Zeichner et al. (2012), which consists of relational patterns with semantic inference labels annotated. The dataset includes 5,555 pairs extracted by Reverb (Fader et al., 2011), 2,447 pairs with inference relation and 3,108 pairs (the rest) without one.\r\n\r\nSource: [Composing Distributed Representations of Relational Patterns](https://www.aclweb.org/anthology/P16-1215.pdf)", "variants": ["Relational Pattern Similarity Dataset"], "title": "Composing Distributed Representations of Relational Patterns"}
{"id": "BraTS 2016", "contents": "BRATS 2016 is a brain tumor segmentation dataset. It shares the same training set as BRATS 2015, which consists of 220 HHG and 54 LGG. Its testing dataset consists of 191 cases with unknown grades.\r\nImage Source: [https://sites.google.com/site/braintumorsegmentation/home/brats_2016](https://sites.google.com/site/braintumorsegmentation/home/brats_2016)", "variants": ["BraTS 2016"], "title": "The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)"}
{"id": "UA-DETRAC", "contents": "Consists of 100 challenging video sequences captured from real-world traffic scenes (over 140,000 frames with rich annotations, including occlusion, weather, vehicle category, truncation, and vehicle bounding boxes) for object detection, object tracking and MOT system. \r\n\r\nSource: [UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking](/paper/ua-detrac-a-new-benchmark-and-protocol-for)\r\n\r\nImage Source: [UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking](/paper/ua-detrac-a-new-benchmark-and-protocol-for)", "variants": ["UA-DETRAC"], "title": "UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking"}
{"id": "PCD", "contents": "The Arabic dataset is scraped mainly from الموسوعة الشعرية and الديوان. After merging both, the total number of verses is 1,831,770 poetic verses. Each verse is labeled by its meter, the poet who wrote it, and the age which it was written in. There are 22 meters, 3701 poets and 11 ages: Pre-Islamic, Islamic, Umayyad, Mamluk, Abbasid, Ayyubid, Ottoman, Andalusian, era between Umayyad and Abbasid, Fatimid, and finally the modern age. We are only interested in the 16 classic meters which are attributed to Al-Farahidi, and they comprise the majority of the dataset with a total number around 1.7M verses. It is important to note that the verses diacritic states are not consistent. This means that a verse can carry full, semi diacritics, or it can carry nothing.", "variants": ["PCD"], "title": "Learning meters of Arabic and English poems with Recurrent Neural Networks: a step forward for language understanding and synthesis"}
{"id": "CheXpert", "contents": "The **CheXpert** dataset contains 224,316 chest radiographs of 65,240 patients with both frontal and lateral views available. The task is to do automated chest x-ray interpretation, featuring uncertainty labels and radiologist-labeled reference standard evaluation sets.\r\n\r\nSource: [Deep Mining External Imperfect Data for Chest X-ray Disease Screening](https://arxiv.org/abs/2006.03796)\r\nImage Source: [https://stanfordmlgroup.github.io/competitions/chexpert/](https://stanfordmlgroup.github.io/competitions/chexpert/)", "variants": ["CheXpert"], "title": "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison"}
{"id": "So2Sat LCZ42", "contents": "So2Sat LCZ42 consists of local climate zone (LCZ) labels of about half a million Sentinel-1 and Sentinel-2 image patches in 42 urban agglomerations (plus 10 additional smaller areas) across the globe. This dataset was labeled by 15 domain experts following a carefully designed labeling work flow and evaluation process over a period of six months. \r\n\r\nSource: [So2Sat LCZ42: A Benchmark Dataset for Global Local Climate Zones Classification](/paper/so2sat-lcz42-a-benchmark-dataset-for-global)\r\nImage Source: [Zhu et al](https://paperswithcode.com/paper/so2sat-lcz42-a-benchmark-dataset-for-global)", "variants": ["So2Sat LCZ42"], "title": "So2Sat LCZ42: A Benchmark Dataset for Global Local Climate Zones Classification"}
{"id": "CrowdFix", "contents": "Contributes dataset: (1) reviewing the dynamics behind saliency and crowds. (2) using eye tracking to create a dynamic human eye fixation dataset over a new set of crowd videos gathered from the Internet. The videos are annotated into three distinct density levels. \r\n\r\nSource: [CrowdFix: An Eyetracking Dataset of Real Life Crowd Videos](/paper/crowdfix-an-eyetracking-data-set-of-human)", "variants": ["CrowdFix"], "title": "CrowdFix: An Eyetracking Dataset of Real Life Crowd Videos"}
{"id": "WMCA", "contents": "The Wide Multi Channel Presentation Attack (WMCA) database consists of 1941 short video recordings of both bonafide and presentation attacks from 72 different identities. The data is recorded from several channels including color, depth, infra-red, and thermal.\r\n\r\nAdditionally, the pulse reading data for bonafide recordings is also provided.\r\n\r\nPreprocessed images for some of the channels are also provided for part of the data used in the reference publication.\r\n\r\nThe WMCA database is produced at Idiap within the framework of “IARPA BATL” and “H2020 TESLA” projects and it is intended for investigation of presentation attack detection (PAD) methods for face recognition systems.", "variants": ["WMCA"], "title": "Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network"}
{"id": "SUN360", "contents": "The goal of the **SUN360** panorama database is to provide academic researchers in computer vision, computer graphics and computational photography, cognition and neuroscience, human perception, machine learning and data mining, with a comprehensive collection of annotated panoramas covering 360x180-degree full view for a large variety of environmental scenes, places and the objects within. To build the core of the dataset, the authors download a huge number of high-resolution panorama images from the Internet, and group them into different place categories. Then, they designed a WebGL annotation tool for annotating the polygons and cuboids for objects in the scene.\r\n\r\nSource: [Scene UNderstanding 360° panorama](https://vision.cs.princeton.edu/projects/2012/SUN360/data/)\r\nImage Source: [http://3dvision.princeton.edu/projects/2012/SUN360/](http://3dvision.princeton.edu/projects/2012/SUN360/)", "variants": ["SUN360"], "title": "Recognizing scene viewpoint using panoramic place representation"}
{"id": "DBpedia", "contents": "**DBpedia** (from \"DB\" for \"database\") is a project aiming to extract structured content from the information created in the Wikipedia project. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.\r\n\r\nSource: [https://en.wikipedia.org/wiki/DBpedia](https://en.wikipedia.org/wiki/DBpedia)", "variants": ["DBpedia"], "title": "DBpedia: A Nucleus for a Web of Open Data"}
{"id": "Charades-Ego", "contents": "Contains 68,536 activity instances in 68.8 hours of first and third-person video, making it one of the largest and most diverse egocentric datasets available. Charades-Ego furthermore shares activity classes, scripts, and methodology with the Charades dataset, that consist of additional 82.3 hours of third-person video with 66,500 activity instances. \r\n\r\nSource: [Charades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos](https://arxiv.org/pdf/1804.09626v2.pdf)", "variants": ["Charades-Ego"], "title": "Charades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos"}
{"id": "SciQ", "contents": "The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/sciq)", "variants": ["SciQ"], "title": "Crowdsourcing Multiple Choice Science Questions"}
{"id": "TED-LIUM 3", "contents": "**TED-LIUM 3** is an audio dataset collected from TED Talks. It contains:\r\n\r\n- 2351 audio talks in NIST sphere format (SPH), including talks from TED-LIUM 2: be careful, same talks but not same audio files (only these audio file must be used with the TED-LIUM 3 STM files)\r\n- 452 hours of audio\r\n- 2351 aligned automatic transcripts in STM format\r\n- TEDLIUM 2 dev and test data: 19 TED talks in SPH format with corresponding manual transcriptions (cf. ‘legacy’ distribution below).\r\n- Dictionary with pronunciations (159848 entries), same file as the one included in TED-LIUM 2\r\n- Selected monolingual data for language modeling from WMT12 publicly available corpora: these files come from the TED-LIUM 2 release, but have been modified to get a tokenization more relevant for English language", "variants": ["TED-LIUM 3"], "title": "TED-LIUM 3: twice as much data and corpus repartition for experiments on speaker adaptation"}
{"id": "PreCo", "contents": "A large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. \r\n\r\nSource: [PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution](/paper/preco-a-large-scale-dataset-in-preschool)", "variants": ["PreCo"], "title": "PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution"}
{"id": "SynthHands", "contents": "The **SynthHands** dataset is a dataset for hand pose estimation which consists of real captured hand motion retargeted to a virtual hand with natural backgrounds and interactions with different objects. The dataset contains data for male and female hands, both with and without interaction with objects. While the hand and foreground object are synthtically generated using Unity, the motion was obtained from real performances as described in the accompanying paper. In addition, real object textures and background images (depth and color) were used. Ground truth 3D positions are provided for 21 keypoints of the hand.\n\nSource: [Egocentric 6-DoF Tracking of Small Handheld Objects](https://arxiv.org/abs/1804.05870)\nImage Source: [https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm](https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm)", "variants": ["SynthHands"], "title": "Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor"}
{"id": "Weibo NER", "contents": "The **Weibo NER** dataset is a Chinese Named Entity Recognition dataset drawn from the social media website Sina Weibo.\r\n\r\nSource: [Chinese NER Using Lattice LSTM](https://arxiv.org/abs/1805.02023)\r\nImage Source: [https://en.wikipedia.org/wiki/Sina_Weibo](https://en.wikipedia.org/wiki/Sina_Weibo)", "variants": ["Weibo NER"], "title": "Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings"}
{"id": "Flickr30K Entities", "contents": "The **Flickr30K Entities** dataset is an extension to the Flickr30K dataset. It augments the original 158k captions with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. This is used to define a new benchmark for localization of textual entity mentions in an image.\r\n\r\nSource: [http://bryanplummer.com/Flickr30kEntities/](http://bryanplummer.com/Flickr30kEntities/)\r\nImage Source: [http://bryanplummer.com/Flickr30kEntities/](http://bryanplummer.com/Flickr30kEntities/)", "variants": ["Flickr30k Entities Dev", "Flickr30k Entities Test", "Flickr30K Entities"], "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}
{"id": "GASP", "contents": "**GASP** is a dataset composed by a list of cited abstracts associated with the corresponding source abstract. The dataset is composed by a training set of 100000 elements, a test set and a validation set of 10000 each.\nThe goal is to generate a paper abstract given cited paper's abstracts and model the human creativity behind the process.\n\nSource: [https://github.com/ART-Group-it/GASP](https://github.com/ART-Group-it/GASP)", "variants": ["GASP"], "title": "GASP! Generating Abstracts of Scientific Papers from Abstracts of Cited Papers"}
{"id": "Synthetic Human Model Dataset", "contents": "A synthetic dataset for evaluating non-rigid 3D human reconstruction based on conventional RGB-D cameras. The dataset consist of seven motion sequences of a single human model. \r\n\r\nSource: [Synthetic Human Model Dataset for Skeleton Driven Non-rigid Motion Tracking and 3D Reconstruction](/paper/synthetic-human-model-dataset-for-skeleton)", "variants": ["Synthetic Human Model Dataset"], "title": "Synthetic Human Model Dataset for Skeleton Driven Non-rigid Motion Tracking and 3D Reconstruction"}
{"id": "HACS", "contents": "HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.\r\n\r\nAuthors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled\r\nfrom 492K, 6K and 6K videos, respectively.", "variants": ["HACS"], "title": "SLAC: A Sparsely Labeled Dataset for Action Classification and Localization"}
{"id": "11k Hands", "contents": "A large dataset of human hand images (dorsal and palmar sides) with detailed ground-truth information for gender recognition and biometric identification.\r\n\r\nSource: [11K Hands: Gender recognition and biometric identification using a large dataset of hand images](/paper/11k-hands-gender-recognition-and-biometric)", "variants": ["11k Hands"], "title": "11K Hands: Gender recognition and biometric identification using a large dataset of hand images"}
{"id": "Turing Change Point Dataset", "contents": "Specifically designed for the evaluation of change point detection algorithms, consisting of 37 time series from various domains. \r\n\r\nSource: [An Evaluation of Change Point Detection Algorithms](/paper/an-evaluation-of-change-point-detection)", "variants": ["Turing Change Point Dataset"], "title": "An Evaluation of Change Point Detection Algorithms"}
{"id": "QUVA Repetition", "contents": "QUVA Repetition dataset consists of 100 videos displaying a wide variety of repetitive video dynamics, including swimming, stirring, cutting, combing and music-making. All videos have been annotated with individual cycle bounds and a total repetition count.\r\n\r\nSource: [Real-world Repetition Estimation by Div, Grad and Curl](https://tomrunia.github.io/projects/repetition/)", "variants": ["QUVA Repetition"], "title": "Real-World Repetition Estimation by Div, Grad and Curl"}
{"id": "DeepFashion", "contents": "**DeepFashion** is a dataset containing around 800K diverse fashion images with their rich annotations (46 categories, 1,000 descriptive attributes, bounding boxes and landmark information) ranging from well-posed product images to real-world-like consumer photos.\r\n\r\nSource: [A Benchmark for Inpainting of Clothing Images with Irregular Holes](https://arxiv.org/abs/2007.05080)", "variants": ["Deep-Fashion", "DeepFashion"], "title": "DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations"}
{"id": "LITIS Rouen", "contents": "The LITIS-Rouen dataset  is a dataset for audio scenes. It consists of 3026 examples of 19 scene categories. Each class is specific to a location such as a train station or an open market. The audio recordings have a duration of 30 seconds and a sampling rate of 22050 Hz. The dataset has a total duration of 1500 minutes.\n\nSource: [Spatio-Temporal Attention Pooling for Audio Scene Classification](https://arxiv.org/abs/1904.03543)\nImage Source: [https://www.researchgate.net/figure/Summary-of-Litis-Rouen-audio-scene-dataset_tbl1_329608235](https://www.researchgate.net/figure/Summary-of-Litis-Rouen-audio-scene-dataset_tbl1_329608235)", "variants": ["LITIS Rouen"], "title": "Histogram of Gradients of Time–Frequency Representations for Audio Scene Classification"}
{"id": "inD Dataset", "contents": "The **inD** dataset is a new dataset of naturalistic vehicle trajectories recorded at German intersections. Using a drone, typical limitations of established traffic data collection methods like occlusions are overcome. Traffic was recorded at four different locations. The trajectory for each road user and its type is extracted. Using state-of-the-art computer vision algorithms, the positional error is typically less than 10 centimetres. The dataset is applicable on many tasks such as road user prediction, driver modeling, scenario-based safety validation of automated driving systems or data-driven development of HAD system components.", "variants": ["inD Dataset"], "title": "The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories at German Intersections"}
{"id": "MonoPerfCap Dataset", "contents": "MonoPerfCap is a benchmark dataset for human 3D performance capture from monocular video input  consisting of around 40k frames, which covers a variety of different scenarios.", "variants": ["MonoPerfCap Dataset"], "title": "MonoPerfCap: Human Performance Capture From Monocular Video"}
{"id": "SciCite", "contents": "**SciCite** is a dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL-ARC.\n\nSource: [Structural Scaffolds for Citation Intent Classification in Scientific Publications](https://arxiv.org/abs/1904.01608)\nImage Source: [https://arxiv.org/pdf/1904.01608v2.pdf](https://arxiv.org/pdf/1904.01608v2.pdf)", "variants": ["ScienceCite", "SciCite"], "title": "Structural Scaffolds for Citation Intent Classification in Scientific Publications"}
{"id": "FER+", "contents": "The **FER+** dataset is an extension of the original FER dataset, where the images have been re-labelled into one of 8 emotion types: neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt.\r\n\r\nSource: [https://github.com/Microsoft/FERPlus](https://github.com/Microsoft/FERPlus)\r\nImage Source: [https://github.com/Microsoft/FERPlus](https://github.com/Microsoft/FERPlus)", "variants": ["FER+", "FERPlus"], "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution"}
{"id": "VIsual PERception (VIPER)", "contents": "VIPER is a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. \r\n\r\nSource: [Playing for Benchmarks](/paper/playing-for-benchmarks)\r\n\r\nImage Source: [Playing for Benchmarks](/paper/playing-for-benchmarks)", "variants": ["VIsual PERception (VIPER)"], "title": "Playing for Benchmarks"}
{"id": "RLLab Framework", "contents": "A benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. \r\n\r\nSource: [Benchmarking Deep Reinforcement Learning for Continuous Control](/paper/benchmarking-deep-reinforcement-learning-for)", "variants": ["RLLab Framework"], "title": "Benchmarking Deep Reinforcement Learning for Continuous Control"}
{"id": "CoSal2015", "contents": "Cosal2015 is a large-scale dataset for co-saliency detection which consists of 2,015 images of 50 categories, and each group suffers from various challenging factors such as complex environments, occlusion issues, target appearance variations and background clutters, etc. All these increase the difficulty for accurate co-saliency detection.\n\nSource: [Adaptive Graph Convolutional Network with Attention Graph Clustering for Co-saliency Detection](https://arxiv.org/abs/2003.06167)\nImage Source: [https://arxiv.org/pdf/1604.07090.pdf](https://arxiv.org/pdf/1604.07090.pdf)", "variants": ["CoSal2015"], "title": "Detection of Co-salient Objects by Looking Deep and Wide"}
{"id": "KITTI", "contents": "**KITTI** (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. [Álvarez et al.](http://yann.lecun.com/exdb/publis/pdf/alvarez-eccv-12.pdf) generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. [Zhang et al.](http://www-video.eecs.berkeley.edu/papers/rzhang/zhang-icra-submission.pdf) annotated 252 (140 for training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. [Ros et al.](http://refbase.cvc.uab.es/files/rrg2015.pdf) labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.\r\n\r\nSource: [A Review on Deep Learning Techniques Applied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\r\nImage Source: [http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)", "variants": ["KITTI", "KITTI Cars Easy", "KITTI Cars Easy val", "KITTI Cars Hard", "KITTI Cars Hard val", "KITTI Cars Moderate", "KITTI Cars Moderate val", "KITTI Cyclist Easy val", "KITTI Cyclist Hard val", "KITTI Cyclist Moderate val", "KITTI Cyclists Easy", "KITTI Cyclists Hard", "KITTI Cyclists Moderate", "KITTI Depth Completion", "KITTI Eigen split", "KITTI Eigen split unsupervised", "KITTI Pedestrian Easy val", "KITTI Pedestrian Hard val", "KITTI Pedestrian Moderate val", "KITTI Pedestrians Easy", "KITTI Pedestrians Hard", "KITTI Pedestrians Moderate", "KITTI Semantic Segmentation", "PreSIL to KITTI", "KITTI 2012", "KITTI 2012 - 2x upscaling", "KITTI 2012 - 4x upscaling", "KITTI 2012 unsupervised", "KITTI2012", "KITTI 2015", "KITTI 2015 - 2x upscaling", "KITTI 2015 - 4x upscaling", "KITTI 2015 unsupervised", "KITTI2015", "KITTI Tracking test", "KITTI Depth Completion Eigen Split", "KITTI Horizon", "KITTI Object Tracking Evaluation 2012", "KITTI Panoptic Segmentation", "KITTI Pedestrian Hard", "KITTI Pedestrians Moderate val", "KITTI 2012 - unsupervised", "KITTI 2015 - unsupervised", "KITTI Novel View Synthesis"], "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite"}
{"id": "CQR", "contents": "CQR is an extension to the Stanford Dialogue Corpus. It contains crowd-sourced rewrites to facilitate research in dialogue state tracking using natural language as the interface.\r\n\r\nSource: [A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (CQR)](/paper/a-dataset-for-resolving-referring-expressions)", "variants": ["CQR"], "title": "A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (CQR)"}
{"id": "ShopSign", "contents": "A newly developed natural scene text dataset of Chinese shop signs in street views. \r\n\r\nSource: [ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views](/paper/shopsign-a-diverse-scene-text-dataset-of)", "variants": ["ShopSign"], "title": "ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views"}
{"id": "HCU400", "contents": "The dataset consists of the features associated with 402 5-second sound samples.\nThe 402 sounds range from easily identifiable everyday sounds to intentionally obscured artificial ones. The dataset aims to lower the barrier for the study of aural phenomenology as the largest available audio dataset to include an analysis of causal attribution. Each sample has been annotated with crowd-sourced descriptions, as well as familiarity, imageability, arousal, and valence ratings.\n\nSource: [https://github.com/mitmedialab/HCU400](https://github.com/mitmedialab/HCU400)", "variants": ["HCU400"], "title": "HCU400: An Annotated Dataset for Exploring Aural Phenomenology Through Causal Uncertainty"}
{"id": "MRPC", "contents": "Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases).\r\n\r\nSource: [Exploiting Semantic Annotations and Q-Learning for Constructing an Efficient Hierarchy/Graph Texts Organization](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313059/)\r\nImage Source: [https://www.aclweb.org/anthology/I05-5002.pdf](https://www.aclweb.org/anthology/I05-5002.pdf)", "variants": ["MRPC", "MRPC Dev"], "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}
{"id": "Hopkins155", "contents": "The Hopkins 155 dataset consists of 156 video sequences of two or three motions. Each video sequence motion corresponds to a low-dimensional subspace. There are 39−550 data vectors drawn from two or three motions for each video sequence.\r\n\r\nSource: [Symmetric low-rank representation for subspace clustering](https://arxiv.org/abs/1410.8618)\r\nImage Source: [http://www.vision.jhu.edu/data/hopkins155/](http://www.vision.jhu.edu/data/hopkins155/)", "variants": ["Hopkins155"], "title": "A Benchmark for the Comparison of 3-D Motion Segmentation Algorithms"}
{"id": "Atari-HEAD", "contents": "**Atari-HEAD** is a dataset of human actions and eye movements recorded while playing Atari videos games. For every game frame, its corresponding image frame, the human keystroke action, the reaction time to make that action, the gaze positions, and immediate reward returned by the environment were recorded. The gaze data was recorded using an EyeLink 1000 eye tracker at 1000Hz. The human subjects are amateur players who are familiar with the games. The human subjects were only allowed to play for 15 minutes and were required to rest for at least 15 minutes before the next trial. Data was collected from 4 subjects, 16 games, 175 15-minute trials, and a total of 2.97 million frames/demonstrations.\n\nSource: [https://zenodo.org/record/2587121](https://zenodo.org/record/2587121)\nImage Source: [https://arxiv.org/abs/1903.06754](https://arxiv.org/abs/1903.06754)", "variants": ["Atari-HEAD"], "title": "Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset"}
{"id": "TVSum", "contents": "The **Title-based Video Summarization** (**TVSum**) dataset serves as a benchmark to validate video summarization techniques. It contains 50 videos of various genres (e.g., news, how-to, documentary, vlog, egocentric) and 1,000 annotations of shot-level importance scores obtained via crowdsourcing (20 per video). The video and annotation data permits an automatic evaluation of various video summarization techniques, without having to conduct (expensive) user study.\r\n\r\nSource: [https://github.com/yalesong/tvsum](https://github.com/yalesong/tvsum)\r\nImage Source: [https://github.com/yalesong/tvsum](https://github.com/yalesong/tvsum)", "variants": ["TvSum", "TVSum"], "title": "TVSum: Summarizing web videos using titles"}
{"id": "HeadQA", "contents": "HeadQA is a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. \r\n\r\nSource: [HEAD-QA: A Healthcare Dataset for Complex Reasoning](/paper/head-qa-a-healthcare-dataset-for-complex)\r\nImage Source: [https://arxiv.org/pdf/1906.04701v1.pdf](https://arxiv.org/pdf/1906.04701v1.pdf)", "variants": ["HeadQA"], "title": "HEAD-QA: A Healthcare Dataset for Complex Reasoning"}
{"id": "CIHP", "contents": "The **Crowd Instance-level Human Parsing** (**CIHP**) dataset has 38,280 diverse human images. Each image in CIHP is labeled with pixel-wise annotations on 20 categories and instance-level identification. The dataset can be used for the human part segmentation task.\r\n\r\nSource: [Parsing R-CNN for Instance-Level Human Analysis](https://arxiv.org/abs/1811.12596)\r\nImage Source: [https://arxiv.org/abs/1808.00157](https://arxiv.org/abs/1808.00157)", "variants": ["CIHP"], "title": "Instance-level Human Parsing via Part Grouping Network"}
{"id": "Kannada-MNIST", "contents": "The **Kannada-MNIST** dataset is a drop-in substitute for the standard MNIST dataset for the Kannada language.\n\nSource: [https://github.com/vinayprabhu/Kannada_MNIST](https://github.com/vinayprabhu/Kannada_MNIST)\nImage Source: [https://github.com/vinayprabhu/Kannada_MNIST](https://github.com/vinayprabhu/Kannada_MNIST)", "variants": ["Kannada-MNIST"], "title": "Kannada-MNIST: A new handwritten digits dataset for the Kannada language"}
{"id": "BUFF", "contents": "**BUFF** consists of 5 subjects, 3 male and 2 female wearing 2 clothing styles: a) t-shirt and long pants and b) a soccer outfit.\r\nThey perform 3 different motions i) hips ii) tilt_twist_left iii) shoulders_mill.\r\n\r\nSource: [http://buff.is.tue.mpg.de/](http://buff.is.tue.mpg.de/)\r\nImage Source: [http://buff.is.tue.mpg.de/](http://buff.is.tue.mpg.de/)", "variants": ["BUFF"], "title": "Detailed, Accurate, Human Shape Estimation from Clothed 3D Scan Sequences"}
{"id": "MNIST-8M", "contents": "MNIST8M is derived from the MNIST dataset by applying random deformations and translations to the dataset.\r\n\r\nSource: [Scalable and Sustainable Deep Learningvia Randomized Hashing](https://arxiv.org/abs/1602.08194)", "variants": ["MNIST-8M"], "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks"}
{"id": "NomBank", "contents": "**NomBank** is an annotation project at New York University that is related to the PropBank project at the University of Colorado.  The goal is to mark the sets of arguments that cooccur with nouns in the PropBank Corpus (the Wall Street Journal Corpus of the Penn Treebank), just as PropBank records such information for verbs.  As a side effect of the annotation process, the authors are producing a number of other resources including various dictionaries, as well as PropBank style lexical entries called frame files. These resources help the user label the various arguments and adjuncts of the head nouns with roles (sets of argument labels for each sense of each noun).  NYU and U of Colorado are making a coordinated effort to insure that, when possible, role definitions are consistent across parts of speech. For example, PropBank's frame file for the verb \"decide\" was used in the annotation of the noun \"decision\".\r\n\r\nSource: [Nombank](https://nlp.cs.nyu.edu/meyers/NomBank.html)\r\nImage Source: [https://nlp.cs.nyu.edu/meyers/NomBank.html](https://nlp.cs.nyu.edu/meyers/NomBank.html)", "variants": ["NomBank"], "title": "The NomBank Project: An Interim Report"}
{"id": "ScanNet", "contents": "**ScanNet** is an instance-level indoor RGB-D dataset that includes both 2D and 3D data. It is a collection of labeled voxels rather than points or objects. Up to now, ScanNet v2, the newest version of ScanNet, has collected 1513 annotated scans with an approximate 90% surface coverage. In the semantic segmentation task, this dataset is marked in 20 classes of annotated 3D voxelized objects.\r\n\r\nSource: [A Review of Point Cloud Semantic Segmentation](https://arxiv.org/abs/1908.08854)\r\nImage Source: [http://www.scan-net.org/](http://www.scan-net.org/)", "variants": ["ScanNet", "ScanNetV1", "ScanNet(v2)"], "title": "ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes"}
{"id": "WebKB", "contents": "**WebKB** is a dataset that includes web pages from computer science departments of various universities. 4,518 web pages are categorized into 6 imbalanced categories (Student, Faculty, Staff, Department, Course, Project). Additionally there is Other miscellanea category that is not comparable to the rest.\r\n\r\nSource: [Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation](https://arxiv.org/abs/1606.04429)", "variants": ["WebKB"], "title": "The IAM-database: an English sentence database for offline handwriting recognition"}
{"id": "Proto Summ", "contents": "This is a large-scale court judgment dataset, where each judgment is a summary of the case description with a patternized style. It contains 2,003,390 court judgment documents. The case description is used as the input, and the court judgment as the summary. The average lengths of the input documents and summaries are 595.15 words and 273.57 words respectively.\n\nSource: [https://arxiv.org/pdf/1909.08837.pdf](https://arxiv.org/pdf/1909.08837.pdf)", "variants": ["Proto Summ"], "title": "How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing"}
{"id": "VisDial", "contents": "**Visual Dialog** (**VisDial**) dataset contains human annotated questions based on images of MS COCO dataset. This dataset was developed by pairing two subjects on Amazon Mechanical Turk to chat about an image. One person was assigned the job of a ‘questioner’ and the other person acted as an ‘answerer’. The questioner sees only the text description of an image (i.e., an image caption from MS COCO dataset) and the original image remains hidden to the questioner. Their task is to ask questions about this hidden image to “imagine the scene better”. The answerer sees the image, caption and answers the questions asked by the questioner. The two of them can continue the conversation by asking and answering questions for 10 rounds at max.\r\n\r\n**VisDial v1.0** contains 123K dialogues on MS COCO (2017 training set) for training split, 2K dialogues with validation images for validation split and 8K dialogues on test set for test-standard set. The previously released v0.5 and v0.9 versions of VisDial dataset (corresponding to older splits of MS COCO) are considered deprecated.\r\n\r\nSource: [Granular Multimodal Attention Networks for Visual Dialog](https://arxiv.org/abs/1910.05728)\r\nImage Source: [https://arxiv.org/pdf/1611.08669.pdf](https://arxiv.org/pdf/1611.08669.pdf)", "variants": ["VisDial", "Visual Dialog  v0.9", "Visual Dialog v1.0 test-std", "VisDial v0.9 val", "VisDial v1.0 test-std", "Visual Dialog v0.9", "Visual Dialog v1.0"], "title": "Visual Dialog"}
{"id": "CosmoFlow", "contents": "The latest CosmoFlow dataset includes around 10,000 cosmological N-body dark matter simulations. The simulations are run using MUSIC to generate the initial conditions, and are evolved with pyCOLA, a multithreaded Python/Cython N-body code. The output of these simulations is then binned into a 3D histogram of particle counts in a cube of size 512x512x512, which is sampled at 4 different redshifts.", "variants": ["CosmoFlow"], "title": "CosmoFlow: Using Deep Learning to Learn the Universe at Scale"}
{"id": "AeroRIT", "contents": "AeroRIT is a hyperspectral dataset to facilitate aerial hyperspectral scene understanding.\r\n\r\nSource: [AeroRIT: A New Scene for Hyperspectral Image Analysis](/paper/aerorit-a-new-scene-for-hyperspectral-image)\r\nImage Source: [https://github.com/aneesh3108/AeroRIT](https://github.com/aneesh3108/AeroRIT)", "variants": ["AeroRIT"], "title": "AeroRIT: A New Scene for Hyperspectral Image Analysis"}
{"id": "COS960", "contents": "A benchmark dataset with 960 pairs of Chinese wOrd Similarity, where all the words have two morphemes in three Part of Speech (POS) tags with their human annotated similarity rather than relatedness. \r\n\r\nSource: [COS960: A Chinese Word Similarity Dataset of 960 Word Pairs](/paper/190600247)", "variants": ["COS960"], "title": "COS960: A Chinese Word Similarity Dataset of 960 Word Pairs"}
{"id": "CrossTask", "contents": "**CrossTask** dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\r\n\r\nSource: [CrossTask](https://github.com/DmZhukov/CrossTask)\r\nImage Source: [https://arxiv.org/pdf/1903.08225v2.pdf](https://arxiv.org/pdf/1903.08225v2.pdf)", "variants": ["CrossTask"], "title": "Cross-Task Weakly Supervised Learning From Instructional Videos"}
{"id": "Virtual KITTI", "contents": "**Virtual KITTI** is a photo-realistic synthetic video dataset designed to learn and evaluate computer vision models for several video understanding tasks: object detection and multi-object tracking, scene-level and instance-level semantic segmentation, optical flow, and depth estimation.\r\n\r\nVirtual KITTI contains 50 high-resolution monocular videos (21,260 frames) generated from five different virtual worlds in urban settings under different imaging and weather conditions. These worlds were created using the Unity game engine and a novel real-to-virtual cloning method. These photo-realistic synthetic videos are automatically, exactly, and fully annotated for 2D and 3D multi-object tracking and at the pixel level with category, instance, flow, and depth labels (cf. below for download links).\r\n\r\nSource: [https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/](https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/)\r\nImage Source: [https://arxiv.org/pdf/1605.06457.pdf](https://arxiv.org/pdf/1605.06457.pdf)", "variants": ["Virtual KITTI to BDD100K", "Virtual KITTI 2", "Virtual KITTI"], "title": "VirtualWorlds as Proxy for Multi-object Tracking Analysis"}
{"id": "COCO-Tasks", "contents": "Comprises about 40,000 images where the most suitable objects for 14 tasks have been annotated.\r\n\r\nSource: [What Object Should I Use? - Task Driven Object Detection](/paper/what-object-should-i-use-task-driven-object)", "variants": ["COCO-Tasks"], "title": "What Object Should I Use? - Task Driven Object Detection"}
{"id": "Controversial News Topic Datasets", "contents": "Corpus of controversial news articles extracted from Twitter. Contains news from three different topics: Beef Ban – controversy over the slaughter and sale of beef on religious grounds (1543\r\narticles) is localised to a particular region, mainly Indian subcontinent, while Gun Control – restrictions on carrying, using, or purchasing firearms (6494 articles) and Capital Punishment – use of the death penalty (7905 articles) are\r\ntopical in various regions around the world.\r\n\r\nSource: [Comparative Document Summarisation via Classification](/paper/comparative-document-summarisation-via)", "variants": ["Controversial News Topic Datasets"], "title": "Comparative Document Summarisation via Classification"}
{"id": "CLaRO", "contents": "CLaRO is a new dataset of 234 Competency Questions that had been processed automatically into 106 patterns. The coverage of CLaRO, with its 93 main templates and 41 linguistic variants, is about 90% for unseen questions.\r\n\r\nSource: [CLaRO: a Data-driven CNL for Specifying Competency Questions](https://arxiv.org/pdf/1907.07378)", "variants": ["CLaRO"], "title": "CLaRO: a Data-driven CNL for Specifying Competency Questions"}
{"id": "MALF", "contents": "The **MALF** dataset is a large dataset with 5,250 images annotated with multiple facial attributes and it is specifically constructed for fine grained evaluation.\n\nSource: [Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results](https://arxiv.org/abs/1804.10275)\nImage Source: [http://www.cbsr.ia.ac.cn/faceevaluation/](http://www.cbsr.ia.ac.cn/faceevaluation/)", "variants": ["MALF"], "title": "Fine-grained evaluation on face detection in the wild"}
{"id": "JHU CoSTAR Block Stacking Dataset", "contents": "Involves data where a robot interacts with 5.1 cm colored blocks to complete an order-fulfillment style block stacking task. It contains dynamic scenes and real time-series data in a less constrained environment than comparable datasets. There are nearly 12,000 stacking attempts and over 2 million frames of real data. \r\n\r\nSource: [The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints](/paper/training-frankensteins-creature-to-stack)", "variants": ["JHU CoSTAR Block Stacking Dataset"], "title": "The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints"}
{"id": "Flickr30k", "contents": "The **Flickr30k** dataset contains 31,000 images collected from Flickr, together with 5 reference sentences provided by human annotators.\r\n\r\nSource: [Guiding Long-Short Term Memory for Image Caption Generation](https://arxiv.org/abs/1509.04942)\r\n\r\nImage Source: [Dual-Path Convolutional Image-Text Embedding with Instance Loss\r\n](https://arxiv.org/abs/1711.05535)", "variants": ["Flickr30k", "Flickr", "Flickr30k Captions test", "Flickr30K 1K test"], "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}
{"id": "CATS", "contents": "A dataset consisting of stereo thermal, stereo color, and cross-modality image pairs with high accuracy ground truth (< 2mm) generated from a LiDAR. The authors scanned 100 cluttered indoor and 80 outdoor scenes featuring challenging environments and conditions. CATS contains approximately 1400 images of pedestrians, vehicles, electronics, and other thermally interesting objects in different environmental conditions, including nighttime, daytime, and foggy scenes.\r\n\r\nSource: [CATS: A Color and Thermal Stereo Benchmark](/paper/cats-a-color-and-thermal-stereo-benchmark)", "variants": ["Cats-and-Dogs", "CATS"], "title": "CATS: A Color and Thermal Stereo Benchmark"}
{"id": "3R-Scan", "contents": "A novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. \r\n\r\nSource: [RIO: 3D Object Instance Re-Localization in Changing Indoor Environments](/paper/rio-3d-object-instance-re-localization-in)", "variants": ["3R-Scan"], "title": "RIO: 3D Object Instance Re-Localization in Changing Indoor Environments"}
{"id": "Chinese AI and Law (CAIL) 2018", "contents": "Large-scale Chinese legal dataset for judgment prediction. \\dataset contains more than 2.6  million criminal cases published by the Supreme People's Court of China, which are several times larger than other datasets in existing works on judgment prediction.\r\n\r\nSource: [CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction](/paper/cail2018-a-large-scale-legal-dataset-for)", "variants": ["Chinese AI and Law (CAIL) 2018"], "title": "CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction"}
{"id": "PASCAL Context", "contents": "The **PASCAL Context** dataset is an extension of the PASCAL VOC 2010 detection challenge, and it contains pixel-wise labels for all training images. It contains more than 400 classes (including the original 20 classes plus backgrounds from PASCAL VOC segmentation), divided into three categories (objects, stuff, and hybrids). Many of the object categories of this dataset are too sparse and; therefore, a subset of 59 frequent classes are usually selected for use.\r\n\r\nSource: [Image Segmentation Using Deep Learning:A Survey](https://arxiv.org/abs/2001.05566)\r\nImage Source: [https://cs.stanford.edu/~roozbeh/pascal-context/](https://cs.stanford.edu/~roozbeh/pascal-context/)", "variants": ["PASCAL Context"], "title": "The Role of Context for Object Detection and Semantic Segmentation in the Wild"}
{"id": "DPC-Captions", "contents": "This is an open-source image captions dataset for the aesthetic evaluation of images.\nThe dataset is called **DPC-Captions**, which contains comments of up to five aesthetic attributes of one image through knowledge transfer from a full-annotated small-scale dataset.\n\nSource: [https://github.com/BestiVictory/DPC-Captions](https://github.com/BestiVictory/DPC-Captions)", "variants": ["DPC-Captions"], "title": "Aesthetic Attributes Assessment of Images"}
{"id": "ADE20K", "contents": "The **ADE20K** semantic segmentation dataset contains more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels. There are totally 150 semantic categories, which include stuffs like sky, road, grass, and discrete objects like person, car, bed.\r\n\r\nSource: [Cooperative Image Segmentation and Restoration in Adverse Environmental Conditions](https://arxiv.org/abs/1911.00679)\r\nImage Source: [https://groups.csail.mit.edu/vision/datasets/ADE20K/](https://groups.csail.mit.edu/vision/datasets/ADE20K/)", "variants": ["ADE20K", "ADE20K Labels-to-Photos", "ADE20K-Outdoor Labels-to-Photos", "ADE20K val"], "title": "Scene Parsing through ADE20K Dataset"}
{"id": "BlendedMVS", "contents": "**BlendedMVS** is a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. The dataset was created by applying a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, these mesh models were rendered to color images and depth maps. \r\n\r\nSource: [BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks](/paper/blendedmvs-a-large-scale-dataset-for)", "variants": ["BlendedMVS"], "title": "BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks"}
{"id": "TutorialVQA", "contents": "**TutorialVQA** is a new type of dataset used to find answer spans in tutorial videos. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software.\r\n\r\nSource: [TutorialVQA: Question Answering Dataset for Tutorial Videos](https://arxiv.org/pdf/1912.01046v2.pdf)", "variants": ["TutorialVQA"], "title": "TutorialVQA: Question Answering Dataset for Tutorial Videos"}
{"id": "CLAD", "contents": "CLAD (Compled and Long Activities Dataset) is an activity dataset which exhibits real-life and diverse scenarios of complex, temporally-extended human activities and actions. The dataset consists of a set of videos of actors performing everyday activities in a natural and unscripted manner. The dataset was recorded using a static Kinect 2 sensor which is commonly used on many robotic platforms. The dataset comprises of RGB-D images, point cloud data, automatically generated skeleton tracks in addition to crowdsourced annotations. \r\n\r\nSource: [Complex and Long Activities Dataset](http://archive.researchdata.leeds.ac.uk/254/)", "variants": ["CLAD"], "title": "CLAD: A Complex and Long Activities Dataset with Rich Crowdsourced Annotations"}
{"id": "DUTS", "contents": "**DUTS** is a saliency detection dataset containing 10,553 training images and 5,019 test images. All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set. Both the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects.\r\n\r\nSource: [http://saliencydetection.net/duts/](http://saliencydetection.net/duts/)\r\nImage Source: [https://ieeexplore.ieee.org/document/8099887](https://ieeexplore.ieee.org/document/8099887)", "variants": ["DUTS-test", "DUTS-TE", "DUTS"], "title": "Learning to Detect Salient Objects with Image-Level Supervision"}
{"id": "BosphorusSign22k", "contents": "BosphorusSign22k is a benchmark dataset for vision-based user-independent isolated Sign Language Recognition (SLR). The dataset is based on the BosphorusSign (Camgoz et al., 2016c) corpus which was collected with the purpose of helping both linguistic and computer science communities. It contains isolated videos of Turkish Sign Language glosses from three different domains: Health, finance and commonly used everyday signs. Videos in this dataset were performed by six native signers, which makes this dataset valuable for user independent sign language studies.\r\n\r\nSource: [BosphorusSign22k Sign Language Recognition Dataset](https://arxiv.org/abs/2004.01283)", "variants": ["BosphorusSign22k"], "title": "BosphorusSign22k Sign Language Recognition Dataset"}
{"id": "WikiSplit", "contents": "Contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task.\r\n\r\nSource: [Learning To Split and Rephrase From Wikipedia Edit History](/paper/learning-to-split-and-rephrase-from-wikipedia)", "variants": ["WikiSplit1.0", "WikiSplit"], "title": "Learning To Split and Rephrase From Wikipedia Edit History"}
{"id": "WikiArt", "contents": "**WikiArt** contains painting from 195 different artists. The dataset has 42129 images for training and 10628 images for testing.\r\n\r\nSource: [Adding New Tasks to a Single Network with Weight Transformations using Binary Masks](https://arxiv.org/abs/1805.11119)\r\nImage Source: [https://towardsdatascience.com/the-non-treachery-of-dataset-df1f6cbe577e](https://towardsdatascience.com/the-non-treachery-of-dataset-df1f6cbe577e)", "variants": ["Wikiart (Fine-grained 6 Tasks)", "WikiArt"], "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature"}
{"id": "RUSLAN", "contents": "RUSLAN is a Russian spoken language corpus for text-to-speech task. RUSLAN contains 22,200 audio samples with text annotations – more than 31 hours of high-quality speech of one person – being one of the largest annotated Russian corpus in terms of speech duration for a single speaker. \r\n\r\nSource: [RUSLAN: Russian Spoken Language Corpus for Speech Synthesis](/paper/ruslan-russian-spoken-language-corpus-for)", "variants": ["RUSLAN"], "title": "RUSLAN: Russian Spoken Language Corpus for Speech Synthesis"}
{"id": "CMU DoG", "contents": "This is a document grounded dataset for text conversations. \"Document Grounded Conversations\" are conversations that are about the contents of a specified document. In this dataset the specified documents are Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation.\r\n\r\nSource: [https://github.com/festvox/datasets-CMU_DoG](https://github.com/festvox/datasets-CMU_DoG)\r\nImage Source: [https://arxiv.org/pdf/1809.07358v1.pdf](https://arxiv.org/pdf/1809.07358v1.pdf)", "variants": ["CMU DoG"], "title": "A Dataset for Document Grounded Conversations"}
{"id": "RobotPush", "contents": "**RobotPush** is a dataset for object singulation – the task of separating cluttered objects through physical interaction. The dataset contains 3456 training images with labels and 1024 validation images with labels. It consists of simulated and real-world data collected from a PR2 robot that equipped with a Kinect 2 camera. The dataset also contains ground truth instance segmentation masks for 110 images in the test set.\r\n\r\nSource: [http://robotpush.cs.uni-freiburg.de/](http://robotpush.cs.uni-freiburg.de/)\nImage Source: [http://robotpush.cs.uni-freiburg.de/](http://robotpush.cs.uni-freiburg.de/)", "variants": ["RobotPush"], "title": "Learning to Singulate Objects using a Push Proposal Network"}
{"id": "ScenicOrNot", "contents": "ScenicOrNot (SoN) is a dataset of 185,548 images with associated natural beauty rating histograms. Each image in the dataset was rated at least five times. The images also have metadata like title and location.\r\n\r\nSource: [Understanding and Mapping Natural Beauty](https://arxiv.org/abs/1612.03142)", "variants": ["ScenicOrNot"], "title": "Understanding and Mapping Natural Beauty"}
{"id": "SIDOD", "contents": "SIDOD is a new, publicly-available image dataset generated by the NVIDIA Deep Learning Data Synthesizer intended for use in object detection, pose estimation, and tracking applications. This dataset contains 144k stereo image pairs that synthetically combine 18 camera viewpoints of three photorealistic virtual environments with up to 10 objects (chosen randomly from the 21 object models of the YCB dataset) and flying distractors. \r\n\r\nSource: [SIDOD: A Synthetic Image Dataset for 3D Object Pose Recognition with Distractors](/paper/sidod-a-synthetic-image-dataset-for-3d-object)\r\nImage Source: [https://research.nvidia.com/publication/2019-06_SIDOD%3A-A-Synthetic](https://research.nvidia.com/publication/2019-06_SIDOD%3A-A-Synthetic)", "variants": ["SIDOD"], "title": "SIDOD: A Synthetic Image Dataset for 3D Object Pose Recognition With Distractors"}
{"id": "Human3.6M", "contents": "The **Human3.6M** dataset is one of the largest motion capture datasets, which consists of 3.6 million human poses and corresponding images captured by a high-speed motion capture system. There are 4 high-resolution progressive scan cameras to acquire video data at 50 Hz. The dataset contains activities by 11 professional actors in 17 scenarios: discussion, smoking, taking photo, talking on the phone, etc., as well as provides accurate 3D joint positions and high-resolution videos.\r\n\r\nSource: [Space-Time Representation of People Based on 3D Skeletal Data: A Review](https://arxiv.org/abs/1601.01006)\r\n\r\nImage Source: [Yu et al](https://www.researchgate.net/publication/320271480_Coupled_Multiview_Auto-Encoders_with_Locality-Sensitivity_for_3D_Human_Pose_Estimation)", "variants": ["Human3.6M", "Human3 6M", "Human 3.6 M"], "title": "Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments"}
{"id": "Common Voice", "contents": "**Common Voice** is an audio dataset that consists of a unique MP3 and corresponding text file. There are 9,283 recorded hours in the dataset. The dataset also includes demographic metadata like age, sex, and accent. The dataset consists of 7,335 validated hours in 60 languages.", "variants": ["Common Voice"], "title": "Common Voice: A Massively-Multilingual Speech Corpus"}
{"id": "CADC", "contents": "Collected with the Autonomoose autonomous vehicle platform, based on a modified Lincoln MKZ.\r\n\r\nSource: [Canadian Adverse Driving Conditions Dataset](/paper/canadian-adverse-driving-conditions-dataset)", "variants": ["CADC"], "title": "Canadian Adverse Driving Conditions Dataset"}
{"id": "ARCD", "contents": "Composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). \r\n\r\nSource: [Neural Arabic Question Answering](/paper/neural-arabic-question-answering)", "variants": ["ARCD"], "title": "Neural Arabic Question Answering"}
{"id": "NavigationNet", "contents": "NavigationNet is a computer vision dataset and benchmark to allow the utilization of deep reinforcement learning on scene-understanding-based indoor navigation.\r\n\r\nSource: [NavigationNet: A Large-scale Interactive Indoor Navigation Dataset](/paper/navigationnet-a-large-scale-interactive)\r\nImage Source: [https://arxiv.org/pdf/1808.08374v1.pdf](https://arxiv.org/pdf/1808.08374v1.pdf)", "variants": ["NavigationNet"], "title": "NavigationNet: A Large-scale Interactive Indoor Navigation Dataset"}
{"id": "Penn Treebank", "contents": "The English **Penn Treebank** (**PTB**) corpus, and in particular the section of the corpus corresponding to the articles of Wall Street Journal (WSJ), is one of the most known and used corpus for the evaluation of models for sequence labelling. The task consists of annotating each word with its Part-of-Speech tag. In the most common split of this corpus,  sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).\r\nThe corpus is also commonly used for character-level and word-level Language Modelling.\r\n\r\nSource: [Seq2Biseq: Bidirectional Output-wise Recurrent Neural Networks for Sequence Modelling](https://arxiv.org/abs/1904.04733)\r\nImage Source: [https://dl.acm.org/doi/10.5555/972470.972475](https://dl.acm.org/doi/10.5555/972470.972475)", "variants": ["Penn Treebank", "Penn Treebank (Character Level)", "Penn Treebank (Word Level)", "Penn Treebank (Character Level) 3x1000 LSTM - 500 Epochs", "PTB", "PTB dataset, ECG lead II"], "title": "ImageNet: A large-scale hierarchical image database"}
{"id": "Email-EU", "contents": "EmailEU is a directed temporal network constructed from email exchanges in a large European research institution for a 803-day period. It contains 986 email addresses as nodes and 332,334 emails as edges with timestamps. There are 42 ground truth departments in the dataset.\n\nSource: [gl2vec: Learning Feature Representation Using Graphlets for Directed Networks](https://arxiv.org/abs/1812.05473)", "variants": ["Email-EU"], "title": "Local Higher-Order Graph Clustering"}
{"id": "PHD²", "contents": "The dataset contains information on what video segments a specific user considers a highlight. Having this kind of data allows for strong personalization models, as specific examples of what a user is interested in help models obtain a fine-grained understanding of that specific user.\r\n\r\nThe data consists of YouTube videos, from which gifs.com users manually extracted their highlights, by creating GIFs from a segment of the full video. Thus, the dataset is similar to PHD-GIFS, with two major differences.\r\n\r\n- Each selection is associated with a user, which is what allows personalization.\r\n- instead of visual matching to find the position in the video from which a GIF was selected, PHD-GIFS uses the timestamps. Thus, the ground truth is free from any alignment errors.\r\n\r\nThe training set contains highlights from 12,972 users. The test set contains highlights from 850 users. \r\n\r\nSource: [Personalized Highlight Detection Dataset](https://github.com/gifs/personalized-highlights-dataset)", "variants": ["PHD²"], "title": "PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation"}
{"id": "PubMed PICO Element Detection Dataset", "contents": "PICO is a framework to formulate a well-defined focused clinical question. This framework identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). The PubMed PICO Element Detection dataset is a dataset for evaluating models that automatically detect PICO elements.\n\nSource: [https://github.com/jind11/PubMed-PICO-Detection](https://github.com/jind11/PubMed-PICO-Detection)", "variants": ["PubMed PICO Element Detection Dataset"], "title": "PICO Element Detection in Medical Text via Long Short-Term Memory Neural Networks"}
{"id": "SCUT-CTW1500", "contents": "The **SCUT-CTW1500** dataset contains 1,500 images: 1,000 for training and 500 for testing. In particular, it provides 10,751 cropped text instance images, including 3,530 with curved text. The images are manually harvested from the Internet, image libraries such as Google Open-Image, or phone cameras. The dataset contains a lot of horizontal and multi-oriented text.\r\n\r\nSource: [Text Recognition in the Wild: A Survey](https://arxiv.org/abs/2005.03492)\r\nImage Source: [https://github.com/Yuliang-Liu/Curve-Text-Detector](https://github.com/Yuliang-Liu/Curve-Text-Detector)", "variants": ["SCUT-CTW1500"], "title": "Detecting Curve Text in the Wild: New Dataset and New Solution"}
{"id": "REFUGE Challenge", "contents": "REFUGE Challenge provides a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one.\r\n\r\nSource: [REFUGE Challenge: A Unified Framework for Evaluating Automated Methods for Glaucoma Assessment from Fundus Photographs](https://arxiv.org/pdf/1910.03667v1.pdf)\r\nImage Source: [Orlando et al](https://arxiv.org/pdf/1910.03667v1.pdf)", "variants": ["REFUGE Challenge"], "title": "REFUGE Challenge: A Unified Framework for Evaluating Automated Methods for Glaucoma Assessment from Fundus Photographs"}
{"id": "comma 2k19", "contents": "comma 2k19 is a dataset of over 33 hours of commute in California's 280 highway. This means 2019 segments, 1 minute long each, on a 20km section of highway driving between California's San Jose and San Francisco. The dataset was collected using comma EONs that have sensors similar to those of any modern smartphone including a road-facing camera, phone GPS, thermometers and a 9-axis IMU. \r\n\r\nSource: [A Commute in Data: The comma2k19 Dataset](/paper/a-commute-in-data-the-comma2k19-dataset)", "variants": ["comma 2k19"], "title": "A Commute in Data: The comma2k19 Dataset"}
{"id": "WikiReading", "contents": "WikiReading is a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs).\r\n\r\nSource: [WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia](https://www.aclweb.org/anthology/P16-1145.pdf)\r\nImage Source: [Hewlett et al](https://arxiv.org/pdf/1608.03542v2.pdf)", "variants": ["WikiReading"], "title": "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia"}
{"id": "Houses Dataset", "contents": "This dataset is used for predicting house prices from both images and textual information. It is composed of 535 sample houses from California, USA.\n\nSource: [https://github.com/emanhamed/Houses-dataset](https://github.com/emanhamed/Houses-dataset)", "variants": ["Houses Dataset"], "title": "House price estimation from visual and textual features"}
{"id": "LAMBADA", "contents": "The **LAMBADA** (LAnguage Modeling Broadened to Account for Discourse Aspects) benchmark is an open-ended cloze task which consists of about 10,000 passages from BooksCorpus where a missing target word is predicted in the last sentence of each passage. The missing word is constrained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples were further filtered to ensure that missing words could not be guessed without the context, ensuring that models attempting the dataset would need to reason over the entire paragraph to answer questions.\r\n\r\nSource: [Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)\r\nImage Source: [https://arxiv.org/pdf/1606.06031.pdf](https://arxiv.org/pdf/1606.06031.pdf)", "variants": ["LAMBADA"], "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}
{"id": "RIT-18", "contents": "The RIT-18 dataset was built for the semantic segmentation of remote sensing imagery. It was collected with the Tetracam Micro-MCA6 multispectral imaging sensor flown on-board a DJI-1000 octocopter. \r\n\r\nThe features this dataset include 1) very-high resolution multispectral imagery from a drone, 2) six-spectral VNIR bands, and 3) 18 object classes (plus background) with a severely unbalanced class distribution.\r\n\r\nSource: [Algorithms for Semantic Segmentation of Multispectral Remote Sensing Imagery using Deep Learning](/paper/algorithms-for-semantic-segmentation-of)\r\nImage Source: [https://github.com/rmkemker/RIT-18](https://github.com/rmkemker/RIT-18)", "variants": ["RIT-18"], "title": "Algorithms for Semantic Segmentation of Multispectral Remote Sensing Imagery using Deep Learning"}
{"id": "SUN Attribute", "contents": "The **SUN Attribute** dataset consists of 14,340 images from 717 scene categories, and each category is annotated with a taxonomy of 102 discriminate attributes. The dataset can be used for high-level scene understanding and fine-grained scene recognition.\r\n\r\nSource: [Zero-Shot Learning with Multi-Battery Factor Analysis](https://arxiv.org/abs/1606.09349)\r\nImage Source: [https://cs.brown.edu/~gmpatter/sunattributes.html](https://cs.brown.edu/~gmpatter/sunattributes.html)", "variants": ["SUN Attribute"], "title": "The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding"}
{"id": "medisim", "contents": "**medisim** is a collection of new large-scale medical term similarity datasets based on SNOMED-CT.\n\nSource: [https://github.com/babylonhealth/medisim](https://github.com/babylonhealth/medisim)", "variants": ["medisim"], "title": "Can Embeddings Adequately Represent Medical Terminology? New Large-Scale Medical Term Similarity Datasets Have the Answer!"}
{"id": "Slim", "contents": "This dataset consists of virtual scenes rendered in MuJoCo with multiple views each presented in multiple modalities: image, and synthetic or natural language descriptions. Each scene consists of two or three objects placed on a square walled room, and for each of the 10 camera viewpoint the authors rendered a 3D view of the scene as seen from that viewpoint as well as a synthetically generated description of the scene.\r\n\r\nSource: [GitHub](https://github.com/deepmind/slim-dataset)", "variants": ["Slim"], "title": "Encoding Spatial Relations from Natural Language"}
{"id": "MIT Traffic", "contents": "**MIT Traffic** is a dataset for research on activity analysis and crowded scenes. It includes a traffic video sequence of 90 minutes long. It is recorded by a stationary camera. The size of the scene is 720 by 480 and it is divided into 20 clips.\r\n\r\nSource: [MIT Traffic Dataset](http://mmlab.ie.cuhk.edu.hk/datasets/mit_traffic/index.html)", "variants": ["MIT Traffic"], "title": "Unsupervised Activity Perception in Crowded and Complicated Scenes Using Hierarchical Bayesian Models"}
{"id": "Open Entity", "contents": "The **Open Entity** dataset is a collection of about 6,000 sentences with fine-grained entity types annotations. The entity types are free-form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Sentences were sampled from Gigaword, OntoNotes and web articles. On average each sentence has 5 labels.\r\n\r\nSource: [Ultra-Fine Entity Typing](https://paperswithcode.com/paper/ultra-fine-entity-typing/)\r\nImage Source: [Ultra-Fine Entity Typing](https://paperswithcode.com/paper/ultra-fine-entity-typing/)", "variants": [" Open Entity", "Open Entity"], "title": "Ultra-Fine Entity Typing"}
{"id": "Goldfinch", "contents": "Goldfinch is a dataset for fine-grained recognition challenges. It contains a list of bird, butterfly, aircraft, and dog categories with relevant Google image search and Flickr search URLs. In addition, it also includes a set of active learning annotations on dog categories.\r\n\r\nSource: [The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition](/paper/the-unreasonable-effectiveness-of-noisy-data)", "variants": ["Goldfinch"], "title": "The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition"}
{"id": "INRIA Aerial Image Labeling", "contents": "The **INRIA Aerial Image Labeling** dataset is comprised of 360 RGB tiles of 5000×5000px with a spatial resolution of 30cm/px on 10 cities across the globe. Half of the cities are used for training and are associated to a public ground truth of building footprints. The rest of the dataset is used only for evaluation with a hidden ground truth. The dataset was constructed by combining public domain imagery and public domain official building footprints.\r\n\r\nSource: [Distance transform regression for spatially-aware deep semantic segmentation](https://arxiv.org/abs/1909.01671)\r\nImage Source: [https://project.inria.fr/aerialimagelabeling/](https://project.inria.fr/aerialimagelabeling/)", "variants": ["INRIA Aerial Image Labeling"], "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}
{"id": "Aesthetics Text Corpus", "contents": "An exhaustive list of stop lemmas created from 12 corpora across multiple domains, consisting of over 13 million words, from which more than 200,000 lemmas were generated, and 11 publicly available stop word lists comprising over 1000 words, from which nearly 400 unique lemmas were generated.\r\n\r\nSource: [Novel Language Resources for Hindi: An Aesthetics Text Corpus and a Comprehensive Stop Lemma List](/paper/novel-language-resources-for-hindi-an)", "variants": ["Aesthetics Text Corpus"], "title": "Novel Language Resources for Hindi: An Aesthetics Text Corpus and a Comprehensive Stop Lemma List"}
{"id": "Mapillary Vistas Dataset", "contents": "Mapillary Vistas Dataset is a diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world.\r\n\r\nSource: [Mapillary Vistas Dataset](https://www.mapillary.com/dataset/vistas?lat=20&lng=0&z=1.5&pKey=pBBmjuJ8yU1r2ROYRzWmFg)\r\n\r\nImage Source: [Neuhold et al](https://openaccess.thecvf.com/content_ICCV_2017/papers/Neuhold_The_Mapillary_Vistas_ICCV_2017_paper.pdf)", "variants": ["Mapillary val", "Mapillary Vistas Dataset"], "title": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes"}
{"id": "Moviescope", "contents": "Moviescope is a large-scale dataset of 5,000 movies with corresponding video trailers, posters, plots and metadata. Moviescope is based on the IMDB 5000 dataset consisting of 5.043 movie records. It is augmented by crawling video trailers associated with each movie from YouTube and text plots from Wikipedia.\r\n\r\nSource: [Moviescope: Large-scale Analysis of Movies using Multiple Modalities](/paper/moviescope-large-scale-analysis-of-movies)", "variants": ["Moviescope"], "title": "Moviescope: Large-scale Analysis of Movies using Multiple Modalities"}
{"id": "CAMO", "contents": "Camouflaged Object (CAMO) dataset specifically designed for the task of camouflaged object segmentation. We focus on two categories, i.e., naturally camouflaged objects and artificially camouflaged objects, which usually correspond to animals and humans in the real world, respectively. Camouflaged object images consists of 1250 images (1000 images for the training set and 250 images for the testing set). Non-camouflaged object images are collected from the MS-COCO dataset (1000 images for the training set and 250 images for the testing set). CAMO has objectness mask ground-truth.", "variants": ["CAMO"], "title": "Anabranch network for camouflaged object segmentation"}
{"id": "Large Age-Gap", "contents": "**Large Age-Gap (LAG)** is a dataset for face verification, The dataset contains 3,828 images of 1,010 celebrities. For each identity at least one child/young image and one adult/old image are present.", "variants": ["Large Age-Gap"], "title": "Large age-gap face verification by feature injection in deep networks"}
{"id": "ImageNet-A", "contents": "The **ImageNet-A** dataset consists of real-world, unmodified, and naturally occurring examples that are misclassified by ResNet models.\r\n\r\nSource: [On Robustness and Transferability of Convolutional Neural Networks](https://arxiv.org/abs/2007.08558)\r\nImage Source: [https://github.com/hendrycks/natural-adv-examples](https://github.com/hendrycks/natural-adv-examples)", "variants": ["ImageNet-A"], "title": "Natural Adversarial Examples"}
{"id": "Dialog-based Language Learning dataset", "contents": "Dialog-based Language Learning dataset is designed to measure how well models can perform at learning as a student given a teacher’s textual responses to the student’s answer (as well as potentially receiving an external real-valued reward signal). \r\n\r\nSource: [Dialog-based Language Learning dataset](https://research.fb.com/downloads/babi/)", "variants": ["Dialog-based Language Learning dataset"], "title": "Dialog-based Language Learning"}
{"id": "BioGRID", "contents": "**BioGRID** is a biomedical interaction repository with data compiled through comprehensive curation efforts. The current index is version 4.2.192 and searches 75,868 publications for 1,997,840 protein and genetic interactions, 29,093 chemical interactions and 959,750 post translational modifications from major model organism species.\r\n\r\nSource: [https://thebiogrid.org/](https://thebiogrid.org/)", "variants": ["BioGRID", "BioGRID(yeast)", "BioGRID (human)"], "title": "BioGRID: a general repository for interaction datasets"}
{"id": "Jericho", "contents": "Jericho is a learning environment for man-made Interactive Fiction (IF) games.\r\n\r\nSource: [Interactive Fiction Games: A Colossal Adventure](https://arxiv.org/pdf/1909.05398.pdf)", "variants": ["Jericho"], "title": "Interactive Fiction Games: A Colossal Adventure"}
{"id": "NYTWIT", "contents": "A collection of over 2,500 novel English words published in the New York Times between November 2017 and March 2019, manually annotated for their class of novelty (such as lexical derivation, dialectal variation, blending, or compounding). \r\n\r\nSource: [NYTWIT: A Dataset of Novel Words in the New York Times](/paper/nytwit-a-dataset-of-novel-words-in-the-new)", "variants": ["NYTWIT"], "title": "NYTWIT: A Dataset of Novel Words in the New York Times"}
{"id": "EgoHands", "contents": "The EgoHands dataset contains 48 Google Glass videos of complex, first-person interactions between two people. The main intention of this dataset is to enable better, data-driven approaches to understanding hands in first-person computer vision. The dataset offers\r\n\r\n* high quality, pixel-level segmentations of hands\r\n* the possibility to semantically distinguish between the observer’s hands and someone else’s hands, as well as left and right hands\r\n* virtually unconstrained hand poses as actors freely engage in a set of joint activities\r\n* lots of data with 15,053 ground-truth labeled hands\r\n \r\nSource: [Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions](/paper/lending-a-hand-detecting-hands-and)", "variants": ["EgoHands"], "title": "Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions"}
{"id": "Advice Seeking Questions", "contents": "The Advice-Seeking Questions (ASQ) dataset is a collection of personal narratives with advice-seeking questions. The dataset has been split into train, test, heldout sets, with 8865, 2500, 10000 test instances each. This dataset is used to train and evaluate methods that can infer what is the advice-seeking goal behind a personal narrative. This task is formulated as a cloze test, where the goal is to identify which of two advice-seeking questions was removed from a given narrative.\n\nSource: [https://github.com/CornellNLP/ASQ](https://github.com/CornellNLP/ASQ)", "variants": ["Advice Seeking Questions"], "title": "Asking the Right Question: Inferring Advice-Seeking Intentions from Personal Narratives"}
{"id": "ELI5", "contents": "ELI5 is a dataset for long-form question answering. It contains 270K complex, diverse questions that require explanatory multi-sentence answers. Web search results are used as evidence documents to answer each question.\r\n\r\nELI5 is also a task in Dodecadialogue.\r\n\r\nSource: [ELI5](https://facebookresearch.github.io/ELI5/)\r\nImage Source: [https://arxiv.org/pdf/1907.09190v1.pdf](https://arxiv.org/pdf/1907.09190v1.pdf)", "variants": ["ELI5"], "title": "ELI5: Long Form Question Answering"}
{"id": "StockNet", "contents": "The **StockNet** dataset is a comprehensive dataset for stock movement prediction from tweets and historical stock prices.\nIt consists of two-year price movements from 01/01/2014 to 01/01/2016 of 88 stocks, coming from all the 8 stocks in the Conglomerates sector and the top 10 stocks in capital size in each of the other 8 sectors.\n\nSource: [https://github.com/yumoxu/stocknet-dataset](https://github.com/yumoxu/stocknet-dataset)", "variants": ["stocknet", "StockNet"], "title": "Stock Movement Prediction from Tweets and Historical Prices"}
{"id": "ADE-Affordance", "contents": "ADE-Affordance is a new dataset that builds upon ADE20k, which contains annotations enabling such rich visual reasoning. \r\n\r\nSource: [Learning to Act Properly: Predicting and Explaining Affordances from Images](https://arxiv.org/pdf/1712.07576)", "variants": ["ADE-Affordance"], "title": "Learning to Act Properly: Predicting and Explaining Affordances from Images"}
{"id": "YUP++", "contents": "A new and challenging video database of dynamic scenes that more than doubles the size of those previously available. This dataset is explicitly split into two subsets of equal size that contain videos with and without camera motion to allow for systematic study of how this variable interacts with the defining dynamics of the scene per se. \r\n\r\nSource: [Temporal Residual Networks for Dynamic Scene Recognition](/paper/temporal-residual-networks-for-dynamic-scene)", "variants": ["YUP++"], "title": "Temporal Residual Networks for Dynamic Scene Recognition"}
{"id": "PS-Battles", "contents": "The PS-Battles dataset is gathered from a large community of image manipulation enthusiasts and provides a basis for media derivation and manipulation detection in the visual domain. The dataset consists of 102'028 images grouped into 11'142 subsets, each containing the original image as well as a varying number of manipulated derivatives.\r\n\r\nSource: [The PS-Battles Dataset - an Image Collection for Image Manipulation Detection](/paper/180404866)\r\nImage Source: [Heller et al](https://arxiv.org/pdf/1804.04866v1.pdf)", "variants": ["PS-Battles"], "title": "The PS-Battles Dataset - an Image Collection for Image Manipulation Detection"}
{"id": "Virtual Gallery", "contents": "The Virtual Gallery dataset is a synthetic dataset that targets multiple challenges such as varying lighting conditions and different occlusion levels for various tasks such as depth estimation, instance segmentation and visual localization.\r\n\r\nIt consists of a scene containing 3-4 rooms, in which a total of 42 free-for-use famous paintings are placed on the walls.\r\n\r\nThe virtual model and the captured images were generated with Unity software, allowing us to extract ground-truth information such as depth, semantic and instance segmentation, 2D-2D and 2D-3D correspondences.\r\n\r\nSource: [Visual Localization by Learning Objects-Of-Interest Dense Match Regression](https://europe.naverlabs.com/research/3d-vision/virtual-gallery-dataset/)\r\nImage Source: [https://europe.naverlabs.com/research/3d-vision/virtual-gallery-dataset/](https://europe.naverlabs.com/research/3d-vision/virtual-gallery-dataset/)", "variants": ["Virtual Gallery"], "title": "Visual Localization by Learning Objects-Of-Interest Dense Match Regression"}
{"id": "LABR", "contents": "LABR is a large sentiment analysis dataset to-date for the Arabic language. It consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars. \r\n\r\nSource: [LABR: A Large Scale Arabic Book Reviews Dataset](https://www.aclweb.org/anthology/P13-2088)", "variants": ["LABR (2-class, unbalanced)", "LABR"], "title": "LABR: A Large Scale Arabic Book Reviews Dataset"}
{"id": "TUM monoVO", "contents": "**TUM monoVO** is a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes.\nAll sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence.\nIn contrast to existing datasets, all sequences are photometrically calibrated: the dataset creators provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting).\n\nSource: [https://vision.in.tum.de/data/datasets/mono-dataset](https://vision.in.tum.de/data/datasets/mono-dataset)\nImage Source: [https://vision.in.tum.de/data/datasets/mono-dataset](https://vision.in.tum.de/data/datasets/mono-dataset)", "variants": ["TUM monoVO"], "title": "A Photometrically Calibrated Benchmark For Monocular Visual Odometry"}
{"id": "STL-10", "contents": "The **STL-10** is an image dataset derived from ImageNet and popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes (such as birds, cats, trucks), among which 5,000 images are partitioned for training while the remaining 8,000 images for testing. All the images are color images with 96×96 pixels in size.\r\n\r\nSource: [Unsupervised Feature Learning with C-SVDDNet](https://arxiv.org/abs/1412.7259)\r\nImage Source: [https://cs.stanford.edu/~acoates/stl10/](https://cs.stanford.edu/~acoates/stl10/)", "variants": ["STL-10", "STL-10, 1000 Labels", "STL-10, 5000 Labels"], "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning"}
{"id": "PreSIL", "contents": "Consists of over 50,000 frames and includes high-definition images with full resolution depth information, semantic segmentation (images), point-wise segmentation (point clouds), and detailed annotations for all vehicles and people. \r\n\r\nSource: [Precise Synthetic Image and LiDAR (PreSIL) Dataset for Autonomous Vehicle Perception](/paper/precise-synthetic-image-and-lidar-presil)", "variants": ["PreSIL"], "title": "Precise Synthetic Image and LiDAR (PreSIL) Dataset for Autonomous Vehicle Perception"}
{"id": "O-HAZE", "contents": "The O-Haze dataset contains 35 hazy images (size 2833×4657 pixels) for training. It has 5 hazy images for validation along with their corresponding ground truth images.\r\n\r\nSource: [Single image dehazing for a variety of haze scenarios using back projected pyramid network](https://arxiv.org/abs/2008.06713)\r\nImage Source: [https://data.vision.ee.ethz.ch/cvl/ntire18//o-haze/](https://data.vision.ee.ethz.ch/cvl/ntire18//o-haze/)", "variants": ["O-Haze", "O-HAZE"], "title": "O-HAZE: A Dehazing Benchmark with Real Hazy and Haze-Free Outdoor Images"}
{"id": "Quora", "contents": "**Quora** Question Pair consists of over 400k question pairs based on actual quora.com questions. Each pair contains a binary value indicating whether the two questions are paraphrase or not. The training-dev-test splits for this dataset are provided in.\r\n\r\nSource: [Semantic Sentence Matching with Densely-connectedRecurrent and Co-attentive Information](https://arxiv.org/abs/1805.11360)\r\nImage Source: [https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)", "variants": ["Quora Question Pairs", "quora", "Quora"], "title": "VisDA: The Visual Domain Adaptation Challenge"}
{"id": "CoVoST", "contents": "CoVoST is a multilingual speech-to-text translation corpus from 11 languages into English, diversified with over 11,000 speakers and over 60 accents. \r\n\r\nSource: [CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus](/paper/covost-a-diverse-multilingual-speech-to-text)", "variants": ["CoVoST"], "title": "CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus"}
{"id": "AQA-7", "contents": "Consists of 1106 action samples from seven actions with quality scores as measured by expert human judges.\r\n\r\nSource: [Action Quality Assessment Across Multiple Actions](/paper/action-quality-assessment-across-multiple)", "variants": ["AQA-7"], "title": "Action Quality Assessment Across Multiple Actions"}
{"id": "ShapenetRender", "contents": "**ShapenetRender**er is an extension of the ShapeNet Core dataset which has more variation in camera angles. For each mesh model, the dataset provides 36 views with smaller variation and 36 views with larger variation. The resolution of the newly rendered images is 224x224 in contrast to the 137x137 original resolution. Additionally, each RGB image is paired with a depth image, a normal map and an albedo image.\n\nSource: [https://github.com/Xharlie/ShapenetRender_more_variation](https://github.com/Xharlie/ShapenetRender_more_variation)\nImage Source: [https://github.com/Xharlie/ShapenetRender_more_variation](https://github.com/Xharlie/ShapenetRender_more_variation)", "variants": ["ShapenetRender"], "title": "DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction"}
{"id": "ApolloScape", "contents": "**ApolloScape** is a large dataset consisting of over 140,000 video frames (73 street scene videos) from various locations in China under varying weather conditions. Pixel-wise semantic annotation of the recorded data is provided in 2D, with point-wise semantic annotation in 3D for 28 classes. In addition, the dataset contains lane marking annotations in 2D.\r\n\r\nSource: [A2D2: Audi Autonomous Driving Dataset](https://arxiv.org/abs/2004.06320)\r\nImage Source: [https://arxiv.org/pdf/1803.06184.pdf](https://arxiv.org/pdf/1803.06184.pdf)", "variants": ["Apolloscape", "ApolloScape"], "title": "The ApolloScape Open Dataset for Autonomous Driving and its Application"}
{"id": "FDST", "contents": "The **Fudan-ShanghaiTech** dataset (**FDST**) is a dataset for video crowd counting. It contains 15K frames with about 394K annotated heads captured from 13 different scenes\n\nSource: [https://arxiv.org/abs/1907.07911](https://arxiv.org/abs/1907.07911)", "variants": ["FDST"], "title": "Locality-Constrained Spatial Transformer Network for Video Crowd Counting"}
{"id": "CASIA-SURF", "contents": "Dataset for face anti-spoofing in terms of both subjects and modalities. Specifically, it consists of  subjects with  videos and each sample has  modalities (i.e., RGB, Depth and IR). \r\n\r\nSource: [CASIA-SURF: A Large-scale Multi-modal Benchmark for Face Anti-spoofing](/paper/casia-surf-a-large-scale-multi-modal)", "variants": ["CASIA-SURF"], "title": "CASIA-SURF: A Large-Scale Multi-Modal Benchmark for Face Anti-Spoofing"}
{"id": "MMD", "contents": "The MMD (MultiModal Dialongs) dataset is a dataset for multimodal domain-aware conversations. It consists of over 150K conversation sessions between shoppers and sales agents, annotated by a group of in-house annotators using a semi-automated manually intense iterative process. \r\n\r\nSource: [Towards Building Large Scale Multimodal Domain-Aware Conversation Systems](/paper/towards-building-large-scale-multimodal)", "variants": ["MMD"], "title": "Towards Building Large Scale Multimodal Domain-Aware Conversation Systems"}
{"id": "COCO-CN", "contents": "COCO-CN is a bilingual image description dataset enriching MS-COCO with manually written Chinese sentences and tags. The new dataset can be used for multiple tasks including image tagging, captioning and retrieval, all in a cross-lingual setting.\r\n\r\nSource: [COCO-CN](https://github.com/li-xirong/coco-cn)", "variants": ["COCO-CN"], "title": "COCO-CN for Cross-Lingual Image Tagging, Captioning, and Retrieval"}
{"id": "Minecraft Segmentation", "contents": "**Minecraft Segmentation** is a segmentation dataset for the [Minecraft House](https://www.paperswithcode.com/dataset/minecraft-house) that adds semantic\r\nsegmentation labels for sub-components of the house. There are 2050 houses in total and 1038 distinct labels of subcomponents.", "variants": ["Minecraft Segmentation"], "title": "CraftAssist: A Framework for Dialogue-enabled Interactive Agents"}
{"id": "HSD", "contents": "An annotated dataset is released to enable dynamic scene classification that includes 80 hours of diverse high quality driving video data clips collected in the San Francisco Bay area. The dataset includes temporal annotations for road places, road types, weather, and road surface conditions. \r\n\r\nSource: [Dynamic Traffic Scene Classification with Space-Time Coherence](/paper/dynamic-traffic-scene-classification-with)", "variants": ["HSD"], "title": "Dynamic Traffic Scene Classification with Space-Time Coherence"}
{"id": "Poser", "contents": "The **Poser** dataset is a dataset for pose estimation which consists of 1927 training and 418 test images. These images are synthetically generated and tuned to unimodal predictions. The images were generated using the Poser software package.\n\nSource: [Overlapping Cover Local Regression Machines](https://arxiv.org/abs/1701.01218)\nImage Source: [https://www.researchgate.net/figure/Test-data-used-in-the-user-study-Left-the-pose-pictures-shown-to-the-user-Middle-the_fig17_221847487](https://www.researchgate.net/figure/Test-data-used-in-the-user-study-Left-the-pose-pictures-shown-to-the-user-Middle-the_fig17_221847487)", "variants": ["Poser"], "title": "Recovering 3D human pose from monocular images"}
{"id": "Visual Relationship Detection Dataset", "contents": "A dataset containing 5000 images with 37,993 thousand relationships. The dataset contains 100 object categories and 70 predicate categories connecting those objects together.\r\n\r\nSource: [Visual Relationship Detection with Language prior and Softmax](/paper/visual-relationship-detection-with-language-1)", "variants": ["Visual Relationship Detection Dataset"], "title": "Visual Relationship Detection with Language prior and Softmax"}
{"id": "WHU", "contents": "Created for MVS tasks and is a large-scale multi-view aerial dataset generated from a highly accurate 3D digital surface model produced from thousands of real aerial images with precise camera parameters.\r\n\r\nSource: [A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view Stereo Reconstruction from An Open Aerial Dataset](/paper/a-novel-recurrent-encoder-decoder-structure)", "variants": ["WHU"], "title": "A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view Stereo Reconstruction from An Open Aerial Dataset"}
{"id": "ExDark", "contents": "The **Exclusively Dark** (ExDARK) dataset is a collection of 7,363 low-light images from very low-light environments to twilight (i.e 10 different conditions) with 12 object classes (similar to PASCAL VOC) annotated on both image class level and local object bounding boxes.\n\nSource: [https://github.com/cs-chan/Exclusively-Dark-Image-Dataset](https://github.com/cs-chan/Exclusively-Dark-Image-Dataset)", "variants": ["ExDark"], "title": "Getting to Know Low-light Images with The Exclusively Dark Dataset"}
{"id": "Top Comment or Flop Comment?", "contents": "This dataset comprises four files of IDs of either strongly or weakly engaging online news comments (please see the paper for details):\r\n\"Top comments\" are 1) the top 10% comments in the politics section of The Guardian with the largest relative number of *replies* received (3111 samples) and 2) the top 10% comments in the politics section with the largest relative number of *upvotes* received (11081 samples)\r\n\r\n\"Flop comments\" are 1) the flop 10% comments in the politics section of The Guardian with the smallest relative number of *replies* received (3111 samples) and 2) the flop 10% comments in the politics section with the smallest relative number of *upvotes* received (11081 samples)", "variants": ["Top Comment or Flop Comment?"], "title": "Top Comment or Flop Comment? Predicting and Explaining User Engagement in Online News Discussions"}
{"id": "MLB-YouTube Dataset", "contents": "The MLB-YouTube dataset is a new, large-scale dataset consisting of 20 baseball games from the 2017 MLB post-season available on YouTube with over 42 hours of video footage. The dataset consists of two components: segmented videos for activity recognition and continuous videos for activity classification. It is quite challenging as it is created from TV broadcast baseball games where multiple different activities share the camera angle. Further, the motion/appearance difference between the various activities is quite small.\n\nSource: [https://github.com/piergiaj/mlb-youtube](https://github.com/piergiaj/mlb-youtube)\nImage Source: [https://github.com/piergiaj/mlb-youtube](https://github.com/piergiaj/mlb-youtube)", "variants": ["MLB-YouTube Dataset"], "title": "Fine-grained Activity Recognition in Baseball Videos"}
{"id": "QAMR", "contents": "**Question-Answer Meaning Representation** (**QAMR**) represents a predicate-argument structure of a sentence with a set of question-answer pairs, so that annotations can be easily provided by non-experts. QAMR is a dataset of over 5,000 sentences and 100,000 questions created by crowdsourcing workers.", "variants": ["QAMR"], "title": "Crowdsourcing Question-Answer Meaning Representations"}
{"id": "DiveFace", "contents": "A new face annotation dataset with balanced distribution between genders and ethnic origins. \r\n\r\nSource: [SensitiveNets: Learning Agnostic Representations with Application to Face Images](/paper/sensitivenets-learning-agnostic)", "variants": ["DiveFace"], "title": "SensitiveNets: Learning Agnostic Representations with Application to Face Recognition"}
{"id": "ToyADMOS", "contents": "**ToyADMOS** dataset is a machine operating sounds dataset of approximately 540 hours of normal machine operating sounds and over 12,000 samples of anomalous sounds collected with four microphones at a 48kHz sampling rate, prepared by Yuma Koizumi and members in NTT Media Intelligence Laboratories. The ToyADMOS dataset is designed for anomaly detection in machine operating sounds (ADMOS) research. It is designed for three tasks of ADMOS: product inspection (toy car), fault diagnosis for fixed machine (toy conveyor), and fault diagnosis for moving machine (toy train).\n\nSource: [https://github.com/YumaKoizumi/ToyADMOS-dataset](https://github.com/YumaKoizumi/ToyADMOS-dataset)", "variants": ["ToyADMOS"], "title": "ToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous Sound Detection"}
{"id": "YouTube-100M", "contents": "The **YouTube-100M** data set consists of 100 million YouTube videos: 70M training videos, 10M evaluation videos, and 20M validation videos. Videos average 4.6 minutes each for a total of 5.4M training hours. Each of these videos is labeled with 1 or more topic identifiers from a set of 30,871 labels. There are an average of around 5 labels per video. The labels are assigned automatically based on a combination of metadata (title, description, comments, etc.), context, and image content for each video. The labels apply to the entire video and range from very generic (e.g. “Song”) to very specific (e.g. “Cormorant”).\r\nBeing machine generated, the labels are not 100% accurate and of the 30K labels, some are clearly acoustically relevant (“Trumpet”) and others are less so (“Web Page”). Videos often bear annotations with multiple degrees of specificity. For example, videos labeled with “Trumpet” are often labeled “Entertainment” as well, although no hierarchy is enforced.\r\n\r\nSource: [https://arxiv.org/pdf/1609.09430.pdf](https://arxiv.org/pdf/1609.09430.pdf)\nImage Source: [https://arxiv.org/pdf/1609.09430.pdf](https://arxiv.org/pdf/1609.09430.pdf)", "variants": ["YouTube-100M"], "title": "CNN architectures for large-scale audio classification"}
{"id": "MultiRC", "contents": "**MultiRC** (**Multi-Sentence Reading Comprehension**) is a dataset of short paragraphs and multi-sentence questions, i.e., questions that can be answered by combining information from multiple sentences of the paragraph.\r\nThe dataset was designed with three key challenges in mind:\r\n* The number of correct answer-options for each question is not pre-specified. This removes the over-reliance on answer-options and forces them to decide on the correctness of each candidate answer independently of others. In other words, the task is not to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually.\r\n* The correct answer(s) is not required to be a span in the text.\r\n* The paragraphs in the dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets.\r\nThe entire corpus consists of around 10K questions (including about 6K multiple-sentence questions). The 60% of the data is released as training and development data. The rest of the data is saved for evaluation and every few months a new unseen additional data is included for evaluation to prevent unintentional overfitting over time.\r\n\r\nSource: [https://cogcomp.seas.upenn.edu/multirc/](https://cogcomp.seas.upenn.edu/multirc/)\r\nImage Source: [https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set/](https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set/)", "variants": ["MultiRC"], "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"}
{"id": "CPP", "contents": "A benchmark dataset that consists of 99,000+ sentences for Chinese polyphone disambiguation. \r\n\r\nSource: [g2pM: A Neural Grapheme-to-Phoneme Conversion Package for Mandarin Chinese Based on a New Open Benchmark Dataset](/paper/g2pm-a-neural-grapheme-to-phoneme-conversion)", "variants": ["CPP"], "title": "g2pM: A Neural Grapheme-to-Phoneme Conversion Package for MandarinChinese Based on a New Open Benchmark Dataset"}
{"id": "Noun-Noun Compound Dataset", "contents": "The noun–noun compounds dataset created by Fares (2016) consists of compounds annotated with two different taxonomies of relations; that is, for each noun–noun compound there are two distinct relations, drawing on different linguistic schools. The dataset was derived from existing linguistic resources, such as NomBank (Meyers et al., 2004) and the Prague Czech-English Dependency Treebank 2.0 (PCEDT; Hajič et al., 2012).\r\n\r\nSource: [Noun-Noun Compound Dataset](https://github.com/ltgoslo/fun-nom)", "variants": ["Noun-Noun Compound Dataset"], "title": "Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation"}
{"id": "University-1652", "contents": "Contains data from three platforms, i.e., synthetic drones, satellites and ground cameras of 1,652 university buildings around the world. University-1652 is a drone-based geo-localization dataset and enables two new tasks, i.e., drone-view target localization and drone navigation. \r\n\r\nSource: [University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization](/paper/university-1652-a-multi-view-multi-source)", "variants": ["University-1652"], "title": "University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization"}
{"id": "Pinterest", "contents": "The **Pinterest** dataset contains more than 1 million images associated to Pinterest users’ who have “pinned” them.\n\nSource: [https://openaccess.thecvf.com/content_iccv_2015/papers/Geng_Learning_Image_and_ICCV_2015_paper.pdf](https://openaccess.thecvf.com/content_iccv_2015/papers/Geng_Learning_Image_and_ICCV_2015_paper.pdf)", "variants": ["Pinterest"], "title": "Learning Image and User Features for Recommendation in Social Networks"}
{"id": "Bianet", "contents": "Bianet is a parallel news corpus in Turkish, Kurdish and English\r\nIt contains 3,214 Turkish articles with their sentence-aligned Kurdish or English translations from the Bianet online newspaper.\r\n\r\nSource: [Bianet: A Parallel News Corpus in Turkish, Kurdish and English](/paper/bianet-a-parallel-news-corpus-in-turkish)", "variants": ["Bianet"], "title": "Bianet: A Parallel News Corpus in Turkish, Kurdish and English"}
{"id": "JParaCrawl", "contents": "JParaCrawl is a parallel corpus for English-Japanese, for which the amount of publicly available parallel corpora is still limited. The parallel corpus was constructed by broadly crawling the web and automatically aligning parallel sentences. The corpus amassed over 8.7 million sentence pairs.\r\n\r\nSource: [JParaCrawl: A Large Scale Web-Based English-Japanese Parallel Corpus](https://arxiv.org/pdf/1911.10668)", "variants": ["JParaCrawl"], "title": "JParaCrawl: A Large Scale Web-Based English-Japanese Parallel Corpus"}
{"id": "IgboNLP Datasets", "contents": "IgboNLP is a standard machine translation benchmark dataset for Igbo. It consists of 10,000 English-Igbo human-level quality sentence pairs mostly from the news domain.\r\n\r\nSource: [Igbo-English Machine Translation: An Evaluation Benchmark](/paper/igbo-english-machine-translation-an)", "variants": ["IgboNLP Datasets"], "title": "Igbo-English Machine Translation: An Evaluation Benchmark"}
{"id": "BSDS500", "contents": "Berkeley Segmentation Data Set 500 (**BSDS500**) is a standard benchmark for contour detection. This dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images with carefully annotated boundaries collected from multiple users. The dataset is divided into three parts: 200 for training, 100 for validation and the rest 200 for test.\r\n\r\nSource: [Object Contour Detection with a Fully Convolutional Encoder-Decoder Network](https://arxiv.org/abs/1603.04530)\r\nImage Source: [https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html)", "variants": ["BSDS500", "BSDS500 (Quality 10 Color)", "BSDS500 (Quality 20 Color)", "BSDS500 (Quality 30 Color)", "BSDS500 (Quality 10 Grayscale)", "BSDS500 (Quality 20 Grayscale)", "BSDS500 (Quality 30 Grayscale)"], "title": "Contour Detection and Hierarchical Image Segmentation"}
{"id": "WSVD", "contents": "The Web Stereo Video Dataset consists of 553 stereoscopic videos from YouTube. This dataset has a wide variety of scene types, and features many nonrigid objects.", "variants": ["WSVD"], "title": "Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes"}
{"id": "SmartCity", "contents": "SmartCity consists of 50 images in total collected from ten city scenes including office entrance, sidewalk, atrium, shopping mall etc.. Unlike the existing crowd counting datasets with images of hundreds/thousands of pedestrians and nearly all the images being taken outdoors, SmartCity has few pedestrians in images and consists of both outdoor and indoor scenes: the average number of pedestrians is only 7.4 with minimum being 1 and maximum being 14.\r\n\r\nSource: [SmartCity](https://github.com/miao0913/SaCNN-CrowdCounting-Tencent_Youtu)", "variants": ["SmartCity"], "title": "Crowd Counting via Scale-Adaptive Convolutional Neural Network"}
{"id": "SEMAINE", "contents": "The **SEMAINE** videos dataset contains spontaneous data capturing the audiovisual interaction between a human and an operator undertaking the role of an avatar with four personalities: Poppy (happy), Obadiah (gloomy), Spike (angry) and Prudence (pragmatic). The audiovisual sequences have been recorded at a video rate of 25 fps (352 x 288 pixels). The dataset consists of audiovisual interaction between a human and an operator undertaking the role of an agent (Sensitive Artificial Agent). SEMAINE video clips have been annotated with couples of epistemic states such as agreement, interested, certain, concentration, and thoughtful with continuous rating (within the range [1,-1]) where -1 indicates most negative rating (i.e: No concentration at all) and +1 defines the highest (Most concentration). Twenty-four recording sessions are used in the Solid SAL scenario. Recordings are made of both the user and the operator, and there are usually four character interactions in each recording session, providing a total of 95 character interactions and 190 video clips.\r\n\r\nSource: [ROBUST MODELING OF EPISTEMIC MENTAL STATES](https://arxiv.org/abs/2005.13982)", "variants": ["SEMAINE"], "title": "The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent"}
{"id": "LJSpeech", "contents": "This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours. The texts were published between 1884 and 1964, and are in the public domain. The audio was recorded in 2016-17 by the LibriVox project and is also in the public domain.\r\n\r\nSource: [The LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/)\r\nImage Source: [https://keithito.com/LJ-Speech-Dataset/](https://keithito.com/LJ-Speech-Dataset/)\r\nAudio Source: [https://keithito.com/LJ-Speech-Dataset/](https://keithito.com/LJ-Speech-Dataset/)", "variants": ["LJSpeech"], "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering"}
{"id": "VOT2017", "contents": "**VOT2017** is a Visual Object Tracking dataset for different tasks that contains 60 short sequences annotated with 6 different attributes.\r\n\r\nSource: [GradNet: Gradient-Guided Network for Visual Object Tracking](https://arxiv.org/abs/1909.06800)\r\nImage Source: [https://ieeexplore.ieee.org/document/8265440](https://ieeexplore.ieee.org/document/8265440)", "variants": ["VOT2017/18", "VOT2017"], "title": "The Visual Object Tracking VOT2017 Challenge Results"}
{"id": "FairFace", "contents": "**FairFace** is a face image dataset which is race balanced. It contains 108,501 images from 7 different race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups.\r\n\r\nSource: [https://github.com/joojs/fairface](https://github.com/joojs/fairface)", "variants": ["FairFace"], "title": "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age"}
{"id": "Deep Fakes Dataset", "contents": "The Deep Fakes Dataset is a collection of \"in the wild\" portrait videos for deepfake detection. The videos in the dataset are diverse real-world samples in terms of the source generative model, resolution, compression, illumination, aspect-ratio, frame rate, motion, pose, cosmetics, occlusion, content, and context. They originate from various sources such as news articles, forums, apps, and research presentations; totalling up to 142 videos, 32 minutes, and 17 GBs. Synthetic videos are matched with their original counterparts when possible. \r\n\r\nSource: [Deepfakes dataset](http://cs.binghamton.edu/~ncilsal2/DeepFakesDataset/)", "variants": ["Deep Fakes Dataset"], "title": "FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals"}
{"id": "EuroSAT", "contents": "**Eurosat** is a dataset and deep learning benchmark for land use and land cover classification. The dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images.\r\n\r\nSource: [EuroSAT](https://github.com/phelber/eurosat)\r\nImage Source: [EuroSAT](https://github.com/phelber/eurosat)", "variants": ["EuroSAT"], "title": "EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification"}
{"id": "esXNLI", "contents": "**esXNLI** is a bilingual NLI dataset. It comprises 2,490 examples from 5 different genres that were originally annotated in Spanish, and translated into English by professional translators. It serves as a counterpoint to XNLI, which was originally annotated in English and translated into 14 other languages, including Spanish. The dataset was conceived to be used in conjunction with the XNLI development set to analyse the effect of translation in cross-lingual transfer learning.\n\nSource: [https://github.com/artetxem/esxnli](https://github.com/artetxem/esxnli)", "variants": ["esXNLI"], "title": "Translation Artifacts in Cross-lingual Transfer Learning"}
{"id": "CJRC", "contents": "The Chinese judicial reading comprehension (CJRC) dataset contains approximately 10K documents and almost 50K questions with answers. The documents come from judgment documents and the questions are annotated by law experts. \r\n\r\nSource: [CJRC: A Reliable Human-Annotated Benchmark DataSet for Chinese Judicial Reading Comprehension](/paper/cjrc-a-reliable-human-annotated-benchmark)", "variants": ["CJRC Dev", "CJRC"], "title": "CJRC: A Reliable Human-Annotated Benchmark DataSet for Chinese Judicial Reading Comprehension"}
{"id": "UASOL", "contents": "The UASOL an RGB-D stereo dataset, that contains 160902 frames, filmed at 33 different scenes, each with between 2 k and 10 k frames. The frames show different paths from the perspective of a pedestrian, including sidewalks, trails, roads, etc. The images were extracted from video files with 15 fps at HD2K resolution with a size of 2280 × 1282 pixels. The dataset also provides a GPS geolocalization tag for each second of the sequences and reflects different climatological conditions. It also involved up to 4 different persons filming the dataset at different moments of the day.\r\n\r\nWe propose a [train, validation and test split](https://www.nature.com/articles/s41597-019-0168-5/tables/4) to train the network. \r\nAdditionally, we introduce a subset of [676 pairs of RGB Stereo images and their respective depth](https://osf.io/64532/files/), which we extracted randomly from the entire dataset. This given test set is introduced to make comparability possible between the different methods trained with the dataset.", "variants": ["UASOL"], "title": "UASOL, a large-scale high-resolution outdoor stereo dataset"}
{"id": "FC100", "contents": "The **FC100** dataset (**Fewshot-CIFAR100**) is a newly split dataset based on CIFAR-100 for few-shot learning. It contains 20 high-level categories which are divided into 12, 4, 4 categories for training, validation and test. There are 60, 20, 20 low-level classes in the corresponding split containing 600 images of size 32 × 32 per class. Smaller image size makes it more challenging for few-shot learning.\r\n\r\nSource: [Prototype Rectification for Few-Shot Learning](https://arxiv.org/abs/1911.10713)", "variants": ["FC100 5-way (1-shot)", "FC100 5-way (5-shot)", "FC100", "FC100 5-way (10-shot)", "Fewshot-CIFAR100 - 1-Shot Learning", "Fewshot-CIFAR100 - 10-Shot Learning", "Fewshot-CIFAR100 - 5-Shot Learning"], "title": "TADAM: Task dependent adaptive metric for improved few-shot learning"}
{"id": "Lazaro Corpus", "contents": "A corpus of 21,570 newspaper headlines written in European Spanish annotated with emergent anglicisms.\r\n\r\nSource: [An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines](/paper/an-annotated-corpus-of-emerging-anglicisms-in)", "variants": ["Lazaro Corpus"], "title": "An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines"}
{"id": "VisPro", "contents": "VisPro dataset contains coreference annotation of 29,722 pronouns from 5,000 dialogues.\r\n\r\nSource: [VisPro](https://github.com/HKUST-KnowComp/Visual_PCR)", "variants": ["VisPro"], "title": "What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues"}
{"id": "Ward2ICU", "contents": "**Ward2ICU** is a vital signs dataset of inpatients from the general ward. It contains vital signs with class labels indicating patient transitions from the ward to intensive care units\n\nSource: [https://github.com/3778/Ward2ICU](https://github.com/3778/Ward2ICU)", "variants": ["Ward2ICU"], "title": "Ward2ICU: A Vital Signs Dataset of Inpatients from the General Ward"}
{"id": "PCam", "contents": "**PatchCamelyon** is an image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue. PCam provides a new benchmark for machine learning models: bigger than CIFAR10, smaller than ImageNet, trainable on a single GPU.", "variants": ["PCam"], "title": "Rotation Equivariant CNNs for Digital Pathology"}
{"id": "Ford AV Dataset", "contents": "A challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times during 2017-18. \r\n\r\nSource: [Ford Multi-AV Seasonal Dataset](/paper/ford-multi-av-seasonal-dataset)", "variants": ["Ford AV Dataset"], "title": "Ford Multi-AV Seasonal Dataset"}
{"id": "Europarl ConcoDisco Dataset", "contents": "The ConcoDisco Corpus is an English-French parallel corpus with discourse relations (DRs) and discourse connectives (DCs) annotations.\n\nSource: [https://github.com/mjlaali/Europarl-ConcoDisco](https://github.com/mjlaali/Europarl-ConcoDisco)\nImage Source: [https://github.com/mjlaali/Europarl-ConcoDisco](https://github.com/mjlaali/Europarl-ConcoDisco)", "variants": ["Europarl ConcoDisco Dataset"], "title": "Improving Discourse Relation Projection to Build Discourse Annotated Corpora"}
{"id": "MICC-SRI", "contents": "The dataset contains 11,913 frame pairs of urban driving footage with and without moving objects, synthetically generated with the CARLA simulator. All frames are available both as RGB images and semantic segmentations. RGB images are non-photorealistic being rendered by a game engine, while semantic segmentations are similar to a real-world segmentations. The dataset is designed to provide a supervision for semantic road inpainting tasks.", "variants": ["MICC-SRI"], "title": "Semantic Road Layout Understanding by Generative Adversarial Inpainting"}
{"id": "Middlebury 2006", "contents": "The **Middlebury 2006** is a stereo dataset of indoor scenes with multiple handcrafted layouts.\n\nSource: [https://vision.middlebury.edu/stereo/data/scenes2006/](https://vision.middlebury.edu/stereo/data/scenes2006/)\nImage Source: [https://vision.middlebury.edu/stereo/data/scenes2006/](https://vision.middlebury.edu/stereo/data/scenes2006/)", "variants": ["Middlebury 2006"], "title": "Evaluation of Cost Functions for Stereo Matching"}
{"id": "ReDWeb", "contents": "The ReDWeb dataset consists of 3600 RGB-RD image pairs collected from the Web. This dataset covers a wide range of scenes and features various non-rigid objects.", "variants": ["ReDWeb"], "title": "Monocular Relative Depth Perception with Web Stereo Data Supervision"}
{"id": "LabelMe", "contents": "**LabelMe** database is a large collection of images with ground truth labels for object detection and recognition. The annotations come from two different sources, including the LabelMe online annotation tool.\r\n\r\nSource: [LabelMe: A Database and Web-Based Tool for Image Annotation](https://people.csail.mit.edu/brussell/research/AIM-2005-025-new.pdf)\r\nImage Source: [Russell et al](https://people.csail.mit.edu/brussell/research/AIM-2005-025-new.pdf)", "variants": ["LabelMe"], "title": "LabelMe: A Database and Web-Based Tool for Image Annotation"}
{"id": "PASCAL-S", "contents": "**PASCAL-S** is a dataset for salient object detection consisting of a set of 850 images from PASCAL VOC 2010 validation set with multiple salient objects on the scenes.\r\n\r\nSource: [Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection](https://arxiv.org/abs/1909.04366)", "variants": ["PASCAL-S"], "title": "The Secrets of Salient Object Segmentation"}
{"id": "SpatialSense Benchmark", "contents": "SpatialSense Benchmark is a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. \r\n\r\nSource: [SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition](/paper/spatialsense-an-adversarially-crowdsourced)", "variants": ["SpatialSense Benchmark"], "title": "SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition"}
{"id": "CoS-E", "contents": "CoS-E consists of human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations\r\n\r\nSource: [Explain Yourself! Leveraging Language Models for Commonsense Reasoning](https://arxiv.org/pdf/1906.02361v1.pdf)", "variants": ["CoS-E"], "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning"}
{"id": "CULane", "contents": "**CULane** is a large scale challenging dataset for academic research on traffic lane detection. It is collected by cameras mounted on six different vehicles driven by different drivers in Beijing. More than 55 hours of videos were collected and 133,235 frames were extracted. The dataset is divided into 88880 images for training set, 9675 for validation set, and 34680 for test set. The test set is divided into normal and 8 challenging categories.\r\n\r\nSource: [https://xingangpan.github.io/projects/CULane.html](https://xingangpan.github.io/projects/CULane.html)\r\nImage Source: [https://xingangpan.github.io/projects/CULane.html](https://xingangpan.github.io/projects/CULane.html)", "variants": ["CULane"], "title": "Spatial As Deep: Spatial CNN for Traffic Scene Understanding"}
{"id": "Book Cover Dataset", "contents": "A new challenging dataset that can be used for many pattern recognition tasks.\r\n\r\nSource: [Judging a Book By its Cover](/paper/judging-a-book-by-its-cover)", "variants": ["Book Cover Dataset"], "title": "Judging a Book By its Cover"}
{"id": "X-ray and Visible Spectra Circular Motion Images Dataset", "contents": "Collections of images of the same rotating plastic object made in X-ray and visible spectra. Both parts of the dataset contain 400 images. The images are maid every 0.5 degrees of the object axial rotation. The collection of images is designed for evaluation of the performance of circular motion estimation algorithms as well as for the study of X-ray nature influence on the image analysis algorithms such as keypoints detection and description.\r\n\r\nSource: [X-ray and Visible Spectra Circular Motion Images Dataset](/paper/x-ray-and-visible-spectra-circular-motion)", "variants": ["X-ray and Visible Spectra Circular Motion Images Dataset"], "title": "X-ray and Visible Spectra Circular Motion Images Dataset"}
{"id": "Sentence Compression", "contents": "Sentence Compression is a dataset where the syntactic trees of the compressions are subtrees of their uncompressed counterparts, and hence where supervised systems which require a structural alignment between the input and output can be successfully trained. \r\n\r\nSource: [Overcoming the Lack of Parallel Data in Sentence Compression](/paper/overcoming-the-lack-of-parallel-data-in)", "variants": ["Google Dataset", "Sentence Compression"], "title": "Overcoming the Lack of Parallel Data in Sentence Compression"}
{"id": "EyeCar", "contents": "EyeCar is a dataset of driving videos of vehicles involved in rear-end collisions paired with eye fixation data captured from human subjects. It contains 21 front-view videos that were captured in various traffic, weather, and day light conditions. Each video is 30sec in length and contains typical driving tasks (e.g., lanekeeping, merging-in, and braking) ending to rear-end collisions.\r\n\r\nSource: [MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning](https://arxiv.org/abs/1912.07773)", "variants": ["EyeCar"], "title": "EyeCar: Modeling the Visual Attention Allocation of Drivers in Semi-Autonomous Vehicles"}
{"id": "MS MARCO", "contents": "The **MS MARCO** (Microsoft MAchine Reading Comprehension) is a collection of datasets focused on deep learning in search.\r\nThe first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. Over time the collection was extended with a 1,000,000 question dataset, a natural language generation dataset, a passage ranking dataset, keyphrase extraction dataset, crawling dataset, and a conversational search.\r\n\r\nSource: [https://microsoft.github.io/msmarco/](https://microsoft.github.io/msmarco/)\r\nImage Source: [https://arxiv.org/pdf/1809.08267.pdf](https://arxiv.org/pdf/1809.08267.pdf)", "variants": ["MS MARCO"], "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"}
{"id": "THUMOS14", "contents": "The **THUMOS14** dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.\r\n\r\nSource: [Learning to Localize Actions from Moments](https://arxiv.org/abs/2008.13705)\r\nImage Source: [http://crcv.ucf.edu/THUMOS14/](http://crcv.ucf.edu/THUMOS14/)", "variants": ["THUMOS 2014", "THUMOS' 14", "THUMOS’14", "THUMOS14", "THUMOS'14"], "title": "Jointly Learning Energy Expenditures and Activities Using Egocentric Multimodal Signals"}
{"id": "GTA5", "contents": "The **GTA5** dataset contains 24966 synthetic images with pixel level semantic annotation. The images have been rendered using the open-world video game **Grand Theft Auto 5** and are all from the car perspective in the streets of American-style virtual cities. There are 19 semantic classes which are compatible with the ones of Cityscapes dataset.\r\n\r\nSource: [Adversarial Learning and Self-Teaching Techniques for Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/1909.00781)\r\nImage Source: [Richter et al](https://arxiv.org/pdf/1608.02192v1.pdf)", "variants": ["GTA5", "GTAV-to-Cityscapes Labels", "GTA5 to Cityscapes"], "title": "Playing for Data: Ground Truth from Computer Games"}
{"id": "Places", "contents": "The **Places** dataset is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5,000 images per category.\r\n\r\nSource: [Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey](https://arxiv.org/abs/1902.06162)\r\nImage Source: [http://places.csail.mit.edu/browser.html](http://places.csail.mit.edu/browser.html)", "variants": ["Places2", "Places2 val", "Places205", "Places"], "title": "Places: A 10 Million Image Database for Scene Recognition"}
{"id": "PASTEL", "contents": "PASTEL is a parallelly annotated stylistic language dataset. The dataset consists of ~41K parallel sentences and 8.3K parallel stories annotated across different personas.\r\n\r\nSource: [PASTEL](https://github.com/dykang/PASTEL)", "variants": ["PASTEL"], "title": "(Male, Bachelor) and (Female, Ph.D) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas"}
{"id": "bAbI", "contents": "The **bAbI** dataset is a textual QA benchmark composed of 20 different tasks. Each task is designed to test a different reasoning skill, such as deduction, induction, and coreference resolution. Some of the tasks need relational reasoning, for instance, to compare the size of different entities. Each sample is composed of a question, an answer, and a set of facts. There are two versions of the dataset, referring to different dataset sizes: bAbI-1k and bAbI-10k. The bAbI-10k version of the dataset consists of 10,000 training samples per task.\r\n\r\nSource: [Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module](https://arxiv.org/abs/1805.09354)\r\nImage Source: [https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/)", "variants": ["bAbi", "bAbI"], "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"}
{"id": "LRS3-TED", "contents": "LRS3-TED is a multi-modal dataset for visual and audio-visual speech recognition. It includes face tracks from over 400 hours of TED and TEDx videos, along with the corresponding subtitles and word alignment boundaries. The new dataset is substantially larger in scale compared to other public datasets that are available for general research. \r\n\r\nSource: [LRS3-TED: a large-scale dataset for visual speech recognition](https://arxiv.org/pdf/1809.00496v2.pdf)", "variants": ["LRS3-TED"], "title": "LRS3-TED: a large-scale dataset for visual speech recognition"}
{"id": "Freiburg Terrains", "contents": "**Freiburg Terrains** consist of three parts: 3.7 hours of audio recordings of the microphone pointed at the robot wheels. It also contains 24K RGB images from the camera mounted on top of the robot. The dataset creators also provide the SLAM poses for each data collection run. The dataset can be used for terrain classification which is useful for agent navigation tasks.\n\nSource: [http://deepterrain.cs.uni-freiburg.de/](http://deepterrain.cs.uni-freiburg.de/)\nImage Source: [http://deepterrain.cs.uni-freiburg.de/](http://deepterrain.cs.uni-freiburg.de/)", "variants": ["Freiburg Terrains"], "title": "Self-Supervised Visual Terrain Classification from Unsupervised Acoustic Feature Learning"}
{"id": "DrivingStereo", "contents": "DrivingStereo contains over 180k images covering a diverse set of driving scenarios, which is hundreds of times larger than the KITTI Stereo dataset. High-quality labels of disparity are produced by a model-guided filtering strategy from multi-frame LiDAR points. \r\n\r\nSource: [DrivingStereo: A Large-Scale Dataset for Stereo Matching in Autonomous Driving Scenarios](/paper/drivingstereo-a-large-scale-dataset-for)\r\nImage Source: [https://drivingstereo-dataset.github.io/](https://drivingstereo-dataset.github.io/)", "variants": ["DrivingStereo"], "title": "DrivingStereo: A Large-Scale Dataset for Stereo Matching in Autonomous Driving Scenarios"}
{"id": "Microsoft Research Social Media Conversation Corpus", "contents": "Microsoft Research Social Media Conversation Corpus consists of 127M context-message-response triples from the Twitter FireHose, covering the 3-month period June 2012 through August 2012. Only those triples where context and response were generated by the same user were extracted. To minimize noise, only triples that contained at least one frequent bigram that appeared more than 3 times in the corpus was selected. This produced a corpus of 29M Twitter triples.\r\n\r\nSource: [A Neural Network Approach to Context-Sensitive Generation of Conversational Responses](/paper/a-neural-network-approach-to-context-1)", "variants": ["Microsoft Research Social Media Conversation Corpus"], "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses"}
{"id": "MultiBooked", "contents": "MultiBooked is a dataset for supervised aspect-level sentiment analysis in Basque and Catalan, both of which are under-resourced languages. \r\n\r\nSource: [MultiBooked: A Corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification](/paper/multibooked-a-corpus-of-basque-and-catalan)", "variants": ["MultiBooked"], "title": "MultiBooked: A Corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification"}
{"id": "The Contextual TV Dataset", "contents": "Using the Experience-Sampling Method (ESM), participants are asked to report TV consumption multiple times each day for a five week period. Through self-reported data, authors decrease uncertainty of exposure to content, and allow collection of non-trivial information, such as how much attention is paid to the TV. The data is structured to accommodate quantitative analyses, e.g. in the CARS community, and is publicly available under the name **Contextual TV (CTV)** dataset.\r\n\r\nSource: [Kristoffersen et al.](https://arxiv.org/pdf/1808.00337v2.pdf)\r\n\r\nImage source: [Kristoffersen et al.](https://arxiv.org/pdf/1808.00337v2.pdf)", "variants": ["The Contextual TV Dataset"], "title": "The Importance of Context When Recommending TV Content: Dataset and Algorithms"}
{"id": "SAMM Long Videos", "contents": "The **SAMM Long Videos** dataset consists of 147 long videos with 343 macro-expressions and 159 micro-expressions. The dataset is FACS-coded with detailed Action Units.\r\n\r\nSource: [SAMM Long Videos: A Spontaneous Facial Micro- and Macro-Expressions Dataset](/paper/samm-long-videos-a-spontaneous-facial-micro)", "variants": ["SAMM Long Videos"], "title": "SAMM Long Videos: A Spontaneous Facial Micro- and Macro-Expressions Dataset"}
{"id": "ModelNet", "contents": "The **ModelNet**40 dataset contains synthetic object point clouds. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular because of its various categories, clean shapes, well-constructed dataset, etc. The original ModelNet40 consists of 12,311 CAD-generated meshes in 40 categories (such as airplane, car, plant, lamp), of which 9,843 are used for training while the rest 2,468 are reserved for testing. The corresponding point cloud data points are uniformly sampled from the mesh surfaces, and then further preprocessed by moving to the origin and scaling into a unit sphere.\r\n\r\nSource: [Geometric Feedback Network for Point Cloud Classification](https://arxiv.org/abs/1911.12885)", "variants": ["ModelNet40", "ModelNet10", "ModelNet"], "title": "3D ShapeNets: A deep representation for volumetric shapes"}
{"id": "ICLabel", "contents": "An Independent components (IC) dataset containing spatiotemporal measures for over 200,000 ICs from more than 6,000 EEG recordings.\r\n\r\nSource: [ICLabel: An automated electroencephalographic independent component classifier, dataset, and website](/paper/iclabel-an-automated-electroencephalographic)", "variants": ["ICLabel"], "title": "ICLabel: An automated electroencephalographic independent component classifier, dataset, and website"}
{"id": "MIMIC-CXR", "contents": "**MIMIC-CXR** from Massachusetts Institute of Technology presents 371,920 chest X-rays associated with 227,943 imaging studies from 65,079 patients. The studies were performed at Beth Israel Deaconess Medical Center in Boston, MA.\n\nSource: [Can we trust deep learning models diagnosis? The impact of domain shift in chest radiograph classification](https://arxiv.org/abs/1909.01940)\nImage Source: [https://arxiv.org/abs/1901.07042](https://arxiv.org/abs/1901.07042)", "variants": ["MIMIC-CXR"], "title": "MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs"}
{"id": "TPIC17", "contents": "Image dataset with about 600K Flickr photos.\r\n\r\nSource: [Sequential Prediction of Social Media Popularity with Deep Temporal Context Networks](/paper/sequential-prediction-of-social-media)", "variants": ["TPIC17"], "title": "Sequential Prediction of Social Media Popularity with Deep Temporal Context Networks"}
{"id": "PROMISE12", "contents": "The **PROMISE12** dataset was made available for the MICCAI 2012 prostate segmentation challenge. Magnetic Resonance (MR) images (T2-weighted) of 50 patients with various diseases were acquired at different locations with several MRI vendors and scanning protocols.\r\n\r\nSource: [Constrained Deep Networks: Lagrangian Optimization via Log-Barrier Extensions](https://arxiv.org/abs/1904.04205)\r\nImage Source: [https://promise12.grand-challenge.org/](https://promise12.grand-challenge.org/)", "variants": ["PROMISE 2012", "PROMISE12"], "title": "Evaluation of prostate segmentation algorithms for MRI : the PROMISE12 challenge"}
{"id": "CityPersons", "contents": "The **CityPersons** dataset is a subset of Cityscapes which only consists of person annotations. There are 2975 images for training, 500 and 1575 images for validation and testing. The average of the number of pedestrians in an image is 7. The visible-region and full-body annotations are provided.\r\n\r\nSource: [NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing](https://arxiv.org/abs/2003.12729)\r\nImage Source: [https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons](https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons)", "variants": ["CityPersons"], "title": "CityPersons: A Diverse Dataset for Pedestrian Detection"}
{"id": "C3", "contents": "C3 is a free-form multiple-Choice Chinese machine reading Comprehension dataset.", "variants": ["C3"], "title": "Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"}
{"id": "Frames Dataset", "contents": "This dataset is dialog dataset collected in a Wizard-of-Oz fashion. Two humans talked to each other via a chat interface. One was playing the role of the user and the other one was playing the role of the conversational agent. The latter is called a wizard as a reference to the Wizard of Oz, the man behind the curtain. The wizards had access to a database of 250+ packages, each composed of a hotel and round-trip flights. The users were asked to find the best deal. This resulted in complex dialogues where a user would often consider different options, compare packages, and progressively build the description of her ideal trip.\r\n\r\nSource: [A Frame Tracking Model for Memory-Enhanced Dialogue Systems](/paper/a-frame-tracking-model-for-memory-enhanced)", "variants": ["Frames Dataset"], "title": "A Frame Tracking Model for Memory-Enhanced Dialogue Systems"}
{"id": "PlantDoc", "contents": "PlantDoc is a dataset for visual plant disease detection. The dataset contains 2,598 data points in total across 13 plant species and up to 17 classes of diseases, involving approximately 300 human hours of effort in annotating internet scraped images.\r\n\r\nSource: [PlantDoc: A Dataset for Visual Plant Disease Detection](/paper/plantdoc-a-dataset-for-visual-plant-disease)", "variants": ["PlantDoc"], "title": "PlantDoc: A Dataset for Visual Plant Disease Detection"}
{"id": "MIMIC-III", "contents": "The Medical Information Mart for Intensive Care III (**MIMIC-III**) dataset is a large, de-identified, and publicly-available collection of medical records. Each record in the dataset includes ICD-9 codes, which identify diagnoses and procedures performed. Each code is partitioned into sub-codes, which often include specific circumstantial details. The dataset consists of 112,000 clinical reports records (average length 709.3 tokens) and 1,159 top-level ICD-9 codes. Each report is assigned to 7.6 codes, on average.\r\n\r\nSource: [Interaction Matching for Long-Tail Multi-Label Classification](https://arxiv.org/abs/2005.08805)", "variants": ["MIMIC-III"], "title": "HMDB: A large video database for human motion recognition"}
{"id": "MeQSum", "contents": "**MeQSum** is a dataset for medical question summarization. It contains 1,000 summarized consumer health questions.\n\nSource: [https://www.aclweb.org/anthology/P19-1215.pdf](https://www.aclweb.org/anthology/P19-1215.pdf)\nImage Source: [https://www.aclweb.org/anthology/P19-1215.pdf](https://www.aclweb.org/anthology/P19-1215.pdf)", "variants": ["MeQSum"], "title": "On the Summarization of Consumer Health Questions"}
{"id": "NCBI Disease Corpus", "contents": "NCBI Disease Corpus is a large-scale disease corpus consisting of 6900 disease mentions in 793 PubMed citations, derived from an earlier corpus. The corpus contains rich annotations, was developed by a team of 12 annotators (two people per annotation) and covers all sentences in a PubMed abstract. Disease mentions are categorized into Specific Disease, Disease Class, Composite Mention and Modifier categories.\r\n\r\nSource: [An improved corpus of disease mentions in PubMed citations](/paper/an-improved-corpus-of-disease-mentions-in)", "variants": ["NCBI Disease Corpus"], "title": "An improved corpus of disease mentions in PubMed citations"}
{"id": "SVIRO", "contents": "Contains bounding boxes for object detection, instance segmentation masks, keypoints for pose estimation and depth images for each synthetic scenery as well as images for each individual seat for classification. \r\n\r\nSource: [SVIRO: Synthetic Vehicle Interior Rear Seat Occupancy Dataset and Benchmark](/paper/sviro-synthetic-vehicle-interior-rear-seat)", "variants": ["SVIRO"], "title": "SVIRO: Synthetic Vehicle Interior Rear Seat Occupancy Dataset and Benchmark"}
{"id": "DADA-2000", "contents": "DADA-2000 is a large-scale benchmark with 2000 video sequences (named as DADA-2000) is contributed with laborious annotation for driver attention (fixation, saccade, focusing time), accident objects/intervals, as well as the accident categories, and superior performance to state-of-the-arts are provided by thorough evaluations. \r\n\r\nSource: [DADA: A Large-scale Benchmark and Model for Driver Attention Prediction in Accidental Scenarios](https://arxiv.org/pdf/1912.12148)\r\nImage Source: [Fang et al](https://arxiv.org/pdf/1912.12148)", "variants": ["DADA-2000"], "title": "DADA: A Large-scale Benchmark and Model for Driver Attention Prediction in Accidental Scenarios"}
{"id": "UT Zappos50K", "contents": "**UT Zappos50K** is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories — shoes, sandals, slippers, and boots — followed by functional types and individual brands. The shoes are centered on a white background and pictured in the same orientation for convenient analysis.\r\n\r\nSource: [UT Zappos50K](http://vision.cs.utexas.edu/projects/finegrained/utzap50k/)", "variants": ["UT Zappos50K"], "title": "Fine-Grained Visual Comparisons with Local Learning"}
{"id": "IStego100K", "contents": "Contains 208,104 images with the same size of 1024*1024. Among them, 200,000 images (100,000 cover-stego image pairs) are divided as the training set and the remaining 8,104 as testing set.\r\n\r\nSource: [IStego100K: Large-scale Image Steganalysis Dataset](/paper/istego100k-large-scale-image-steganalysis)", "variants": ["IStego100K"], "title": "IStego100K: Large-scale Image Steganalysis Dataset"}
{"id": "ImageCLEF-DA", "contents": "The **ImageCLEF-DA** dataset is a benchmark dataset for ImageCLEF 2014 domain adaptation challenge, which contains three domains: Caltech-256 (C), ImageNet ILSVRC 2012 (I) and Pascal VOC 2012 (P). For each domain, there are 12 categories and 50 images in each category.\r\n\r\nSource: [Domain-Symmetric Networks for Adversarial Domain Adaptation](https://arxiv.org/abs/1904.04663)\r\nImage Source: [https://www.imageclef.org/2014/adaptation](https://www.imageclef.org/2014/adaptation)", "variants": ["ImageCLEF-DA"], "title": "Deep Transfer Learning with Joint Adaptation Networks"}
{"id": "MegaDepth", "contents": "The MegaDepth dataset is a dataset for single-view depth prediction that includes 196 different locations reconstructed from COLMAP SfM/MVS.\r\n\r\nSource: [MegaDepth: Learning Single-View Depth Prediction from Internet Photos](/paper/megadepth-learning-single-view-depth)", "variants": ["MegaDepth"], "title": "MegaDepth: Learning Single-View Depth Prediction from Internet Photos"}
{"id": "LSLF", "contents": "Consists of a large number of unconstrained multi-view and partially occluded faces.\r\n\r\nSource: [Large-scale Datasets: Faces with Partial Occlusions and Pose Variations in the Wild](/paper/large-scale-datasets-faces-with-partial)", "variants": ["LSLF"], "title": "Large-scale Datasets: Faces with Partial Occlusions and Pose Variations in the Wild"}
{"id": "Logo-2K+", "contents": "**Logo-2K+**:A Large-Scale Logo Dataset for Scalable Logo Classiﬁcation\nThe Logo-2K+ dataset contains a diverse range of logo classes from real-world logo images. It contains 167,140 images with 10 root categories and 2,341 leaf categories.\nThe 10 different root categories are: Food, Clothes, Institution, Accessories, Transportation, Electronic, Necessities, Cosmetic, Leisure and Medical.\n\nSource: [https://github.com/msn199959/Logo-2k-plus-Dataset](https://github.com/msn199959/Logo-2k-plus-Dataset)\nImage Source: [https://github.com/msn199959/Logo-2k-plus-Dataset](https://github.com/msn199959/Logo-2k-plus-Dataset)", "variants": ["Logo-2K+"], "title": "Logo-2K+: A Large-Scale Logo Dataset for Scalable Logo Classification"}
{"id": "Creative Flow+ Dataset", "contents": "Includes 3000 animated sequences rendered using styles randomly selected from 40 textured line styles and 38 shading styles, spanning the range between flat cartoon fill and wildly sketchy shading. The dataset includes 124K+ train set frames and 10K test set frames rendered at 1500x1500 resolution, far surpassing the largest available optical flow datasets in size.\r\n\r\nSource: [Creative Flow+ Dataset](/paper/creative-flow-dataset)", "variants": ["Creative Flow+ Dataset"], "title": "Creative Flow+ Dataset"}
{"id": "IntrA", "contents": "**IntrA** is an open-access 3D intracranial aneurysm dataset that makes the application of points-based and mesh-based classification and segmentation models available. This dataset can be used to diagnose intracranial aneurysms and to extract the neck for a clipping operation in medicine and other areas of deep learning, such as normal estimation and surface reconstruction.\r\n\r\n103 3D models of entire brain vessels are collected by reconstructing scanned 2D MRA images of patients (the raw 2D MRA images are not published due to medical ethics).\r\n1909 blood vessel segments are generated automatically from the complete models, including 1694 healthy vessel segments and 215 aneurysm segments for diagnosis.\r\n116 aneurysm segments are divided and annotated manually by medical experts; the scale of each aneurysm segment is based on the need for a preoperative examination.\r\nGeodesic distance matrices are computed and included for each annotated 3D segment, because the expression of the geodesic distance is more accurate than Euclidean distance according to the shape of vessels.\r\n\r\nSource: [https://github.com/intra3d2019/IntrA](https://github.com/intra3d2019/IntrA)\nImage Source: [https://github.com/intra3d2019/IntrA](https://github.com/intra3d2019/IntrA)", "variants": ["IntrA"], "title": "IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning"}
{"id": "ANETAC", "contents": "An English-Arabic named entity transliteration and classification dataset built from freely available parallel translation corpora. The dataset contains 79,924 instances, each instance is a triplet (e, a, c), where e is the English named entity, a is its Arabic transliteration and c is its class that can be either a Person, a Location, or an Organization. The ANETAC dataset is mainly aimed for the researchers that are working on Arabic named entity transliteration, but it can also be used for named entity classification purposes.\r\n\r\nSource: [ANETAC: Arabic Named Entity Transliteration and Classification Dataset](/paper/anetac-arabic-named-entity-transliteration)", "variants": ["ANETAC"], "title": "ANETAC: Arabic Named Entity Transliteration and Classification Dataset"}
{"id": "BookTest", "contents": "BookTest is a new dataset similar to the popular Children’s Book Test (CBT), however more than 60 times larger.\r\n\r\nSource: [BookTest](https://arxiv.org/pdf/1610.00956.pdf)", "variants": ["BookTest"], "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension"}
{"id": "RoadTracer", "contents": "**RoadTracer** is a dataset for extraction of road networks from aerial images. It consists of a large\r\ncorpus of high-resolution satellite imagery and ground truth\r\nroad network graphs covering the urban core of forty cities\r\nacross six countries. For each city, the dataset covers a region of approximately 24 sq km around the city center. The satellite imagery is obtained from Google at 60 cm/pixel resolution, and the road network from OSM.\r\n\r\nThe dataset is split into a training set with 25 cities and a\r\ntest set with 15 other cities.\r\n\r\nSource: [RoadTracer: Automatic Extraction of Road Networks from Aerial Images](/paper/roadtracer-automatic-extraction-of-road)\r\nImage Source: [Bastani et al](https://paperswithcode.com/paper/roadtracer-automatic-extraction-of-road)", "variants": ["RoadTracer"], "title": "RoadTracer: Automatic Extraction of Road Networks from Aerial Images"}
{"id": "MAFL", "contents": "The **MAFL** dataset contains manually annotated facial landmark locations for 19,000 training and 1,000 test images.\r\n\r\nSource: [Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance](https://arxiv.org/abs/1806.06503)\r\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html](http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html)", "variants": ["MAFL"], "title": "Facial Landmark Detection by Deep Multi-task Learning"}
{"id": "TallyQA", "contents": "TallyQA is a large-scale dataset for open-ended counting.\r\n\r\nSource: [TallyQA: Answering Complex Counting Questions](/paper/tallyqa-answering-complex-counting-questions)", "variants": ["TallyQA"], "title": "TallyQA: Answering Complex Counting Questions"}
{"id": "EMBER", "contents": "A labeled benchmark dataset for training machine learning models to statically detect malicious Windows portable executable files. The dataset includes features extracted from 1.1M binary files: 900K training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test samples (100K malicious, 100K benign).\r\n\r\nSource: [EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models](https://arxiv.org/pdf/1804.04637v2.pdf)", "variants": ["EMBER"], "title": "EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models"}
{"id": "WordNet-feelings", "contents": "WordNet-feelings, is an affective dataset that identifies 3664 word senses as feelings, and associates each of these with one of the 9 categories of feeling. The 9 different categories are: Actions, Anger, Attention, Attraction, Hedonics, Other, Physiological, Social, Wellbeing.\r\n\r\nSource: [WordNet-feelings: A linguistic categorisation of human feelings](https://arxiv.org/abs/1811.02435)", "variants": ["WordNet-feelings"], "title": "WordNet-feelings: A linguistic categorisation of human feelings"}
{"id": "RAD", "contents": "The dataset is useful for query-adaptive video summarization and annotated with diversity and query-specific relevance labels. \r\n\r\nSource: [Query-adaptive Video Summarization via Quality-aware Relevance Estimation](/paper/query-adaptive-video-summarization-via)", "variants": ["RAD"], "title": "Query-adaptive Video Summarization via Quality-aware Relevance Estimation"}
{"id": "ReferItGame", "contents": "The ReferIt dataset contains 130,525 expressions for referring to 96,654 objects in 19,894 images of natural scenes.\r\n\r\nSource: [BiLingUNet: Image Segmentation by Modulating Top-Down and Bottom-Up Visual Processing with Referring Expressions](https://arxiv.org/abs/2003.12739)\r\nImage Source: [http://tamaraberg.com/referitgame/](http://tamaraberg.com/referitgame/)", "variants": ["ReferItGame"], "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes"}
{"id": "NIPS4Bplus", "contents": "**NIPS4Bplus** is a richly annotated birdsong audio dataset, that is comprised of recordings containing bird vocalisations along with their active species tags plus the temporal annotations acquired for them. It consists of around 687 recordings, 87 classes, species tags, annotations. The total duration of audio is around 1 hour.\n\nSource: [https://peerj.com/articles/cs-223.pdf](https://peerj.com/articles/cs-223.pdf)\nImage Source: [https://peerj.com/articles/cs-223.pdf](https://peerj.com/articles/cs-223.pdf)", "variants": ["NIPS4Bplus"], "title": "NIPS4Bplus: a richly annotated birdsong audio dataset"}
{"id": "VidSet", "contents": "A large video dataset with dynamic content.\r\n\r\nSource: [Deep Homography Estimation for Dynamic Scenes](/paper/deep-homography-estimation-for-dynamic-scenes)", "variants": ["VidSet"], "title": "Deep Homography Estimation for Dynamic Scenes"}
{"id": "DFW", "contents": "Contains over 11000 images of 1000 identities with different types of disguise accessories. The dataset is collected from the Internet, resulting in unconstrained face images similar to real world settings. \r\n\r\nSource: [Recognizing Disguised Faces in the Wild](/paper/recognizing-disguised-faces-in-the-wild)", "variants": ["Disguised Faces in the Wild", "Disguised Faces in the Wild 2019", "DFW"], "title": "Recognizing Disguised Faces in the Wild"}
{"id": "3D Ken Burns Dataset", "contents": "Provides a large-scale synthetic dataset which contains accurate ground truth depth of various photo-realistic scenes.\r\n\r\nSource: [3D Ken Burns Effect from a Single Image](/paper/3d-ken-burns-effect-from-a-single-image)", "variants": ["3D Ken Burns Dataset"], "title": "3D Ken Burns effect from a single image"}
{"id": "Wikipedia Generation", "contents": "**Wikipedia Generation** is a dataset for article generation from Wikipedia from references at the end of Wikipedia page and the top 10 search results for the Wikipedia topic.", "variants": ["Wikipedia Generation"], "title": "Generating Wikipedia by Summarizing Long Sequences"}
{"id": "OpenSubtitles", "contents": "OpenSubtitles is collection of multilingual parallel corpora. The dataset is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages.", "variants": ["OpenSubtitles"], "title": "OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles"}
{"id": "MRPC", "contents": "Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases).\r\n\r\nSource: [Exploiting Semantic Annotations and Q-Learning for Constructing an Efficient Hierarchy/Graph Texts Organization](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313059/)\r\nImage Source: [https://www.aclweb.org/anthology/I05-5002.pdf](https://www.aclweb.org/anthology/I05-5002.pdf)", "variants": ["MRPC", "MRPC Dev"], "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}
{"id": "McMaster", "contents": "The **McMaster** dataset is a dataset for color demosaicing, which contains 18 cropped images of size 500×500.\r\n\r\nSource: [FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising](https://arxiv.org/abs/1710.04026)\r\nImage Source: [https://www4.comp.polyu.edu.hk/~cslzhang/paper/LMMSEdemosaicing.pdf](https://www4.comp.polyu.edu.hk/~cslzhang/paper/LMMSEdemosaicing.pdf)", "variants": ["McMaster sigma15", "McMaster sigma25", "McMaster sigma35", "McMaster sigma50", "McMaster sigma75", "McMaster"], "title": "Color Demosaicking by Local Directional Interpolation and Nonlocal Adaptive Thresholding"}
{"id": "Talk the Walk", "contents": "Talk The Walk is a large-scale dialogue dataset grounded in\r\naction and perception. The task involves two agents (a “guide” and a “tourist”)\r\nthat communicate via natural language in order to achieve a common goal: having\r\nthe tourist navigate to a given target location.", "variants": ["Talk the Walk"], "title": "Talk the Walk: Navigating New York City through Grounded Dialogue"}
{"id": "Query-Focused Video Summarization Dataset", "contents": "Collects dense per-video-shot concept annotations.\r\n\r\nSource: [Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach](https://arxiv.org/pdf/1707.04960v1.pdf)", "variants": ["Query-Focused Video Summarization Dataset"], "title": "Query-Focused Video Summarization: Dataset, Evaluation, and a Memory Network Based Approach"}
{"id": "RSICD", "contents": "The **Remote Sensing Image Captioning Dataset** (**RSICD**) is a dataset for remote sensing image captioning task. It contains more than ten thousands remote sensing images which are collected from Google Earth, Baidu Map, MapABC and Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images is 10921, with five sentences descriptions per image.\n\nSource: [https://github.com/201528014227051/RSICD_optimal](https://github.com/201528014227051/RSICD_optimal)\nImage Source: [https://github.com/201528014227051/RSICD_optimal](https://github.com/201528014227051/RSICD_optimal)", "variants": ["RSICD"], "title": "Exploring Models and Data for Remote Sensing Image Caption Generation"}
{"id": "Volleyball", "contents": "**Volleyball** is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.\r\n\r\nSource: [https://github.com/mostafa-saad/deep-activity-rec#dataset](https://github.com/mostafa-saad/deep-activity-rec#dataset)\r\nImage Source: [https://github.com/mostafa-saad/deep-activity-rec#dataset](https://github.com/mostafa-saad/deep-activity-rec#dataset)", "variants": ["Volleyball"], "title": "A Hierarchical Deep Temporal Model for Group Activity Recognition"}
{"id": "Brno-Urban-Dataset", "contents": "This self-driving dataset collected in Brno, Czech Republic contains data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy.\n\nSource: [https://arxiv.org/abs/1909.06897](https://arxiv.org/abs/1909.06897)\nImage Source: [https://github.com/RoboticsBUT/Brno-Urban-Dataset](https://github.com/RoboticsBUT/Brno-Urban-Dataset)", "variants": ["Brno-Urban-Dataset"], "title": "Brno Urban Dataset -- The New Data for Self-Driving Agents and Mapping Tasks"}
{"id": "Cityscapes", "contents": "**Cityscapes** is a large-scale database which focuses on semantic understanding of urban street scenes. It provides semantic, instance-wise, and dense pixel annotations for 30 classes grouped into 8 categories (flat surfaces, humans, vehicles, constructions, objects, nature, sky, and void). The dataset consists of around 5000 fine annotated images and 20000 coarse annotated ones. Data was captured in 50 cities during several months, daytimes, and good weather conditions. It was originally recorded as video so the frames were manually selected to have the following features: large number of dynamic objects, varying scene layout, and varying background.\r\n\r\nSource: [A Review on Deep Learning Techniques Applied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\r\nImage Source: [https://www.cityscapes-dataset.com/dataset-overview/](https://www.cityscapes-dataset.com/dataset-overview/)", "variants": ["Cityscapes 100 samples labeled", "Cityscapes 12.5% labeled", "Cityscapes 25% labeled", "Cityscapes 50% labeled", "Cityscapes Labels-to-Photo", "Cityscapes Photo-to-Labels", "Cityscapes test", "Cityscapes val", "Cityscapes-25K 256x512", "Cityscapes-5K 256x512", "Cityscapes", "Cityscapes 10% labeled"], "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding"}
{"id": "DukeMTMC-attribute", "contents": "The images in **DukeMTMC-attribute** dataset comes from Duke University. There are 1812 identities and 34183 annotated bounding boxes in the DukeMTMC-attribute dataset. This dataset contains 702 identities for training and 1110 identities for testing, corresponding to 16522 and 17661 images respectively. The attributes are annotated in the identity level, every image in this dataset is annotated with 23 attributes.\r\n\r\n**NOTE**: This dataset [has been retracted](https://exposing.ai/duke_mtmc/).\r\n\r\nSource: [Pedestrian Attribute Recognition: A Survey](https://arxiv.org/abs/1901.07474)\r\nImage Source: [http://irip.buaa.edu.cn/mars_duke_attributes/index.html](http://irip.buaa.edu.cn/mars_duke_attributes/index.html)", "variants": ["DukeMTMC-attribute"], "title": "Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro"}
{"id": "eSCAPE", "contents": "Consists of millions of entries in which the MT element of the training triplets has been obtained by translating the source side of publicly-available parallel corpora, and using the target side as an artificial human post-edit. Translations are obtained both with phrase-based and neural models.\r\n\r\nSource: [eSCAPE: a Large-scale Synthetic Corpus for Automatic Post-Editing](/paper/escape-a-large-scale-synthetic-corpus-for)", "variants": ["eSCAPE"], "title": "eSCAPE: a Large-scale Synthetic Corpus for Automatic Post-Editing"}
{"id": "Cumulo", "contents": "A benchmark dataset for training and evaluating global cloud classification models. It consists of one year of 1km resolution MODIS hyperspectral imagery merged with pixel-width 'tracks' of CloudSat cloud labels. \r\n\r\nSource: [Cumulo: A Dataset for Learning Cloud Classes](/paper/cumulo-a-dataset-for-learning-cloud-classes)", "variants": ["Cumulo"], "title": "Cumulo: A Dataset for Learning Cloud Classes"}
{"id": "VocalFolds", "contents": "The Vocal Folds dataset is a dataset for automatic segmentation of laryngeal endoscopic images.\nThe dataset consists of 8 sequences from 2 patients containing 536 hand segmented in vivo colour images of the larynx during two different resection interventions with a resolution of 512x512 pixels.\n\nSource: [https://github.com/imesluh/vocalfolds](https://github.com/imesluh/vocalfolds)\nImage Source: [https://github.com/imesluh/vocalfolds](https://github.com/imesluh/vocalfolds)", "variants": ["VocalFolds"], "title": "A dataset of laryngeal endoscopic images with comparative study on convolution neural network-based semantic segmentation"}
{"id": "MuST-Cinema", "contents": "MuST-Cinema is a Multilingual Speech-to-Subtitles corpus ideal for building subtitle-oriented machine and speech translation systems.\r\nIt comprises audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations.\r\n\r\nMuST-Cinema was built by annotating MuST-C with subtitle breaks based on the original subtitle files. Special symbols have been inserted in the aligned sentences to mark subtitle breaks as follows:\r\n\r\n- <eob>: block break (breaks between subtitle blocks)\r\n- <eol>: line breaks (breaks between lines inside the same subtitle block)\r\n\r\nSource: [MuST-Cinema](https://ict.fbk.eu/must-cinema/)", "variants": ["MuST-Cinema"], "title": "MuST-Cinema: a Speech-to-Subtitles corpus"}
{"id": "DuoRC", "contents": "DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\r\n\r\n**Why another RC dataset?**\r\n\r\nDuoRC pushes the NLP community to address challenges on incorporating knowledge and reasoning in neural architectures for reading comprehension. It poses several interesting challenges such as:\r\n\r\n- DuoRC using parallel plots is especially designed to contain a large number of questions with low lexical overlap between questions and their corresponding passages\r\n- It requires models to go beyond the content of the given passage itself and incorporate world-knowledge, background knowledge, and common-sense knowledge to arrive at the answer\r\n- It revolves around narrative passages from movie plots describing complex events and therefore naturally require complex reasoning (e.g. temporal reasoning, entailment, long-distance anaphoras, etc.) across multiple sentences to infer the answer to questions\r\n- Several of the questions in DuoRC, while seeming relevant, cannot actually be answered from the given passage. This requires the model to detect the unanswerability of questions. This aspect is important for machines to achieve in industrial settings in particular\r\n\r\nSource: [DuoRC](https://duorc.github.io/)", "variants": ["DuoRC"], "title": "DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension"}
{"id": "CommonsenseQA", "contents": "The **CommonsenseQA** is a dataset for commonsense question answering task. The dataset consists of 12,247 questions with 5 choices each.\r\nThe dataset was generated by Amazon Mechanical Turk workers in the following process (an example is provided in parentheses):\r\n\r\n1. a crowd worker observes a source concept from ConceptNet (“River”) and three target concepts (“Waterfall”, “Bridge”, “Valley”) that are all related by the same ConceptNet relation (“AtLocation”),\r\n2. the worker authors three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not, (“Where on a river can you hold a cup upright to catch water on a sunny day?”, “Where can I stand on a river to see water falling without getting wet?”, “I’m crossing the river, my feet are wet but my body is dry, where am I?”)\r\n3. for each question, another worker chooses one additional distractor from Concept Net (“pebble”, “stream”, “bank”), and the author another distractor (“mountain”, “bottom”, “island”) manually.\r\n\r\nSource: [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge](https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge/)\r\nImage Source: [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge](https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge/)", "variants": ["CommonsenseQA"], "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}
{"id": "CUHK02", "contents": "CUHK02 is a dataset for person re-identification. It contains 1,816 identities from two disjoint camera views. Each identity has two samples per camera view making a total of 7,264 images. It is used for Person Re-identification.\r\n\r\nImage Source: [Locally Aligned Feature Transforms across Views](https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Locally_Aligned_Feature_2013_CVPR_paper.pdf)", "variants": ["CUHK02"], "title": "Locally Aligned Feature Transforms across Views"}
{"id": "HRF", "contents": "The **HRF** dataset is a dataset for retinal vessel segmentation which comprises 45 images and is organized as 15 subsets. Each subset contains one healthy fundus image, one image of patient with diabetic retinopathy and one glaucoma image. The image sizes are 3,304 x 2,336, with a training/testing image split of 22/23.\r\n\r\nSource: [Connection Sensitive Attention U-NET for Accurate Retinal Vessel Segmentation](https://arxiv.org/abs/1903.05558)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-fundus-images-from-HRF-database-with-corresponding-hand-labelled-gold_fig1_260625531](https://www.researchgate.net/figure/Examples-of-fundus-images-from-HRF-database-with-corresponding-hand-labelled-gold_fig1_260625531)", "variants": ["HRF"], "title": "Robust Vessel Segmentation in Fundus Images"}
{"id": "JAFFE", "contents": "The **JAFFE** dataset consists of 213 images of different facial expressions from 10 different Japanese female subjects. Each subject was asked to do 7 facial expressions (6 basic facial expressions and neutral) and the images were annotated with average semantic ratings on each facial expression by 60 annotators.\r\n\r\nSource: [Balanced k-Means and Min-Cut Clustering](https://arxiv.org/abs/1411.6235)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-facial-expression-images-from-the-JAFFE-database_fig11_51873190](https://www.researchgate.net/figure/Examples-of-facial-expression-images-from-the-JAFFE-database_fig11_51873190)", "variants": ["JAFFE"], "title": "Web-based database for facial expression analysis"}
{"id": "PIRM", "contents": "The PIRM dataset consists of 200 images, which are divided into two equal sets for validation and testing. These images cover diverse contents, including people, objects, environments, flora, natural scenery, etc. Images vary in size, and are typically ~300K pixels in resolution.", "variants": ["PIRM-test", "PIRM"], "title": "2018 PIRM Challenge on Perceptual Image Super-resolution"}
{"id": "Simulated Flying Shapes", "contents": "The dataset consists of 90 000 grayscale videos that show two objects of equal shape and size in which one object approaches the other one. The object speed during the process of approaching is hereby modelled by a proportional-derivative controller. Overall, three different shapes (Rectangle, Triangle and Circle) are provided. Initial conﬁguration of the objects such as position and color were randomly sampled. Different from the moving MNIST dataset, the samples comprise a goal-oriented task, namely one object has to fully cover the other object rather than randomly moving, making it better suitable for testing prediction capabilities of an ML model.\nFor instance, one can use it as a toy dataset to investigate the capacity and output behavior of a deep neural network before testing it on real-world data.\n\nSource: [https://github.com/ferreirafabio/FlyingShapesDataset](https://github.com/ferreirafabio/FlyingShapesDataset)\nImage Source: [https://github.com/ferreirafabio/FlyingShapesDataset](https://github.com/ferreirafabio/FlyingShapesDataset)", "variants": ["Simulated Flying Shapes"], "title": "Introducing the Simulated Flying Shapes and Simulated Planar Manipulator Datasets"}
{"id": "VRD", "contents": "The Visual Relationship Dataset (**VRD**) contains 4000 images for training and 1000 for testing annotated with visual relationships. Bounding boxes are annotated with a label containing 100 unary predicates. These labels refer to animals, vehicles, clothes and generic objects. Pairs of bounding boxes are annotated with a label containing 70 binary predicates. These labels refer to actions, prepositions, spatial relations, comparatives or preposition phrases. The dataset has 37993 instances of visual relationships and 6672 types of relationships. 1877 instances of relationships occur only in the test set and they are used to evaluate the zero-shot learning scenario.\r\n\r\nSource: [Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation](https://arxiv.org/abs/1910.00462)\r\nImage Source: [https://cs.stanford.edu/people/ranjaykrishna/vrd/](https://cs.stanford.edu/people/ranjaykrishna/vrd/)", "variants": ["VRD", "VRD Predicate Detection", "VRD Phrase Detection", "VRD Relationship Detection"], "title": "Visual Relationship Detection with Language Priors"}
{"id": "TrackingNet", "contents": "**TrackingNet** is a large-scale tracking dataset consisting of videos in the wild. It has a total of 30,643 videos split into 30,132 training videos and 511 testing videos, with an average of 470,9 frames.\r\n\r\nSource: [Learning the Model Update for Siamese Trackers](https://arxiv.org/abs/1908.00855)\r\nImage Source: [https://arxiv.org/abs/1803.10794](https://arxiv.org/abs/1803.10794)", "variants": ["TrackingNet"], "title": "TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild"}
{"id": "V-COCO", "contents": "**Verbs in COCO** (**V-COCO**) is a dataset that builds off COCO for human-object interaction detection. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person has annotations for 29 action categories and there are no interaction labels including objects.\r\n\r\nSource: [Visual Compositional Learning for Human-Object Interaction Detection](https://arxiv.org/abs/2007.12407)\r\nImage Source: [https://www.researchgate.net/figure/Pose-estimation-and-action-recognition-results-on-the-V-COCO-Dataset-16-which-has_fig9_339477856](https://www.researchgate.net/figure/Pose-estimation-and-action-recognition-results-on-the-V-COCO-Dataset-16-which-has_fig9_339477856)", "variants": ["V-COCO"], "title": "Visual Semantic Role Labeling"}
{"id": "Charades-STA", "contents": "Charades-STA is a new dataset built on top of Charades by adding sentence temporal annotations.\r\n\r\nSource: [TALL: Temporal Activity Localization via Language Query](/paper/tall-temporal-activity-localization-via)", "variants": ["Charades-STA"], "title": "TALL: Temporal Activity Localization via Language Query"}
{"id": "MSD", "contents": "The Million Song Dataset is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.\r\n\r\nThe core of the dataset is the feature analysis and metadata for one million songs, provided by The Echo Nest. The dataset does not include any audio, only the derived features. Note, however, that sample audio can be fetched from services like 7digital, using [code]( https://github.com/tbertinmahieux/MSongsDB/tree/master/Tasks_Demos/Preview7digital) provided by the authors.\r\n\r\n\r\nSource: [http://millionsongdataset.com/](http://millionsongdataset.com/)", "variants": ["Million Song Dataset", "MSD"], "title": "Where Is My Mirror?"}
{"id": "COVID-19 Image Data Collection", "contents": "Contains hundreds of frontal view X-rays and is the largest public resource for COVID-19 image and prognostic data, making it a necessary resource to develop and evaluate tools to aid in the treatment of COVID-19.\r\n\r\nSource: [COVID-19 Image Data Collection](/paper/covid-19-image-data-collection)", "variants": ["COVID-19 Image Data Collection"], "title": "COVID-19 Image Data Collection"}
{"id": "Out the Window", "contents": "The Out the Window (OTW) dataset is a crowdsourced activity dataset containing 5,668 instances of 17 activities from the NIST Activities in Extended Video (ActEV) challenge. These videos are crowdsourced from workers on the Amazon Mechanical Turk using a novel scenario acting strategy, which collects multiple instances of natural activities per scenario. \r\n\r\nSource: [Out the Window: A Crowd-Sourced Dataset for Activity Classification in Security Video](/paper/out-the-window-a-crowd-sourced-dataset-for)", "variants": ["Out the Window"], "title": "Out the Window: A Crowd-Sourced Dataset for Activity Classification in Surveillance Video"}
{"id": "Kinetics-700", "contents": "Kinetics-700 is a video dataset of 650,000 clips that covers 700 human action classes. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 700 video clips. Each clip is annotated with an action class and lasts approximately 10 seconds.", "variants": ["Kinetics-700"], "title": "A Short Note on the Kinetics-700 Human Action Dataset"}
{"id": "Amazon Fine Foods", "contents": "Amazon Fine Foods is a dataset that consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plaintext review.\r\n\r\nSource: [https://snap.stanford.edu/data/web-FineFoods.html](https://snap.stanford.edu/data/web-FineFoods.html)", "variants": ["Amazon Fine Foods"], "title": "From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews"}
{"id": "Event-focused Emotion Corpora for German and English", "contents": "A corpus designed in analogy to the well-established English ISEAR emotion dataset.\r\n\r\nSource: [Crowdsourcing and Validating Event-focused Emotion Corpora for German and English](/paper/crowdsourcing-and-validating-event-focused)", "variants": ["Event-focused Emotion Corpora for German and English"], "title": "Crowdsourcing and Validating Event-focused Emotion Corpora for German and English"}
{"id": "BiPaR", "contents": "**BiPaR** is a manually annotated bilingual parallel novel-style machine reading comprehension (MRC) dataset, developed to support monolingual, multilingual and cross-lingual reading comprehension on novels. The biggest difference between BiPaR and existing reading comprehension datasets is that each triple (Passage, Question, Answer) in BiPaR is written in parallel in two languages. BiPaR is diverse in prefixes of questions, answer types and relationships between questions and passages. Answering the questions requires reading comprehension skills of coreference resolution, multi-sentence reasoning, and understanding of implicit causality.\r\n\r\nSource: [BiPaR](https://multinlp.github.io/BiPaR/)", "variants": ["BiPaR"], "title": "BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels"}
{"id": "FakeNewsAMT & Celebrity", "contents": "**FakeNewsAMT & Celebrity** include two novel datasets for the task of fake news detection, covering seven different news domains.", "variants": ["FakeNewsAMT & Celebrity"], "title": "Automatic Detection of Fake News"}
{"id": "MedleyDB 2.0", "contents": "**MedleyDB 2.0** is a superset of the MedleyDB – a dataset of annotated, royalty-free multitrack recordings. The second iteration of the dataset includes 74 new multitrack recordings resulting in 194 songs in total.\n\nSource: [https://medleydb.weebly.com/](https://medleydb.weebly.com/)\nImage Source: [https://medleydb.weebly.com/](https://medleydb.weebly.com/)\nAudio Source: [https://zenodo.org/record/1438309](https://zenodo.org/record/1438309)", "variants": ["MedleyDB 2.0"], "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents"}
{"id": "Netflix Prize", "contents": "**Netflix Prize** consists of about 100,000,000 ratings for 17,770 movies given by 480,189 users. Each rating in the training dataset consists of four entries: user, movie, date of grade, grade. Users and movies are represented with integer IDs, while ratings range from 1 to 5.\r\n\r\nSource: [Indian Regional Movie Dataset for Recommender Systems](https://arxiv.org/abs/1801.02203)\r\nImage Source: [https://www.netflixprize.com/](https://www.netflixprize.com/)", "variants": ["Netflix Prize"], "title": "The Replay-Mobile Face Presentation-Attack Database"}
{"id": "DFDC", "contents": "The DFDC (Deepfake Detection Challenge) is a dataset for deepface detection consisting of more than 100,000 videos.\r\n\r\nThe DFDC dataset consists of two versions:\r\n\r\n- Preview dataset. with 5k videos. Featuring two facial modification algorithms.\r\n- Full dataset, with 124k videos. Featuring eight facial modification algorithms\r\n\r\nSource: [The Deepfake Detection Challenge (DFDC) Preview Dataset](/paper/the-deepfake-detection-challenge-dfdc-preview)", "variants": ["DFDC"], "title": "The Deepfake Detection Challenge (DFDC) Preview Dataset"}
{"id": "Unite the People", "contents": "Unite The People is a dataset for 3D body estimation. The images come from the Leeds Sports Pose dataset and its extended version, as well as the single person tagged people from the MPII Human Pose Dataset. The images are labeled with different types of annotations such as segmentation labels, pose or 3D.", "variants": ["Unite the People"], "title": "Unite the People: Closing the Loop Between 3D and 2D Human Representations"}
{"id": "UTD-MHAD", "contents": "The **UTD-MHAD** dataset consists of 27 different actions performed by 8 subjects. Each subject repeated the action for 4 times, resulting in 861 action sequences in total. The RGB, depth, skeleton and the inertial sensor signals were recorded.\r\n\r\nSource: [Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints for Action Recognition](https://arxiv.org/abs/1711.05941)\r\nImage Source: [https://www.researchgate.net/figure/Sample-shots-of-the-27-actions-in-the-UTD-MHAD-database_fig12_283090976](https://www.researchgate.net/figure/Sample-shots-of-the-27-actions-in-the-UTD-MHAD-database_fig12_283090976)", "variants": ["UTD-MHAD"], "title": "UTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor"}
{"id": "Diabetic Retinopathy Detection Dataset", "contents": "A large scale of retina image dataset.\r\n\r\nSource: [Diabetic Retinopathy Detection via Deep Convolutional Networks for Discriminative Localization and Visual Explanation](/paper/diabetic-retinopathy-detection-via-deep)", "variants": ["Diabetic Retinopathy Detection Dataset"], "title": "Diabetic Retinopathy Detection via Deep Convolutional Networks for Discriminative Localization and Visual Explanation"}
{"id": "CVUSA", "contents": "A large dataset containing millions of pairs of ground-level and aerial/satellite images from across the United States.\r\n\r\nSource: [http://mvrl.cs.uky.edu/datasets/cvusa/](http://mvrl.cs.uky.edu/datasets/cvusa/)\r\nImage Source: [https://arxiv.org/pdf/1612.02709.pdf](https://arxiv.org/pdf/1612.02709.pdf)", "variants": ["cvusa", "CVUSA"], "title": "Wide-Area Image Geolocalization with Aerial Reference Imagery"}
{"id": "AMASS", "contents": "AMASS is a large database of human motion unifying different optical marker-based motion capture datasets by representing them within a common framework and parameterization. AMASS is readily useful for animation, visualization, and generating training data for deep learning.", "variants": ["AMASS"], "title": "AMASS: Archive of Motion Capture As Surface Shapes"}
{"id": "OTB-2013", "contents": "OTB2013 is the previous version of the current OTB2015 Visual Tracker Benchmark. It contains only 50 tracking sequences, as opposed to the 100 sequences in the current version of the benchmark.\r\n\r\nSource: [Marvasti-Zadeh](https://arxiv.org/abs/2004.01382)", "variants": ["OTB-2013"], "title": "Online Object Tracking: A Benchmark"}
{"id": "Twitter100k", "contents": "Twitter100k is a large-scale dataset for weakly supervised cross-media retrieval.\r\n\r\nSource: [Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media Retrieval](/paper/twitter100k-a-real-world-dataset-for-weakly)\r\nImage Source: [https://arxiv.org/pdf/1703.06618v1.pdf](https://arxiv.org/pdf/1703.06618v1.pdf)", "variants": ["Twitter100k"], "title": "Twitter100k: A Real-World Dataset for Weakly Supervised Cross-Media Retrieval"}
{"id": "DocRED", "contents": "**DocRED** (Document-Level Relation Extraction Dataset) is a relation extraction dataset constructed from Wikipedia and Wikidata. Each document in the dataset is human-annotated with named entity mentions, coreference information, intra- and inter-sentence relations, and supporting evidence. DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document. Along with the human-annotated data, the dataset provides large-scale distantly supervised data.\r\n\r\nDocRED contains 132,375 entities and 56,354 relational facts annotated on 5,053 Wikipedia documents. In addition to the human-annotated data, the dataset provides large-scale distantly supervised data over 101,873 documents.\r\n\r\nSource: [DocRED: A Large-Scale Document-Level Relation Extraction Dataset](https://paperswithcode.com/paper/docred-a-large-scale-document-level-relation/)\r\nImage Source: [DocRED: A Large-Scale Document-Level Relation Extraction Dataset](https://paperswithcode.com/paper/docred-a-large-scale-document-level-relation/)", "variants": ["DocRED"], "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset"}
{"id": "WritingPrompts", "contents": "WritingPrompts is a large dataset of 300K human-written stories paired with writing prompts from an online forum. \r\n\r\nSource: [Hierarchical Neural Story Generation](https://arxiv.org/pdf/1805.04833v1.pdf)", "variants": ["WritingPrompts"], "title": "Hierarchical Neural Story Generation"}
{"id": "PA-HMDB51", "contents": "The **Privacy Annotated HMDB51** (**PA-HMDB51**) dataset is a video-based dataset for evaluating pirvacy protection in visual action recognition algorithms. The dataset contains both target task labels (action) and selected privacy attributes (skin color, face, gender, nudity, and relationship) annotated on a per-frame basis.\n\nSource: [https://github.com/VITA-Group/PA-HMDB51](https://github.com/VITA-Group/PA-HMDB51)", "variants": ["PA-HMDB51"], "title": "Privacy-Preserving Deep Visual Recognition: An Adversarial Learning Framework and A New Dataset"}
{"id": "CARLA", "contents": "**CARLA** (CAR Learning to Act) is an open simulator for urban driving, developed as an open-source layer over Unreal Engine 4. Technically, it operates similarly to, as an open source layer over Unreal Engine 4 that provides sensors in the form of RGB cameras (with customizable positions), ground truth depth maps, ground truth semantic segmentation maps with 12 semantic classes designed for driving (road, lane marking, traffic sign, sidewalk and so on), bounding boxes for dynamic objects in the environment, and measurements of the agent itself (vehicle location and orientation).\r\n\r\nSource: [Synthetic Data for Deep Learning](https://arxiv.org/abs/1909.11512)", "variants": ["CARLA"], "title": "CARLA: An Open Urban Driving Simulator"}
{"id": "TWT-16", "contents": "The TWT16 dataset contains ~30k conversations in Twitter, collected from January to June 2016.\n\nSource: [https://arxiv.org/pdf/1903.07319.pdf](https://arxiv.org/pdf/1903.07319.pdf)\nImage Source: [https://github.com/zengjichuan/Topic_Disc](https://github.com/zengjichuan/Topic_Disc)", "variants": ["TWT-16"], "title": "What You Say and How You Say it: Joint Modeling of Topics and Discourse in Microblog Conversations"}
{"id": "Bulgarian Reading Comprehension Dataset", "contents": "A dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects -history, biology, geography and philosophy-, and 412 additional questions from online quizzes in history. \r\n\r\nSource: [Beyond English-Only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian](/paper/beyond-english-only-reading-comprehension)", "variants": ["Bulgarian Reading Comprehension Dataset"], "title": "Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian"}
{"id": "StarCraft II Learning Environment", "contents": "The **StarCraft II Learning Environment** (S2LE) is a reinforcement learning environment based on the game StarCraft II. The environment consists of three sub-components: a Linux StarCraft II binary, the StarCraft II API and PySC2. The StarCraft II API allows programmatic control of StarCraft II. It can be used to start a game, get observations, take actions, and review replays. PyC2 is a Python environment that wraps the StarCraft II API to ease the interaction between Python reinforcement learning agents and StarCraft II. It defines an action and observation specification, and includes a random agent and a handful of rule-based agents as examples. It also includes some mini-games as challenges and visualization tools to understand what the agent can see and do.\n\nSource: [https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)\nImage Source: [https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)", "variants": ["StarCraft II Learning Environment"], "title": "StarCraft II: A New Challenge for Reinforcement Learning"}
{"id": "VGG Face", "contents": "The **VGG Face** dataset is face identity recognition dataset that consists of 2,622 identities. It contains over 2.6 million images.\r\n\r\nSource: [https://www.robots.ox.ac.uk/~vgg/data/vgg_face/](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/)\r\nImage Source: [http://www.bmva.org/bmvc/2015/papers/paper041/paper041.pdf](http://www.bmva.org/bmvc/2015/papers/paper041/paper041.pdf)", "variants": ["VGG Face"], "title": "Deep Face Recognition."}
{"id": "Headlines dataset", "contents": "The **Headlines dataset** for sarcasm detection is collected from two news website. TheOnion aims at producing sarcastic versions of current events. The dataset includes all the headlines from News in Brief and News in Photos categories (which are sarcastic) and real (and non-sarcastic) news headlines from HuffPost.\r\nThis dataset has following advantages over the existing Twitter datasets:\r\n\r\n* Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.\r\n* Furthermore, since the sole purpose of TheOnion is to publish sarcastic news, the dataset has high-quality labels with much less noise as compared to Twitter datasets.\r\n* Unlike tweets which are replies to other tweets, the obtained news headlines are self-contained.\r\n\r\nSource: [https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection](https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection)\nImage Source: [https://github.com/rishabhmisra/Headlines-Dataset-For-Sarcasm-Detection](https://github.com/rishabhmisra/Headlines-Dataset-For-Sarcasm-Detection)", "variants": ["Headlines dataset"], "title": "Sarcasm Detection using Hybrid Neural Network"}
{"id": "PISC", "contents": "The People in Social Context (PISC) dataset is a dataset that focuses on social relationships. It consists of 22,670 images of 9 types of social relationships. It has annotations for the bounding boxes of all people, as well as the social relationship between all pairs of people in the images. In addition, it also contains occupation annotation. \r\n\r\nSource: [PISC](https://zenodo.org/record/1059155)", "variants": ["PISC"], "title": "Dual-Glance Model for Deciphering Social Relationships"}
{"id": "DIODE", "contents": "Diode Dense Indoor/Outdoor DEpth (**DIODE**) is the first standard dataset for monocular depth estimation comprising diverse indoor and outdoor scenes acquired with the same hardware setup. The training set consists of 8574 indoor and 16884 outdoor samples from 20 scans each. The validation set contains 325 indoor and 446 outdoor samples with each set from 10 different scans. The ground truth density for the indoor training and validation splits are approximately 99.54% and 99%, respectively. The density of the outdoor sets are naturally lower with 67.19% for training and 78.33% for validation subsets. The indoor and outdoor ranges for the dataset are 50m and 300m, respectively.\r\n\r\nSource: [Bidirectional Attention Network for Monocular Depth Estimation](https://arxiv.org/abs/2009.00743)\r\nImage Source: [https://diode-dataset.org/](https://diode-dataset.org/)", "variants": ["DIODE"], "title": "DIODE: A Dense Indoor and Outdoor DEpth Dataset"}
{"id": "Czech restaurant information", "contents": "Czech restaurant information is a dataset for NLG in task-oriented spoken dialogue systems with Czech as the target language. It originated as a translation of the English San Francisco Restaurants dataset by Wen et al. (2015).\r\n\r\nSource: [https://github.com/UFAL-DSG/cs_restaurant_dataset](https://github.com/UFAL-DSG/cs_restaurant_dataset)", "variants": ["Czech restaurant information"], "title": "Neural Generation for Czech: Data and Baselines"}
{"id": "Diseases in Neurology Case Reports Dataset", "contents": "Extracts diseases and syndromes (DsSs) from more than 65,000 neurology case reports from 66 journals in PubMed over the last six decades from 1955 to 2017.\r\n\r\nSource: [Exploring Diseases and Syndromes in Neurology Case Reports from 1955 to 2017 with Text Mining](/paper/exploring-diseases-and-syndromes-in-neurology)", "variants": ["Diseases in Neurology Case Reports Dataset"], "title": "Exploring Diseases and Syndromes in Neurology Case Reports from 1955 to 2017 with Text Mining"}
{"id": "HICO", "contents": "**HICO** is a benchmark for recognizing human-object interactions (HOI). \r\n\r\nKey features:\r\n\r\n- A diverse set of interactions with common object categories\r\n- A list of well-defined, sense-based HOI categories\r\n- An exhaustive labeling of co-occurring interactions with an object category in each image\r\n- The annotation of each HOI instance (i.e. a human and an object bounding box with an interaction class label) in all images\r\n\r\nSource: [HICO: A Benchmark for Recognizing Human-Object Interactions in Images](http://openaccess.thecvf.com/content_iccv_2015/papers/Chao_HICO_A_Benchmark_ICCV_2015_paper.pdf)", "variants": ["HICO"], "title": "HICO: A Benchmark for Recognizing Human-Object Interactions in Images"}
{"id": "Silhouettes", "contents": "The Caltech 101 **Silhouettes** dataset consists of 4,100 training samples, 2,264 validation samples and 2,307 test samples. The datast is based on CalTech 101 image annotations. Each image in the CalTech 101 data set includes a high-quality polygon outline of the primary object in the scene. To create the **CalTech 101 Silhouettes** data set, the authors center and scale each outline and render it on a DxD pixel image-plane. The outline is rendered as a filled, black polygon on a white background. Many object classes exhibit silhouettes that have distinctive class-specific features. A relatively small number of classes like soccer ball, pizza, stop sign, and yin-yang are indistinguishable based on shape, but have been left-in in the data.\r\n\r\nSource: [1 Introduction](https://arxiv.org/abs/1506.04557)\r\nImage Source: [https://people.cs.umass.edu/~marlin/data.shtml](https://people.cs.umass.edu/~marlin/data.shtml)", "variants": ["Silhouettes"], "title": "Meme-tracking and the dynamics of the news cycle"}
{"id": "Aff-Wild2", "contents": "Aff-Wild2 is an extension of the Aff-Wild dataset for affect recognition. It approximately doubles the number of included video frames and the number of subjects; thus, improving the variability of the included behaviors and of the involved persons. \r\n\r\nSource: [Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition](https://arxiv.org/pdf/1811.07770v2.pdf)", "variants": ["Aff-Wild2"], "title": "Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition"}
{"id": "ICDAR 2017", "contents": "ICDAR2017 is a dataset for scene text detection.\n\nSource: [Scale-Invariant Multi-Oriented Text Detection in Wild Scene Images](https://arxiv.org/abs/2002.06423)\nImage Source: [https://rrc.cvc.uab.es/?ch=7](https://rrc.cvc.uab.es/?ch=7)", "variants": [" ICDAR 2017 MLT", "ICDAR 2017 MLT", "ICDAR 2017"], "title": "ICDAR2017 Robust Reading Challenge on COCO-Text"}
{"id": "SoccerData", "contents": "A dataset of 4562 images of which 4152 images contain a soccer ball.\r\n\r\nSource: [Utilizing Temporal Information in Deep Convolutional Network for Efficient Soccer Ball Detection and Tracking](/paper/utilizing-temporal-information-in)", "variants": ["SoccerData"], "title": "Utilizing Temporal Information in Deep Convolutional Network for Efficient Soccer Ball Detection and Tracking"}
{"id": "Icons-50", "contents": "Icons-50 is a dataset for studying surface variation robustness.\r\n\r\nSource: [Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations](/paper/benchmarking-neural-network-robustness-to)", "variants": ["Icons-50"], "title": "Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations"}
{"id": "MEIR", "contents": "MEIR is a substantially challenging dataset over that which has been previously available to support research into image repurposing detection. The new dataset includes location, person, and organization manipulations on real-world data sourced from Flickr.\r\n\r\nSource: [Deep Multimodal Image-Repurposing Detection](https://arxiv.org/pdf/1808.06686.pdf)", "variants": ["MEIR"], "title": "Deep Multimodal Image-Repurposing Detection"}
{"id": "SARC", "contents": "This dataset was designed for contextual investigations, with related works making considerable usage of said context. The dataset was constructed by scraping Reddit comments; with sarcastic entries being self-annotated by authors through the use of the \\s token, which indicates sarcastic intent on the website. Posts on Reddit are often in response to another comment; SARC incorporates this information through the addition of the parent comment and further child comments surrounding a post. \r\n\r\nSource: [DEEP AND DENSE SARCASM DETECTION](https://arxiv.org/pdf/1911.07474v2.pdf)", "variants": ["SARC (all-bal)", "SARC (pol-bal)", "SARC (pol-unbal)", "SARC"], "title": "A Large Self-Annotated Corpus for Sarcasm"}
{"id": "BookCorpus", "contents": "**BookCorpus** is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).\r\n\r\nSource: [Temporal Event Knowledge Acquisition via Identifying Narratives](https://arxiv.org/abs/1805.10956)", "variants": ["BookCorpus"], "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}
{"id": "JTA", "contents": "JTA is a dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. It is up to now the vastest dataset (about 500.000 frames, almost 10 million body poses) of human body parts for people tracking in urban scenarios. \r\n\r\nSource: [Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World](/paper/learning-to-detect-and-track-visible-and)", "variants": ["JTA"], "title": "Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World"}
{"id": "LibriCSS", "contents": "Continuous speech separation (CSS) is an approach to handling overlapped speech in conversational audio signals. A real recorded dataset, called **LibriCSS**, is derived from LibriSpeech by concatenating the corpus utterances to simulate a conversation and capturing the audio replays with far-field microphones.\r\n\r\nSource: [https://github.com/chenzhuo1011/libri_css](https://github.com/chenzhuo1011/libri_css)", "variants": ["LibriCSS"], "title": "Continuous speech separation: dataset and analysis"}
{"id": "FrameNet", "contents": "**FrameNet** is a linguistic knowledge graph containing information about lexical and predicate argument semantics of the English language. FrameNet contains two distinct entity classes: frames and lexical units, where a frame is a meaning and a lexical unit is a single meaning for a word.\r\n\r\nSource: [Retrofitting Distributional Embeddings to Knowledge Graphswith Functional Relations](https://arxiv.org/abs/1708.00112)", "variants": ["FrameNet"], "title": "The Berkeley FrameNet Project"}
{"id": "Spot the Difference Corpus", "contents": "Spot the Difference Corpus is a corpus of task-oriented spontaneous dialogues which contains 54 interactions between pairs of subjects interacting to find differences in two very similar scenes. The corpus includes rich transcriptions, annotations, audio and video.\r\n\r\nSource: [The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions](https://arxiv.org/pdf/1805.05091.pdf)", "variants": ["Spot the Difference Corpus"], "title": "The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions"}
{"id": "Sports-1M", "contents": "The **Sports-1M** dataset consists of over a million videos from YouTube. The videos in the dataset can be obtained through the YouTube URL specified by the authors. Approximately 7% (as of 2016) of the videos have been removed by the YouTube uploaders since the dataset was compiled. However, there are still over a million videos in the dataset with 487 sports-related categories with 1,000 to 3,000 videos per category. The videos are automatically labelled with 487 sports classes using the YouTube Topics API by analyzing the text metadata associated with the videos (e.g. tags, descriptions). Approximately 5% of the videos are annotated with more than one class.\r\n\r\nSource: [Review of Action Recognition and Detection Methods](https://arxiv.org/abs/1610.06906)\r\n\r\nImage Source: [Computer Vision for Sports](https://www.researchgate.net/publication/316477606_Computer_vision_for_sports_Current_applications_and_research_topics)", "variants": ["Sports-1M"], "title": "Large-Scale Video Classification with Convolutional Neural Networks"}
{"id": "SYNTHIA", "contents": "The **SYNTHIA** dataset is a synthetic dataset that consists of 9400 multi-viewpoint photo-realistic frames rendered from a virtual city and comes with pixel-level semantic annotations for 13 classes. Each frame has resolution of 1280 × 960.\r\n\r\nSource: [Orientation-aware Semantic Segmentation on Icosahedron Spheres](https://arxiv.org/abs/1907.12849)\r\nImage Source: [https://synthia-dataset.net/](https://synthia-dataset.net/)", "variants": ["SYNTHIA Fall-to-Winter", "SYNTHIA-to-Cityscapes", "SYNTHIA", "SYNTHIA-CVPR’16", "Synthia Novel View Synthesis"], "title": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes"}
{"id": "PHYRE", "contents": "Benchmark for physical reasoning that contains a set of simple classical mechanics puzzles in a 2D physical environment. The benchmark is designed to encourage the development of learning algorithms that are sample-efficient and generalize well across puzzles. \r\n\r\nSource: [PHYRE: A New Benchmark for Physical Reasoning](/paper/phyre-a-new-benchmark-for-physical-reasoning)", "variants": ["PHYRE"], "title": "PHYRE: A New Benchmark for Physical Reasoning"}
{"id": "InstaCities1M", "contents": "InstaCities1M is a dataset of social media images with associated text. It consists of Instagram images associated associated with one of the 10 most populated English speaking cities all over the world. It has 100K images for each city, which makes a total of 1M images, split in 800K training images, 50K validation images and 150K testing images. All images were resized to 300x300 pixels.\r\n\r\nSource: [The InstaCities1M Dataset](https://gombru.github.io/2018/08/01/InstaCities1M/)", "variants": ["InstaCities1M"], "title": "Learning to Learn from Web Data through Deep Semantic Embeddings"}
{"id": "BioASQ", "contents": "**BioASQ** is a question answering dataset. Instances in the BioASQ dataset are composed of a question (Q), human-annotated answers (A), and the relevant contexts (C) (also called snippets).\r\n\r\nSource: [Transferability of Natural Language Inference to Biomedical Question Answering](https://arxiv.org/abs/2007.00217)\r\nImage Source: [http://participants-area.bioasq.org/datasets/](http://participants-area.bioasq.org/datasets/)", "variants": ["BioASQ"], "title": "An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition"}
{"id": "Stanford Light Field", "contents": "The **Stanford Light Field** Archive is a collection of several light fields for research in computer graphics and vision.\r\n\r\nSource: [http://lightfield.stanford.edu/](http://lightfield.stanford.edu/)\r\nImage Source: [http://lightfield.stanford.edu/](http://lightfield.stanford.edu/)", "variants": ["Stanford Light Field"], "title": "eTrust: understanding trust evolution in an online world"}
{"id": "CODEBRIM", "contents": "Dataset for multi-target classification of five commonly appearing concrete defects.\r\n\r\nSource: [Meta-learning Convolutional Neural Architectures for Multi-target Concrete Defect Classification with the COncrete DEfect BRidge IMage Dataset](/paper/190408486)", "variants": ["CODEBRIM"], "title": "Meta-Learning Convolutional Neural Architectures for Multi-Target Concrete Defect Classification With the COncrete DEfect BRidge IMage Dataset"}
{"id": "XKCDColors", "contents": "A balanced dataset of color names and RGB values for training classifiers.\n\nSource: [https://github.com/Smoltbob/XKCDColors-Dataset](https://github.com/Smoltbob/XKCDColors-Dataset)", "variants": ["XKCDColors"], "title": "Color inference from semantic labeling for person search in videos"}
{"id": "Kite", "contents": "The Kite database is a multi-modal dataset for the control of unmanned aerial vehicles (UAVs). There are three modalities present in the dataset:\r\n\r\n- Language, represented by the commands issued to the UAV\r\n- Audio, represented by the spoken instantiation of the commands\r\n- Visual, represented by an image that is likely to be seen when the command is issued\r\n\r\nThe dataset was created by the members of the [SpeeD](https://speed.pub.ro/) team.\r\n\r\nSource: [Kite Dataset](http://kite.speed.pub.ro/)", "variants": ["Kite"], "title": "Kite: Automatic speech recognition for unmanned aerial vehicles"}
{"id": "ODSQA", "contents": "The **ODSQA** dataset is a spoken dataset for question answering in Chinese. It contains more than three thousand questions from 20 different speakers.\n\nSource: [https://github.com/chiahsuan156/ODSQA](https://github.com/chiahsuan156/ODSQA)", "variants": ["ODSQA"], "title": "ODSQA: Open-Domain Spoken Question Answering Dataset"}
{"id": "Letter", "contents": "Letter Recognition Data Set is a handwritten digit dataset. The task is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.\r\n\r\nSource: [UCL Machine Learning Repository Letter Recognition](https://archive.ics.uci.edu/ml/datasets/Letter+Recognition)\r\nImage Source: [http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf](http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf)", "variants": ["LetterA-J", "Letter"], "title": "Letter Recognition Using Holland-Style Adaptive Classifiers"}
{"id": "MSRDailyActivity3D", "contents": "**DailyActivity3D** dataset is a daily activity dataset captured by a Kinect device. There are 16 activity types: drink, eat, read book, call cellphone, write on a paper, use laptop, use vacuum cleaner, cheer up, sit still, toss paper, play game, lay down on sofa, walk, play guitar, stand up, sit down. If possible, each subject performs an activity in two different poses: “sitting on sofa” and “standing”. The total number of the activity samples is 320.\r\nThis dataset is designed to cover human’s daily activities in the living room. When the performer stands close to the sofa or sits on the sofa, the 3D joint positions extracted by the skeleton tracker are very noisy. Moreover, most of the activities involve the humans-object interactions. Thus this dataset is more challenging.\r\n\r\nSource: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06247813.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06247813.pdf)\r\nImage Source: [https://www.researchgate.net/publication/308001852_Automatic_Learning_of_Articulated_Skeletons_Based_on_Mean_of_3D_Joints_for_Efficient_Action_Recognition](https://www.researchgate.net/publication/308001852_Automatic_Learning_of_Articulated_Skeletons_Based_on_Mean_of_3D_Joints_for_Efficient_Action_Recognition)", "variants": ["MSR Daily Activity3D", "MSR Daily Activity3D dataset", "MSRDailyActivity3D"], "title": "Mining actionlet ensemble for action recognition with depth cameras"}
{"id": "IITB Corridor", "contents": "An abnormal activity data-set for research use that contains 4,83,566 annotated frames.\r\n\r\nSource: [Multi-timescale Trajectory Prediction for Abnormal Human Activity Detection](/paper/multi-timescale-trajectory-prediction-for)", "variants": ["IITB Corridor"], "title": "Multi-timescale Trajectory Prediction for Abnormal Human Activity Detection"}
{"id": "ToM QA", "contents": "The data consists of a set of 3 task types and 4 question types, creating 12 total scenarios. The tasks are grouped into stories, which are denoted by the numbering at the start of each line.\r\n\r\nSource: [ToM QA](https://github.com/kayburns/tom-qa-dataset)", "variants": ["ToM QA"], "title": "Evaluating Theory of Mind in Question Answering"}
{"id": "Cross-Modal Comments Dataset", "contents": "Cross Modal Automatic Commenting (CMAC) is a task which aims to automatically generate comments for graphic news. The CMAC dataset is a large-scale dataset for this task which consists of 24,134 graphic news. Each instance is composed of several news photos, news title, news body, and corresponding high-quality comments.\n\nSource: [https://github.com/lancopku/CMAC](https://github.com/lancopku/CMAC)", "variants": ["Cross-Modal Comments Dataset"], "title": "Cross-Modal Commentator: Automatic Machine Commenting Based on Cross-Modal Information"}
{"id": "GENIA", "contents": "The **GENIA** corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology.\r\n\r\nThe corpus contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms “human”, “blood cells”, and “transcription factors”. The corpus has been annotated with various levels of linguistic and semantic information.\r\n\r\nThe primary categories of annotation in the GENIA corpus and the corresponding subcorpora are:\r\n\r\n* Part-of-Speech annotation\r\n* Constituency (phrase structure) syntactic annotation\r\n* Term annotation\r\n* Event annotation\r\n* Relation annotation\r\n* Coreference annotation\r\n\r\nSource: [http://www.geniaproject.org/genia-corpus](http://www.geniaproject.org/genia-corpus)\r\nImage Source: [http://www.geniaproject.org/genia-corpus](http://www.geniaproject.org/genia-corpus)", "variants": ["GENIA", "GENIA - LAS", "GENIA - UAS", "GENIA 2013"], "title": "T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-Less Objects"}
{"id": "Manga109", "contents": "**Manga109** has been compiled by the Aizawa Yamasaki Matsui Laboratory, Department of Information and Communication Engineering, the Graduate School of Information Science and Technology, the University of Tokyo. The compilation is intended for use in academic research on the media processing of Japanese manga. Manga109 is composed of 109 manga volumes drawn by professional manga artists in Japan. These manga were commercially made available to the public between the 1970s and 2010s, and encompass a wide range of target readerships and genres (see the table in Explore for further details.) Most of the manga in the compilation are available at the manga library “Manga Library Z” (formerly the “Zeppan Manga Toshokan” library of out-of-print manga).\r\n\r\nSource: [Manga109](http://www.manga109.org/en/)\r\nImage Source: [https://arxiv.org/pdf/1510.04389v1.pdf](https://arxiv.org/pdf/1510.04389v1.pdf)", "variants": ["Manga109 - 16x upscaling", "Manga109 - 2x upscaling", "Manga109 - 3x upscaling", "Manga109 - 4x upscaling", "Manga109 - 8x upscaling", "Manga109"], "title": "Sketch-based Manga Retrieval using Manga109 Dataset"}
{"id": "Stylized ImageNet", "contents": "The Stylized-ImageNet dataset is created by removing local texture cues in ImageNet while retaining global shape information on natural images via AdaIN style transfer. This nudges CNNs towards learning more about shapes and less about local textures.\r\n\r\nSource: [Adversarial Examples Improve Image Recognition](https://arxiv.org/abs/1911.09665)\r\nImage Source: [https://github.com/rgeirhos/Stylized-ImageNet](https://github.com/rgeirhos/Stylized-ImageNet)", "variants": ["Stylized ImageNet"], "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness"}
{"id": "Waymo Open Dataset", "contents": "The Waymo Open Dataset is comprised of high resolution sensor data collected by autonomous vehicles operated by the Waymo Driver in a wide variety of conditions. \r\n\r\nThe Waymo Open Dataset currently contains 1,950 segments. The authors plan to grow this dataset in the future. Currently the datasets includes:\r\n\r\n* 1,950 segments of 20s each, collected at 10Hz (390,000 frames) in diverse geographies and conditions\r\n* Sensor data\r\n    * 1 mid-range lidar\r\n    * 4 short-range lidars\r\n    * 5 cameras (front and sides)\r\n    * Synchronized lidar and camera data\r\n    * Lidar to camera projections\r\n    * Sensor calibrations and vehicle poses\r\n* Labeled data\r\n    * Labels for 4 object classes - Vehicles, Pedestrians, Cyclists, Signs\r\n    * High-quality labels for lidar data in 1,200 segments\r\n    * 12.6M 3D bounding box labels with tracking IDs on lidar data\r\n    * High-quality labels for camera data in 1,000 segments\r\n    * 11.8M 2D bounding box labels with tracking IDs on camera data", "variants": ["Waymo Open Dataset"], "title": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset"}
{"id": "comma.ai", "contents": "The comma.ai dataset consists of 7 and a quarter hours of largely highway driving.", "variants": ["Comma.ai", "comma.ai"], "title": "Learning a Driving Simulator"}
{"id": "Set5", "contents": "The **Set5** dataset is a dataset consisting of 5 images (“baby”, “bird”, “butterfly”, “head”, “woman”) commonly used for testing performance of Image Super-Resolution models.\r\nImage Source: [http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html)", "variants": ["Set5", "Set5 - 2x upscaling", "Set5 - 3x upscaling", "Set5 - 4x upscaling", "Set5 - 8x upscaling", "Set5-2x"], "title": "Low-Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding"}
{"id": "Horse-10", "contents": "**Horse-10, an animal pose estimation dataset**\r\n\r\nPose estimation is an important tool for measuring behavior, and thus widely used in technology, medicine and biology. Due to innovations in both deep learning algorithms and large-scale datasets pose estimation on humans has gotten very powerful. However, typical human pose estimation benchmarks, such as MPII pose and COCO, contain many different individuals (>10K) in different contexts, but only very few example postures per individual. In real world application of pose estimation, users want to estimate the location of user-defined bodyparts by only labeling a few hundred frames on a small subset of individuals, yet want this to generalize to new individuals. Thus, one naturally asks the following question: Assume you have trained an algorithm that performs with high accuracy on a given (individual) animal for the whole repertoire of movement  - how well will it generalize to different individuals that have slightly or a dramatically different appearance? Unlike in common human pose estimation benchmarks here the setting is that datasets have many (annotated) poses per individual (>200) but only few individuals (1-25).   \r\n\r\nTo allow the field to tackle this challenge, we developed a novel benchmark, called Horse-10, comprising 30 diverse Thoroughbred horses, for which 22 body parts were labeled by an expert in *8,114* frames (animal pose estimation). Horses have various coat colors and the “in-the-wild” aspect of the collected data at various Thoroughbred yearling sales and farms added additional complexity. \r\n\r\nMoreover, we present Horse-C to contrast the domain shift inherent in the Horse-10 dataset with domain shift induced by common image corruptions.", "variants": ["Horse-10"], "title": "Pretraining boosts out-of-domain robustness for pose estimation"}
{"id": "WinoBias", "contents": "**WinoBias** contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenarios where entities could be interacting in plausible ways. Templates were selected to be challenging and designed to cover cases requiring semantics and syntax separately.\r\n\r\nSource: [WinoBias](https://uclanlp.github.io/corefBias/overview)\r\nImage Source: [https://uclanlp.github.io/corefBias/overview](https://uclanlp.github.io/corefBias/overview)", "variants": ["WinoBias"], "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods"}
{"id": "Twitter Cyberthreat Detection Dataset", "contents": "Twitter Cyberthreat Detection Dataset is a dataset that contains tweets from two sets of accounts related to cybersecurity. The tweets are annotated with different information such as whether they contain security-related information and named entities.\r\n\r\nSource: [https://arxiv.org/pdf/1904.01127.pdf](https://arxiv.org/pdf/1904.01127.pdf)", "variants": ["Twitter Cyberthreat Detection Dataset"], "title": "Cyberthreat Detection from Twitter using Deep Neural Networks"}
{"id": "HowTo100M", "contents": "HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:\r\n\r\n- 136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)\r\n- 23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness\r\n\r\nEach video is associated with a narration available as subtitles automatically downloaded from Youtube.\r\n\r\nSource: [HowTo100M](https://www.di.ens.fr/willow/research/howto100m/)", "variants": ["Howto100M-QA", "HowTo100M"], "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips"}
{"id": "YT-BB", "contents": "YouTube-BoundingBoxes (YT-BB) is a large-scale data set of video URLs with densely-sampled object bounding box annotations. The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the MS COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. \r\n\r\nSource: [YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video](https://arxiv.org/pdf/1702.00824.pdf)", "variants": ["YT-BB"], "title": "YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video"}
{"id": "UAVDT", "contents": "UAVDT is a large scale challenging UAV Detection and Tracking benchmark (i.e., about 80, 000 representative frames from 10 hours raw videos) for 3 important fundamental tasks, i.e., object DETection\r\n(DET), Single Object Tracking (SOT) and Multiple Object Tracking (MOT).\r\n\r\nThe dataset is captured by UAVs in various complex scenarios. The objects of\r\ninterest in this benchmark are vehicles. The frames are manually annotated with bounding boxes and some useful attributes, e.g., vehicle category and occlusion. \r\n\r\nThe UAVDT benchmark consists of 100 video sequences, which are selected\r\nfrom over 10 hours of videos taken with an UAV platform at a number of locations in urban areas, representing various common scenes including squares, arterial streets, toll stations, highways, crossings and T-junctions. The videos\r\nare recorded at 30 frames per seconds (fps), with the JPEG image resolution of 1080 × 540 pixels.", "variants": ["UAVDT"], "title": "The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking"}
{"id": "MultiSenti", "contents": "MultiSenti presents a labeled dataset called MultiSenti for sentiment classification of code-switched informal short text, (2) explore the feasibility of adapting resources from a resource-rich language for an informal one, and (3) propose a deep learning-based model for sentiment classification of code-switched informal short text.\r\n\r\nSource: [Adapting Deep Learning for Sentiment Classification of Code-Switched Informal Short Text](/paper/adapting-deep-learning-for-sentiment)", "variants": ["MultiSenti"], "title": "Adapting Deep Learning for Sentiment Classification of Code-Switched Informal Short Text"}
{"id": "COFW", "contents": "The **Caltech Occluded Faces in the Wild** (**COFW**) dataset is designed to present faces in real-world conditions. Faces show large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food, hands, microphones, etc.). All images were hand annotated using the same 29 landmarks as in LFPW. Both the landmark positions as well as their occluded/unoccluded state were annotated. The faces are occluded to different degrees, with large variations in the type of occlusions encountered. COFW has an average occlusion of over 23.\r\n\r\nSource: [http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset](http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset)\r\nImage Source: [http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset](http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset)", "variants": ["COFW"], "title": "Robust Face Landmark Estimation under Occlusion"}
{"id": "Cholec80", "contents": "Cholec80 is an endoscopic video dataset containing 80 videos of cholecystectomy surgeries performed by 13 surgeons. The videos are captured at 25 fps and downsampled to 1 fps for processing. The whole dataset is labeled with the phase and tool presence annotations. The phases have been defined by a senior surgeon in Strasbourg hospital, France. Since the tools are sometimes hardly visible in the images and thus difficult to be recognized visually, a tool is defined as present in an image if at least half of the tool tip is visible.\r\n\r\nSource: [EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos](/paper/endonet-a-deep-architecture-for-recognition)[https://arxiv.org/pdf/1602.03012.pdf]", "variants": ["Cholec80"], "title": "EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos"}
{"id": "Texygen Platform", "contents": "Texygen is a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and facilitate the sharing of fine-tuned open-source implementations among researchers for their work. As a consequence, this would help in improving the reproductivity and reliability of future research work in text generation.\r\n\r\nSource: [Texygen Platform](https://github.com/geek-ai/Texygen)", "variants": ["Texygen Platform"], "title": "Texygen: A Benchmarking Platform for Text Generation Models"}
{"id": "LOGO-Net", "contents": "A large-scale logo image database for logo detection and brand recognition from real-world product images. \r\n\r\nSource: [LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with Deep Region-based Convolutional Networks](/paper/logo-net-large-scale-deep-logo-detection-and)", "variants": ["LOGO-Net"], "title": "LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with Deep Region-based Convolutional Networks"}
{"id": "CoNLL 2002", "contents": "The shared task of CoNLL-2002 concerns language-independent named entity recognition. The types of named entities include: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The participants of the shared task were offered training and test data for at least two languages. Information sources other than the training data might have been used in this shared task.\r\n\r\nSource: [CoNLL 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\r\nImage Source: [https://www.aclweb.org/anthology/W02-2024.pdf](https://www.aclweb.org/anthology/W02-2024.pdf)", "variants": ["CoNLL 2002 (Spanish)", "CoNLL 2002 (Dutch)", "CoNLL 2002"], "title": "SParC: Cross-Domain Semantic Parsing in Context"}
{"id": "Composed Quora", "contents": "The **Composed Quora** dataset consists of questions extracted from Quora that are grouped together if they are asking the same thing. The dataset contains 60,400 groups of questions, each group with at least 3 questions that are asking the same.\n\nSource: [https://arxiv.org/pdf/1911.02747.pdf](https://arxiv.org/pdf/1911.02747.pdf)", "variants": ["Composed Quora"], "title": "Query-bag Matching with Mutual Coverage for Information-seeking Conversations in E-commerce"}
{"id": "FSDnoisy18k", "contents": "The **FSDnoisy18k** dataset is an open dataset containing 42.5 hours of audio across 20 sound event classes, including a small amount of manually-labeled data and a larger quantity of real-world noisy data. The audio content is taken from Freesound, and the dataset was curated using the Freesound Annotator. The noisy set of FSDnoisy18k consists of 15,813 audio clips (38.8h), and the test set consists of 947 audio clips (1.4h) with correct labels. The dataset features two main types of label noise: in-vocabulary (IV) and out-of-vocabulary (OOV). IV applies when, given an observed label that is incorrect or incomplete, the true or missing label is part of the target class set. Analogously, OOV means that the true or missing label is not covered by those 20 classes.\r\n\r\nSource: [Model-agnostic Approaches to Handling Noisy Labels When Training Sound Event Classifiers](https://arxiv.org/abs/1910.12004)\nImage Source: [http://www.eduardofonseca.net/FSDnoisy18k/](http://www.eduardofonseca.net/FSDnoisy18k/)", "variants": ["FSDnoisy18k"], "title": "Learning Sound Event Classifiers from Web Audio with Noisy Labels"}
{"id": "UG^2", "contents": "Contains three difficult real-world scenarios: uncontrolled videos taken by UAVs and manned gliders, as well as controlled videos taken on the ground. Over 160,000 annotated frames forhundreds of ImageNet classes are available, which are used for baseline experiments that assess the impact of known and unknown image artifacts and other conditions on common deep learning-based object classification approaches.\r\n\r\nSource: [UG^2: a Video Benchmark for Assessing the Impact of Image Restoration and Enhancement on Automatic Visual Recognition](/paper/ug2-a-video-benchmark-for-assessing-the)", "variants": ["UG^2"], "title": "UG^2: a Video Benchmark for Assessing the Impact of Image Restoration andEnhancement on Automatic Visual Recognition"}
{"id": "Yeast", "contents": "Yeast dataset consists of a protein-protein interaction network. Interaction detection methods have led to the discovery of thousands of interactions between proteins, and discerning relevance within large-scale data sets is important to present-day biology.\r\n\r\nSource: [http://vlado.fmf.uni-lj.si/pub/networks/data/bio/Yeast/Yeast.htm](http://vlado.fmf.uni-lj.si/pub/networks/data/bio/Yeast/Yeast.htm)", "variants": ["Yeast"], "title": "Deep Metric Learning via Lifted Structured Feature Embedding"}
{"id": "UniMiB SHAR", "contents": "Includes 11,771 samples of both human activities and falls performed by 30 subjects of ages ranging from 18 to 60 years. Samples are divided in 17 fine grained classes grouped in two coarse grained classes: one containing samples of 9 types of activities of daily living (ADL) and the other containing samples of 8 types of falls. The dataset has been stored to include all the information useful to select samples according to different criteria, such as the type of ADL, the age, the gender, and so on. \r\n\r\nSource: [UniMiB SHAR: a new dataset for human activity recognition using acceleration data from smartphones](/paper/unimib-shar-a-new-dataset-for-human-activity)", "variants": ["UniMiB SHAR"], "title": "UniMiB SHAR: a new dataset for human activity recognition using acceleration data from smartphones"}
{"id": "CAD-120", "contents": "The CAD-60 and **CAD-120** data sets comprise of RGB-D video sequences of humans performing activities which are recording using the Microsoft Kinect sensor. Being able to detect human activities is important for making personal assistant robots useful in performing assistive tasks. The CAD dataset comprises twelve different activities (composed of several sub-activities) performed by four people in different environments, such as a kitchen, a living room, and office, etc.\r\n\r\nSource: [https://www.re3data.org/repository/r3d100012216](https://www.re3data.org/repository/r3d100012216)\r\nImage Source: [https://www.researchgate.net/figure/The-CAD-120-dataset-A-Examples-of-high-level-activities-from-the-dataset-B-A_fig3_335424041](https://www.researchgate.net/figure/The-CAD-120-dataset-A-Examples-of-high-level-activities-from-the-dataset-B-A_fig3_335424041)", "variants": ["CAD-120"], "title": "Learning human activities and object affordances from RGB-D videos"}
{"id": "Horne 2017 Fake News Data", "contents": "The **Horne 2017 Fake News Data** contains two independed news datasets:\r\n\r\n1. Buzzfeed Political News Data:\r\n\r\n    * News originally analyzed by Craig Silverman of Buzzfeed News in article entitled \" This Analysis Shows How Viral Fake Election News Stories Outperformed Real News On Facebook.\"\r\n    * BuzzFeed News used keyword search on the content analysis tool BuzzSumo to find news stories\r\n    * Post the analysis of Buzzfeed News, the authors collect the body text and body title of all articles and use the ground truth as set by Buzzfeed as actual ground truth.\r\n    * This data set has fewer clear restrictions on the ground truth, including opinion-based real stories and satire-based fake stories. In our study, the authors manually filter this data set down to contain only \"hard\" news stories and malicious fake news stories. This repository contains the whole dataset with no filtering.\r\n\r\n2. Random Political News Data:\r\n    * Randomly collected from three types of sources during 2016.\r\n    * Sources ground truth determined through: Business Insider’s “Most Trusted” list and Zimdars 2016 Fake news list\r\n    * Sources:\r\n        - Real: Wall Street Journal, The Economist, BBC, NPR, ABC, CBS, USA Today, The Guardian, NBC, The Washington Post\r\n        - Satire: The Onion, Huffington Post Satire, Borowitz Report, The Beaverton, Satire Wire, and Faking News\r\n        - Fake: Ending The Fed, True Pundit, abcnews.com.co, DC Gazette, Liberty Writers News, Before its News, InfoWars, Real News Right Now\r\n\r\nSource: [https://github.com/BenjaminDHorne/fakenewsdata1](https://github.com/BenjaminDHorne/fakenewsdata1)", "variants": ["Horne 2017 Fake News Data"], "title": "This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive Content in Text Body, More Similar to Satire than Real News"}
{"id": "STACKEX", "contents": "STACKEX expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. \r\n\r\nSource: [One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases](https://arxiv.org/pdf/1810.05241v4.pdf)", "variants": ["STACKEX"], "title": "Generating Diverse Numbers of Diverse Keyphrases"}
{"id": "LINEMOD", "contents": "**LINEMOD** is an RGB+D dataset, which has become a de facto standard benchmark for 6D pose estimation. The dataset contains poorly textured objects in a cluttered scene. The dataset contains 15 object sequences. The images in each object sequence contain multiple objects, however, only one object is annotated with the ground-truth class label, bounding box, and 6D pose. The camera intrinsic matrix is also provided with the dataset.\r\n\r\nSource: [Deep-6DPose: Recovering 6D Object Pose from a Single RGB Image](https://arxiv.org/abs/1802.10367)", "variants": ["LineMOD", "Occlusion LineMOD", "Synth Objects-to-LINEMOD", "LINEMOD"], "title": "Model Based Training, Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes"}
{"id": "SoccerDB", "contents": "Comprises of 171,191 video segments from 346 high-quality soccer games. The database contains 702,096 bounding boxes, 37,709 essential event labels with time boundary and 17,115 highlight annotations for object detection, action recognition, temporal action localization, and highlight detection tasks. \r\n\r\nSource: [SoccerDB: A Large-Scale Database for Comprehensive Video Understanding](/paper/comprehensive-soccer-video-understanding)", "variants": ["SoccerDB"], "title": "Comprehensive Soccer Video Understanding: Towards Human-comparable Video Understanding System in Constrained Environment"}
{"id": "Obstacle Tower", "contents": "Obstacle Tower is a high fidelity, 3D, 3rd person, procedurally generated environment for reinforcement learning. An agent playing Obstacle Tower must learn to solve both low-level control and high-level planning problems in tandem while learning from pixels and a sparse reward signal. Unlike other benchmarks such as the Arcade Learning Environment, evaluation of agent performance in Obstacle Tower is based on an agent’s ability to perform well on unseen instances of the environment.\r\n\r\nSource: [Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning](/paper/obstacle-tower-a-generalization-challenge-in)", "variants": ["Obstacle Tower", "Obstacle Tower (No Gen) fixed", "Obstacle Tower (No Gen) varied", "Obstacle Tower (Strong Gen) fixed", "Obstacle Tower (Strong Gen) varied", "Obstacle Tower (Weak Gen) fixed", "Obstacle Tower (Weak Gen) varied"], "title": "Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning"}
{"id": "emrQA", "contents": "emrQA has 1 million question-logical form and 400,000+ questionanswer evidence pairs.\r\n\r\nSource: [emrQA: A Large Corpus for Question Answering on Electronic Medical Records](https://arxiv.org/pdf/1809.00732.pdf)\r\nImage Source: [https://arxiv.org/pdf/1809.00732v1.pdf](https://arxiv.org/pdf/1809.00732v1.pdf)", "variants": ["emrQA"], "title": "emrQA: A Large Corpus for Question Answering on Electronic Medical Records"}
{"id": "Advising Corpus", "contents": "Advising Corpus is a dataset based on an entirely new collection of dialogues in which university students are being advised which classes to take. These were collected at the University of Michigan with IRB approval. They were released as part of DSTC 7 track 1 and used again in DSTC 8 track 2.\r\n\r\nSource: [Gunasekara et al.](http://workshop.colips.org/dstc7/papers/dstc7_task1_final_report.pdf)\r\n\r\nImage source: [Gunasekara et al.](http://workshop.colips.org/dstc7/papers/dstc7_task1_final_report.pdf)", "variants": ["Advising Corpus"], "title": "DSTC7 Task 1: Noetic End-to-End Response Selection"}
{"id": "AQUA-RAT", "contents": "Algebra Question Answering with Rationales (AQUA-RAT) is a dataset that contains algebraic word problems with rationales. The dataset consists of about 100,000 algebraic word problems with natural language rationales. Each problem is a json object consisting of four parts:\r\n* question - A natural language definition of the problem to solve\r\n* options - 5 possible options (A, B, C, D and E), among which one is correct\r\n* rationale - A natural language description of the solution to the problem\r\n* correct - The correct option\r\n\r\nSource: [https://github.com/deepmind/AQuA](https://github.com/deepmind/AQuA)", "variants": ["AQUA-RAT"], "title": "Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems"}
{"id": "Verse", "contents": "Verse is a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. \r\n\r\nSource: [Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings](https://www.aclweb.org/anthology/N16-1022.pdf)", "variants": ["Verse"], "title": "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings"}
{"id": "BiasBios", "contents": "", "variants": ["BiasBios"], "title": "Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting"}
{"id": "DeepFluoroLabeling-IPCAI2020", "contents": "This collection contains data and code associated with the IPCAI/IJCARS 2020 paper “Automatic Annotation of Hip Anatomy in Fluoroscopy for Robust and Efficient 2D/3D Registration.” The data hosted here consists of annotated datasets of actual hip fluoroscopy, CT and derived data from six lower torso cadaveric specimens. Documentation and examples for using the dataset and Python code for training and testing the proposed models are also included. Higher-level information, including clinical motivations, prior works, algorithmic details, applications to 2D/3D registration, and experimental details, may be found in the companion paper which is available at [https://arxiv.org/abs/1911.07042](https://arxiv.org/abs/1911.07042) or [https://doi.org/10.1007/s11548-020-02162-7](https://doi.org/10.1007/s11548-020-02162-7). We hope that this code and data will be useful in the development of new computer-assisted capabilities that leverage fluoroscopy.", "variants": ["DeepFluoroLabeling-IPCAI2020"], "title": "Automatic Annotation of Hip Anatomy in Fluoroscopy for Robust and Efficient 2D/3D Registration"}
{"id": "ImageNet-Sketch", "contents": "ImageNet-Sketch data set consists of 50000 images, 50 images for each of the 1000 ImageNet classes. The data set is constructed with Google Image queries \"sketch of __\", where __ is the standard class name. Only within the \"black and white\" color scheme is searched. 100 images are initially queried for every class, and the pulled images are cleaned by deleting the irrelevant images and images that are for similar but different classes. For some classes, there are less than 50 images after manually cleaning, and then the data set is augmented by flipping and rotating the images.\r\n\r\nSource: [ImageNet-Sketch](https://github.com/HaohanWang/ImageNet-Sketch)\r\nImage Source: [https://github.com/HaohanWang/ImageNet-Sketch](https://github.com/HaohanWang/ImageNet-Sketch)", "variants": ["ImageNet-Sketch"], "title": "Learning Robust Global Representations by Penalizing Local Predictive Power"}
{"id": "Mafiascum", "contents": "A collection of over 700 games of Mafia, in which players are randomly assigned either deceptive or non-deceptive roles and then interact via forum postings. Over 9000 documents were compiled from the dataset, which each contained all messages written by a single player in a single game. This corpus was used to construct a set of hand-picked linguistic features based on prior deception research, as well as a set of average word vectors enriched with subword information. \r\n\r\nSource: [The Mafiascum Dataset: A Large Text Corpus for Deception Detection](/paper/the-mafiascum-dataset-a-large-text-corpus-for)", "variants": ["Mafiascum"], "title": "The Mafiascum Dataset: A Large Text Corpus for Deception Detection"}
{"id": "EMOTIC", "contents": "EMOTIC is a dataset of images of people in a diverse set of natural situations, annotated with their apparent emotion.\r\n\r\nSource: [Context Based Emotion Recognition using EMOTIC Dataset](https://arxiv.org/pdf/2003.13401v1.pdf)", "variants": ["EMOTIC"], "title": "Context Based Emotion Recognition using EMOTIC Dataset"}
{"id": "How2", "contents": "The **How2** dataset contains 13,500 videos, or 300 hours of speech, and is split into 185,187 training, 2022 development (dev), and 2361 test utterances. It has subtitles in English and crowdsourced Portuguese translations.\r\n\r\nSource: [exploring multiview correlations in open-domain videos](https://arxiv.org/abs/1811.08890)", "variants": ["How2", "How2 300h"], "title": "How2: A Large-scale Dataset for Multimodal Language Understanding"}
{"id": "Evidence Inference", "contents": "Evidence Inference is a corpus for this task comprising 10,000+ prompts coupled with full-text articles describing RCTs. \r\n\r\nSource: [Inferring Which Medical Treatments Work from Reports of Clinical Trials](/paper/inferring-which-medical-treatments-work-from)", "variants": ["Evidence Inference"], "title": "Inferring Which Medical Treatments Work from Reports of Clinical Trials"}
{"id": "Q-Traffic", "contents": "**Q-Traffic** is a large-scale traffic prediction dataset, which consists of three sub-datasets: query sub-dataset, traffic speed sub-dataset and road network sub-dataset.\n\nSource: [https://github.com/JingqingZ/BaiduTraffic](https://github.com/JingqingZ/BaiduTraffic)\nImage Source: [https://github.com/JingqingZ/BaiduTraffic](https://github.com/JingqingZ/BaiduTraffic)", "variants": ["Q-Traffic"], "title": "Deep Sequence Learning with Auxiliary Information for Traffic Prediction"}
{"id": "WHOI-Plankton", "contents": "WHOI-Plankton is a collection of annotated plankton images. It contains > 3.5 million images of microscopic marine plankton, organized according to category labels provided by researchers at the Woods Hole Oceanographic Institution (WHOI). The images are currently placed into one of 103 categories.\r\n\r\nSource: [WHOI-Plankton- A Large Scale Fine Grained Visual Recognition Benchmark Dataset for Plankton Classification](/paper/whoi-plankton-a-large-scale-fine-grained)", "variants": ["WHOI-Plankton"], "title": "WHOI-Plankton- A Large Scale Fine Grained Visual Recognition Benchmark Dataset for Plankton Classification"}
{"id": "Localized Narratives", "contents": "We propose Localized Narratives, a new form of multimodal image annotations connecting vision and language. We ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. Since the voice and the mouse pointer are synchronized, we can localize every single word in the description. This dense visual grounding takes the form of a mouse trace segment per word and is unique to our data. We annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and ADE20K datasets, and 671k images of Open Images, all of which we make publicly available. We provide an extensive analysis of these annotations showing they are diverse, accurate, and efficient to produce. We also demonstrate their utility on the application of controlled image captioning.", "variants": ["Localized Narratives"], "title": "Connecting Vision and Language with Localized Narratives"}
{"id": "Quoref", "contents": "Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems. In this span-selection benchmark containing 24K questions over 4.7K paragraphs from Wikipedia, a system must resolve hard coreferences before selecting the appropriate span(s) in the paragraphs for answering questions.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/quoref)", "variants": ["Quoref"], "title": "Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning"}
{"id": "MSD", "contents": "The Million Song Dataset is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.\r\n\r\nThe core of the dataset is the feature analysis and metadata for one million songs, provided by The Echo Nest. The dataset does not include any audio, only the derived features. Note, however, that sample audio can be fetched from services like 7digital, using [code]( https://github.com/tbertinmahieux/MSongsDB/tree/master/Tasks_Demos/Preview7digital) provided by the authors.\r\n\r\n\r\nSource: [http://millionsongdataset.com/](http://millionsongdataset.com/)", "variants": ["Million Song Dataset", "MSD"], "title": "Where Is My Mirror?"}
{"id": "MPII Human Pose", "contents": "**MPII Human Pose** Dataset is a dataset for human pose estimation. It consists of around 25k images extracted from online videos. Each image contains one or more people, with over 40k people annotated in total. Among the 40k samples, ∼28k samples are for training and the remainder are for testing. Overall the dataset covers 410 human activities and each image is provided with an activity label. Images were extracted from a YouTube video and provided with preceding and following un-annotated frames.\r\n\r\nSource: [Accelerating Large-Kernel Convolution Using Summed-Area Tables](https://arxiv.org/abs/1906.11367)\r\nImage Source: [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)", "variants": ["MPII Human Pose", "MPII Single Person"], "title": "2D Human Pose Estimation: New Benchmark and State of the Art Analysis"}
{"id": "PIQA", "contents": "PIQA is a dataset for commonsense reasoning, and was created to investigate the physical knowledge of existing models in NLP. \r\n\r\nSource: [PIQA](https://yonatanbisk.com/piqa/)", "variants": ["PIQA"], "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}
{"id": "Sim10k", "contents": "SIM10k is a synthetic dataset containing 10,000 images, which is rendered from the video game Grand Theft Auto V (GTA5).\r\n\r\nSource: [Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation](https://arxiv.org/abs/2003.10275)\r\nImage Source: [https://arxiv.org/pdf/1610.01983.pdf](https://arxiv.org/pdf/1610.01983.pdf)", "variants": ["SIM10K to Cityscapes", "SIM10K to BDD100K", "Sim10k"], "title": "Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?"}
{"id": "Office-Home", "contents": "**Office-Home** is a benchmark dataset for domain adaptation which contains 4 domains where each domain consists of 65 categories. The four domains are: Art – artistic images in the form of sketches, paintings, ornamentation, etc.; Clipart – collection of clipart images; Product – images of objects without a background and Real-World – images of objects captured with a regular camera. It contains 15,500 images, with an average of around 70 images per class and a maximum of 99 images in a class.\r\n\r\nSource: [Multi-component Image Translation for Deep Domain Generalization](https://arxiv.org/abs/1812.08974)\r\n\r\nImage Source: [Wen et al](https://www.researchgate.net/publication/329023199_Exploiting_Local_Feature_Patterns_for_Unsupervised_Domain_Adaptation)", "variants": ["Office-Home", "Office-Home (RS-UT imbalance)"], "title": "Deep Hashing Network for Unsupervised Domain Adaptation"}
{"id": "FIW", "contents": "FIW is a large and comprehensive database available for kinship recognition. FIW is made up of 11,932 natural family photos of 1,000 families-- nearly 10x more than the next-to-largest, [Family-101](family101) database. Also, it contains 656,954 image pairs split between the 11 relationships, which is much larger than the 2nd to largest [KinFaceW-II](kinfacew) with 2,000 pairs for only 4 kinship types.", "variants": ["FIW"], "title": "Families in the Wild (FIW): Large-Scale Kinship Image Database and Benchmarks"}
{"id": "Icentia11K", "contents": "Public ECG dataset of continuous raw signals for representation learning containing 11 thousand patients and 2 billion labelled beats.\r\n\r\nSource: [Icentia11K: An Unsupervised Representation Learning Dataset for Arrhythmia Subtype Discovery](/paper/icentia11k-an-unsupervised-representation)", "variants": ["Icentia11K"], "title": "Icentia11K: An Unsupervised Representation Learning Dataset for Arrhythmia Subtype Discovery"}
{"id": "LiTS17", "contents": "**LiTS17** is a liver tumor segmentation benchmark. The data and segmentations are provided by various clinical sites around the world. The training data set contains 130 CT scans and the test data set 70 CT scans.\nImage Source: [https://arxiv.org/pdf/1707.07734.pdf](https://arxiv.org/pdf/1707.07734.pdf)", "variants": ["LiTS2017", "LiTS17"], "title": "The Liver Tumor Segmentation Benchmark (LiTS)"}
{"id": "ADVIO", "contents": "Provides a wide range of raw sensor data that is accessible on almost any modern-day smartphone together with a high-quality ground-truth track. \r\n\r\nSource: [ADVIO: An authentic dataset for visual-inertial odometry](/paper/advio-an-authentic-dataset-for-visual)", "variants": ["ADVIO"], "title": "ADVIO: An authentic dataset for visual-inertial odometry"}
{"id": "UI-PRMD", "contents": "UI-PRMD is a data set of movements related to common exercises performed by patients in physical therapy and rehabilitation programs. The data set consists of 10 rehabilitation exercises. A sample of 10 healthy individuals repeated each exercise 10 times in front of two sensory systems for motion capturing: a Vicon optical tracker, and a Kinect camera. The data is presented as positions and angles of the body joints in the skeletal models provided by the Vicon and Kinect mocap systems.", "variants": ["UI-PRMD"], "title": "A Deep Learning Framework for Assessing Physical Rehabilitation Exercises"}
{"id": "FIVR-200K", "contents": "The FIVR-200K dataset has been collected to simulate the problem of Fine-grained Incident Video Retrieval (FIVR). The dataset comprises 225,960 videos associated with 4,687 Wikipedia events and 100 selected video queries.", "variants": ["FIVR-200K"], "title": "FIVR: Fine-grained Incident Video Retrieval"}
{"id": "UBC3V Dataset", "contents": "~6 million synthetic depth frames for pose estimation from multiple cameras.\r\n\r\nSource: [Real-Time Human Motion Capture with Multiple Depth Cameras](/paper/real-time-human-motion-capture-with-multiple)", "variants": ["UBC3V Dataset"], "title": "Real-Time Human Motion Capture with Multiple Depth Cameras"}
{"id": "JSUT Corpus", "contents": "JSUT Corpus is a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist.\r\n\r\nSource: [JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis](/paper/jsut-corpus-free-large-scale-japanese-speech)", "variants": ["JSUT Corpus"], "title": "JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis"}
{"id": "PCDS", "contents": "Contains over 4,500 videos recorded at the entrance doors of buses in normal and cluttered conditions. It also proposes an efficient method for counting people in real-world cluttered scenes related to public transportations using depth videos. \r\n\r\nSource: [Benchmark data and method for real-time people counting in cluttered scenes using depth sensors](/paper/benchmark-data-and-method-for-real-time)", "variants": ["PCDS"], "title": "Benchmark Data and Method for Real-Time People Counting in Cluttered Scenes Using Depth Sensors"}
{"id": "KorSTS", "contents": "**KorSTS** is a dataset for semantic textural similarity (STS) in Korean. The dataset is constructed by automatically the STS-B dataset. To ensure translation quality, two professional translators with at least seven years of experience who specialize in academic papers/books as well as business contracts post-edited a half of the dataset each and cross-checked each other’s translation afterward.\r\nThe KorSTS dataset comprises 5,749 training examples translated automatically and 2,879 evaluation examples translated manually.\r\n\r\nSource: [https://github.com/kakaobrain/KorNLUDatasets](https://github.com/kakaobrain/KorNLUDatasets)\r\nImage Source: [https://github.com/kakaobrain/KorNLUDatasets](https://github.com/kakaobrain/KorNLUDatasets)", "variants": ["KorSTS"], "title": "KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding"}
{"id": "TVSeries", "contents": "A realistic dataset composed of 27 episodes from 6 popular TV series. The dataset spans over 16 hours of footage annotated with 30 action classes, totaling 6,231 action instances.\r\n\r\nSource: [Online Action Detection](/paper/online-action-detection)", "variants": ["TVSeries"], "title": "Online Action Detection"}
{"id": "Fashion IQ", "contents": "Fashion IQ support and advance research on interactive fashion image retrieval. Fashion IQ is the first fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual attribute labels for these images.\r\n\r\nSource: [Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback](/paper/the-fashion-iq-dataset-retrieving-images-by)", "variants": ["FashionIQ", "Fashion IQ"], "title": "The Fashion IQ Dataset: Retrieving Images by Combining Side Information and Relative Natural Language Feedback."}
{"id": "XQuAD", "contents": "XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently, the dataset is entirely parallel across 11 languages.\r\n\r\nSource: [XQuAD](https://github.com/deepmind/xquad)\r\nImage Source: [https://arxiv.org/pdf/1910.11856v3.pdf](https://arxiv.org/pdf/1910.11856v3.pdf)", "variants": ["XQuAD"], "title": "On the Cross-lingual Transferability of Monolingual Representations"}
{"id": "UCLA Aerial Event Dataset", "contents": "The UCLA Aerial Event Dataest has been captured by a low-cost hex-rotor with a GoPro camera, which is able to eliminate the high frequency vibration of the camera and hold in air autonomously through a GPS and a barometer. It can also fly 20 ∼ 90m above the ground and stays 5 minutes in air. \r\n\r\nThis hex-rotor has been used to take the set of videos in the dataset, captured in different places: hiking routes, parking lots, camping sites, picnic areas with shelters, restrooms, tables, trash bins and BBQ ovens. By detecting/tracking humans and objects in the videos, the videos can be annotated with events.\r\n\r\nThe original videos are pre-processed, including camera calibration and frame registration. After pre-processing, there are totally 27 videos in the dataset, the length of which ranges from 2 minutes to 5 minutes. Each video has annotations with hierarchical semantic information of objects, roles, events and groups in the videos.\r\n\r\nSource: [Joint Inference of Groups, Events and Human Roles in Aerial Videos](http://www.stat.ucla.edu/~tianmin.shu/AerialVideo/AerialVideo.html)", "variants": ["UCLA Aerial Event Dataset"], "title": "Joint inference of groups, events and human roles in aerial videos"}
{"id": "SALICON", "contents": "The SALIency in CONtext (**SALICON**) dataset contains 10,000 training images, 5,000 validation images and 5,000 test images for saliency prediction. This dataset has been created by annotating saliency in images from MS COCO.\r\nThe ground-truth saliency annotations include fixations generated from mouse trajectories. To improve the data quality, isolated fixations with low local density have been excluded.\r\nThe training and validation sets, provided with ground truth, contain the following data fields: image, resolution and gaze.\r\nThe testing data contains only the image and resolution fields.\r\n\r\nSource: [DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations](https://arxiv.org/abs/1510.02927)\r\nImage Source: [http://salicon.net/explore/](http://salicon.net/explore/)", "variants": ["SALICON->WebpageSaliency - 1-shot", "SALICON->WebpageSaliency - EUB", "SALICON", "SALICON->WebpageSaliency - 10-shot ", "SALICON->WebpageSaliency - 5-shot "], "title": "SALICON: Saliency in Context"}
{"id": "ModaNet", "contents": "ModaNet is a street fashion images dataset consisting of annotations related to RGB images. ModaNet provides multiple polygon annotations for each image. Each polygon is associated with a label from 13 meta fashion categories. The annotations are based on images in the PaperDoll image set, which has only a few hundred images annotated by the superpixel-based tool.\r\n\r\nSource: [ModaNet](https://github.com/eBay/modanet)\r\nImage Source: [ModaNet](https://github.com/eBay/modanet)", "variants": ["ModaNet Dev", "ModaNet"], "title": "ModaNet: A Large-scale Street Fashion Dataset with Polygon Annotations"}
{"id": "LSP", "contents": "The **Leeds Sports Pose** (**LSP**) dataset is widely used as the benchmark for human pose estimation. The original LSP dataset contains 2,000 images of sportspersons gathered from Flickr, 1000 for training and 1000 for testing. Each image is annotated with 14 joint locations, where left and right joints are consistently labelled from a person-centric viewpoint. The extended LSP dataset contains additional 10,000 images labeled for training.\r\n\r\nSource: [Deep Deformation Network for Object Landmark Localization](https://arxiv.org/abs/1605.01014)\r\n\r\nImage: [Sumer et al](https://www.researchgate.net/figure/Pose-estimation-results-in-Leeds-Sports-Pose-dataset-First-images-are-from-test-set-with_fig3_322058596)", "variants": ["Leeds Sports Poses", " Leeds Sports Pose", "LSP"], "title": "Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation"}
{"id": "LVIS", "contents": "LVIS is a dataset for long tail instance segmentation. It has annotations for over 1000 object categories in 164k images.\r\n\r\nSource: [LVIS](https://arxiv.org/pdf/1908.03195.pdf)", "variants": ["LVIS v1.0", "LVIS"], "title": "LVIS: A Dataset for Large Vocabulary Instance Segmentation"}
{"id": "BigPatent", "contents": "Consists of 1.3 million records of U.S. patent documents along with human written abstractive summaries.\r\n\r\nSource: [BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization](https://arxiv.org/pdf/1906.03741v1.pdf)", "variants": ["BigPatent"], "title": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization"}
{"id": "SBIC", "contents": "To support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups.\r\n\r\nSource: [Social Bias Frames: Reasoning about Social and Power Implications of Language](/paper/social-bias-frames-reasoning-about-social-and)", "variants": ["SBIC"], "title": "Social Bias Frames: Reasoning about Social and Power Implications of Language"}
{"id": "MSVD", "contents": "The **Microsoft Research Video Description Corpus** (**MSVD**) dataset consists of about 120K sentences collected during the summer of 2010. Workers on Mechanical Turk were paid to watch a short video snippet and then summarize the action in a single sentence. The result is a set of roughly parallel descriptions of more than 2,000 video snippets. Because the workers were urged to complete the task in the language of their choice, both paraphrase and bilingual alternations are captured in the data.\r\n\r\nSource: [https://www.microsoft.com/en-us/download/details.aspx?id=52422&from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2F](https://www.microsoft.com/en-us/download/details.aspx?id=52422&from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2F)\r\nImage Source: [https://arxiv.org/pdf/1609.06782.pdf](https://arxiv.org/pdf/1609.06782.pdf)", "variants": ["MSVD", "MSVD-QA"], "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"}
{"id": "LectureBank", "contents": "**LectureBank** Dataset is a manually collected dataset of lecture slides. It contains 1,352 online lecture files from 60 courses covering 5 different domains, including Natural Language Processing (nlp), Machine Learning (ml), Artificial Intelligence (ai), Deep Learning (dl) and Information Retrieval (ir). In addition, it also contains the corresponding annotations for each slide.\n\nSource: [https://github.com/Yale-LILY/LectureBank](https://github.com/Yale-LILY/LectureBank)\nImage Source: [https://github.com/Yale-LILY/LectureBank](https://github.com/Yale-LILY/LectureBank)", "variants": ["LectureBank"], "title": "What Should I Learn First: Introducing LectureBank for NLP Education and Prerequisite Chain Learning"}
{"id": "HS-SOD", "contents": "HS-SOD is a hyperspectral salient object detection dataset with a collection of 60 hyperspectral images with their respective ground-truth binary images and representative rendered colour images (sRGB).\r\n\r\nSource: [Hyperspectral Image Dataset for Benchmarking on Salient Object Detection](/paper/hyperspectral-image-dataset-for-benchmarking)", "variants": ["HS-SOD"], "title": "Hyperspectral Image Dataset for Benchmarking on Salient Object Detection"}
{"id": "FoodX-251", "contents": "FoodX-251 is a dataset of 251 fine-grained classes with 118k training, 12k validation and 28k test images. Human verified labels are made available for the training and test images. The classes are fine-grained and visually similar, for example, different types of cakes, sandwiches, puddings, soups, and pastas.\r\n\r\nSource: [FoodX-251: A Dataset for Fine-grained Food Classification](/paper/foodx-251-a-dataset-for-fine-grained-food)\r\nImage Source: [Kaur et al](https://arxiv.org/pdf/1907.06167v1.pdf)", "variants": ["FoodX-251"], "title": "FoodX-251: A Dataset for Fine-grained Food Classification"}
{"id": "NAB", "contents": "**The First Temporal Benchmark Designed to Evaluate  Real-time Anomaly Detectors Benchmark**\r\n\r\nThe growth of the Internet of Things has created an abundance of streaming data. Finding anomalies in this data can provide valuable insights into opportunities or failures. Yet it’s difficult to achieve, due to the need to process data in real time, continuously learn and make predictions. How do we evaluate and compare various real-time anomaly detection techniques? \r\n\r\nThe Numenta Anomaly Benchmark (NAB) provides a standard, open source framework for evaluating real-time anomaly detection algorithms on streaming data. Through a controlled, repeatable environment of open-source tools, NAB rewards detectors that find anomalies as soon as possible, trigger no false alarms, and automatically adapt to any changing statistics. \r\n\r\nNAB comprises two main components: a scoring system designed for streaming data and a dataset with labeled, real-world time-series data.\r\n\r\nSource: [Evaluating Real-time Anomaly Detection Algorithms – the Numenta Anomaly Benchmark](http://arxiv.org/abs/1510.03336)\r\nImage Source: [https://numenta.com/machine-intelligence-technology/numenta-anomaly-benchmark/](https://numenta.com/machine-intelligence-technology/numenta-anomaly-benchmark/)", "variants": ["Numenta Anomaly Benchmark", "NAB"], "title": "Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly Benchmark"}
{"id": "SoF", "contents": "The **Specs on Faces** (**SoF**) dataset, a collection of 42,592 (2,662×16) images for 112 persons (66 males and 46 females) who wear glasses under different illumination conditions. The dataset is FREE for reasonable academic fair use. The dataset presents a new challenge regarding face detection and recognition. It is focused on two challenges: harsh illumination environments and face occlusions, which highly affect face detection, recognition, and classification. The glasses are the common natural occlusion in all images of the dataset. However, there are two more synthetic occlusions (nose and mouth) added to each image. Moreover, three image filters, that may evade face detectors and facial recognition systems, were applied to each image. All generated images are categorized into three levels of difficulty (easy, medium, and hard). That enlarges the number of images to be 42,592 images (26,112 male images and 16,480 female images). There is metadata for each image that contains many information such as: the subject ID, facial landmarks, face and glasses rectangles, gender and age labels, year that the photo was taken, facial emotion, glasses type, and more.\n\nSource: [https://sites.google.com/view/sof-dataset](https://sites.google.com/view/sof-dataset)\nImage Source: [https://sites.google.com/view/sof-dataset](https://sites.google.com/view/sof-dataset)", "variants": ["SoF"], "title": "AFIF4: Deep Gender Classification based on AdaBoost-based Fusion of Isolated Facial Features and Foggy Faces"}
{"id": "TextWorld KG", "contents": "**TextWorld KG** is a dynamic Knowledge Graph (KG) extraction dataset. It is based on a set of text-based games generated using. That framework allows to extract the underlying partial KG for every state, i.e., the subgraph that represents the agent’s partial knowledge of the world – what it has observed so far. All games share the same overarching theme: the agent finds itself hungry in a simple modern house with the goal of gathering ingredients and cooking a meal.\n\nSource: [https://arxiv.org/abs/1910.09532](https://arxiv.org/abs/1910.09532)", "variants": ["TextWorld KG"], "title": "Building Dynamic Knowledge Graphs from Text-based Games"}
{"id": "CC-Stories", "contents": "**CC-Stories** (or STORIES) is a dataset for common sense reasoning and language modeling. It was constructed by aggregating documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions in commonsense reasoning tasks. The top 1.0% of highest ranked documents is chosen as the new training corpus.", "variants": ["CC-Stories"], "title": "A Simple Method for Commonsense Reasoning"}
{"id": "Memeify", "contents": "A large-scale dataset of memes with captions and class labels. The dataset consists of 1.1 million meme captions from 128 classes.\r\n\r\nSource: [Memeify: A Large-Scale Meme Generation System](/paper/memeify-a-large-scale-meme-generation-system)", "variants": ["Memeify"], "title": "Memeify: A Large-Scale Meme Generation System"}
{"id": "Rent3D", "contents": "A dataset which contains over 200 apartments.\r\n\r\nSource: [Rent3D: Floor-Plan Priors for Monocular Layout Estimation](/paper/rent3d-floor-plan-priors-for-monocular-layout)", "variants": ["Rent3D"], "title": "Rent3D: Floor-plan priors for monocular layout estimation"}
{"id": "UR-FUNNY", "contents": "For understanding multimodal language used in expressing humor.\r\n\r\nSource: [UR-FUNNY: A Multimodal Language Dataset for Understanding Humor](/paper/ur-funny-a-multimodal-language-dataset-for)", "variants": ["UR-FUNNY"], "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor"}
{"id": "GuessWhat?!", "contents": "**GuessWhat?!** is a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images.\r\n\r\nGuessWhat?! is a cooperative two-player game in which\r\nboth players see the picture of a rich visual scene with several objects. One player – the oracle – is randomly assigned\r\nan object (which could be a person) in the scene. This object is not known by the other player – the questioner –\r\nwhose goal it is to locate the hidden object. To do so, the\r\nquestioner can ask a series of yes-no questions which are\r\nanswered by the oracle.\r\n\r\nSource: [https://paperswithcode.com/paper/guesswhat-visual-object-discovery-through](https://paperswithcode.com/paper/guesswhat-visual-object-discovery-through)\r\nImage Source: [Vries et al](https://arxiv.org/pdf/1611.08481v2.pdf)", "variants": ["GuessWhat?!"], "title": "GuessWhat?! Visual Object Discovery through Multi-modal Dialogue"}
{"id": "EdNet", "contents": "A large-scale hierarchical dataset of diverse student activities collected by Santa, a multi-platform self-study solution equipped with artificial intelligence tutoring system. EdNet contains 131,441,538 interactions from 784,309 students collected over more than 2 years, which is the largest among the ITS datasets released to the public so far.\r\n\r\nSource: [EdNet: A Large-Scale Hierarchical Dataset in Education](/paper/ednet-a-large-scale-hierarchical-dataset-in)", "variants": ["EdNet"], "title": "EdNet: A Large-Scale Hierarchical Dataset in Education"}
{"id": "WiderPerson", "contents": "WiderPerson contains a total of 13,382 images with 399,786 annotations, i.e., 29.87 annotations per image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence, pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario and occlusion, which is suitable to evaluate pedestrian detectors in the wild.\r\n\r\nSource: [WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild](/paper/widerperson-a-diverse-dataset-for-dense)\r\nImage Source: [http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/](http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/)", "variants": ["WiderPerson"], "title": "WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild"}
{"id": "Spades", "contents": "Datasets **Spades** contains 93,319 questions derived from clueweb09 sentences. Specifically, the questions were created by randomly removing an entity, thus producing sentence-denotation pairs.\n\nSource: [Learning an Executable Neural Semantic Parser](https://arxiv.org/abs/1711.05066)\nImage Source: [https://github.com/sivareddyg/graph-parser/blob/master/data/spades/results/graphparser-ccg-supervised-dev.txt](https://github.com/sivareddyg/graph-parser/blob/master/data/spades/results/graphparser-ccg-supervised-dev.txt)", "variants": ["Spades"], "title": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing"}
{"id": "FGVC-Aircraft", "contents": "FGVC-Aircraft contains 10,200 images of aircraft, with 100 images for each of 102 different aircraft model variants, most of which are airplanes. The (main) aircraft in each image is annotated with a tight bounding box and a hierarchical airplane model label.\r\nAircraft models are organized in a four-levels hierarchy. The four levels, from finer to coarser, are:\r\n\r\n* Model, e.g. Boeing 737-76J. Since certain models are nearly visually indistinguishable, this level is not used in the evaluation.\r\n* Variant, e.g. Boeing 737-700. A variant collapses all the models that are visually indistinguishable into one class. The dataset comprises 102 different variants.\r\n* Family, e.g. Boeing 737. The dataset comprises 70 different families.\r\n* Manufacturer, e.g. Boeing. The dataset comprises 41 different manufacturers.\r\nThe data is divided into three equally-sized training, validation and test subsets.\r\n\r\nSource: [https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/](https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/)\r\nImage Source: [Fine-Grained Visual Classification of Aircraft](https://arxiv.org/abs/1306.5151)", "variants": ["FGVC Aircraft", "FGVC-Aircraft"], "title": "Fine-Grained Visual Classification of Aircraft"}
{"id": "Grasp MultiObject", "contents": "Robotic grasp dataset for multi-object multi-grasp evaluation with RGB-D data.\nThis dataset is annotated using the same protocol as Cornell Dataset, and can be used as multi-object extension of Cornell Dataset.\n\nSource: [https://github.com/ivalab/grasp_multiObject](https://github.com/ivalab/grasp_multiObject)\nImage Source: [https://github.com/ivalab/grasp_multiObject](https://github.com/ivalab/grasp_multiObject)", "variants": ["Grasp MultiObject"], "title": "Real-World Multiobject, Multigrasp Detection"}
{"id": "NoReC_fine", "contents": "NoReC_fine is a dataset for fine-grained sentiment analysis in Norwegian, annotated with respect to polar expressions, targets and holders of opinion. \r\n\r\nSource: [A Fine-Grained Sentiment Dataset for Norwegian](/paper/a-fine-grained-sentiment-dataset-for)", "variants": ["NoReC_fine"], "title": "A Fine-Grained Sentiment Dataset for Norwegian"}
{"id": "MVTec D2S", "contents": "**MVTec D2S** is a benchmark for instance-aware semantic segmentation in an industrial domain. It contains 21,000 high-resolution images with pixel-wise labels of all object instances. The objects comprise groceries and everyday products from 60 categories. The benchmark is designed such that it resembles the real-world setting of an automatic checkout, inventory, or warehouse system. The training images only contain objects of a single class on a homogeneous background, while the validation and test sets are much more complex and diverse.\r\n\r\nSource: [MVTec D2S: Densely Segmented Supermarket Dataset](/paper/mvtec-d2s-densely-segmented-supermarket)\r\nImage Source: [https://www.mvtec.com/company/research/datasets/mvtec-d2s](https://www.mvtec.com/company/research/datasets/mvtec-d2s)", "variants": ["MVTec D2S"], "title": "MVTec D2S: Densely Segmented Supermarket Dataset"}
{"id": "TableBank", "contents": "To address the need for a standard open domain table benchmark dataset, the author propose a novel weak supervision approach to automatically create the TableBank, which is orders of magnitude larger than existing human labeled datasets for table analysis. Distinct from traditional weakly supervised training set, our approach can obtain not only large scale but also high quality training data.\r\n\r\nNowadays, there are a great number of electronic documents on the web such as Microsoft Word (.docx) and Latex (.tex) files. These online documents contain mark-up tags for tables in their source code by nature. Intuitively, one can manipulate these source code by adding bounding box using the mark-up language within each document. For Word documents, the internal Office XML code can be modified where the borderline of each table is identified. For Latex documents, the tex code can be also modified where bounding boxes of tables are recognized. In this way, high-quality labeled data is created for a variety of domains such as business documents, official fillings, research papers etc, which is tremendously beneficial for large-scale table analysis tasks.\r\n\r\nThe TableBank dataset totally consists of 417,234 high quality labeled tables as well as their original documents in a variety of domains.\r\n\r\nSource: [TableBank](https://github.com/doc-analysis/TableBank)", "variants": ["TableBank"], "title": "TableBank: Table Benchmark for Image-based Table Detection and Recognition"}
{"id": "TCIA Test & Validation Radiotherapy CT Planning Scan", "contents": "A dataset of 663 deidentified computed tomography (CT) scans acquired in routine clinical practice and with both segmentations taken from clinical practice and segmentations.\r\n\r\nSource: [Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy](/paper/deep-learning-to-achieve-clinically)", "variants": ["TCIA Test & Validation Radiotherapy CT Planning Scan"], "title": "Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy"}
{"id": "Dataset of Legal Documents", "contents": "Dataset of Legal Documents consists of court decisions from 2017 and 2018 were selected for the dataset, published online by the Federal Ministry of Justice and Consumer Protection. The documents originate from seven federal courts: Federal Labour Court (BAG), Federal Fiscal Court (BFH), Federal Court of Justice (BGH), Federal Patent Court (BPatG), Federal Social Court (BSG), Federal Constitutional Court (BVerfG) and Federal Administrative Court (BVerwG).\r\n\r\nThe dataset consists of 66,723 sentences with 2,157,048 tokens. The sizes of the seven court-specific datasets varies between 5,858 and 12,791 sentences, and 177,835 to 404,041 tokens. The distribution of annotations on a per-token basis corresponds to approx. 19-23 %.\r\n\r\nSource: [GitHub](https://github.com/elenanereiss/Legal-Entity-Recognition)", "variants": ["Dataset of Legal Documents"], "title": "A Dataset of German Legal Documents for Named Entity Recognition"}
{"id": "WHAM!", "contents": "The **WSJ0 Hipster Ambient Mixtures** (**WHAM!**) dataset pairs each two-speaker mixture in the wsj0-2mix dataset with a unique noise background scene. It has an extension called [WHAMR!](/dataset/whamr) that adds artificial reverberation to the speech signals in addition to the background noise.\r\n\r\nThe noise audio was collected at various urban locations throughout the San Francisco Bay Area in late 2018. The environments primarily consist of restaurants, cafes, bars, and parks. Audio was recorded using an Apogee Sennheiser binaural microphone on a tripod between 1.0 and 1.5 meters off the ground.", "variants": ["WHAMR!", "WHAM!"], "title": "WHAM!: Extending Speech Separation to Noisy Environments"}
{"id": "Real Bacteria Dataset", "contents": "A genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem.\r\n\r\nSource: [Likelihood Ratios for Out-of-Distribution Detection](/paper/likelihood-ratios-for-out-of-distribution)", "variants": ["Real Bacteria Dataset"], "title": "Likelihood Ratios for Out-of-Distribution Detection"}
{"id": "Meta-Dataset", "contents": "The **Meta-Dataset** benchmark is a large few-shot learning benchmark and consists of multiple datasets of different data distributions. It does not restrict few-shot tasks to have fixed ways and shots, thus representing a more realistic scenario. It consists of 10 datasets from diverse domains: \r\n\r\n* ILSVRC-2012 (the ImageNet dataset, consisting of natural images with 1000 categories)\r\n* Omniglot (hand-written characters, 1623 classes)\r\n* Aircraft (dataset of aircraft images, 100 classes)\r\n* CUB-200-2011 (dataset of Birds, 200 classes)\r\n* Describable Textures (different kinds of texture images with 43 categories)\r\n* Quick Draw (black and white sketches of 345 different categories)\r\n* Fungi (a large dataset of mushrooms with 1500 categories)\r\n* VGG Flower (dataset of flower images with 102 categories), \r\n* Traffic Signs (German traffic sign images with 43 classes)\r\n* MSCOCO (images collected from Flickr, 80 classes). \r\n\r\nAll datasets except Traffic signs and MSCOCO have a training, validation and test split (proportioned roughly into 70%, 15%, 15%). The datasets Traffic Signs and MSCOCO are reserved for testing only.\r\n\r\nSource: [Optimized Generic Feature Learning for Few-shot Classification across Domains](https://arxiv.org/abs/2001.07926)\r\nImage Source: [Triantafillou et al](https://arxiv.org/pdf/1903.03096.pdf)", "variants": ["Meta-Dataset"], "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples"}
{"id": "ParaBank", "contents": "A large-scale English paraphrase dataset that surpasses prior work in both quantity and quality.\r\n\r\nSource: [ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-constrained Neural Machine Translation](/paper/parabank-monolingual-bitext-generation-and)", "variants": ["ParaBank"], "title": "ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-constrained Neural Machine Translation"}
{"id": "BIRD", "contents": "Blocksworld Image Reasoning Dataset (BIRD) contains images of wooden blocks in different configurations, and the sequence of moves to rearrange one configuration to the other. \r\n\r\nSource: [Blocksworld Revisited: Learning and Reasoning to Generate Event-Sequences from Image Pairs](https://arxiv.org/pdf/1905.12042)", "variants": ["Bird-225", "BIRD"], "title": "Blocksworld Revisited: Learning and Reasoning to Generate Event-Sequences from Image Pairs"}
{"id": "ERA", "contents": "Consists of 2,864 videos each with a label from 25 different classes corresponding to an event unfolding 5 seconds. The ERA dataset is designed to have a significant intra-class variation and inter-class similarity and captures dynamic events in various circumstances and at dramatically various scales.\r\n\r\nSource: [ERA: A Dataset and Deep Learning Benchmark for Event Recognition in Aerial Videos](/paper/era-a-dataset-and-deep-learning-benchmark-for)", "variants": ["ERA"], "title": "ERA: A Dataset and Deep Learning Benchmark for Event Recognition in Aerial Videos"}
{"id": "SailX", "contents": "A dataset for grounded language learning that consists of navigational instructions and actions in a maze-like environment.\r\n\r\nSource: [A new dataset and model for learning to understand navigational instructions](/paper/a-new-dataset-and-model-for-learning-to)", "variants": ["SailX"], "title": "A new dataset and model for learning to understand navigational instructions"}
{"id": "Berkeley DeepDrive Video", "contents": "A dataset comprised of real driving videos and GPS/IMU data. The BDDV dataset contains diverse driving scenarios including cities, highways, towns, and rural areas in several major cities in US.\r\n\r\nSource: [End-to-end Learning of Driving Models from Large-scale Video Datasets](/paper/end-to-end-learning-of-driving-models-from)", "variants": ["Berkeley DeepDrive Video"], "title": "End-to-End Learning of Driving Models from Large-Scale Video Datasets"}
{"id": "UW IOM", "contents": "Comprises twenty individuals picking up and placing objects of varying weights to and from cabinet and table locations at various heights.\r\n\r\nSource: [Toward Ergonomic Risk Prediction via Segmentation of Indoor Object Manipulation Actions Using Spatiotemporal Convolutional Networks](/paper/predicting-ergonomic-risks-during-indoor)", "variants": ["UW IOM"], "title": "Toward Ergonomic Risk Prediction via Segmentation of Indoor Object Manipulation Actions Using Spatiotemporal Convolutional Networks"}
{"id": "D-HAZY", "contents": "The **D-HAZY** dataset is generated from NYU depth indoor image collection. D-HAZY contains depth map for each indoor hazy image. It contains 1400+ real images and corresponding depth maps used to synthesize hazy scenes based on Koschmieder’s light propagation mode\r\n\r\nSource: [C2MSNet: A Novel approach for single image haze removal](https://arxiv.org/abs/1801.08406)\r\nImage Source: [https://www.semanticscholar.org/paper/D-HAZY%3A-A-dataset-to-evaluate-quantitatively-Ancuti-Ancuti/9451d0b1bfbba5f3e19c083866f1394aabf7d06c](https://www.semanticscholar.org/paper/D-HAZY%3A-A-dataset-to-evaluate-quantitatively-Ancuti-Ancuti/9451d0b1bfbba5f3e19c083866f1394aabf7d06c)", "variants": ["D-HAZY"], "title": "D-HAZY: A dataset to evaluate quantitatively dehazing algorithms"}
{"id": "PRID2011", "contents": "PRID 2011 is a person reidentification dataset that provides multiple person trajectories recorded from two different static surveillance cameras, monitoring crosswalks and sidewalks. The dataset shows a clean background, and the people in the dataset are rarely occluded. In the dataset, 200 people appear in both views. Among the 200 people, 178 people have more than 20 appearances\n\nSource: [PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification](https://arxiv.org/abs/1705.06011)", "variants": ["PRID2011"], "title": "Mask R-CNN"}
{"id": "Foggy Cityscapes", "contents": "**Foggy Cityscapes** is a synthetic foggy dataset which simulates fog on real scenes. Each foggy image is rendered with a clear image and depth map from Cityscapes. Thus the annotations and data split in Foggy Cityscapes are inherited from Cityscapes.\r\n\r\nSource: [Exploring Object Relation in Mean Teacher for Cross-Domain Detection](https://arxiv.org/abs/1904.11245)\r\nImage Source: [http://people.ee.ethz.ch/~csakarid/SFSU_synthetic/](http://people.ee.ethz.ch/~csakarid/SFSU_synthetic/)", "variants": ["Cityscapes to Foggy Cityscapes", "Cityscapes-to-Foggy Cityscapes", "Foggy Cityscapes"], "title": "Semantic Foggy Scene Understanding with Synthetic Data"}
{"id": "DAQUAR", "contents": "DAQUAR (DAtaset for QUestion Answering on Real-world images) is a dataset of human question answer pairs about images.\r\n\r\nSource: [A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input](/paper/a-multi-world-approach-to-question-answering)\r\nImage Source: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)", "variants": ["DAQUAR"], "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input"}
{"id": "OGTD", "contents": "A manually annotated dataset containing 4,779 posts from Twitter annotated as offensive and not offensive. \r\n\r\nSource: [Offensive Language Identification in Greek](/paper/offensive-language-identification-in-greek)", "variants": ["OGTD"], "title": "Offensive Language Identification in Greek"}
{"id": "NWPU-Crowd", "contents": "NWPU-Crowd consists of 5,109 images, in a total of 2,133,375 annotated heads with points and boxes. Compared with other real-world datasets, it contains various illumination scenes and has the largest density range (0~20,033). \r\n\r\nSource: [NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization](/paper/nwpu-crowd-a-large-scale-benchmark-for-crowd)", "variants": ["NWPU-Crowd"], "title": "NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting"}
{"id": "Raindrop", "contents": "Raindrop is a set of image pairs, where\r\neach pair contains exactly the same background scene, yet\r\none is degraded by raindrops and the other one is free from\r\nraindrops. To obtain this, the images are captured through two pieces of exactly the\r\nsame glass: one sprayed with water, and the other is left\r\nclean. The dataset consists of 1,119 pairs of images, with various\r\nbackground scenes and raindrops. They were captured with a Sony A6000\r\nand a Canon EOS 60.\r\n\r\nSource: [Attentive Generative Adversarial Network for Raindrop Removal from a Single Image](/paper/attentive-generative-adversarial-network-for)", "variants": ["Raindrop"], "title": "Attentive Generative Adversarial Network for Raindrop Removal from A Single Image"}
{"id": "CELEX", "contents": "**CELEX** database comprises three different searchable lexical databases, Dutch, English and German. The lexical data contained in each database is divided into five categories: orthography, phonology, morphology, syntax (word class) and word frequency.\r\n\r\nSource: [Polysemy and Brevity versus Frequency in Language](https://arxiv.org/abs/1904.00812)\r\nImage Source: [https://www.aclweb.org/anthology/W17-7619.pdf](https://www.aclweb.org/anthology/W17-7619.pdf)", "variants": ["CELEX"], "title": "EmotionLines: An Emotion Corpus of Multi-Party Conversations"}
{"id": "MUSE", "contents": "The **MUSE** dataset contains bilingual dictionaries for 110 pairs of languages. For each language pair, the training seed dictionaries contain approximately 5000 word pairs while the evaluation sets contain 1500 word pairs.\r\n\r\nSource: [Filtered Inner Product Projection for Multilingual Embedding Alignment](https://arxiv.org/abs/2006.03652)\r\nImage Source: [https://github.com/facebookresearch/MUSE](https://github.com/facebookresearch/MUSE)", "variants": ["MUSE en-de", "MUSE en-pt", "MUSE", "MUSE English-French", "MUSE English-Portuguese", "MUSE English-Spanish", "MUSE Italian-French"], "title": "Word Translation Without Parallel Data"}
{"id": "EYEDIAP", "contents": "The **EYEDIAP** dataset is a dataset for gaze estimation from remote RGB, and RGB-D (standard vision and depth), cameras. The recording methodology was designed by systematically including, and isolating, most of the variables which affect the remote gaze estimation algorithms:\r\n\r\n* Head pose variations.\r\n* Person variation.\r\n* Changes in ambient and sensing condition.\r\n* Types of target: screen or 3D object.\r\n\r\nSource: [https://www.idiap.ch/dataset/eyediap](https://www.idiap.ch/dataset/eyediap)\r\nImage Source: [https://www.idiap.ch/dataset/eyediap](https://www.idiap.ch/dataset/eyediap)", "variants": ["EYEDIAP (screen target)", "EYEDIAP (floating target)", "EYEDIAP"], "title": "EYEDIAP: a database for the development and evaluation of gaze estimation algorithms from RGB and RGB-D cameras"}
{"id": "TACO", "contents": "**TACO** is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches. These images are manually labelled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms. The annotations are provided in COCO format.\n\nSource: [https://github.com/pedropro/TACO](https://github.com/pedropro/TACO)\nImage Source: [https://github.com/pedropro/TACO](https://github.com/pedropro/TACO)", "variants": ["TACO"], "title": "TACO: Trash Annotations in Context for Litter Detection"}
{"id": "MuTual", "contents": "**MuTual** is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified from Chinese high school English listening comprehension test data. It tests dialogue reasoning via next utterance prediction.\n\nSource: [https://github.com/Nealcly/MuTual](https://github.com/Nealcly/MuTual)\nImage Source: [https://github.com/Nealcly/MuTual](https://github.com/Nealcly/MuTual)", "variants": ["MuTual"], "title": "MuTual: A Dataset for Multi-Turn Dialogue Reasoning"}
{"id": "AVA-Speech", "contents": "Contains densely labeled speech activity in YouTube videos, with the goal of creating a shared, available dataset for this task. \r\n\r\nSource: [AVA-Speech: A Densely Labeled Dataset of Speech Activity in Movies](/paper/ava-speech-a-densely-labeled-dataset-of)", "variants": ["AVA-Speech"], "title": "AVA-Speech: A Densely Labeled Dataset of Speech Activity in Movies"}
{"id": "HASY", "contents": "HASY is a dataset of single symbols similar to MNIST. It contains 168,233 instances of 369 classes. HASY contains two challenges: A classification challenge with 10 pre-defined folds for 10-fold cross-validation and a verification challenge.\r\n\r\nSource: [The HASYv2 dataset](/paper/the-hasyv2-dataset)", "variants": ["HASY"], "title": "The HASYv2 dataset"}
{"id": "SAMSum Corpus", "contents": "A new dataset with abstractive dialogue summaries.\r\n\r\nSource: [SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization](/paper/samsum-corpus-a-human-annotated-dialogue-1)", "variants": ["SAMSum Corpus"], "title": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization"}
{"id": "TITAN", "contents": "TITAN consists of 700 labeled video-clips (with odometry) captured from a moving vehicle on highly interactive urban traffic scenes in Tokyo. The dataset includes 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes that are organized hierarchically corresponding to atomic, simple/complex-contextual, transportive, and communicative actions. \r\n\r\nSource: [TITAN: Future Forecast using Action Priors](/paper/titan-future-forecast-using-action-priors)", "variants": ["TITAN"], "title": "TITAN: Future Forecast using Action Priors"}
{"id": "Lorenz Dataset", "contents": "The Lorenz dataset contains 100000 time-series with length 24. The data has 5 modes and it is obtained using the Lorenz equation with 5 different seed values.\r\n\r\nSource: [Lorenz Dataset](https://git.opendfki.de/koochali/forgan)", "variants": ["Lorenz dataset", "Lorenz Dataset"], "title": "Probabilistic Forecasting of Sensory Data with Generative Adversarial Networks - ForGAN"}
{"id": "FCDB", "contents": "Consists of 76 million geo-tagged images in 16 cosmopolitan cities.\r\n\r\nSource: [Changing Fashion Cultures](/paper/changing-fashion-cultures)", "variants": ["FCDB"], "title": "Changing Fashion Cultures"}
{"id": "xBD", "contents": "The xBD dataset contains over 45,000KM2 of polygon labeled pre and post disaster imagery. The dataset provides the post-disaster imagery with transposed polygons from pre over the buildings, with damage classification labels.\r\n\r\nSource: [xBD](https://github.com/DIUx-xView/xview2-baseline)\r\nImage Source: [Gupta et al](https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.pdf)", "variants": ["xBD"], "title": "xBD: A Dataset for Assessing Building Damage from Satellite Imagery"}
{"id": "Long-term visual localization", "contents": "Long-term visual localization provides a benchmark datasets aimed at evaluating 6 DoF pose estimation accuracy over large appearance variations caused by changes in seasonal (summer, winter, spring, etc.) and illumination (dawn, day, sunset, night) conditions. Each dataset consists of a set of reference images, together with their corresponding ground truth poses, and a set of query images.\r\n\r\nSource: [Long-term visual localization](http://visuallocalization.net/)\r\nImage Source: [https://www.visuallocalization.net/](https://www.visuallocalization.net/)", "variants": ["Long-term visual localization"], "title": "Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization"}
{"id": "Dynamic FAUST", "contents": "Dynamic FAUST extends the FAUST dataset to dynamic 4D data. It consists of high-resolution 4D scans of human subjects in motion, captured at 60 fps.\r\n\r\nSource: [Dynamic FAUST: Registering Human Bodies in Motion](/paper/dynamic-faust-registering-human-bodies-in)", "variants": ["Dynamic FAUST"], "title": "Dynamic FAUST: Registering Human Bodies in Motion"}
{"id": "Deep Thermal Imaging Dataset", "contents": "The **Deep Thermal Imaging dataset** consists of two main datasets:\r\n\r\n- **DeepTherm I** (Indoor materials) - 15 indoor materials were used to create the dataset DeepTherm I which consists of 14,860 processed thermal images (average count of data for each individual class: 990.7, SD=425.9; 400-600 images of each material per each variable). The dataset was created by recording thermal image sequences in a room with different lighting levels (bright / dark), with/without air-conditioning, different places (on a floor or a desk) and from different perspectives (Figure 5). The spatial temperature patterns were collected from different angles and different distances (between 10 and 50 cm, from the camera lens to the material). The data was collected five times in about 3 weeks. \r\n\r\n- **DeepTherm II** (Outdoor materials) - 17 outdoor materials were targeted. The data collection process produced the DeepTherm II dataset which includes 26,584 labelled thermal images. The average number of collected spatial thermal patterns from each material was 1563.8 (SD=295.3; about 300-500 images of each material per each condition).\r\n\r\nImage source: [Cho et al.](https://arxiv.org/pdf/1803.02310v1.pdf)\r\n\r\nSource: [Cho et al.](https://arxiv.org/pdf/1803.02310v1.pdf)", "variants": ["Deep Thermal Imaging Dataset"], "title": "Deep Thermal Imaging: Proximate Material Type Recognition in the Wild through Deep Learning of Spatial Surface Temperature Patterns"}
{"id": "MLDoc", "contents": "**Multilingual Document Classification Corpus** (**MLDoc**) is a cross-lingual document classification dataset covering English, German, French, Spanish, Italian, Russian, Japanese and Chinese. It is a subset of the Reuters Corpus Volume 2 selected according to the following design choices:\r\n\r\n* uniform class coverage: same number of examples for each class and language,\r\n* official train / development / test split: for each language a training data of different sizes (1K, 2K, 5K and 10K stories), a development (1K) and a test corpus (4K) are provided (with exception of Spanish and Russian with 9458 and 5216 training documents respectively.\r\n\r\nSource: [A Corpus for Multilingual Document Classification in Eight Languages](https://paperswithcode.com/paper/a-corpus-for-multilingual-document/)", "variants": ["MLDoc Zero-Shot English-to-Chinese", "MLDoc Zero-Shot English-to-French", "MLDoc Zero-Shot English-to-German", "MLDoc Zero-Shot English-to-Italian", "MLDoc Zero-Shot English-to-Japanese", "MLDoc Zero-Shot English-to-Russian", "MLDoc Zero-Shot English-to-Spanish", "MLDoc Zero-Shot German-to-French", "MLDoc"], "title": "A Corpus for Multilingual Document Classification in Eight Languages"}
{"id": "Friendster", "contents": "**Friendster** is an on-line gaming network. Before re-launching as a game website, Friendster was a social networking site where users can form friendship edge each other. Friendster social network also allows users form a group which other members can then join. The Friendster dataset consist of ground-truth communities (based on user-defined groups) and the social network from induced subgraph of the nodes that either belong to at least one community or are connected to other nodes that belong to at least one community.\r\n\r\nSource: [https://snap.stanford.edu/data/com-Friendster.html](https://snap.stanford.edu/data/com-Friendster.html)", "variants": ["Friendster"], "title": "Defining and evaluating network communities based on ground-truth"}
{"id": "VITON", "contents": "VITON was a dataset for virtual try-on of clothing items. It consisted of 16,253 pairs of images of a person and a clothing item .\r\n\r\nThe authors have removed the dataset and it is no longer publicly available due to copyright issues.", "variants": ["VITON"], "title": "VITON: An Image-Based Virtual Try-on Network"}
{"id": "FIGER", "contents": "The **FIGER** dataset is an entity recognition dataset where entities are labelled using fine-grained system 112 tags, such as *person/doctor*, *art/written_work* and *building/hotel*. The tags are derivied from Freebase types. The training set consists of Wikipedia articles automatically annotated with distant supervision approach that utilizes the information encoded in anchor links. The test set was annotated manually.\r\n\r\nSource: [http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5152](http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5152)", "variants": ["FIGER"], "title": "DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion"}
{"id": "ATIS", "contents": "The **ATIS** (**Airline Travel Information Systems**) is a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories. The original split contains 4478, 500 and 893 intent-labeled reference utterances in train, development and test set respectively.\r\n\r\nSource: [Spoken Language Intent Detection using Confusion2Vec](https://arxiv.org/abs/1904.03576)", "variants": ["ATIS"], "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark"}
{"id": "ReQA", "contents": "Retrieval Question-Answering (ReQA) benchmark tests a model’s ability to retrieve relevant answers efficiently from a large set of documents.\r\n\r\nSource: [ReQA: An Evaluation for End-to-End Answer Retrieval Models](https://arxiv.org/pdf/1907.04780.pdf)", "variants": ["ReQA"], "title": "ReQA: An Evaluation for End-to-End Answer Retrieval Models"}
{"id": "Medical Case Report Corpus", "contents": "Medical Case Report Corpus is a new corpus comprising annotations of medical entities in case reports, originating from PubMed Central's open access library. \r\n\r\nSource: [Named Entities in Medical Case Reports: Corpus and Experiments](/paper/named-entities-in-medical-case-reports-corpus)", "variants": ["Medical Case Report Corpus"], "title": "Named Entities in Medical Case Reports: Corpus and Experiments"}
{"id": "OpenWebText", "contents": "**OpenWebText** is an open-source recreation of the [WebText](/dataset/webtext) corpus. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).\r\n\r\nSource: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)", "variants": ["OpenWebText"], "title": "STRING v10: protein–protein interaction networks, integrated over the tree of life"}
{"id": "IG-3.5B-17k", "contents": "**IG-3.5B-17k** is an internal Facebook AI Research dataset for training image classification models. It consists of hashtags for up to 3.5 billion public Instagram images.", "variants": ["IG-3.5B-17k"], "title": "Exploring the Limits of Weakly Supervised Pretraining"}
{"id": "SQuAD", "contents": "The **Stanford Question Answering Dataset** (**SQuAD**) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.\r\n\r\nSource: [Deep Learning Based Text Classification: A Comprehensive Review](https://arxiv.org/abs/2004.03705)\r\nImage Source: [https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Prime_number.html](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Prime_number.html)", "variants": ["SQuAD1.1", "SQuAD1.1 dev", "SQuAD2.0", "SQuAD 2.0", "SQuAD2.0 dev", "SQuAD"], "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}
{"id": "PANDORA", "contents": "PANDORA is the first large-scale dataset of Reddit comments labeled with three personality models (including the well-established Big 5 model) and demographics (age, gender, and location) for more than 10k users.\r\n\r\nSource: [PANDORA Talks: Personality and Demographics on Reddit](https://arxiv.org/pdf/2004.04460)", "variants": ["PANDORA"], "title": "PANDORA Talks: Personality and Demographics on Reddit"}
{"id": "JSS Dataset", "contents": "The **Jejueo Single Speaker Speech** (JSS) dataset consists of 10k high-quality audio files recorded by a native Jejueo speaker and a transcript file.\n\nSource: [https://arxiv.org/abs/1911.12071](https://arxiv.org/abs/1911.12071)", "variants": ["JSS Dataset"], "title": "Jejueo Datasets for Machine Translation and Speech Synthesis"}
{"id": "ZINC", "contents": "**ZINC** is a free database of commercially-available compounds for virtual screening. ZINC contains over 230 million purchasable compounds in ready-to-dock, 3D formats. ZINC also contains over 750 million purchasable compounds that can be searched for analogs.\r\n\r\nSource: [ZINC15](http://zinc15.docking.org/)\r\nImage Source: [https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559](https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559)", "variants": ["ZINC", "ZINC 100k", "ZINC-500k"], "title": "ZINC: A Free Tool to Discover Chemistry for Biology"}
{"id": "Humicroedit", "contents": "Humicroedit is a humorous headline dataset. The data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. The authors carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline.\r\n\r\nSource: [\"President Vows to Cut <Taxes> Hair\": Dataset and Analysis of Creative Text Editing for Humorous Headlines](/paper/190600274)\r\nImage Source: [https://arxiv.org/pdf/1906.00274v1.pdf](https://arxiv.org/pdf/1906.00274v1.pdf)", "variants": ["Humicroedit"], "title": "\"President Vows to CutHair\": Dataset and Analysis of Creative Text Editing for Humorous Headlines"}
{"id": "DAiSEE", "contents": "DAiSEE is a multi-label video classification dataset comprising of 9,068 video snippets captured from 112 users for recognizing the user affective states of boredom, confusion, engagement, and frustration \"in the wild\". The dataset has four levels of labels namely - very low, low, high, and very high for each of the affective states, which are crowd annotated and correlated with a gold standard annotation created using a team of expert psychologists. \r\n\r\nSource: [DAiSEE: Towards User Engagement Recognition in the Wild](/paper/daisee-towards-user-engagement-recognition-in)", "variants": ["DAiSEE"], "title": "DAiSEE: Towards User Engagement Recognition in the Wild"}
{"id": "YCB-Video", "contents": "The **YCB-Video** dataset is a large-scale video dataset for 6D object pose estimation. provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames.\r\n\r\nSource: [https://rse-lab.cs.washington.edu/projects/posecnn/](https://rse-lab.cs.washington.edu/projects/posecnn/)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-refined-poses-on-the-YCB-Video-dataset-which-use-results-from-PoseCNN-Xiang_fig6_339663565](https://www.researchgate.net/figure/Examples-of-refined-poses-on-the-YCB-Video-dataset-which-use-results-from-PoseCNN-Xiang_fig6_339663565)", "variants": ["YCB-Video"], "title": "PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes"}
{"id": "SART", "contents": "**SART** is a collection of three datasets for Similarity, Analogies and Relatedness for the Tatar language.\nThe three subsets are:\n* Similarity dataset - 202 pairs of words along with averaged human scores of similarity degree between the words (in 0-to-10 scale). For example, \"йорт, бина, 7.69\".\n* Relatedness dataset - 252 pairs of words along with averaged human scores of relatedness degree between the words. For example, \"урам, балалар, 5.38\".\n* Analogies dataset - set of analytical questions of the form A:B::C:D, meaning A to B as C to D, and D is to be predicted. For example, \"Әнкара Төркия Париж Франция\". Contains 34 categories, and in total 30 144 questions.\n\nSource: [https://github.com/tat-nlp/SART](https://github.com/tat-nlp/SART)", "variants": ["SART"], "title": "SART - Similarity, Analogies, and Relatedness for Tatar Language: New Benchmark Datasets for Word Embeddings Evaluation"}
{"id": "MMDB", "contents": "Multimodal Dyadic Behavior (MMDB) dataset is a unique collection of multimodal (video, audio, and physiological) recordings of the social and communicative behavior of toddlers. The MMDB contains 160 sessions of 3-5 minute semi-structured play interaction between a trained adult examiner and a child between the age of 15 and 30 months. The MMDB dataset supports a novel problem domain for activity recognition, which consists of the decoding of dyadic social interactions between adults and children in a developmental context.\r\n\r\nSource: [MMDB](http://cbs.ic.gatech.edu/mmdb/)", "variants": ["MMDB"], "title": "Decoding Children's Social Behavior"}
{"id": "word2word", "contents": "word2word contains easy-to-use word translations for 3,564 language pairs.\r\n\r\n- A large collection of freely & publicly available bilingual lexicons for 3,564 language pairs across 62 unique languages.\r\n- Easy-to-use Python interface for accessing top-k word translations and for building a new bilingual lexicon from a custom parallel corpus.\r\n- Constructed using a simple approach that yields bilingual lexicons with high coverage and competitive translation quality.\r\n\r\nSource: [word2word](https://pypi.org/project/word2word/)", "variants": ["word2word"], "title": "word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs"}
{"id": "Almawave-SLU", "contents": "Almawave-SLU is the first Italian dataset for Spoken Language Understanding (SLU). It is derived through a semi-automatic procedure and is used as a benchmark of various open source and commercial systems.\r\n\r\nSource: [Almawave-SLU: A new dataset for SLU in Italian](https://arxiv.org/pdf/1907.07526)", "variants": ["Almawave-SLU"], "title": "Almawave-SLU: A new dataset for SLU in Italian"}
{"id": "Facebook Post Reactions", "contents": "Collects posts (and their reactions) from Facebook pages of large supermarket chains.\r\n\r\nSource: [Social Emotion Mining Techniques for Facebook Posts Reaction Prediction](/paper/social-emotion-mining-techniques-for-facebook)", "variants": ["Facebook Post Reactions"], "title": "Social Emotion Mining Techniques for Facebook Posts Reaction Prediction"}
{"id": "VisDrone", "contents": "**VisDrone** is a large-scale benchmark with carefully annotated ground-truth for various important computer vision tasks, to make vision meet drones. The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining, Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). Note that, the dataset was collected using various drone platforms (i.e., drones with different models), in different scenarios, and under various weather and lighting conditions. These frames are manually annotated with more than 2.6 million bounding boxes of targets of frequent interests, such as pedestrians, cars, bicycles, and tricycles. Some important attributes including scene visibility, object class and occlusion, are also provided for better data utilization.\n\nSource: [https://github.com/VisDrone/VisDrone-Dataset](https://github.com/VisDrone/VisDrone-Dataset)\nImage Source: [https://arxiv.org/pdf/2001.06303.pdf](https://arxiv.org/pdf/2001.06303.pdf)", "variants": ["VisDrone"], "title": "Vision Meets Drones: Past, Present and Future"}
{"id": "DensePose-Track", "contents": "DensePose-Track is a dataset of videos where selected frames are annotated in the traditional DensePose manner.\r\n\r\nSource: [Slim DensePose: Thrifty Learning from Sparse Annotations and Motion Cues](https://arxiv.org/pdf/1906.05706)", "variants": ["DensePose-Track"], "title": "Slim DensePose: Thrifty Learning From Sparse Annotations and Motion Cues"}
{"id": "RotoWire", "contents": "This dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box- and line-scores. Summaries taken from rotowire.com are referred to as the \"rotowire\" data.  There are 4853 distinct rotowire summaries, covering NBA games played between 1/1/2014 and 3/29/2017; some games have multiple summaries. The summaries have been randomly split into training, validation, and test sets consisting of 3398, 727, and 728 summaries, respectively.\r\n\r\nSource: [Challenges in Data-to-Document Generation](https://arxiv.org/abs/1707.08052)\r\nImage Source: [https://arxiv.org/pdf/1707.08052v1.pdf](https://arxiv.org/pdf/1707.08052v1.pdf)", "variants": ["RotoWire (Relation Generation)", "Rotowire (Content Selection)", "RotoWire (Content Ordering)", "RotoWire"], "title": "Challenges in Data-to-Document Generation"}
{"id": "INRIA-Horse", "contents": "The **INRIA-Horse** dataset consists of 170 horse images and 170 images without horses. All horses in all images are annotated with a bounding-box. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The horses are mostly unoccluded, taken from approximately the side viewpoint, and face the same direction.\n\nSource: [Dynamical And-Or Graph Learning for Object Shape Modeling and Detection](https://arxiv.org/abs/1502.00741)\nImage Source: [http://calvin-vision.net/datasets/inria-horses/](http://calvin-vision.net/datasets/inria-horses/)", "variants": ["INRIA-Horse"], "title": "Histograms of oriented gradients for human detection"}
{"id": "GCDC", "contents": "A corpus of real-world texts.\r\n\r\nSource: [Discourse Coherence in the Wild: A Dataset, Evaluation and Methods](/paper/discourse-coherence-in-the-wild-a-dataset)", "variants": ["GCDC"], "title": "Discourse Coherence in the Wild: A Dataset, Evaluation and Methods"}
{"id": "CPH", "contents": "A large-scale database including substantial CU partition data for HEVC intra- and inter-modes. This enables deep learning on the CU partition.\r\n\r\nSource: [Reducing Complexity of HEVC: A Deep Learning Approach](/paper/reducing-complexity-of-hevc-a-deep-learning)", "variants": ["CPH"], "title": "Reducing Complexity of HEVC: A Deep Learning Approach"}
{"id": "RPC", "contents": "RPC is a large-scale retail product checkout dataset and collects 200 retail SKUs. The collected SKUs can be divided into 17 meta categories, i.e., puffed food, dried fruit, dried food, instant drink, instant noodles, dessert, drink, alcohol, milk, canned food, chocolate, gum, candy, seasoner, personal hygiene, tissue, stationery.\r\n\r\nSource: [RPC: A Large-Scale Retail Product Checkout Dataset](/paper/rpc-a-large-scale-retail-product-checkout)\r\nImage Source: [Wei et al](https://arxiv.org/pdf/1901.07249.pdf)", "variants": ["RPC"], "title": "RPC: A Large-Scale Retail Product Checkout Dataset"}
{"id": "MIMII", "contents": "**Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection** (MIMII) is a sound dataset\r\nof industrial machine sounds.\r\n\r\nSource: [MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection](/paper/mimii-dataset-sound-dataset-for)", "variants": ["MIMII"], "title": "MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection"}
{"id": "Speech Commands", "contents": "**Speech Commands** is an audio dataset of spoken words designed to help train and evaluate keyword spotting systems.", "variants": ["Speech Commands"], "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition"}
{"id": "IJB-B", "contents": "The **IJB-B** dataset is a template-based face dataset that contains 1845 subjects with 11,754 images, 55,025 frames and 7,011 videos where a template consists of a varying number of still images and video frames from different sources. These images and videos are collected from the Internet and are totally unconstrained, with large variations in pose, illumination, image quality etc. In addition, the dataset comes with protocols for 1-to-1 template-based face verification, 1-to-N template-based open-set face identification, and 1-to-N open-set video face identification.\r\n\r\nSource: [An Automatic System for Unconstrained Video-Based Face Recognition](https://arxiv.org/abs/1812.04058)\r\nImage Source: [https://www.vislab.ucr.edu/Biometrics2017/program_slides/Noblis_CVPRW_IJBB.pdf](https://www.vislab.ucr.edu/Biometrics2017/program_slides/Noblis_CVPRW_IJBB.pdf)", "variants": ["IJB-B"], "title": "IARPA Janus Benchmark-B Face Dataset"}
{"id": "Penn Action", "contents": "The **Penn Action** Dataset contains 2326 video sequences of 15 different actions and human joint annotations for each sequence.\r\n\r\nSource: [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)\r\nImage Source: [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)", "variants": ["UPenn Action", "Penn Action"], "title": "From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding"}
{"id": "ProPara", "contents": "The **ProPara** dataset is designed to train and test comprehension of simple paragraphs describing processes (e.g., photosynthesis), designed for the task of predicting, tracking, and answering questions about how entities change during the process.\r\n\r\nProPara aims to promote the research in natural language understanding in the context of procedural text. This requires identifying the actions described in the paragraph and tracking state changes happening to the entities involved. The comprehension task is treated as that of predicting, tracking, and answering questions about how entities change during the procedure. The dataset contains 488 paragraphs and 3,300 sentences. Each paragraph is richly annotated with the existence and locations of all the main entities (the “participants”) at every time step (sentence) throughout the procedure (~81,000 annotations).\r\n\r\nProPara paragraphs are natural (authored by crowdsourcing) rather than synthetic (e.g., in bAbI). Workers were given a prompt (e.g., “What happens during photosynthesis?”) and then asked to author a series of sentences describing the sequence of events in the procedure. From these sentences, participant entities and their existence and locations were identified. The goal of the challenge is to predict the existence and location of each participant, based on sentences in the paragraph.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/propara)", "variants": ["ProPara"], "title": "Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural Text"}
{"id": "IP102", "contents": "IP102 contains more than 75,000 images belonging to 102 categories, which exhibit a natural long-tailed distribution.\r\n\r\nSource: [IP102: A Large-Scale Benchmark Dataset for Insect Pest Recognition](/paper/ip102-a-large-scale-benchmark-dataset-for)", "variants": ["IP102"], "title": "IP102: A Large-Scale Benchmark Dataset for Insect Pest Recognition"}
{"id": "MovieGraphs", "contents": "Provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions.\r\n\r\nSource: [MovieGraphs: Towards Understanding Human-Centric Situations from Videos](https://arxiv.org/pdf/1712.06761v2.pdf)", "variants": ["MovieGraphs"], "title": "MovieGraphs: Towards Understanding Human-Centric Situations from Videos"}
{"id": "Partial-iLIDS", "contents": "Partial iLIDS is a dataset for occluded person person re-identification. It contains a total of 476 images of 119 people captured by 4 non-overlapping cameras. Some images contain people occluded by other individuals or luggage.\n\nSource: [Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification](https://arxiv.org/abs/1904.04975)\nImage Source: [https://arxiv.org/pdf/1904.04975.pdf](https://arxiv.org/pdf/1904.04975.pdf)", "variants": ["Partial-iLIDS"], "title": "Person re-identification by probabilistic relative distance comparison"}
{"id": "ShEMO", "contents": "The database includes 3000 semi-natural utterances, equivalent to 3 hours and 25 minutes of speech data extracted from online radio plays. The ShEMO covers speech samples of 87 native-Persian speakers for five basic emotions including anger, fear, happiness, sadness and surprise, as well as neutral state.\r\n\r\nSource: [ShEMO -- A Large-Scale Validated Database for Persian Speech Emotion Detection](/paper/shemo-a-large-scale-validated-database-for)", "variants": ["ShEMO"], "title": "ShEMO -- A Large-Scale Validated Database for Persian Speech Emotion Detection"}
{"id": "SberQuAD", "contents": "A large scale analog of Stanford SQuAD in the Russian language - is a valuable resource that has not been properly presented to the scientific community. \r\n\r\nSource: [SberQuAD -- Russian Reading Comprehension Dataset: Description and Analysis](/paper/sberquad-russian-reading-comprehension)", "variants": ["SberQuAD"], "title": "SberQuAD -- Russian Reading Comprehension Dataset: Description and Analysis"}
{"id": "Florentine", "contents": "The **Florentine** dataset is a dataset of facial gestures which contains facial clips from 160 subjects (both male and female), where gestures were artificially generated according to a specific request, or genuinely given due to a shown stimulus. 1032 clips were captured for posed expressions and 1745 clips for induced facial expressions amounting to a total of 2777 video clips. Genuine facial expressions were induced in subjects using visual stimuli, i.e. videos selected randomly from a bank of Youtube videos to generate a specific emotion.\n\nSource: [Deep video gesture recognition using illumination invariants](https://arxiv.org/abs/1603.06531)\nImage Source: [https://www.micc.unifi.it/resources/datasets/florence-3d-faces/](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)", "variants": ["Florentine"], "title": "Combining Multiple Kernel Methods on Riemannian Manifold for Emotion Recognition in the Wild"}
{"id": "VOT2016", "contents": "**VOT2016** is a video dataset for visual object tracking. It contains 60 video clips and 21,646 corresponding ground truth maps with pixel-wise annotation of salient objects.\r\n\r\nSource: [Video Saliency Detection by 3D Convolutional Neural Networks](https://arxiv.org/abs/1807.04514)\r\nImage Source: [https://www.researchgate.net/profile/Mohamed_Abdelpakey/publication/327850473/figure/fig3/AS:674547829338114@1537836143562/Visual-results-on-VOT2016-data-set-for-four-sequences.png](https://www.researchgate.net/profile/Mohamed_Abdelpakey/publication/327850473/figure/fig3/AS:674547829338114@1537836143562/Visual-results-on-VOT2016-data-set-for-four-sequences.png)", "variants": ["VOT-2016", "VOT2016"], "title": "The Visual Object Tracking VOT2016 Challenge Results"}
{"id": "Image and Video Advertisements", "contents": "The **Image and Video Advertisements** collection consists of an image dataset of 64,832 image ads, and a video dataset of 3,477 ads. The data contains rich annotations encompassing the topic and sentiment of the ads, questions and answers describing what actions the viewer is prompted to take and the reasoning that the ad presents to persuade the viewer (\"What should I do according to this ad, and why should I do it? \"), and symbolic references ads make (e.g. a dove symbolizes peace). \r\n\r\nSource: [Automatic Understanding of Image and Video Advertisements](/paper/automatic-understanding-of-image-and-video)", "variants": ["Image and Video Advertisements"], "title": "Automatic Understanding of Image and Video Advertisements"}
{"id": "AISHELL-1", "contents": "AISHELL-1 is a corpus for speech recognition research and building speech recognition systems for Mandarin. \r\n\r\nSource: [AISHELL-1: An Open-Source Mandarin Speech Corpus and A Speech Recognition Baseline](/paper/aishell-1-an-open-source-mandarin-speech)", "variants": ["AISHELL-1"], "title": "AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline"}
{"id": "AMR Bank", "contents": "The **AMR Bank** is a set of English sentences paired with simple, readable semantic representations. Version 3.0 released in 2020 consists of 59,255 sentences.\r\n\r\nEach AMR is a single rooted, directed graph. AMRs include PropBank semantic roles, within-sentence coreference, named entities and types, modality, negation, questions, quantities, and so on.\r\n\r\nThe image presents an AMR of a sample sentence “The boy wants to go”.\r\n\r\nSource: [https://amr.isi.edu/](https://amr.isi.edu/)\r\nImage Source: [https://amr.isi.edu/language.html](https://amr.isi.edu/language.html)", "variants": ["AMR (english, MRP 2020)", "AMR Bank"], "title": "Abstract Meaning Representation for Sembanking"}
{"id": "ReDial", "contents": "ReDial (Recommendation Dialogues) is an annotated dataset of dialogues, where users recommend movies to each other. The dataset consists of over 10,000 conversations centered around the theme of providing movie recommendations. \r\n\r\nSource: [Towards Deep Conversational Recommendations](/paper/towards-deep-conversational-recommendations)", "variants": ["ReDial"], "title": "Towards Deep Conversational Recommendations"}
{"id": "CMRC 2017", "contents": "Contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set.\r\n\r\nSource: [Dataset for the First Evaluation on Chinese Machine Reading Comprehension](/paper/dataset-for-the-first-evaluation-on-chinese)", "variants": ["CMRC 2017"], "title": "Dataset for the First Evaluation on Chinese Machine Reading Comprehension"}
{"id": "SPARE3D", "contents": "Contains three types of 2D-3D reasoning tasks on view consistency, camera pose, and shape generation, with increasing difficulty. \r\n\r\nSource: [SPARE3D: A Dataset for SPAtial REasoning on Three-View Line Drawings](/paper/spare3d-a-dataset-for-spatial-reasoning-on)", "variants": ["SPARE3D"], "title": "SPARE3D: A Dataset for SPAtial REasoning on Three-View Line Drawings"}
{"id": "BreizhCrops", "contents": "**BreizhCrops** is a satellite image time series dataset for crop type classification. It consists on aggregated label data and Sentinel-2 top-of-atmosphere as well as bottom-of-atmosphere time series in the region of Brittany (Breizh in local language), north-east France.\n\nSource: [https://github.com/TUM-LMF/BreizhCrops](https://github.com/TUM-LMF/BreizhCrops)", "variants": ["BreizhCrops"], "title": "BreizhCrops: A Satellite Time Series Dataset for Crop Type Identification"}
{"id": "MUSIC", "contents": "The Multi-Spectral Imaging via Computed Tomography (MUSIC) dataset is a two-part (2D- and 3D spectral) open access dataset for advanced image analysis of spectral radiographic (x-ray) scans, their tomographic reconstruction and the detection of specific materials within such scans. The scans operate at a photon energy range of around 20 keV up to 160 keV.\r\n\r\nThe dataset includes — for 2D- as well as 3D spectral data — the corrected (e.g. calibrated) radiographic projections, their tomographic reconstructions (based on 37 projections of 256 detector pixels into a 100×100 pixel CT image per slice) and the corresponding set of segmentation variants.\r\n\r\nSource: [Multi-Spectral Imaging via Computed Tomography (MUSIC)](http://easi-cil.compute.dtu.dk/index.php/datasets/music/)", "variants": ["MUSIC"], "title": "Multi-Spectral Imaging via Computed Tomography (MUSIC) - Comparing Unsupervised Spectral Segmentations for Material Differentiation"}
{"id": "Places-LT", "contents": "**Places-LT** has an imbalanced training set with 62,500 images for 365 classes from Places-2. The class frequencies follow a natural power law distribution with a maximum number of 4,980 images per class and a minimum number of 5 images per class. The validation and testing sets are balanced and contain 20 and 100 images per class respectively.\r\n\r\nSource: [Long-Tailed Recognition Using Class-Balanced Experts](https://arxiv.org/abs/2004.03706)", "variants": ["Places-LT"], "title": "Large-Scale Long-Tailed Recognition in an Open World"}
{"id": "Google Refexp", "contents": "A new large-scale dataset for referring expressions, based on MS-COCO.\r\n\r\nSource: [Generation and Comprehension of Unambiguous Object Descriptions](/paper/generation-and-comprehension-of-unambiguous)", "variants": ["Google Refexp"], "title": "Generation and Comprehension of Unambiguous Object Descriptions"}
{"id": "AG’s Corpus", "contents": "Antonio Gulli’s corpus of news articles is a collection of more than 1 million news articles. The articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004.\r\nThe dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non - commercial activity.\r\n\r\nA subset of this corpus, AG News, consisting of the 4 largest classes is a popular topic classification dataset.\r\n\r\nSource: [AG's corpus of news articles](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)", "variants": ["AG’s Corpus"], "title": "Neural Module Networks"}
{"id": "SWAG", "contents": "Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). SWAG (Situations With Adversarial Generations) is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.\r\n\r\nThe dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans. The authors aim for SWAG to be a benchmark for evaluating grounded commonsense NLI and for learning representations.\r\n\r\nSource: [SWAG](https://rowanzellers.com/swag/)\r\nImage Source: [Zellers et al](https://arxiv.org/pdf/1808.05326v1.pdf)", "variants": ["SWAG"], "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"}
{"id": "Helsinki Prosody Corpus", "contents": "The Helsinki Prosody Corpus is a dataset for predicting prosodic prominence from written text. The prosodic annotations are automatically generated, high quality prosodic for the 'clean' subsets of LibriTTS corpus (Zen et al., 2019), comprising of 262.5 hours of read speech from 1230 speakers. The transcribed sentences were aligned and then prosodically annotated with word-level acoustic prominence labels.\r\n\r\nSource: [Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations](https://arxiv.org/abs/1908.02262)", "variants": ["Helsinki Prosody Corpus"], "title": "Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations"}
{"id": "Jamendo Corpus", "contents": "The **Jamendo Corpus** is a voice detection dataset consisting of 93 songs with Creative Commons license from the [Jamendo](http://www.jamendo.com/) free music sharing website. Segments of each song are annotated as “voice” (sung or spoken) or “no-voice”. The songs constitute a total of about 6 hours of music. The files are all from different artists and represent various genres from mainstream commercial music. The Jamendo audio files are coded in stereo Vorbis OGG 44.1kHz with 112KB/s bitrate. The original split contains 61, 16 and 16 songs in training, validation and testing set, respectively.\n\nSource: [Vocal detection in music with support vector machines](https://perso.telecom-paristech.fr/grichard/Publications/Icassp08_ramona.pdf)\nAudio Source: [https://zenodo.org/record/2585988](https://zenodo.org/record/2585988)", "variants": ["Jamendo Corpus"], "title": "Kernel Additive Models for Source Separation"}
{"id": "CN-CELEB", "contents": "CN-Celeb is a large-scale speaker recognition dataset collected `in the wild'. This dataset contains more than 130,000 utterances from 1,000 Chinese celebrities, and covers 11 different genres in real world.\r\n\r\nSource: [CN-CELEB: a challenging Chinese speaker recognition dataset](/paper/cn-celeb-a-challenging-chinese-speaker)", "variants": ["CN-CELEB"], "title": "CN-CELEB: a challenging Chinese speaker recognition dataset"}
{"id": "LEAF Benchmark", "contents": "A suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments.\r\n\r\nSource: [LEAF: A Benchmark for Federated Settings](/paper/leaf-a-benchmark-for-federated-settings)", "variants": ["LEAF Benchmark"], "title": "LEAF: A Benchmark for Federated Settings"}
{"id": "Multi-species fruit flower detection datasets", "contents": "This dataset consists of four sets of flower images, from three different species: apple, peach, and pear, and accompanying ground truth images. The images were acquired under a range of imaging conditions. These datasets support work in an accompanying paper that demonstrates a flower identification algorithm that is robust to uncontrolled environments and applicable to different flower species. While this data is primarily provided to support that paper, other researchers interested in flower detection may also use the dataset to develop new algorithms. Flower detection is a problem of interest in orchard crops because it is related to management of fruit load.\r\n\r\nSource: [Multi-species fruit flower detection datasets](https://data.nal.usda.gov/dataset/data-multi-species-fruit-flower-detection-using-refined-semantic-segmentation-network)", "variants": ["Multi-species fruit flower detection datasets"], "title": "Multispecies fruit flower detection using a refined semantic segmentation network"}
{"id": "Office-31", "contents": "The Office dataset contains 31 object categories in three domains: Amazon, DSLR and Webcam. The 31 categories in the dataset consist of objects commonly encountered in office settings, such as keyboards, file cabinets, and laptops. The Amazon domain contains on average 90 images per class and 2817 images in total. As these images were captured from a website of online merchants, they are captured against clean background and at a unified scale. The DSLR domain contains 498 low-noise high resolution images (4288×2848). There are 5 objects per category. Each object was captured from different viewpoints on average 3 times. For Webcam, the 795 images of low resolution (640×480) exhibit significant noise and color as well as white balance artifacts.\r\n\r\nSource: [Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors](https://arxiv.org/abs/1611.08195)\r\nImage Source: [https://www.researchgate.net/publication/310953258](https://www.researchgate.net/publication/310953258)", "variants": ["Office-31"], "title": "T.: Adapting visual category models to new domains. In: ECCV"}
{"id": "SUN360", "contents": "The goal of the **SUN360** panorama database is to provide academic researchers in computer vision, computer graphics and computational photography, cognition and neuroscience, human perception, machine learning and data mining, with a comprehensive collection of annotated panoramas covering 360x180-degree full view for a large variety of environmental scenes, places and the objects within. To build the core of the dataset, the authors download a huge number of high-resolution panorama images from the Internet, and group them into different place categories. Then, they designed a WebGL annotation tool for annotating the polygons and cuboids for objects in the scene.\r\n\r\nSource: [Scene UNderstanding 360° panorama](https://vision.cs.princeton.edu/projects/2012/SUN360/data/)\r\nImage Source: [http://3dvision.princeton.edu/projects/2012/SUN360/](http://3dvision.princeton.edu/projects/2012/SUN360/)", "variants": ["SUN360"], "title": "Recognizing scene viewpoint using panoramic place representation"}
{"id": "CUHK-Shadow", "contents": "Collects shadow images for multiple scenarios and compiled a new dataset of 10,500 shadow images, each with labeled ground-truth mask, for supporting shadow detection in the complex world. The dataset covers a rich variety of scene categories, with diverse shadow sizes, locations, contrasts, and types. \r\n\r\nSource: [Revisiting Shadow Detection: A New Benchmark Dataset for Complex World](/paper/revisiting-shadow-detection-a-new-benchmark)", "variants": ["CUHK-Shadow"], "title": "Revisiting Shadow Detection: A New Benchmark Dataset for Complex World"}
{"id": "KPTimes", "contents": "KPTimes is a large-scale dataset of news texts paired with editor-curated keyphrases. \r\n\r\nSource: [KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents](/paper/kptimes-a-large-scale-dataset-for-keyphrase-1)", "variants": ["KPTimes"], "title": "KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents"}
{"id": "CompCars", "contents": "The **Comprehensive Cars (CompCars)** dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The web-nature data contains 163 car makes with 1,716 car models. There are a total of 136,726 images capturing the entire cars and 27,618 images capturing the car parts. The full car images are labeled with bounding boxes and viewpoints. Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car. The surveillance-nature data contains 50,000 car images captured in the front view. \r\n\r\nThe dataset can be used for the tasks of:\r\n\r\n- Fine-grained classification\r\n- Attribute prediction\r\n- Car model verification\r\n\r\nThe dataset can be also used for other tasks such as image ranking, multi-task learning, and 3D reconstruction.", "variants": ["CompCars"], "title": "A large-scale car dataset for fine-grained categorization and verification"}
{"id": "Open Images V4", "contents": "Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images) are provided. The images often show complex scenes with several objects (8 annotated objects per image on average). Visual relationships between them are annotated, which support visual relationship detection, an emerging task that requires structured reasoning.\r\n\r\nSource: [The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale](https://arxiv.org/pdf/1811.00982)\r\nImage Source: [https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html)", "variants": ["Open Images V4"], "title": "The Open Images Dataset V4"}
{"id": "Moving MNIST", "contents": "The **Moving MNIST** dataset contains 10,000 video sequences, each consisting of 20 frames. In each video sequence, two digits move independently around the frame, which has a spatial resolution of 64×64 pixels. The digits frequently intersect with each other and bounce off the edges of the frame\r\n\r\nSource: [Mutual Suppression Network for Video Prediction using Disentangled Features](https://arxiv.org/abs/1804.04810)\r\nImage Source: [http://www.cs.toronto.edu/~nitish/unsupervised_video/](http://www.cs.toronto.edu/~nitish/unsupervised_video/)", "variants": ["Moving MNIST"], "title": "Unsupervised Learning of Video Representations using LSTMs"}
{"id": "MLe2e", "contents": "MLe2 is a dataset for the evaluation of scene text end-to-end reading systems and all intermediate stages such as text detection, script identification and text recognition. The dataset contains a total of 711 scene images covering four different scripts (Latin, Chinese, Kannada, and Hangul).\r\n\r\nSource: [A fine-grained approach to scene text script identification](https://arxiv.org/abs/1602.07475)", "variants": ["MLe2e"], "title": "A Fine-Grained Approach to Scene Text Script Identification"}
{"id": "Tencent ML-Images", "contents": "Tencent ML-Images is a large open-source multi-label image database, including 17,609,752 training and 88,739 validation image URLs, which are annotated with up to 11,166 categories.\r\n\r\nSource: [Tencent ML-Images](https://github.com/Tencent/tencent-ml-images)", "variants": ["Tencent ML-Images"], "title": "Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning"}
{"id": "Trans10K", "contents": "A large-scale dataset for transparent object segmentation, named Trans10K, consisting of 10,428 images of real scenarios with carefully manual annotations, which are 10 times larger than the existing datasets. \r\n\r\nSource: [Segmenting Transparent Objects in the Wild](/paper/segmenting-transparent-objects-in-the-wild)", "variants": ["Trans10K"], "title": "Segmenting Transparent Objects in the Wild"}
{"id": "OneStopEnglish", "contents": "Useful for through two applications - automatic readability assessment and automatic text simplification. The corpus consists of 189 texts, each in three versions (567 in total).\r\n\r\nSource: [OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification](/paper/onestopenglish-corpus-a-new-corpus-for)", "variants": ["OneStopEnglish"], "title": "OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification"}
{"id": "OTB", "contents": "Object Tracking Benchmark (**OTB**) is a visual tracking benchmark that is widely used to evaluate the performance of a visual tracking algorithm. The dataset contains a total of 100 sequences and each is annotated frame-by-frame with bounding boxes and 11 challenge attributes. [OTB-2013](otb-2013) dataset contains 51 sequences and the [OTB-2015](otb-2015) dataset contains all 100 sequences of the OTB dataset.\r\n\r\nSource: [Deep Meta Learning for Real-Time Target-Aware Visual Tracking](https://arxiv.org/abs/1712.09153)", "variants": ["OTB-2015", "OTB-2013", "OTB-50", "OTB"], "title": "Object Tracking Benchmark"}
{"id": "iKala", "contents": "The **iKala** dataset is a singing voice separation dataset that comprises of 252 30-second excerpts sampled from 206 iKala songs (plus 100 hidden excerpts reserved for MIREX data mining contest). The music accompaniment and the singing voice are recorded at the left and right channels respectively. Additionally, the human-labeled pitch contours and timestamped lyrics are provided.\r\n\r\nThis dataset is not available anymore.\r\n\r\nSource: [http://mac.citi.sinica.edu.tw/ikala/](http://mac.citi.sinica.edu.tw/ikala/)", "variants": ["iKala"], "title": "Vocal activity informed singing voice separation with the iKala dataset"}
{"id": "ConceptNet", "contents": "ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. \r\n\r\nSource: [ConceptNet 5.5: An Open Multilingual Graph of General Knowledge](https://arxiv.org/pdf/1612.03975v2.pdf)\r\nImage Source: [Speer et al](https://arxiv.org/pdf/1612.03975v2.pdf)", "variants": ["ConceptNet"], "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"}
{"id": "Meta-World Benchmark", "contents": "An open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks.\r\n\r\nSource: [Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning](/paper/meta-world-a-benchmark-and-evaluation-for)", "variants": ["Meta-World Benchmark"], "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"}
{"id": "FrameNet", "contents": "**FrameNet** is a linguistic knowledge graph containing information about lexical and predicate argument semantics of the English language. FrameNet contains two distinct entity classes: frames and lexical units, where a frame is a meaning and a lexical unit is a single meaning for a word.\r\n\r\nSource: [Retrofitting Distributional Embeddings to Knowledge Graphswith Functional Relations](https://arxiv.org/abs/1708.00112)", "variants": ["FrameNet"], "title": "Interactive facial feature localization"}
{"id": "AOLP", "contents": "The application-oriented license plate (**AOLP**) benchmark database has 2049 images of Taiwan license plates. This database is categorized into three subsets: access control (AC) with 681 samples, traffic law enforcement (LE) with 757 samples, and road patrol (RP) with 611 samples. AC refers to the cases that a vehicle passes a fixed passage with a lower speed or full stop. This is the easiest situation. The images are captured under different illuminations and different weather conditions. LE refers to the cases that a vehicle violates traffic laws and is captured by roadside camera. The background are really cluttered, with road sign and multiple plates in one image. RP refers to the cases that the camera is held on a patrolling vehicle, and the images are taken with arbitrary viewpoints and distances.\n\nSource: [Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs](https://arxiv.org/abs/1601.05610)\nImage Source: [http://aolpr.ntust.edu.tw/lab/index.html](http://aolpr.ntust.edu.tw/lab/index.html)", "variants": ["AOLP-RP", "AOLP"], "title": "Application-Oriented License Plate Recognition"}
{"id": "Structured3D", "contents": "**Structured3D** is a large-scale photo-realistic dataset containing 3.5K house designs (a) created by professional designers with a variety of ground truth 3D structure annotations (b) and generate photo-realistic 2D images (c).\r\nThe dataset consists of rendering images and corresponding ground truth annotations (e.g., semantic, albedo, depth, surface normal, layout) under different lighting and furniture configurations.\r\n\r\nSource: [https://github.com/bertjiazheng/Structured3D](https://github.com/bertjiazheng/Structured3D)\r\nImage Source: [https://github.com/bertjiazheng/Structured3D](https://github.com/bertjiazheng/Structured3D)", "variants": ["Structured3D"], "title": "Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling"}
{"id": "iNaturalist", "contents": "The iNaturalist 2017 dataset (iNat) contains 675,170 training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images per category. For example, the largest super-category “Plantae (Plant)” has 196,613 images from 2,101 categories; whereas the smallest super-category “Protozoa” only has 381 images from 4 categories.\r\n\r\nSource: [Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning](https://arxiv.org/abs/1806.06193)\r\nImage Source: [https://github.com/visipedia/inat_comp/tree/master/2017](https://github.com/visipedia/inat_comp/tree/master/2017)", "variants": ["iNaturalist", "iNaturalist (227-way multi-shot)", "iNaturalist 2018"], "title": "The iNaturalist Species Classification and Detection Dataset"}
{"id": "LFSD", "contents": "The **Light Field Saliency Database** (**LFSD**) contains 100 light fields with 360×360 spatial resolution. A rough focal stack and an all-focus image are provided for each light field. The images in this dataset usually have one salient foreground object and a background with good color contrast.\r\n\r\nSource: [Light Field Saliency Detection with Deep Convolutional Networks](https://arxiv.org/abs/1906.08331)\r\nImage Source: [https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/](https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/)", "variants": ["LFSD"], "title": "Saliency Detection on Light Field"}
{"id": "Arxiv GR-QC", "contents": "**Arxiv GR-QC** (General Relativity and Quantum Cosmology) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to General Relativity and Quantum Cosmology category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.\n\nSource: [https://snap.stanford.edu/data/ca-GrQc.html](https://snap.stanford.edu/data/ca-GrQc.html)", "variants": ["arXiv-GrQc 2-clique", "arXiv-GrQc 3-clique", "Arxiv GR-QC"], "title": "Graph evolution: Densification and shrinking diameters"}
{"id": "LOL", "contents": "The **LOL** dataset is composed of 500 low-light and normal-light image pairs and divided into 485 training pairs and 15 testing pairs. The low-light images contain noise produced during the photo capture process. Most of the images are indoor scenes. All the images have a resolution of 400×600.\r\n\r\nSource: [Unsupervised Real-world Low-light Image Enhancement with Decoupled Networks](https://arxiv.org/abs/2005.02818)\r\nImage Source: [https://daooshee.github.io/BMVC2018website/](https://daooshee.github.io/BMVC2018website/)", "variants": ["LOL"], "title": "Deep Retinex Decomposition for Low-Light Enhancement"}
{"id": "COMP6", "contents": "**COMP6** is a benchmark for evaluating the extensibility of machine-learning based molecular potentials. It contains a diverse set of organic molecules.\n\nSource: [https://github.com/isayev/COMP6](https://github.com/isayev/COMP6)", "variants": ["COMP6"], "title": "Less is more: sampling chemical space with active learning"}
{"id": "ISTD", "contents": "The Image Shadow Triplets dataset (**ISTD**) is a dataset for shadow understanding that contains 1870 image triplets of shadow image, shadow mask, and shadow-free image.\r\n\r\nSource: [ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal](https://arxiv.org/abs/1908.01323)\r\nImage Source: [Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal](https://paperswithcode.com/paper/stacked-conditional-generative-adversarial/)", "variants": ["ISTD"], "title": "Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal"}
{"id": "K2HPD", "contents": "Includes 100K depth images under challenging scenarios.\r\n\r\nSource: [Human Pose Estimation from Depth Images via Inference Embedded Multi-task Learning](/paper/human-pose-estimation-from-depth-images-via)", "variants": ["K2HPD"], "title": "Human Pose Estimation from Depth Images via Inference Embedded Multi-task Learning"}
{"id": "iCartoonFace", "contents": "The **iCartoonFace** dataset is a large-scale dataset that can be used for two different tasks: cartoon face detection and cartoon face recognition.\n\nSource: [https://github.com/luxiangju-PersonAI/iCartoonFace](https://github.com/luxiangju-PersonAI/iCartoonFace)\nImage Source: [https://github.com/luxiangju-PersonAI/iCartoonFace](https://github.com/luxiangju-PersonAI/iCartoonFace)", "variants": ["iCartoonFace"], "title": "iCartoonFace: A Benchmark of Cartoon Person Recognition"}
{"id": "iSAID", "contents": "iSAID contains 655,451 object instances for 15 categories across 2,806 high-resolution images. The images of iSAID is the same as the DOTA-v1.0 dataset, which are manily collected from the Google Earth, some are taken by satellite JL-1, the others are taken by satellite GF-2 of the China Centre for Resources Satellite Data and Application.\r\n\r\nSource: [iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images](/paper/isaid-a-large-scale-dataset-for-instance)\r\nImage Source: [iSAID](https://captain-whu.github.io/iSAID/index.html)", "variants": ["iSAID"], "title": "iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images"}
{"id": "PKU-Reid", "contents": "This dataset contains 114 individuals including 1824 images captured from two disjoint camera views. For each person, eight images are captured from eight different orientations under one camera view and are normalized to 128x48 pixels. This dataset is also split into two parts randomly. One contains 57 individuals for training, and the other contains 57 individuals for testing.\n\nSource: [https://github.com/charliememory/PKU-Reid-Dataset](https://github.com/charliememory/PKU-Reid-Dataset)\nImage Source: [https://arxiv.org/pdf/1605.02464.pdf](https://arxiv.org/pdf/1605.02464.pdf)", "variants": ["PKU-Reid"], "title": "Orientation Driven Bag of Appearances for Person Re-identification"}
{"id": "Oktoberfest Food Dataset", "contents": "A realistic, diverse, and challenging dataset for object detection on images. The data was recorded at a beer tent in Germany and consists of 15 different categories of food and drink items. \r\n\r\nSource: [Oktoberfest Food Dataset](/paper/oktoberfest-food-dataset)", "variants": ["Oktoberfest Food Dataset"], "title": "Oktoberfest Food Dataset"}
{"id": "CSAbstruct Dataset", "contents": "CSAbstruct is a new dataset of annotated computer science abstracts with sentence labels according to their rhetorical roles. The key difference between this dataset and PUBMED-RCT is that PubMed abstracts are written according to a predefined structure, whereas computer science papers are free-form. Therefore, there is more variety in writing styles in CSABSTRUCT. CSABSTRUCT is collected from the Semantic Scholar corpus (Ammar et al., 2018). Each sentence is annotated by 5 workers on the Figure-eight platform,6 with one of 5 categories {BACKGROUND, OBJECTIVE, METHOD, RESULT, OTHER}.\r\n\r\nSource: [Pretrained Language Models for Sequential Sentence Classification](https://arxiv.org/pdf/1909.04054.pdf)", "variants": ["CSAbstruct Dataset"], "title": "Pretrained Language Models for Sequential Sentence Classification"}
{"id": "E-GMD", "contents": "Expanded Groove MIDI dataset (E-GMD) is an automatic drum transcription (ADT) dataset that contains 444 hours of audio from 43 drum kits, making it an order of magnitude larger than similar datasets, and the first with human-performed velocity annotations.\r\n\r\nSource: [Improving Perceptual Quality of Drum Transcription with the Expanded Groove MIDI Dataset](https://arxiv.org/pdf/2004.00188)", "variants": ["E-GMD"], "title": "Improving Perceptual Quality of Drum Transcription with the Expanded Groove MIDI Dataset"}
{"id": "ZuBuD+", "contents": "A more balanced version of ZuBuD.\r\n\r\nSource: [A location-aware embedding technique for accurate landmark recognition](/paper/a-location-aware-embedding-technique-for)", "variants": ["ZuBuD+"], "title": "A location-aware embedding technique for accurate landmark recognition"}
{"id": "ScanRefer Dataset", "contents": "Contains 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.\r\n\r\nSource: [ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language](/paper/scanrefer-3d-object-localization-in-rgb-d)", "variants": ["ScanRefer Dataset"], "title": "ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language"}
{"id": "LibriVoxDeEn", "contents": "LibriVoxDeEn is a corpus of sentence-aligned triples of German audio, German text, and English translation, based on German audiobooks. The speech translation data consist of 110 hours of audio material aligned to over 50k parallel sentences. An even larger dataset comprising 547 hours of German speech aligned to German text is available for speech recognition. The audio data is read speech and thus low in disfluencies.\r\n\r\nSource: [LibriVoxDeEn: A Corpus for German-to-English Speech Translation and German Speech Recognition](/paper/librivoxdeen-a-corpus-for-german-to-english)", "variants": ["LibriVoxDeEn"], "title": "LibriVoxDeEn: A Corpus for German-to-English Speech Translation and German Speech Recognition"}
{"id": "MPI3D Disentanglement", "contents": "A data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position.\r\n\r\nSource: [On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset](/paper/on-the-transfer-of-inductive-bias-from)", "variants": ["MPI3D Disentanglement"], "title": "On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset"}
{"id": "Wiki-40B", "contents": "A new multilingual language model benchmark that is composed of 40+ languages spanning several scripts and linguistic families containing round 40 billion characters and aimed to accelerate the research of multilingual modeling.\r\n\r\nSource: [Wiki-40B: Multilingual Language Model Dataset](/paper/wiki-40b-multilingual-language-model-dataset)", "variants": ["Wiki-40B"], "title": "Wiki-40B: Multilingual Language Model Dataset"}
{"id": "RuSentRel", "contents": "**RuSentRel** is a corpus of analytical articles translated into Russian texts in the domain of international politics obtained from foreign authoritative sources. The collected articles contain both the author's opinion on the subject matter of the article and a large number of references mentioned between the participants of the described situations. In total, 73 large analytical texts were labeled with about 2000 relations.\r\n\r\nSource: [RuSentRel](https://github.com/nicolay-r/RuSentRel)", "variants": ["RuSentRel"], "title": "Extracting Sentiment Attitudes From Analytical Texts"}
{"id": "MCIC-COCO", "contents": "A large-scale machine comprehension dataset (based on the COCO images and captions).\r\n\r\nSource: [Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task](/paper/understanding-image-and-text-simultaneously-a)", "variants": ["MCIC-COCO"], "title": "Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task"}
{"id": "ChID", "contents": "ChID is a large-scale Chinese IDiom dataset for cloze test. ChID contains 581K passages and 729K blanks, and covers multiple domains. In ChID, the idioms in a passage were replaced with blank symbols. For each blank, a list of candidate idioms including the golden idiom are provided as choice. \r\n\r\nSource: [ChID: A Large-scale Chinese IDiom Dataset for Cloze Test](https://arxiv.org/pdf/1906.01265v3.pdf)\r\nImage Source: [https://arxiv.org/pdf/1906.01265v3.pdf](https://arxiv.org/pdf/1906.01265v3.pdf)", "variants": ["ChID"], "title": "ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"}
{"id": "WikiBio", "contents": "**WikiBio** is a dataset introduced for generating biography notes based on information found in an infobox – a fact table describing a person. Each sample in the dataset contains the infobox and the first sentence of each biography article as reference. On average, each reference sentence has 26.1 words. The corpus contains 728,321 instances, which has been divided into three sub-parts to provide 582,659 for training, 72,831 for validation and 72,831 for testing.\r\n\r\nSource: [Table-to-Text: Describing Table Region with Natural Language](https://arxiv.org/abs/1805.11234)\r\nImage Source: [https://arxiv.org/pdf/1603.07771.pdf](https://arxiv.org/pdf/1603.07771.pdf)", "variants": ["WikiBio"], "title": "Neural Text Generation from Structured Data with Application to the Biography Domain"}
{"id": "Medical Segmentation Decathlon", "contents": "The Medical Segmentation Decathlon is a collection of medical image segmentation datasets. It contains a total of 2,633 three-dimensional images collected across multiple anatomies of interest, multiple modalities and multiple sources. Specifically, it contains data for the following body organs or parts: Brain, Heart, Liver, Hippocampus, Prostate, Lung, Pancreas, Hepatic Vessel, Spleen and Colon.\r\n\r\nSource: [A large annotated medical image dataset for the development and evaluation of segmentation algorithms](https://arxiv.org/pdf/1902.09063.pdf)\r\nImage Source: [Simpson et al](https://arxiv.org/pdf/1902.09063.pdf)", "variants": ["Medical Segmentation Decathlon"], "title": "A large annotated medical image dataset for the development and evaluation of segmentation algorithms"}
{"id": "PSU NRTDB", "contents": "The **PSU Near-Regular Texture Database** is a texture dataset. It covers the spectrum of textures from completely regular to near-regular to irregular. It also includes video of near-regular textures in motion. The database also contains, or will include, test image sets with ground-truth for translation, rotation, reflection/glide-reflection symmetry detection algorithms.\r\n\r\nSource: [Computer Vision Online](https://computervisiononline.com/dataset/1105138706)", "variants": ["PSU NRTDB"], "title": "PatchMatch-Based Automatic Lattice Detection for Near-Regular Textures"}
{"id": "xR-EgoPose", "contents": "xR-EgoPose is an egocentric synthetic dataset for egocentric 3D human pose estimation. It consists of ~380 thousand photo-realistic egocentric camera images in a variety of indoor and outdoor spaces.", "variants": ["xR-EgoPose"], "title": "xR-EgoPose: Egocentric 3D Human Pose From an HMD Camera"}
{"id": "SceneNN", "contents": "SceneNN is an RGB-D scene dataset consisting of more than 100 indoor scenes. The scenes are captured at various places, e.g., offices, dormitory, classrooms, pantry, etc., from University of Massachusetts Boston and Singapore University of Technology and Design.\r\nAll scenes are reconstructed into triangle meshes and have per-vertex and per-pixel annotation. The dataset is additionally enriched with fine-grained information such as axis-aligned bounding boxes, oriented bounding boxes, and object poses.\r\n\r\nSource: [SceneNN: A Scene Meshes Dataset with aNNotations](http://103.24.77.34/scenenn/home/)\r\nImage Source: [http://103.24.77.34/scenenn/home/](http://103.24.77.34/scenenn/home/)", "variants": ["SceneNN"], "title": "SceneNN: A Scene Meshes Dataset with aNNotations"}
{"id": "GloREPlus", "contents": "A distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. \r\n\r\nSource: [Global Textual Relation Embedding for Relational Understanding](/paper/190600550)", "variants": ["GloREPlus"], "title": "Global Textual Relation Embedding for Relational Understanding"}
{"id": "Clotho", "contents": "**Clotho** is an audio captioning dataset, consisting of 4981 audio samples, and each audio sample has five captions (a total of 24 905 captions). Audio samples are of 15 to 30 s duration and captions are eight to 20 words long.\n\nSource: [https://zenodo.org/record/3490684](https://zenodo.org/record/3490684)\nImage Source: [https://arxiv.org/abs/1910.09387](https://arxiv.org/abs/1910.09387)", "variants": ["Clotho"], "title": "Clotho: An Audio Captioning Dataset"}
{"id": "SciERC", "contents": "**SciERC** dataset is a collection of 500 scientific abstract annotated with scientific entities, their relations, and coreference clusters. The abstracts are taken from 12 AI conference/workshop proceedings in four AI communities, from the Semantic Scholar Corpus. SciERC extends previous datasets in scientific articles SemEval 2017 Task 10 and SemEval 2018 Task 7 by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links.\r\n\r\nSource: [http://nlp.cs.washington.edu/sciIE/](http://nlp.cs.washington.edu/sciIE/)\r\nImage Source: [http://nlp.cs.washington.edu/sciIE/](http://nlp.cs.washington.edu/sciIE/)", "variants": ["SciERC"], "title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction"}
{"id": "SentEval", "contents": "SentEval is a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders.\r\n\r\nSource: [SentEval: An Evaluation Toolkit for Universal Sentence Representations](/paper/senteval-an-evaluation-toolkit-for-universal)", "variants": ["SentEval"], "title": "SentEval: An Evaluation Toolkit for Universal Sentence Representations"}
{"id": "MoVi", "contents": "Contains 60 female and 30 male actors performing a collection of 20 predefined everyday actions and sports movements, and one self-chosen movement.\r\n\r\nSource: [MoVi: A Large Multipurpose Motion and Video Dataset](/paper/movi-a-large-multipurpose-motion-and-video)", "variants": ["MoVi"], "title": "MoVi: A Large Multipurpose Motion and Video Dataset"}
{"id": "CSD", "contents": "Comprises 4 different subsets - Flat, House, Priory and Lab - each containing a number of different sequences that can be successfully relocalised against each other. \r\n\r\nSource: [CSD](https://github.com/torrvision/CollaborativeSLAMDataset)", "variants": ["CSD"], "title": "Collaborative Large-Scale Dense 3D Reconstruction with Online Inter-Agent Pose Optimisation"}
{"id": "ManyModalQA", "contents": "Collects the data by scraping Wikipedia and then utilize crowdsourcing to collect question-answer pairs. \r\n\r\nSource: [ManyModalQA: Modality Disambiguation and QA over Diverse Inputs](/paper/manymodalqa-modality-disambiguation-and-qa)", "variants": ["ManyModalQA"], "title": "ManyModalQA: Modality Disambiguation and QA over Diverse Inputs"}
{"id": "CoNLL++", "contents": "CoNLL++ is a corrected version of the CoNLL03 NER dataset where 5.38% of the test sentences have been fixed.\r\n\r\nSource: [CrossWeigh: Training Named Entity Tagger from Imperfect Annotations](/paper/crossweigh-training-named-entity-tagger-from)", "variants": ["CoNLL03", "CoNLL++"], "title": "CrossWeigh: Training Named Entity Tagger from Imperfect Annotations"}
{"id": "MotionSense", "contents": "This dataset includes time-series data generated by accelerometer and gyroscope sensors (attitude, gravity, userAcceleration, and rotationRate). It is collected with an iPhone 6s kept in the participant's front pocket using SensingKit which collects information from Core Motion framework on iOS devices. All data is collected in 50Hz sample rate. A total of 24 participants in a range of gender, age, weight, and height performed 6 activities in 15 trials in the same environment and conditions: downstairs, upstairs, walking, jogging, sitting, and standing.\n\nSource: [https://github.com/mmalekzadeh/motion-sense](https://github.com/mmalekzadeh/motion-sense)\nImage Source: [https://github.com/mmalekzadeh/motion-sense](https://github.com/mmalekzadeh/motion-sense)", "variants": ["MotionSense"], "title": "Protecting Sensory Data against Sensitive Inferences"}
{"id": "XL-R2R", "contents": "The **XL-R2R** dataset is built upon the R2R dataset and extends it with Chinese instructions. XL-R2R preserves the same splits as in R2R and thus consists of train, val-seen, and val-unseen splits with both English and Chinese instructions, and test split with English instructions only.\n\nSource: [https://github.com/zzxslp/Crosslingual-VLN](https://github.com/zzxslp/Crosslingual-VLN)", "variants": ["XL-R2R"], "title": "Cross-Lingual Vision-Language Navigation"}
{"id": "Replica", "contents": "The Replica Dataset is a dataset of high quality reconstructions of a variety of indoor spaces. Each reconstruction has clean dense geometry, high resolution and high dynamic range textures, glass and mirror surface information, planar segmentation as well as semantic class and instance segmentation. \r\n\r\nSource: [The Replica Dataset: A Digital Replica of Indoor Spaces](/paper/the-replica-dataset-a-digital-replica-of)", "variants": ["Replica"], "title": "The Replica Dataset: A Digital Replica of Indoor Spaces"}
{"id": "FDDB-360", "contents": "A 360-degree fisheye-like version of the popular FDDB face detection dataset.\r\n\r\nSource: [FDDB-360: Face Detection in 360-degree Fisheye Images](/paper/fddb-360-face-detection-in-360-degree-fisheye)", "variants": ["FDDB-360"], "title": "FDDB-360: Face Detection in 360-Degree Fisheye Images"}
{"id": "Shmoop Corpus", "contents": "Shmoop Corpus is a dataset of 231 stories that are paired with detailed multi-paragraph summaries for each individual chapter (7,234 chapters), where the summary is chronologically aligned with respect to the story chapter. From the corpus, a set of common NLP tasks are constructed, including Cloze-form question answering and a simplified form of abstractive summarization, as benchmarks for reading comprehension on stories.\r\n\r\nSource: [Shmoop Corpus](http://www.cs.toronto.edu/~makarand/shmoop/)", "variants": ["Shmoop Corpus"], "title": "The Shmoop Corpus: A Dataset of Stories with Loosely Aligned Summaries"}
{"id": "HandNet", "contents": "The HandNet dataset contains depth images of 10 participants' hands non-rigidly deforming in front of a RealSense RGB-D camera. The annotations are generated by a magnetic annotation technique. 6D pose is available for the center of the hand as well as the five fingertips (i.e. position and orientation of each).\r\n\r\nSource: [Rule Of Thumb: Deep derotation for improved fingertip detection](/paper/rule-of-thumb-deep-derotation-for-improved)", "variants": ["HandNet"], "title": "Rule Of Thumb: Deep derotation for improved fingertip detection"}
{"id": "SYSU-CEUS", "contents": "The **SYSU-CEUS** dataset consists of three types of Focal liver lesions (FLLs): 186 HCC instances, 109 HEM instances and 58 FNH instances (i.e.,186 malignant instances and 167 benign instances).\nThis dataset is collected from the First Affiliated Hospital, Sun Yat-sen University. The equipment used was Aplio SSA-770A (Toshiba Medical System).\nAll these instances with resolution 768*576 were taken from different patients, with large variations in appearance and enhancement patterns (e.g. sizes, contrasts, shapes and locations) of the FLLs.\n\nSource: [https://github.com/lemondan/Focal-liver-lesions-dataset-in-CEUS](https://github.com/lemondan/Focal-liver-lesions-dataset-in-CEUS)", "variants": ["SYSU-CEUS"], "title": "Recognizing Focal Liver Lesions in Contrast-Enhanced Ultrasound with Discriminatively Trained Spatio-Temporal Model"}
{"id": "capes", "contents": "Approximately 240,000 documents were collected and aligned using the Hunalign tool.\r\n\r\nSource: [A Parallel Corpus of Theses and Dissertations Abstracts](/paper/a-parallel-corpus-of-theses-and-dissertations)", "variants": ["capes"], "title": "A Parallel Corpus of Theses and Dissertations Abstracts"}
{"id": "KITTI-Depth", "contents": "The **KITTI-Depth** dataset includes depth maps from projected LiDAR point clouds that were matched against the depth estimation from the stereo cameras. The depth images are highly sparse with only 5% of the pixels available and the rest is missing. The dataset has 86k training images, 7k validation images, and 1k test set images on the benchmark server with no access to the ground truth.\n\nSource: [Confidence Propagation through CNNs for Guided Sparse Depth Regression](https://arxiv.org/abs/1811.01791)\nImage Source: [http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction](http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction)", "variants": ["KITTI-Depth"], "title": "Sparsity Invariant CNNs"}
{"id": "Numeric Fused-Head", "contents": "The Numeric Fused-Head dataset consists of ~10K examples of crowd-sourced classified examples, labeled into 7 different categories, from two types. In the first type, Reference, the missing head is referenced explicitly somewhere else in the discourse, either in the same sentence or in surrounding sentences. In the second type, Implicit, the missing head does not appear in the text and needs to be inferred by the reader or hearer based on the context or world knowledge. This category was labeled into the 6 most common categories of the dataset. Models are evaluated based on accuracy.\r\n\r\nSource: [NLP Progress](http://nlpprogress.com/english/missing_elements.html)", "variants": ["Numeric Fused-Head", "Numeric Fused-Head (dev)", "Numeric Fused-Head (test)"], "title": "Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"}
{"id": "Caltech-256", "contents": "**Caltech-256** is an object recognition dataset containing 30,607 real-world images, of different sizes, spanning 257 classes (256 object classes and an additional clutter class). Each class is represented by at least 80 images. The dataset is a superset of the Caltech-101 dataset.\r\n\r\nSource: [Exploiting Non-Linear Redundancy for Neural Model Compression](https://arxiv.org/abs/2005.14070)\r\n\r\nImage Source: [ML4A](https://twitter.com/ml4a_/status/934796379171512322)", "variants": ["Caltech-256, 1024 Labels", "Caltech-256", "Caltech-256 5-way (1-shot)"], "title": "Discovering states and transformations in image collections"}
{"id": "CrowdFlow", "contents": "The **TUB CrowdFlow** is a synthetic dataset that contains 10 sequences showing 5 scenes. Each scene is rendered twice: with a static point of view and a dynamic camera to simulate drone/UAV based surveillance. The scenes are render using Unreal Engine at HD resolution (1280x720) at 25 fps, which is typical for current commercial CCTV surveillance systems. The total number of frames is 3200.\r\n\r\nEach sequence has the following ground-truth data:\r\n\r\n* Optical flow fields\r\n* Person trajectories (up to 1451)\r\n* Dense pixel trajectories\r\n\r\nSource: [Optical Flow Dataset and Benchmark for Visual Crowd Analysis](/paper/optical-flow-dataset-and-benchmark-for-visual)", "variants": ["CrowdFlow"], "title": "Optical Flow Dataset and Benchmark for Visual Crowd Analysis"}
{"id": "YFCC100M", "contents": "YFCC100M is a that dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014.\r\n\r\nSource: [YFCC100M: The New Data in Multimedia Research](https://arxiv.org/pdf/1503.01817v2.pdf)\r\nImage Source: [Thomee et al](https://arxiv.org/pdf/1503.01817v2.pdf)", "variants": ["YFCC100M"], "title": "YFCC100M: The New Data in Multimedia Research"}
{"id": "ComQA", "contents": "ComQA is a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology.\r\n\r\nSource: [ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters](/paper/comqa-a-community-sourced-dataset-for-complex)\r\nImage Source: [http://qa.mpi-inf.mpg.de/comqa/](http://qa.mpi-inf.mpg.de/comqa/)", "variants": ["ComQA"], "title": "ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters"}
{"id": "Kinteract", "contents": "Explicitly created for Human Computer Interaction (HCI).\r\n\r\nSource: [Fast Gesture Recognition with Multiple Stream Discrete HMMs on 3D Skeletons](/paper/fast-gesture-recognition-with-multiple-stream)", "variants": ["Kinteract"], "title": "Fast gesture recognition with Multiple Stream Discrete HMMs on 3D skeletons"}
{"id": "WMT 2018 News", "contents": "News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Chinese, Czech, Estonian, German, Finnish, Russian, Turkish) and additional 1500 sentences from each of the 7 languages translated to English. The sentences were selected from dozens of news websites and translated by professional translators.\r\n\r\nThe training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.\r\nSome training corpora were identical from WMT 2017 (Europarl, Common Crawl, SETIMES2, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (United Nations, CzEng v1.7, News Commentary v13, monolingual news data).\r\nAdditionally, the EU Press Release parallel corpus for German, Finnish and Estonian was added.\r\n\r\nSource: [https://www.statmt.org/wmt18/translation-task.html](https://www.statmt.org/wmt18/translation-task.html)", "variants": ["WMT 2018 News"], "title": "Findings of the 2018 Conference on Machine Translation (WMT18)"}
{"id": "Spaceship Dataset", "contents": "The Spaceship dataset is a dataset for evaluating agents’ ability to learn to solve a class of physics-based tasks. The tasks consist on a spaceship that has to reach a the mothership in 11 steps, in an environment where static planets exert gravitational forces on the spaceship, which induce complex non-linear dynamics on the motion over the 11 steps.\n\nSource: [https://arxiv.org/pdf/1705.02670.pdf](https://arxiv.org/pdf/1705.02670.pdf)", "variants": ["Spaceship Dataset"], "title": "Metacontrol for Adaptive Imagination-Based Optimization"}
{"id": "Human-Parts", "contents": "The **Human-Parts** dataset is a dataset for human body, face and hand detection with ~15k images. It contains ~106k different annotations, with multiple annotations per image.\n\nSource: [https://github.com/xiaojie1017/Human-Parts](https://github.com/xiaojie1017/Human-Parts)\nImage Source: [https://github.com/xiaojie1017/Human-Parts](https://github.com/xiaojie1017/Human-Parts)", "variants": ["Human-Parts"], "title": "Detector-in-Detector: Multi-Level Analysis for Human-Parts"}
{"id": "Worldtree", "contents": "Worldtree is a corpus of explanation graphs, explanatory role ratings, and associated tablestore. It contains explanation graphs for 1,680 questions, and 4,950 tablestore rows across 62 semi-structured tables are provided. This data is intended to be paired with the AI2 Mercury Licensed questions.\r\n\r\nSource: [Explanation Bank](http://cognitiveai.org/explanationbank/)\r\nImage Source: [http://cognitiveai.org/explanationbank/](http://cognitiveai.org/explanationbank/)", "variants": ["Worldtree"], "title": "WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-Hop Inference"}
{"id": "MPII Cooking 2 Dataset", "contents": "A dataset which provides detailed annotations for activity recognition.\r\n\r\nSource: [Recognizing Fine-Grained and Composite Activities using Hand-Centric Features and Script Data](/paper/recognizing-fine-grained-and-composite)", "variants": ["MPII Cooking 2 Dataset"], "title": "Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data"}
{"id": "MPIIGaze", "contents": "**MPIIGaze** is a dataset for appearance-based gaze estimation in the wild. It contains 213,659 images collected from 15 participants during natural everyday laptop use over more than three months. It has a large variability in appearance and illumination.\r\n\r\nSource: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild)\r\nImage Source: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild)", "variants": ["MPII Gaze", "MPIIGaze"], "title": "Appearance-Based Gaze Estimation in the Wild"}
{"id": "Vimeo90K", "contents": "The Vimeo-90K is a large-scale high-quality video dataset for lower-level video processing. It proposes three different video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.\r\n\r\nSource: [https://arxiv.org/pdf/1711.09078.pdf](https://arxiv.org/pdf/1711.09078.pdf)\r\nImage Source: [http://toflow.csail.mit.edu/](http://toflow.csail.mit.edu/)", "variants": ["Vimeo90k", "Vimeo90K"], "title": "Video Enhancement with Task-Oriented Flow"}
{"id": "MOROCO", "contents": "The MOldavian and ROmanian Dialectal COrpus (MOROCO) is a corpus that contains 33,564 samples of text (with over 10 million tokens) collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports and tech. The data set is divided into 21,719 samples for training, 5,921 samples for validation and another 5,924 samples for testing. \r\n\r\nSource: [MOROCO: The Moldavian and Romanian Dialectal Corpus](/paper/moroco-the-moldavian-and-romanian-dialectal)", "variants": ["MOROCO"], "title": "MOROCO: The Moldavian and Romanian Dialectal Corpus"}
{"id": "ETH SfM", "contents": "The **ETH SfM** (structure-from-motion) dataset is a dataset for 3D Reconstruction. The benchmark investigates how different methods perform in terms of building a 3D model from a set of available 2D images.\n\nSource: [SOSNet: Second Order Similarity Regularization forLocal Descriptor Learning](https://arxiv.org/abs/1904.05019)\nImage Source: [https://cvg.ethz.ch/research/symmetries-in-sfm/](https://cvg.ethz.ch/research/symmetries-in-sfm/)", "variants": ["ETH SfM"], "title": "Comparative Evaluation of Hand-Crafted and Learned Local Features"}
{"id": "MOT17", "contents": "The **Multiple Object Tracking 17** (**MOT17**) dataset is a dataset for multiple object tracking. Similar to its previous version MOT16, this challenge contains seven different indoor and outdoor scenes of public places with pedestrians as the objects of interest. A video for each scene is divided into two clips, one for training and the other for testing. The dataset provides detections of objects in the video frames with three detectors, namely SDP, Faster-RCNN and DPM. The challenge accepts both on-line and off-line tracking approaches, where the latter are allowed to use the future video frames to predict tracks.\r\n\r\nSource: [Deep Affinity Network for Multiple Object Tracking](https://arxiv.org/abs/1810.11780)\r\nImage Source: [https://www.researchgate.net/figure/Visualization-of-selected-sequences-from-the-MOT17-benchmark-dataset_fig4_337133502](https://www.researchgate.net/figure/Visualization-of-selected-sequences-from-the-MOT17-benchmark-dataset_fig4_337133502)", "variants": ["MOT17"], "title": "MOT16: A Benchmark for Multi-Object Tracking"}
{"id": "Aff-Wild", "contents": "Aff-Wild is a dataset for emotion recognition from facial images in a variety of head poses, illumination conditions and occlusions.\r\n\r\nSource: [Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond](/paper/deep-affect-prediction-in-the-wild-aff-wild)", "variants": ["Aff-Wild"], "title": "Deep Affect Prediction in-the-Wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond"}
{"id": "WikiText-TL-39", "contents": "WikiText-TL-39 is a benchmark language modeling dataset in Filipino that has 39 million tokens in the training set.\r\n\r\nSource: [Evaluating Language Model Finetuning Techniques for Low-resource Languages](/paper/evaluating-language-model-finetuning)", "variants": ["WikiText-TL-39"], "title": "Evaluating Language Model Finetuning Techniques for Low-resource Languages"}
{"id": "MUTAG", "contents": "In particular, **MUTAG** is a collection of nitroaromatic compounds and the goal is to predict their mutagenicity on Salmonella typhimurium. Input graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms. It includes 188 samples of chemical compounds with 7 discrete node labels.\r\n\r\nSource: [Fast and Deep Graph Neural Networks](https://arxiv.org/abs/1911.08941)", "variants": ["MUTAG"], "title": "Natural Questions: A Benchmark for Question Answering Research"}
{"id": "Talk2Car", "contents": "The **Talk2Car** dataset finds itself at the intersection of various research domains, promoting the development of cross-disciplinary solutions for improving the state-of-the-art in grounding natural language into visual space. The annotations were gathered with the following aspects in mind:\r\nFree-form high quality natural language commands, that stimulate the development of solutions that can operate in the wild.\r\nA realistic task setting. Specifically, the authors consider an autonomous driving setting, where a passenger can control the actions of an Autonomous Vehicle by giving commands in natural language.\r\nThe Talk2Car dataset was build on top of the nuScenes dataset to include an extensive suite of sensor modalities, i.e. semantic maps, GPS, LIDAR, RADAR and 360-degree RGB images annotated with 3D bounding boxes. Such variety of input modalities sets the object referral task on the Talk2Car dataset apart from related challenges, where additional sensor modalities are generally missing.\r\n\r\nSource: [https://talk2car.github.io/](https://talk2car.github.io/)\nImage Source: [https://github.com/talk2car/Talk2Car](https://github.com/talk2car/Talk2Car)", "variants": ["Talk2Car"], "title": "Talk2Car: Taking Control of Your Self-Driving Car"}
{"id": "GameWikiSum", "contents": "**GameWikiSum** is a domain-specific (video game) dataset for multi-document summarization, which is one hundred times larger than commonly used datasets, and in another domain than news. Input documents consist of long professional video game reviews as well as references of their gameplay sections in Wikipedia pages.\n\nSource: [https://github.com/Diego999/GameWikiSum](https://github.com/Diego999/GameWikiSum)", "variants": ["GameWikiSum"], "title": "GameWikiSum: a Novel Large Multi-Document Summarization Dataset"}
{"id": "DeepWeeds", "contents": "The DeepWeeds dataset consists of 17,509 images capturing eight different weed species native to Australia in situ with neighbouring flora.\r\n\r\nSource: [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)\r\nImage Source: [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)", "variants": ["DeepWeeds"], "title": "DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning"}
{"id": "SYNTHIA-AL", "contents": "Specially designed to evaluate active learning for video object detection in road scenes. \r\n\r\nSource: [Temporal Coherence for Active Learning in Videos](/paper/temporal-coherence-for-active-learning-in)", "variants": ["SYNTHIA-AL"], "title": "Temporal Coherence for Active Learning in Videos"}
{"id": "PGM", "contents": "PGM dataset serves as a tool for studying both abstract reasoning and generalisation in models. Generalisation is a multi-faceted phenomenon; there is no single, objective way in which models can or should generalise beyond their experience. The PGM dataset provides a means to measure the generalization ability of models in different ways, each of which may be more or less interesting to researchers depending on their intended training setup and applications.\r\n\r\nSource: [Measuring abstract reasoning in neural networks](https://arxiv.org/pdf/1807.04225v1.pdf)\r\nImage Source: [Barrett et al](https://arxiv.org/abs/1807.04225)", "variants": ["PGM"], "title": "Measuring abstract reasoning in neural networks"}
{"id": "BREAK", "contents": "Break is a question understanding dataset, aimed at training models to reason over complex questions. It features 83,978 natural language questions, annotated with a new meaning representation, Question Decomposition Meaning Representation (QDMR). Each example has the natural question along with its QDMR representation. Break contains human composed questions, sampled from 10 leading question-answering benchmarks over text, images and databases. This dataset was created by a team of NLP researchers at Tel Aviv University and Allen Institute for AI.\r\n\r\nSource: [BREAK](https://allenai.github.io/Break/)", "variants": ["BREAK"], "title": "Break It Down: A Question Understanding Benchmark"}
{"id": "FPDS", "contents": "A benchmark for detecting fallen people lying on the floor. It consists of 6982 images, with a total of 5023 falls and 2275 non falls corresponding to people in conventional situations (standing up, sitting, lying on the sofa or bed, walking, etc). Almost all the images have been captured in indoor environments with very different situations: variation of poses and sizes, occlusions, lighting changes, etc.\r\n\r\nSource: [FPDS](http://agamenon.tsc.uah.es/Investigacion/gram/papers/fall_detection/)", "variants": ["FPDS"], "title": "Fast and Robust Detection of Fallen People from a Mobile Robot"}
{"id": "KP20k", "contents": "**KP20k** is a large-scale scholarly articles dataset with 528K articles for training, 20K articles for validation and 20K articles for testing.\r\n\r\nSource: [Keyphrase Prediction With Pre-trained Language Model](https://arxiv.org/abs/2004.10462)\r\nImage Source: [https://arxiv.org/pdf/1704.06879.pdf](https://arxiv.org/pdf/1704.06879.pdf)", "variants": ["KP20k"], "title": "Deep Keyphrase Generation"}
{"id": "StreetStyle", "contents": "StreetStyle is a large-scale dataset of photos of people annotated with clothing attributes, and use this dataset to train attribute classifiers via deep learning.\r\n\r\nSource: [StreetStyle: Exploring world-wide clothing styles from millions of photos](https://arxiv.org/pdf/1706.01869)\r\nImage Source: [Matzen et al](https://arxiv.org/pdf/1706.01869.pdf)", "variants": ["StreetStyle"], "title": "StreetStyle: Exploring world-wide clothing styles from millions of photos"}
{"id": "HRA", "contents": "A verified-by-experts repository of 3050 human rights violations photographs, labelled with human rights semantic categories, comprising a list of the types of human rights abuses encountered at present. \r\n\r\nSource: [Exploring object-centric and scene-centric CNN features and their complementarity for human rights violations recognition in images](/paper/exploring-object-centric-and-scene-centric)", "variants": ["HRA"], "title": "Exploring Object-Centric and Scene-Centric CNN Features and Their Complementarity for Human Rights Violations Recognition in Images"}
{"id": "JFLEG", "contents": "JFLEG is for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. \r\n\r\nSource: [JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction](https://arxiv.org/pdf/1702.04066v1.pdf)", "variants": ["JFLEG", "Restricted", "Unrestricted", "_Restricted_"], "title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction"}
{"id": "SNLI-VE", "contents": "Visual Entailment (VE) consists of image-sentence pairs whereby a premise is defined by an image, rather than a natural language sentence as in traditional Textual Entailment tasks. The goal of a trained VE model is to predict whether the image semantically entails the text. **SNLI-VE** is a dataset for VE which is based on the Stanford Natural Language Inference corpus and Flickr30k dataset.\n\nSource: [https://github.com/necla-ml/SNLI-VE](https://github.com/necla-ml/SNLI-VE)", "variants": ["SNLI-VE val", "SNLI-VE test", "SNLI-VE"], "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding"}
{"id": "PG-19", "contents": "A new open-vocabulary language modelling benchmark derived from books.\r\n\r\nSource: [Compressive Transformers for Long-Range Sequence Modelling](/paper/compressive-transformers-for-long-range-1)", "variants": ["PG-19"], "title": "Compressive Transformers for Long-Range Sequence Modelling"}
{"id": "People Snapshot Dataset", "contents": "Enables detailed human body model reconstruction in clothing from a single monocular RGB video without requiring a pre scanned template or manually clicked points.\r\n\r\nSource: [Video Based Reconstruction of 3D People Models](/paper/video-based-reconstruction-of-3d-people)", "variants": ["People Snapshot Dataset"], "title": "Video Based Reconstruction of 3D People Models"}
{"id": "VOT2014", "contents": "The dataset comprises 25 short sequences showing various objects in challenging backgrounds. Eight sequences are from the VOT2013 challenge (bolt, bicycle, david, diving, gymnastics, hand, sunshade, woman). The new sequences show complementary objects and backgrounds, for example a fish underwater or a surfer riding a big wave. The sequences were chosen from a large pool of sequences using a methodology based on clustering visual features of object and background so that those 25 sequences sample evenly well the existing pool.\r\n\r\nSource: [VOT2014](https://www.votchallenge.net/vot2014/dataset.html)", "variants": ["VOT2014"], "title": "A Novel Performance Evaluation Methodology for Single-Target Trackers"}
{"id": "AVSD", "contents": "The Audio Visual Scene-Aware Dialog (**AVSD**) dataset, or DSTC7 Track 3, is a audio-visual dataset for dialogue understanding. The goal with the dataset and track was to design systems to generate responses in a dialog about a video, given the dialog history and audio-visual content of the video.\n\nSource: [The Eighth Dialog System Technology Challenge](https://arxiv.org/abs/1911.06394)\nImage Source: [http://workshop.colips.org/dstc7/papers/DSTC7_Task_3_overview_paper.pdf](http://workshop.colips.org/dstc7/papers/DSTC7_Task_3_overview_paper.pdf)", "variants": ["AVSD"], "title": "Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7"}
{"id": "COMA", "contents": "CoMA contains 17,794 meshes of the human face in various expressions\r\n\r\nSource: [DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects](https://arxiv.org/abs/1905.10290)\r\nImage Source: [https://coma.is.tue.mpg.de/](https://coma.is.tue.mpg.de/)", "variants": ["COMA"], "title": "Generating 3D faces using Convolutional Mesh Autoencoders"}
{"id": "QA-SRL", "contents": "**QA-SRL** was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”). The authors collected about 19,000 question-answer pairs from 3,200 sentences.\r\n\r\nSource: [Zero-Shot Relation Extraction via Reading Comprehension](https://arxiv.org/abs/1706.04115)\r\nImage Source: [http://browse.qasrl.org/](http://browse.qasrl.org/)", "variants": ["QA-SRL"], "title": "Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language"}
{"id": "Chinese Literature NER RE", "contents": "Chinese Literature NER RE is a Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text. It is constructed from hundreds of Chinese literature articles.\r\n\r\nSource: [https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset](https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset)\r\nImage Source: [https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset](https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset)", "variants": ["Chinese Literature NER RE"], "title": "A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text"}
{"id": "FaceWarehouse", "contents": "**FaceWarehouse** is a 3D facial expression database that provides the facial geometry of 150 subjects, covering a wide range of ages and ethnic backgrounds.\r\n\r\nSource: [3D Face Reconstruction with Geometry Details from a Single Image](https://arxiv.org/abs/1702.05619)", "variants": ["FaceWarehouse"], "title": "FaceWarehouse: A 3D Facial Expression Database for Visual Computing"}
{"id": "Atari Grand Challenge", "contents": "The **Atari Grand Challenge** dataset is a large dataset of human Atari 2600 replays. It consists of replays for 5 different games:\r\n* Space Invaders (445 episodes, 2M frames)\r\n* Q*bert (659 episodes, 1.6M frames)\r\n* Ms.Pacman (384 episodes, 1.7M frames)\r\n* Video Pinball (211 episodes, 1.5M frames)\r\n* Montezuma’s revenge (668 episodes, 2.7M frames)\r\n\r\nSource: [https://arxiv.org/pdf/1705.10998.pdf](https://arxiv.org/pdf/1705.10998.pdf)\r\nImage Source: [https://arxiv.org/pdf/1705.10998.pdf](https://arxiv.org/pdf/1705.10998.pdf)", "variants": ["Atari Grand Challenge"], "title": "The Atari Grand Challenge Dataset"}
{"id": "ARVSU", "contents": "ARVSU contains a vast body of image variations in visual scenes with an annotated utterance and a corresponding addressee for each scenario.\r\n\r\nSource: [Deep Learning Based Multi-modal Addressee Recognition in Visual Scenes with Utterances](https://arxiv.org/pdf/1809.04288)", "variants": ["ARVSU"], "title": "Deep Learning Based Multi-modal Addressee Recognition in Visual Scenes with Utterances"}
{"id": "SmokEng", "contents": "SmokEng is a dataset of 3144 tweets, which are selected based on the presence of colloquial slang related to smoking and analyze it based on the semantics of the tweet. \r\n\r\nSource: [SmokEng: Towards Fine-grained Classification of Tobacco-related Social Media Text](/paper/smokeng-towards-fine-grained-classification)", "variants": ["SmokEng"], "title": "SmokEng: Towards Fine-grained Classification of Tobacco-related Social Media Text"}
{"id": "MIT Traffic", "contents": "**MIT Traffic** is a dataset for research on activity analysis and crowded scenes. It includes a traffic video sequence of 90 minutes long. It is recorded by a stationary camera. The size of the scene is 720 by 480 and it is divided into 20 clips.\r\n\r\nSource: [MIT Traffic Dataset](http://mmlab.ie.cuhk.edu.hk/datasets/mit_traffic/index.html)", "variants": ["MIT Traffic"], "title": "Learning the Change for Automatic Image Cropping"}
{"id": "Kuzushiji-Kanji", "contents": "Kuzushiji-Kanji is an imbalanced dataset of total 3832 Kanji characters (64x64 grayscale, 140,426 images), ranging from 1,766 examples to only a single example per class. Kuzushiji is a Japanese cursive writing style.\r\n\r\nSource: [Deep Learning for Classical Japanese Literature](/paper/deep-learning-for-classical-japanese)\r\nImage Source: [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist)", "variants": ["Kuzushiji-Kanji"], "title": "Deep Learning for Classical Japanese Literature"}
{"id": "Aqualoc", "contents": "A new underwater dataset that has been recorded in an harbor and provides several sequences with synchronized measurements from a monocular camera, a MEMS-IMU and a pressure sensor.\r\n\r\nSource: [The Aqualoc Dataset: Towards Real-Time Underwater Localization from a Visual-Inertial-Pressure Acquisition System](/paper/the-aqualoc-dataset-towards-real-time)", "variants": ["Aqualoc"], "title": "The Aqualoc Dataset: Towards Real-Time Underwater Localization from a Visual-Inertial-Pressure Acquisition System"}
{"id": "EXPLICIT 3D CHANGE DETECTION USING RAY-TRACING IN SPHERICAL COORDINATES", "contents": "Real and simulated lidar data of indoor and outdoor scenes, before and after geometric scene changes have occurred. Data include lidar scans from multiple viewpoints with provided coordinate transforms, and manually annotated ground-truth regarding which parts of the scene have changed between subsequent scans.", "variants": ["EXPLICIT 3D CHANGE DETECTION USING RAY-TRACING IN SPHERICAL COORDINATES"], "title": "Explicit 3D change detection using ray-tracing in spherical coordinates"}
{"id": "Cluttered Omniglot", "contents": "Dataset for one-shot segmentation.\r\n\r\nSource: [One-Shot Segmentation in Clutter](/paper/one-shot-segmentation-in-clutter)", "variants": ["Cluttered Omniglot"], "title": "One-Shot Segmentation in Clutter"}
{"id": "Mall", "contents": "The **Mall** is a dataset for crowd counting and profiling research. Its images are collected from publicly accessible webcam. It mainly includes 2,000 video frames, and the head position of every pedestrian in all frames is annotated. A total of more than 60,000 pedestrians are annotated in this dataset.\r\n\r\nSource: [Drone Based RGBT Vehicle Detection and Counting: A Challenge](https://arxiv.org/abs/2003.02437)\r\nImage Source: [http://www.bmva.org/bmvc/2012/BMVC/paper021/paper021.pdf](http://www.bmva.org/bmvc/2012/BMVC/paper021/paper021.pdf)", "variants": ["Mall"], "title": "Feature mining for localised crowd counting"}
{"id": "PGR", "contents": "Phenotype-Gene Relations (PGR) is a corpus that consists of 1712 abstracts, 5676 human phenotype annotations, 13835 gene annotations, and 4283 relations. \r\n\r\nSource: [A Silver Standard Corpus of Human Phenotype-Gene Relations](/paper/a-silver-standard-corpus-of-human-phenotype)", "variants": ["PGR"], "title": "A Silver Standard Corpus of Human Phenotype-Gene Relations"}
{"id": "Freiburg Groceries", "contents": "**Freiburg Groceries** is a groceries classification dataset consisting of 5000 images of size 256x256, divided into 25 categories. It has imbalanced class sizes ranging from 97 to 370 images per class. Images were taken in various aspect ratios and padded to squares.\n\nSource: [XNAS: Neural Architecture Search with Expert Advice](https://arxiv.org/abs/1906.08031)\nImage Source: [http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/](http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/)", "variants": ["Freiburg Groceries"], "title": "The Freiburg Groceries Dataset"}
{"id": "AFHQ", "contents": "Animal FacesHQ (AFHQ) is a dataset of animal faces consisting of 15,000 high-quality images at 512 × 512 resolution. The dataset includes three domains of cat, dog, and wildlife, each providing 5000 images. By having multiple (three) domains and diverse images of various\r\nbreeds (≥ eight) per each domain, AFHQ sets a more challenging image-to-image translation problem. \r\nAll images are vertically and horizontally aligned to have the eyes at the center. The low-quality images were discarded by human effort.\r\n\r\nSource: [StarGAN v2: Diverse Image Synthesis for Multiple Domains](https://arxiv.org/abs/1912.01865)", "variants": ["AFHQ"], "title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains"}
{"id": "Ciona17", "contents": "Ciona17 is a semantic segmentation dataset with pixel-level annotations pertaining to invasive species in a marine environment. Diverse outdoor illumination, a range of object shapes, colour, and severe occlusion provide a significant real world challenge for the computer vision community. \r\n\r\nSource: [The Ciona17 Dataset for Semantic Segmentation of Invasive Species in a Marine Aquaculture Environment](https://arxiv.org/pdf/1702.05564)\r\nImage Source: [Galloway et al](https://arxiv.org/pdf/1702.05564)", "variants": ["Ciona17"], "title": "The Ciona17 Dataset for Semantic Segmentation of Invasive Species in a Marine Aquaculture Environment"}
{"id": "Senseval-2", "contents": "There are now many computer programs for automatically determining the sense of a word in context (Word Sense Disambiguation or WSD).  The purpose of SENSEVAL is to evaluate the strengths and weaknesses of such programs with respect to different words, different varieties of language, and different languages.\r\n\r\nSource: [SENSEVAL](http://www.hipposmond.com/senseval2/)\r\nImage Source: [http://www.itri.brighton.ac.uk/events/senseval/](http://www.itri.brighton.ac.uk/events/senseval/)", "variants": ["Senseval-2"], "title": "EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models"}
{"id": "Pars-ABSA", "contents": "Pars-ABSA is a manually annotated Persian dataset, Pars-ABSA, which is verified by 3 native Persian speakers. The dataset consists of 5,114 positive, 3,061 negative and 1,827 neutral data samples from 5,602 unique reviews.\r\n\r\nSource: [Pars-ABSA: an Aspect-based Sentiment Analysis dataset for Persian](/paper/pars-absa-an-aspect-based-sentiment-analysis)", "variants": ["Pars-ABSA"], "title": "Pars-ABSA: an Aspect-based Sentiment Analysis dataset for Persian"}
{"id": "OTEANNv3", "contents": "This dataset contains orthographic samples of words in 19 languages (ar, br, de, en, eno, ent, eo, es, fi, fr, fro, it, ko, nl, pt, ru, sh, tr, zh). Each sample contains two text features: a Word (the textual representation of the word according to its orthography) and a Pronunciation (the highest-surface IPA pronunciation of the word as pronunced in its language).", "variants": ["OTEANNv3"], "title": "OTEANN: Estimating the Transparency of Orthographies with an Artificial Neural Network"}
{"id": "G3D", "contents": "The Gaming 3D Dataset (**G3D**) focuses on real-time action recognition in a gaming scenario. It contains 10 subjects performing 20 gaming actions: “punch right”, “punch left”, “kick right”, “kick left”, “defend”, “golf swing”, “tennis swing forehand”, “tennis swing backhand”, “tennis serve”, “throw bowling ball”, “aim and fire gun”, “walk”, “run”, “jump”, “climb”, “crouch”, “steer a car”, “wave”, “flap” and “clap”.\r\n\r\nSource: [Skeleton Based Action Recognition Using Translation-Scale Invariant Image Mapping And Multi-Scale Deep CNN](https://arxiv.org/abs/1704.05645)\r\nImage Source: [G3D: A gaming action dataset and real time action recognition evaluation framework](https://doi.org/10.1109/CVPRW.2012.6239175)", "variants": ["Gaming 3D (G3D)", "G3D"], "title": "G3D: A gaming action dataset and real time action recognition evaluation framework"}
{"id": "A3D", "contents": "A new dataset of diverse traffic accidents.\r\n\r\nSource: [Unsupervised Traffic Accident Detection in First-Person Videos](/paper/unsupervised-traffic-accident-detection-in)", "variants": ["A3D"], "title": "Unsupervised Traffic Accident Detection in First-Person Videos"}
{"id": "WISDOM", "contents": "Synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. \r\n\r\nSource: [Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data](/paper/segmenting-unknown-3d-objects-from-real-depth)", "variants": ["WISDOM"], "title": "Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Point Clouds"}
{"id": "Dunhuang Grottoes Painting Dataset", "contents": "This dataset provides a large number of training and testing example which is sufficient for a deep learning approach to address Dunhuang Grotto Painting restoration.\r\n\r\nSource: [Dunhuang Grottoes Painting Dataset and Benchmark](/paper/dunhuang-grotto-painting-dataset-and)", "variants": ["Dunhuang Grottoes Painting Dataset"], "title": "Dunhuang Grottoes Painting Dataset and Benchmark"}
{"id": "MIDV-2019", "contents": "Contains video clips shot with modern high-resolution mobile cameras, with strong projective distortions and with low lighting conditions.\r\n\r\nSource: [MIDV-2019: Challenges of the modern mobile-based document OCR](/paper/midv-2019-challenges-of-the-modern-mobile)", "variants": ["MIDV-2019"], "title": "MIDV-2019: Challenges of the modern mobile-based document OCR"}
{"id": "SMM4H", "contents": "Social Media Mining for Health (SMM4H) Shared Task is a massive data source for biomedical and public health applications.\r\n\r\nSource: [SMM4H](https://data.mendeley.com/datasets/rxwfb3tysd/2)", "variants": ["SMM4H"], "title": "Overview of the Fourth Social Media Mining for Health (SMM4H) Shared Tasks at ACL 2019"}
{"id": "OST300", "contents": "**OST300** is an outdoor scene dataset with 300 test images of outdoor scenes, and a training set of 7 categories of images with rich textures.\r\n\r\nSource: [Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform](https://paperswithcode.com/paper/recovering-realistic-texture-in-image-super)\r\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/SFTGAN/](http://mmlab.ie.cuhk.edu.hk/projects/SFTGAN/)", "variants": ["OST300"], "title": "Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform"}
{"id": "DBP15K", "contents": "DBP15k contains four language-specific KGs that are respectively extracted from English (En), Chinese (Zh), French (Fr) and Japanese (Ja) DBpedia, each of which contains around 65k-106k entities. Three sets of 15k alignment labels are constructed to align entities between each of the other three languages and En.\r\n\r\nSource: [Cross-lingual Entity Alignment for Knowledge Graphs with Incidental Supervision from Free Text](https://arxiv.org/abs/2005.00171)", "variants": ["DBP15k zh-en", "dbp15k fr-en", "dbp15k ja-en", "DBP15K"], "title": "Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding"}
{"id": "GolfDB", "contents": "GolfDB is a high-quality video dataset created for general recognition applications in the sport of golf, and specifically for the task of golf swing sequencing.\r\n\r\nSource: [GolfDB: A Video Database for Golf Swing Sequencing](/paper/golfdb-a-video-database-for-golf-swing)", "variants": ["GolfDB"], "title": "GolfDB: A Video Database for Golf Swing Sequencing"}
{"id": "TLL", "contents": "Contains 6016 image-pairs from the wild, shedding light upon a rich and diverse set of criteria employed by human beings.\r\n\r\nSource: [Totally Looks Like - How Humans Compare, Compared to Machines](/paper/totally-looks-like-how-humans-compare)", "variants": ["TLL"], "title": "Totally Looks Like - How Humans Compare, Compared to Machines"}
{"id": "MTL-AQA", "contents": "A new multitask-AQA dataset, the largest to date, comprising of 1412 diving samples.\r\n\r\nSource: [What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment](/paper/what-and-how-well-you-performed-a-multitask)", "variants": ["MTL-AQA"], "title": "What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment"}
{"id": "Electro-Magnetic Emanations Interception Dataset", "contents": "An open data corpus of 123.610 labeled samples, \r\n\r\nSource: [Electro-Magnetic Side-Channel Attack Through Learned Denoising and Classification](/paper/electro-magnetic-side-channel-attack-through)", "variants": ["Electro-Magnetic Emanations Interception Dataset"], "title": "Electro-Magnetic Side-Channel Attack Through Learned Denoising and Classification"}
{"id": "MVB", "contents": "MVB (Multi View Baggage) is a dataset for baggage ReID task which has some essential differences from person ReID. The features of MVB are three-fold. First, MVB is the first publicly released large-scale dataset that contains 4519 baggage identities and 22660 annotated baggage images as well as its surface material labels. Second, all baggage images are captured by specially-designed multi-view camera system to handle pose variation and occlusion, in order to obtain the 3D information of baggage surface as complete as possible. Third, MVB has remarkable inter-class similarity and intra-class dissimilarity, considering the fact that baggage might have very similar appearance while the data is collected in two real airport environments, where imaging factors varies significantly from each other.\r\n\r\nSource: [MVB: A Large-Scale Dataset for Baggage Re-Identification and Merged Siamese Networks](https://arxiv.org/pdf/1907.11366)", "variants": ["MVB"], "title": "MVB: A Large-Scale Dataset for Baggage Re-Identification and Merged Siamese Networks"}
{"id": "Fake News Filipino Dataset", "contents": "Expertly-curated benchmark dataset for fake news detection in Filipino.\r\n\r\nSource: [Localization of Fake News Detection via Multitask Transfer Learning](/paper/localization-of-fake-news-detection-via)", "variants": ["Fake News Filipino Dataset"], "title": "Localization of Fake News Detection via Multitask Transfer Learning"}
{"id": "TriviaQA", "contents": "**TriviaQA** is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets.\r\n\r\nSource: [Episodic Memory Reader: Learning What to Rememberfor Question Answering from Streaming Data](https://arxiv.org/abs/1903.06164)\r\nImage Source: [Joshi et al](https://arxiv.org/pdf/1705.03551v2.pdf)", "variants": ["TriviaQA", "KILT: TriviaQA"], "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}
{"id": "CCMatrix", "contents": "CCMatrix uses ten snapshots of a curated common crawl corpus (Wenzek et al., 2019) totalling 32.7 billion unique sentences. \r\n\r\nSource: [CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB](/paper/ccmatrix-mining-billions-of-high-quality)", "variants": ["CCMatrix"], "title": "CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB"}
{"id": "Lesion Boundary Segmentation Dataset", "contents": "Lesion Boundary Segmentation Dataset is a dataset for lesion segmentation from the ISIC2018 challenge. The dataset contains skin lesions and their corresponding annotations.\r\n\r\nImage source :[]()", "variants": ["Lesion Boundary Segmentation Dataset"], "title": "Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the International Skin Imaging Collaboration (ISIC)"}
{"id": "WebVision", "contents": "The WebVision dataset is designed to facilitate the research on learning visual representation from noisy web data. It is a large scale web images dataset that contains more than 2.4 million of images crawled from the Flickr website and Google Images search. \r\n\r\nThe same 1,000 concepts as the ILSVRC 2012 dataset are used for querying images, such that a bunch of existing approaches can be directly investigated and compared to the models trained from the ILSVRC 2012 dataset, and also makes it possible to study the dataset bias issue in the large scale scenario. The textual information accompanied with those images (e.g., caption, user tags, or description) are also provided as additional meta information. A validation set contains 50,000 images (50 images per category) is provided to facilitate the algorithmic development.", "variants": ["WebVision-1000", "WebVision"], "title": "WebVision Database: Visual Learning and Understanding from Web Data"}
{"id": "LPW", "contents": "**Labeled Pedestrian in the Wild (LPW)** is a pedestrian detection dataset that contains 2,731 pedestrians in three different scenes where each annotated identity is captured by from 2 to 4 cameras. The LPW features a notable scale of 7,694 tracklets with over 590,000 images as well as the cleanliness of its tracklets. It distinguishes from existing datasets in three aspects: large scale with cleanliness, automatically detected bounding boxes and far more crowded scenes with greater age span. This dataset provides a more realistic and challenging benchmark, which facilitates the further exploration of more powerful algorithms.", "variants": ["LPW"], "title": "Region-based Quality Estimation Network for Large-scale Person Re-identification"}
{"id": "RVL-CDIP", "contents": "The **RVL-CDIP** dataset consists of scanned document images belonging to 16 classes such as letter, form, email, resume, memo, etc. The dataset has 320,000 training, 40,000 validation and 40,000 test images. The images are characterized by low quality, noise, and low resolution, typically 100 dpi.\r\n\r\nSource: [Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning](https://arxiv.org/abs/2009.14457)\r\nImage Source: [https://www.cs.cmu.edu/~aharley/rvl-cdip/](https://www.cs.cmu.edu/~aharley/rvl-cdip/)", "variants": ["RVL-CDIP"], "title": "Evaluation of deep convolutional nets for document image classification and retrieval"}
{"id": "Middlebury 2003", "contents": "**Middlebury 2003** is a stereo dataset for indoor scenes.\n\nSource: [https://vision.middlebury.edu/stereo/data/scenes2003/](https://vision.middlebury.edu/stereo/data/scenes2003/)\nImage Source: [https://vision.middlebury.edu/stereo/data/scenes2003/](https://vision.middlebury.edu/stereo/data/scenes2003/)", "variants": ["Middlebury 2003"], "title": "High-Accuracy Stereo Depth Maps Using Structured Light"}
{"id": "CQASUMM", "contents": "CQASUMM is a dataset for CQA (Community Question Answering) summarization, constructed from the 4.4 million Yahoo! Answers L6 dataset. The dataset contains ~300k annotated samples.\r\n\r\nSource: [CQASUMM](https://bitbucket.org/tanya14109/cqasumm/src/master/)", "variants": ["CQASUMM"], "title": "CQASUMM: Building References for Community Question Answering Summarization Corpora"}
{"id": "TuSimple Lane", "contents": "TuSimple Lane is an extension of the TuSimple dataset with 14,336 lane boundaries annotations. Each lane boundary in the dataset is annotated using 7 different classes such as “Single Dashed”, “Double Dashed” or “Single White Continuous”.\r\n\r\nSource: [https://github.com/fabvio/TuSimple-lane-classes](https://github.com/fabvio/TuSimple-lane-classes)\r\nImage Source: [Pizzati et al](https://arxiv.org/pdf/1907.01294v2.pdf)", "variants": ["TuSimple Lane"], "title": "Lane Detection and Classification using Cascaded CNNs"}
{"id": "COG", "contents": "A configurable visual question and answer dataset (COG) to parallel experiments in humans and animals. COG is much simpler than the general problem of video analysis, yet it addresses many of the problems relating to visual and logical reasoning and memory -- problems that remain challenging for modern deep learning architectures. \r\n\r\nSource: [A Dataset and Architecture for Visual Reasoning with a Working Memory](/paper/a-dataset-and-architecture-for-visual)", "variants": ["COG"], "title": "A Dataset and Architecture for Visual Reasoning with a Working Memory"}
{"id": "Spotify Podcast", "contents": "A set of approximately 100K podcast episodes comprised of raw audio files along with accompanying ASR transcripts. This represents over 47,000 hours of transcribed audio, and is an order of magnitude larger than previous speech-to-text corpora. \r\n\r\nSource: [The Spotify Podcast Dataset](/paper/the-spotify-podcasts-dataset)", "variants": ["Spotify Podcast"], "title": "The Spotify Podcasts Dataset"}
{"id": "360-SOD", "contents": "360-SOD contains 500 high-resolution equirectangular images.\r\n\r\nSource: [Distortion-adaptive Salient Object Detection in 360$^\\circ$ Omnidirectional Images](/paper/distortion-adaptive-salient-object-detection)\r\nImage Source: [http://cvteam.net/projects/JSTSP20_DDS/DDS.html](http://cvteam.net/projects/JSTSP20_DDS/DDS.html)", "variants": ["360-SOD"], "title": "Distortion-Adaptive Salient Object Detection in 360$^\\circ$ Omnidirectional Images"}
{"id": "APRICOT", "contents": "APRICOT is a collection of over 1,000 annotated photographs of printed adversarial patches in public locations. The patches target several object categories for three COCO-trained detection models, and the photos represent natural variation in position, distance, lighting conditions, and viewing angle. \r\n\r\nSource: [APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection](/paper/apricot-a-dataset-of-physical-adversarial)\r\nImage Source: [https://apricot.mitre.org/](https://apricot.mitre.org/)", "variants": ["APRICOT"], "title": "APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection"}
{"id": "COIN", "contents": "The **COIN** dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.\r\n\r\nSource: [COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis](/paper/coin-a-large-scale-dataset-for-comprehensive)", "variants": ["COIN"], "title": "COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis"}
{"id": "INTEL-TAU", "contents": "A new large dataset for illumination estimation. This dataset, called INTEL-TAU, contains 7022 images in total, which makes it the largest available high-resolution dataset for illumination estimation research. \r\n\r\nSource: [INTEL-TAU: A Color Constancy Dataset](/paper/intel-tau-a-color-constancy-dataset)", "variants": ["INTEL-TAU"], "title": "INTEL-TAU: A Color Constancy Dataset"}
{"id": "StaQC", "contents": "**StaQC** (Stack Overflow Question-Code pairs) is a large dataset of around 148K Python and 120K SQL domain question-code pairs, which are automatically mined from StackOverflow.\r\n\r\nSource: [https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset)\r\nImage Source: [https://arxiv.org/pdf/1803.09371v1.pdf](https://arxiv.org/pdf/1803.09371v1.pdf)", "variants": ["StaQC"], "title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow"}
{"id": "TrajNet", "contents": "The **TrajNet** Challenge represents a large multi-scenario forecasting benchmark. The challenge consists on  predicting 3161 human trajectories, observing for each trajectory 8 consecutive ground-truth values (3.2 seconds) i.e., t−7,t−6,…,t, in world plane coordinates (the so-called world plane Human-Human protocol) and forecasting the following 12 (4.8 seconds), i.e., t+1,…,t+12. The 8-12-value protocol is consistent with the most trajectory forecasting approaches, usually focused on the 5-dataset ETH-univ + ETH-hotel + UCY-zara01 + UCY-zara02 + UCY-univ. Trajnet extends substantially the 5-dataset scenario by diversifying the training data, thus stressing the flexibility and generalization one approach has to exhibit when it comes to unseen scenery/situations. In fact, TrajNet is a superset of diverse datasets that requires to train on four families of trajectories, namely 1) BIWI Hotel (orthogonal bird’s eye flight view, moving people), 2) Crowds UCY (3 datasets, tilted bird’s eye view, camera mounted on building or utility poles, moving people), 3) MOT PETS (multisensor, different human activities) and 4) Stanford Drone Dataset (8 scenes, high orthogonal bird’s eye flight view, different agents as people, cars etc. ), for a total of 11448 trajectories. Testing is requested on diverse partitions of BIWI Hotel, Crowds UCY, Stanford Drone Dataset, and is evaluated by a specific server (ground-truth testing data is unavailable for applicants).\r\n\r\nSource: [Transformer Networks for Trajectory Forecasting](https://arxiv.org/abs/2003.08111)\r\nImage Source: [http://trajnet.stanford.edu/](http://trajnet.stanford.edu/)", "variants": ["TrajNet++", "TrajNet"], "title": "An Evaluation of Trajectory Prediction Approaches and Notes on the TrajNet Benchmark"}
{"id": "CoDraw", "contents": "The Collaborative Drawing game (**CoDraw**) dataset contains ~10K dialogs consisting of ~138K messages exchanged between human players in the CoDraw game. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language.\n\nSource: [https://github.com/facebookresearch/CoDraw](https://github.com/facebookresearch/CoDraw)\nImage Source: [https://github.com/facebookresearch/CoDraw](https://github.com/facebookresearch/CoDraw)", "variants": ["CoDraw"], "title": "CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication"}
{"id": "ATRW", "contents": "Contains over 8,000 video clips from 92 Amur tigers, with bounding box, pose keypoint, and tiger identity annotations. \r\n\r\nSource: [ATRW: A Benchmark for Amur Tiger Re-identification in the Wild](/paper/amur-tiger-re-identification-in-the-wild)", "variants": ["ATRW"], "title": "Amur Tiger Re-identification in the Wild"}
{"id": "CoLA", "contents": "The **Corpus of Linguistic Acceptability** (**CoLA**) consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. The public version contains 9594 sentences belonging to training and development sets, and excludes 1063 sentences belonging to a held out test set.\r\n\r\nSource: [https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/)\r\nImage Source: [https://arxiv.org/pdf/1805.12471.pdf](https://arxiv.org/pdf/1805.12471.pdf)", "variants": ["CoLA", "CoLA Dev"], "title": "Neural Network Acceptability Judgments"}
{"id": "SPHERE-calorie", "contents": "The dataset contains both RGB and depth images, and the data from two accelerometers, together with ground truth calorie values from a calorimeter for calorie expenditure estimation in home environments. \r\n\r\nSource: [SPEHERE Calorie](https://research-information.bris.ac.uk/en/datasets/sphere-calorie)", "variants": ["SPHERE-calorie"], "title": "Calorie Counter: RGB-Depth Visual Estimation of Energy Expenditure at Home"}
{"id": "Quick, Draw! Dataset", "contents": "The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located.\r\n\r\nSource: [https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)\r\nImage Source: [https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)", "variants": ["Quick, Draw! Dataset"], "title": "A Neural Representation of Sketch Drawings"}
{"id": "Multi-Ego", "contents": "A new multi-view egocentric dataset, Multi-Ego. The dataset is recorded simultaneously by three cameras, covering a wide variety of real-life scenarios. The footage is annotated by multiple individuals under various summarization configurations, with a consensus analysis ensuring a reliable ground truth.\r\n\r\nSource: [Multi-Stream Dynamic Video Summarization](/paper/multi-view-egocentric-video-summarization)", "variants": ["Multi-Ego"], "title": "Multi-View Egocentric Video Summarization"}
{"id": "iSarcasm", "contents": "iSarcasm is a dataset of tweets, each labelled as either sarcastic or non_sarcastic. Each sarcastic tweet is further labelled for one of the following types of ironic speech:\r\n\r\n- sarcasm: tweets that contradict the state of affairs and are critical towards an addressee;\r\n- irony: tweets that contradict the state of affairs but are not obviously critical towards an addressee;\r\n- satire: tweets that appear to support an addressee, but contain underlying disagreement and mocking;\r\n- understatement: tweets that undermine the importance of the state of affairs they refer to;\r\n- overstatement: tweets that describe the state of affairs in obviously exaggerated terms;\r\n- rhetorical question: tweets that include a question whose invited inference (implicature) is obviously contradicting the state of affairs.\r\n\r\nFor each sarastic tweet, there's also:\r\n\r\n- an explanation, in English sentences, as to why it is sarcastic, and\r\n- a rephrase that conveys the same meaning non-sarcastically. Both have been provided by the author of the tweet.\r\n\r\niSarcasm contains 4,484 tweets, out of which 777 are labelled as sarcastic and 3,707 as non-sarcastic. You'll find two files, isarcasm_train.csv and isarcasm_test.csv, each containing 80% and 20% of the examples chosen at random, respectively. Each line in a file has the format tweet_id,sarcasm_label,sarcasm_type, where sarcasm_type are only defined for sarcastic tweets, as specified above.\r\n\r\nSource: [iSarcasm](https://github.com/silviu-oprea/iSarcasm)", "variants": ["iSarcasm"], "title": "iSarcasm: A Dataset of Intended Sarcasm"}
{"id": "SynthCity", "contents": "**SynthCity** is a 367.9M point synthetic full colour Mobile Laser Scanning point cloud. Every point is assigned a label from one of nine categories.\r\n\r\nSource: [SynthCity: A large scale synthetic point cloud](/paper/synthcity-a-large-scale-synthetic-point-cloud)\r\nImage Source: [http://www.synthcity.xyz/](http://www.synthcity.xyz/)", "variants": ["SynthCity"], "title": "SynthCity: A large scale synthetic point cloud"}
{"id": "Cata7", "contents": "Cata7 is the first cataract surgical instrument dataset for semantic segmentation. The dataset consists of seven videos while each video records a complete cataract surgery. All videos are from Beijing Tongren Hospital. Each video is split into a sequence of images, where resolution is 1920×1080 pixels. To reduce redundancy, the videos are downsampled from 30 fps to 1 fps. Also, images without surgical instruments are manually removed. Each image is labeled with precise edges and types of surgical instruments. This dataset contains 2,500 images, which are divided into training and test sets. The training set consists of five video sequences and test set consists of two video sequence.\r\n\r\nSource: [RAUNet: Residual Attention U-Net for Semantic Segmentation of Cataract Surgical Instruments](https://arxiv.org/pdf/1909.10360.pdf)\r\nImage Source: [Ni et al](https://arxiv.org/pdf/1909.10360.pdf)", "variants": ["Cata7"], "title": "RAUNet: Residual Attention U-Net for Semantic Segmentation of Cataract Surgical Instruments"}
{"id": "THCHS-30", "contents": "THCHS-30 is a free Chinese speech database\r\nTHCHS-30 that can be used to build a full-fledged\r\nChinese speech recognition system.\r\n\r\nSource: [THCHS-30 : A Free Chinese Speech Corpus](/paper/thchs-30-a-free-chinese-speech-corpus)", "variants": ["THCHS-30"], "title": "THCHS-30 : A Free Chinese Speech Corpus"}
{"id": "PIT", "contents": "Paraphrase and Semantic Similarity in Twitter (PIT) presents a constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs. \r\n\r\nSource: [SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)](https://www.aclweb.org/anthology/S15-2001.pdf)", "variants": ["PIT"], "title": "SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)"}
{"id": "Social Relation Dataset", "contents": "**Social Relation Dataset** is a dataset for social relation trait prediction from face images. Traits are based on the interpersonal circle proposed by Kiesler, where human relations are divided into 16 segments. Each segment has its opposite side in the circle, such as 'friendly and hostile'. The dataset contains 8,306 images chosen from the internet and movies. Each image is labelled with faces’ bounding boxes and their pairwise relations. \r\n\r\nSource: [Open MMLab](https://openmmlab.com/data.html)", "variants": ["Social Relation Dataset"], "title": "Learning Social Relation Traits from Face Images"}
{"id": "Object Discovery", "contents": "The **Object Discovery** dataset was collected by downloading images from Internet for airplane, car and horse. It is significantly larger and thus, diverse in terms of viewpoints, texture, color etc\r\n\r\nSource: [One shot Joint Colocalization & Cosegmentation](https://arxiv.org/abs/1705.06000)\r\nImage Source: [http://people.csail.mit.edu/mrub/ObjectDiscovery/](http://people.csail.mit.edu/mrub/ObjectDiscovery/)", "variants": ["Object Discovery"], "title": "Unsupervised Joint Object Discovery and Segmentation in Internet Images"}
{"id": "PressurePose", "contents": "A synthetic dataset with 206K pressure images with 3D human poses and shapes.\r\n\r\nSource: [Bodies at Rest: 3D Human Pose and Shape Estimation from a Pressure Image using Synthetic Data](/paper/bodies-at-rest-3d-human-pose-and-shape)", "variants": ["PressurePose"], "title": "Bodies at Rest: 3D Human Pose and Shape Estimation from a Pressure Image using Synthetic Data"}
{"id": "Holl-E", "contents": "Holl-E is a dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie.\r\n\r\nSource: [Towards Exploiting Background Knowledge for Building Conversation Systems](https://arxiv.org/pdf/1809.08205.pdf)", "variants": ["Holl-E"], "title": "Towards Exploiting Background Knowledge for Building Conversation Systems"}
{"id": "FMA", "contents": "The **Free Music Archive** (**FMA**) is a large-scale dataset for evaluating several tasks in Music Information Retrieval. It consists of 343 days of audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies.\r\n\r\nThere are four subsets defined by the authors:\r\n\r\n* Full: the complete dataset,\r\n* Large: the full dataset with audio limited to 30 seconds clips extracted from the middle of the tracks (or entire track if shorter than 30 seconds),\r\n* Medium: a selection of 25,000 30s clips having a single root genre,\r\n* Small: a balanced subset containing 8,000 30s clips with 1,000 clips per one of 8 root genres.\r\n\r\nThe official split into training, validation and test sets (80/10/10) uses stratified sampling to preserve the percentage of tracks per genre. Songs of the same artists are part of one set only.\r\n\r\nSource: [FMA: A Dataset For Music Analysis](https://arxiv.org/pdf/1612.01840.pdf)\r\nAudio Source: [https://github.com/mdeff/fma](https://github.com/mdeff/fma)", "variants": ["Free Music Archive", "FMA"], "title": "FMA: A Dataset For Music Analysis"}
{"id": "C&Z", "contents": "One of the first datasets (if not the first) to highlight the importance of bias and diversity in the community, which started a revolution afterwards. Introduced in 2014 as integral part of a thesis of Master of Science [1,2] at Carnegie Mellon and City University of Hong Kong. It was later expanded by adding synthetic images generated by a GAN architecture at ETH Zürich (in HDCGAN by Curtó et al. 2017). Being then not only the pioneer of talking about the importance of balanced datasets for learning and vision but also for being the first GAN augmented dataset of faces. \r\n\r\nThe original description goes as follows:\r\n\r\nA bias-free dataset, containing human faces from different ethnical groups in a wide variety of illumination conditions and image resolutions. C&Z is enhanced with HDCGAN synthetic images, thus being the first GAN augmented dataset of faces.\r\n\r\nDataset: [https://github.com/curto2/c](https://github.com/curto2/c)\r\n\r\nSupplement (with scripts to handle the labels): [https://github.com/curto2/graphics](https://github.com/curto2/graphics)\r\n\r\n[1] [https://www.curto.hk/c/decurto.pdf](https://www.curto.hk/c/decurto.pdf)\r\n\r\n[2] [https://www.zarza.hk/z/dezarza.pdf](https://www.zarza.hk/z/dezarza.pdf)", "variants": ["C&Z"], "title": "High-resolution Deep Convolutional Generative Adversarial Networks"}
{"id": "Ice Hockey News Dataset", "contents": "Ice Hockey News Dataset is a corpus of Finnish ice hockey news, edited to be suitable for training of end-to-end news generation methods, as well as demonstrate generation of text, which was judged by journalists to be relatively close to a viable product.\r\n\r\nSource: [Template-free Data-to-Text Generation of Finnish Sports News](/paper/template-free-data-to-text-generation-of)", "variants": ["Ice Hockey News Dataset"], "title": "Template-free Data-to-Text Generation of Finnish Sports News"}
{"id": "Live Comment Dataset", "contents": "The Live Comment Dataset is a large-scale dataset with 2,361 videos and 895,929 live comments that were written while the videos were streamed.\r\n\r\nSource: [LiveBot: Generating Live Video Comments Based on Visual and Textual Contexts](/paper/livebot-generating-live-video-comments-based)", "variants": ["Live Comment Dataset"], "title": "LiveBot: Generating Live Video Comments Based on Visual and Textual Contexts"}
{"id": "EyeQ", "contents": "Dataset with 28,792 retinal images from the EyePACS dataset, based on a three-level quality grading system (i.e., `Good', `Usable' and `Reject') for evaluating RIQA methods. \r\n\r\nSource: [Evaluation of Retinal Image Quality Assessment Networks in Different Color-spaces](/paper/evaluation-of-retinal-image-quality)", "variants": ["EyeQ"], "title": "Evaluation of Retinal Image Quality Assessment Networks in Different Color-spaces"}
{"id": "CASIA-HWDB", "contents": "**CASIA-HWDB** is a dataset for handwritten Chinese character recognition. It contains 300 files (240 in HWDB1.1 training set and 60 in HWDB1.1 test set). Each file contains about 3000 isolated gray-scale Chinese character images written by one writer, as well as their corresponding labels.\r\n\r\nSource: [Generating Handwritten Chinese Characters using CycleGAN](https://arxiv.org/abs/1801.08624)\r\nImage Source: [http://www.nlpr.ia.ac.cn/databases/handwriting/Touching_Characters_Databases.html](http://www.nlpr.ia.ac.cn/databases/handwriting/Touching_Characters_Databases.html)", "variants": ["CASIA-HWDB"], "title": "CASIA Online and Offline Chinese Handwriting Databases"}
{"id": "DuReader", "contents": "**DuReader** is a large-scale open-domain Chinese machine reading comprehension dataset. The dataset consists of 200K questions, 420K answers and 1M documents. The questions and documents are based on Baidu Search and Baidu Zhidao. The answers are manually generated. The dataset additionally provides question type annotations – each question was manually annotated as either Entity, Description or YesNo and one of Fact or Opinion.\r\n\r\nSource: [https://arxiv.org/pdf/1711.05073v4.pdf](https://arxiv.org/pdf/1711.05073v4.pdf)\r\nImage Source: [https://arxiv.org/pdf/1711.05073v4.pdf](https://arxiv.org/pdf/1711.05073v4.pdf)", "variants": ["DuReader"], "title": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications"}
{"id": "SegTHOR", "contents": "SegTHOR (Segmentation of THoracic Organs at Risk) is a dataset dedicated to the segmentation of organs at risk (OARs) in the thorax, i.e. the organs surrounding the tumour that must be preserved from irradiations during radiotherapy. In this dataset, the OARs are the heart, the trachea, the aorta and the esophagus, which have varying spatial and appearance characteristics. The dataset includes 60 3D CT scans, divided into a training set of 40 and a test set of 20 patients, where the OARs have been contoured manually by an experienced radiotherapist. \r\n\r\nSource: [SegTHOR: Segmentation of Thoracic Organs at Risk in CT images](/paper/segthor-segmentation-of-thoracic-organs-at)\r\nImage Source: [Lambert et al](https://paperswithcode.com/paper/segthor-segmentation-of-thoracic-organs-at)", "variants": ["SegTHOR"], "title": "SegTHOR: Segmentation of Thoracic Organs at Risk in CT images"}
{"id": "UCF-QNRF", "contents": "The **UCF-QNRF** dataset is a crowd counting dataset and it contains large diversity both in scenes, as well as in background types. It consists of 1535 images high-resolution images from Flickr, Web Search and Hajj footage. The number of people (i.e., the count) varies from 50 to 12,000 across images.\r\n\r\nSource: [Understanding the impact of mistakes on background regions in crowd counting](https://arxiv.org/abs/2003.13759)\r\nImage Source: [https://www.crcv.ucf.edu/data/ucf-qnrf/](https://www.crcv.ucf.edu/data/ucf-qnrf/)", "variants": ["UCF-QNRF"], "title": "Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds"}
{"id": "Interpretable STS", "contents": "A dataset of sentence pairs annotated following the formalization.\r\n\r\nSource: [Interpretable Semantic Textual Similarity: Finding and explaining differences between sentences](/paper/interpretable-semantic-textual-similarity)", "variants": ["Interpretable STS"], "title": "Why do you say they are similar ? Interpretable Semantic Textual Similarity"}
{"id": "MultiSense", "contents": "MultiSense is a dataset of 9,504 images annotated with an English verb and its translation in Spanish and German.\r\n\r\nSource: [Cross-lingual Visual Verb Sense Disambiguation](https://arxiv.org/pdf/1904.05092v2.pdf)", "variants": ["MultiSense"], "title": "Cross-lingual Visual Verb Sense Disambiguation"}
{"id": "ThirdToFirst", "contents": "Two datasets (synthetic and natural/real) containing simultaneously recorded egocentric and exocentric videos.\r\n\r\nSource: [From Third Person to First Person: Dataset and Baselines for Synthesis and Retrieval](/paper/from-third-person-to-first-person-dataset-and)", "variants": ["ThirdToFirst"], "title": "From Third Person to First Person: Dataset and Baselines for Synthesis and Retrieval"}
{"id": "MTNT", "contents": "The Machine Translation of Noisy Text (**MTNT**) dataset is a Machine Translation dataset that consists of noisy comments on Reddit and professionally sourced translation. The translation are between French, Japanese and French, with between 7k and 37k sentence per language pair.\r\n\r\nSource: [https://arxiv.org/abs/1809.00388](https://arxiv.org/abs/1809.00388)\r\nImage Source: [https://github.com/pmichel31415/mtnt](https://github.com/pmichel31415/mtnt)", "variants": ["MTNT"], "title": "MTNT: A Testbed for Machine Translation of Noisy Text"}
{"id": "RecipeQA", "contents": "RecipeQA is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge.\r\n\r\nSource: [RecipeQA](https://hucvl.github.io/recipeqa/)", "variants": ["RecipeQA"], "title": "RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes"}
{"id": "NCI1", "contents": "The **NCI1** dataset comes from the cheminformatics domain, where each input graph is used as representation of a chemical compound: each vertex stands for an atom of the molecule, and edges between vertices represent bonds between atoms. This dataset is relative to anti-cancer screens where the chemicals are assessed as positive or negative to cell lung cancer. Each vertex has an input label representing the corresponding atom type, encoded by a one-hot-encoding scheme into a vector of 0/1 elements.\r\n\r\nSource: [Ring Reservoir Neural Networks for Graphs](https://arxiv.org/abs/2005.05294)", "variants": ["NCI1", "NCI-123"], "title": "Comparison of descriptor spaces for chemical compound retrieval and classification"}
{"id": "Violin", "contents": "Video-and-Language Inference is the task of joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. The **Violin** dataset is a dataset for this task which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels.\n\nSource: [https://github.com/jimmy646/violin](https://github.com/jimmy646/violin)\nImage Source: [https://github.com/jimmy646/violin](https://github.com/jimmy646/violin)", "variants": ["Violin"], "title": "VIOLIN: A Large-Scale Dataset for Video-and-Language Inference"}
{"id": "DARPA", "contents": "Darpa is a dataset consisting of communications between source IPs and destination IPs. This dataset contains different attacks between IPs.\n\nSource: [dynnode2vec: Scalable Dynamic Network Embedding](https://arxiv.org/abs/1812.02356)\nImage Source: [https://archive.ll.mit.edu/ideval/files/1999_DARPA_EvaulationSumPlans.pdf](https://archive.ll.mit.edu/ideval/files/1999_DARPA_EvaulationSumPlans.pdf)", "variants": ["Darpa", "DARPA"], "title": "Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images"}
{"id": "SCAN", "contents": "SCAN is a dataset for grounded navigation which consists of a set of simple compositional navigation commands paired with the corresponding action sequences. \r\n\r\nSource: [Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks](https://arxiv.org/pdf/1711.00350v3.pdf)", "variants": ["SCAN"], "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"}
{"id": "SP-10K", "contents": "A large-scale evaluation set that provides human ratings for the plausibility of 10,000 SP pairs over five SP relations, covering 2,500 most frequent verbs, nouns, and adjectives in American English.\r\n\r\nSource: [SP-10K: A Large-scale Evaluation Set for Selectional Preference Acquisition](/paper/190602123)", "variants": ["SP-10K"], "title": "SP-10K: A Large-scale Evaluation Set for Selectional Preference Acquisition"}
{"id": "ShoeV2", "contents": "**ShoeV2** is a dataset of 2,000 photos and 6648 sketches of shoes. The dataset is designed for fine-grained sketch-based image retrieval.\r\n\r\nSource: [Sketch Me That Shoe](/paper/sketch-me-that-shoe)", "variants": ["ShoeV2"], "title": "Sketch Me That Shoe"}
{"id": "JHU-CROWD", "contents": "(JHU-CROWD) a crowd counting dataset that contains 4,250 images with 1.11 million annotations. This dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level.", "variants": ["JHU-CROWD"], "title": "Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and Benchmark Method"}
{"id": "MPI Sintel", "contents": "MPI (Max Planck Institute) Sintel is a dataset for optical flow evaluation that has 1064 synthesized stereo images and ground truth data for disparity. Sintel is derived from open-source 3D animated short film Sintel. The dataset has 23 different scenes. The stereo images are RGB while the disparity is grayscale. Both have resolution of 1024×436 pixels and 8-bit per channel.\r\n\r\nSource: [Fast Disparity Estimation using Dense Networks*](https://arxiv.org/abs/1805.07499)", "variants": ["MPI Sintel", "Sintel Clean unsupervised", "Sintel Final unsupervised", "Sintel-clean", "Sintel-final", "Sintel-final - unsupervised"], "title": "A naturalistic open source movie for optical flow evaluation"}
{"id": "WoodScape", "contents": "Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of its prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images.\r\n**WoodScape** is an extensive fisheye automotive dataset named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images.\r\n\r\nSource: [https://github.com/valeoai/WoodScape](https://github.com/valeoai/WoodScape)\r\nImage Source: [https://github.com/valeoai/WoodScape](https://github.com/valeoai/WoodScape)", "variants": ["WoodScape"], "title": "WoodScape: A Multi-Task, Multi-Camera Fisheye Dataset for Autonomous Driving"}
{"id": "KaWAT", "contents": "A new word analogy task dataset for Indonesian.\r\n\r\nSource: [KaWAT: A Word Analogy Task Dataset for Indonesian](/paper/kawat-a-word-analogy-task-dataset-for)", "variants": ["KaWAT"], "title": "KaWAT: A Word Analogy Task Dataset for Indonesian"}
{"id": "Open MIC", "contents": "Open MIC (Open Museum Identification Challenge) contains photos of exhibits captured in 10 distinct exhibition spaces of several museums which showcase paintings, timepieces, sculptures, glassware, relics, science exhibits, natural history pieces, ceramics, pottery, tools and indigenous crafts. The goal of Open MIC is to stimulate research in domain adaptation, egocentric recognition and few-shot learning by providing a testbed complementary to the famous Office 31.\r\n\r\nSource: [Museum Exhibit Identification Challenge for Domain Adaptation and Beyond](/paper/museum-exhibit-identification-challenge-for)", "variants": ["Open MIC"], "title": "Museum Exhibit Identification Challenge for Domain Adaptation and Beyond"}
{"id": "LabelMe", "contents": "**LabelMe** database is a large collection of images with ground truth labels for object detection and recognition. The annotations come from two different sources, including the LabelMe online annotation tool.\r\n\r\nSource: [LabelMe: A Database and Web-Based Tool for Image Annotation](https://people.csail.mit.edu/brussell/research/AIM-2005-025-new.pdf)\r\nImage Source: [Russell et al](https://people.csail.mit.edu/brussell/research/AIM-2005-025-new.pdf)", "variants": ["LabelMe"], "title": "CHANGE DETECTION IN REMOTE SENSING IMAGES USING CONDITIONAL ADVERSARIAL NETWORKS"}
{"id": "Twitter Death Hoaxes", "contents": "This is a dataset for detection fake death hoaxes. It consists of of death reports collected from Twitter between 1st January, 2012 and 31st December, 2014. It was collected by tracking the keyword 'RIP', and matching those tweets in which a name is mentioned next to RIP. Matching names were identified by using Wikidata as a database of names. \r\n\r\nThe dataset contains 4,007 death reports, of which 2,301 are real deaths, 1,092 are commemorations and 614 are fake deaths.\r\n\r\nSource: [Early Detection of Social Media Hoaxes at Scale](https://arxiv.org/abs/1801.07311)", "variants": ["Twitter Death Hoaxes"], "title": "Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media."}
{"id": "Incremental Dialog Dataset", "contents": "Simulates unanticipated user needs in the deployment stage.\r\n\r\nSource: [Incremental Learning from Scratch for Task-Oriented Dialogue Systems](/paper/incremental-learning-from-scratch-for-task)", "variants": ["Incremental Dialog Dataset"], "title": "Incremental Learning from Scratch for Task-Oriented Dialogue Systems"}
{"id": "CVL Traffic Signs Dataset", "contents": "A video dataset for recognising traffic signs hosted with the first IEEE Video and Image Processing (VIP) Cup within the IEEE Signal Processing Society.\r\n\r\nSource: [Challenging Environments for Traffic Sign Detection: Reliability Assessment under Inclement Conditions](/paper/challenging-environments-for-traffic-sign)", "variants": ["CVL Traffic Signs Dataset"], "title": "Challenging Environments for Traffic Sign Detection: Reliability Assessment under Inclement Conditions"}
{"id": "SketchyScene", "contents": "SketchyScene is a large-scale dataset of scene sketches to advance research on sketch understanding at both the object and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline, enabling users to efficiently generate large quantities of realistic and diverse scene sketches. SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance masks. The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing scene composition. \r\n\r\nSource: [SketchyScene: Richly-Annotated Scene Sketches](https://openaccess.thecvf.com/content_ECCV_2018/papers/Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper.pdf)\r\nImage Source: [Zoe et al]( SketchyScene: Richly-Annotated Scene Sketches)", "variants": ["SketchyScene"], "title": "SketchyScene: Richly-Annotated Scene Sketches"}
{"id": "VCR", "contents": "**Visual Commonsense Reasoning** (**VCR**) is a large-scale dataset for cognition-level visual understanding. Given a challenging question about an image, machines need to present two sub-tasks: answer correctly and provide a rationale justifying its answer. The VCR dataset contains over 212K (training), 26K (validation) and 25K (testing) questions, answers and rationales derived from 110K movie scenes.\r\n\r\nSource: [Visual Commonsense R-CNN](https://arxiv.org/abs/2002.12204)\r\nImage Source: [From Recognition to Cognition: Visual Commonsense Reasoning](https://paperswithcode.com/paper/from-recognition-to-cognition-visual/)", "variants": ["VCR (Q-A) dev", "VCR (Q-A) test", "VCR (Q-AR) dev", "VCR (Q-AR) test", "VCR (QA-R) dev", "VCR (QA-R) test", "VCR"], "title": "From Recognition to Cognition: Visual Commonsense Reasoning"}
{"id": "RMRC 2014", "contents": "The **RMRC 2014** indoor dataset is a dataset for indoor semantic segmentation. It employs the NYU Depth V2 and Sun3D datasets to define the training set. The test data consists of newly acquired images.\n\nSource: [https://cs.nyu.edu/~silberman/rmrc2014/indoor.php](https://cs.nyu.edu/~silberman/rmrc2014/indoor.php)\nImage Source: [https://cs.nyu.edu/~silberman/rmrc2014/indoor.php](https://cs.nyu.edu/~silberman/rmrc2014/indoor.php)", "variants": ["RMRC 2014"], "title": "Indoor Semantic Segmentation using depth information"}
{"id": "SNIPS", "contents": "The **SNIPS** Natural Language Understanding benchmark is a dataset of over 16,000 crowdsourced queries distributed among 7 user intents of various complexity:\r\n\r\n* SearchCreativeWork (e.g. Find me the I, Robot television show),\r\n* GetWeather (e.g. Is it windy in Boston, MA right now?),\r\n* BookRestaurant (e.g. I want to book a highly rated restaurant in Paris tomorrow night),\r\n* PlayMusic (e.g. Play the last track from Beyoncé off Spotify),\r\n* AddToPlaylist (e.g. Add Diamonds to my roadtrip playlist),\r\n* RateBook (e.g. Give 6 stars to Of Mice and Men),\r\n* SearchScreeningEvent (e.g. Check the showtimes for Wonder Woman in Paris).\r\nThe training set contains of 13,084 utterances, the validation set and the test set contain 700 utterances each, with 100 queries per intent.\r\n\r\nSource: [https://paperswithcode.com/paper/snips-voice-platform-an-embedded-spoken/](https://paperswithcode.com/paper/snips-voice-platform-an-embedded-spoken/)", "variants": ["SNIPS"], "title": "Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces"}
{"id": "FBMS", "contents": "The **Freiburg-Berkeley Motion Segmentation** Dataset (**FBMS**-59) is an extension of the BMS dataset with 33 additional video sequences. A total of 720 frames is annotated. It has pixel-accurate segmentation annotations of moving objects. FBMS-59 comes with a split into a training set and a test set.\r\n\r\nSource: [https://lmb.informatik.uni-freiburg.de/resources/datasets/](https://lmb.informatik.uni-freiburg.de/resources/datasets/)\r\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/](https://lmb.informatik.uni-freiburg.de/resources/datasets/)", "variants": ["FBMS-59", "FBMS"], "title": "Segmentation of Moving Objects by Long Term Video Analysis"}
{"id": "IMPPRES", "contents": "An IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of >25k semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types.\r\n\r\nSource: [Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition](/paper/are-natural-language-inference-models)", "variants": ["IMPPRES"], "title": "Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition"}
{"id": "VATEX", "contents": "**VATEX** is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context.\r\n\r\nSource: [https://arxiv.org/pdf/1904.03493.pdf](https://arxiv.org/pdf/1904.03493.pdf)\r\nImage Source: [https://arxiv.org/pdf/1904.03493.pdf](https://arxiv.org/pdf/1904.03493.pdf)", "variants": ["VATEX"], "title": "VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"}
{"id": "MMED", "contents": "Contains 25,165 textual news articles collected from hundreds of news media sites (e.g., Yahoo News, Google News, CNN News.) and 76,516 image posts shared on Flickr social media, which are annotated according to 412 real-world events. The dataset is collected to explore the problem of organizing heterogeneous data contributed by professionals and amateurs in different data domains, and the problem of transferring event knowledge obtained from one data domain to heterogeneous data domain, thus summarizing the data with different contributors.\r\n\r\nSource: [MMED: A Multi-domain and Multi-modality Event Dataset](/paper/mmed-a-multi-domain-and-multi-modality-event)", "variants": ["MMED"], "title": "MMED: A Multi-domain and Multi-modality Event Dataset"}
{"id": "Epinions", "contents": "The **Epinions** dataset is built form a who-trust-whom online social network of a general consumer review site Epinions.com. Members of the site can decide whether to ''trust'' each other. All the trust relationships interact and form the Web of Trust which is then combined with review ratings to determine which reviews are shown to the user.\nIt contains 75,879 nodes and 50,8837 edges.\n\nSource: [https://snap.stanford.edu/data/soc-Epinions1.html](https://snap.stanford.edu/data/soc-Epinions1.html)", "variants": ["Epinions"], "title": "Trust Management for the Semantic Web"}
{"id": "PWDB", "contents": "# Overview\r\n\r\nThis database of simulated arterial pulse waves is designed to be representative of a sample of pulse waves measured from healthy adults. It contains pulse waves for 4,374 virtual subjects, aged from 25-75 years old (in 10 year increments). The database contains a baseline set of pulse waves for each of the six age groups, created using cardiovascular properties (such as heart rate and arterial stiffness) which are representative of healthy subjects at each age group. It also contains 728 further virtual subjects at each age group, in which each of the cardiovascular properties are varied within normal ranges. This allows for extensive in silico analyses of haemodynamics and the performance of pulse wave analysis algorithms.\r\n\r\n# Data Description\r\n\r\nThe database contains the following [pulse waves](https://github.com/peterhcharlton/pwdb/wiki/pwdb_data.mat#datawaves), sampled at 500 Hz:\r\n\r\n- arterial flow velocity (U),\r\n- luminal area (A),\r\n- pressure (P), and\r\n- photoplethysmogram (PPG).\r\n\r\nThese pulse waves are provided at a range of [measurement sites](https://github.com/peterhcharlton/pwdb/wiki/pwdb_data.mat#datawaves), including:\r\n\r\n- aorta (ascending and descending)\r\n- carotid artery\r\n- brachial artery\r\n- radial artery\r\n- finger\r\n- femoral artery\r\n\r\nThe database also contains numerous [reference variables](https://github.com/peterhcharlton/pwdb/wiki/pwdb_data.mat#datahaemods), mostly relating to cardiovascular properties, such as:\r\n\r\n- heart rate\r\n- cardiac output\r\n- blood pressure\r\n- pulse wave velocity\r\n- age\r\n\r\nThe data are available in three formats: Matlab, CSV and WaveForm Database (WFDB) format. Further details of the formatting and contents of each file are available [here](https://github.com/peterhcharlton/pwdb/wiki/Using-the-Pulse-Wave-Database).\r\n\r\n# Accompanying Publication\r\n\r\nThe database is described in the following publication:\r\n\r\n[Charlton P.H., Mariscal Harana, J., Vennin, S., Li, Y., Chowienczyk, P. & Alastruey, J., **“Modelling arterial pulse waves in healthy ageing: a database for in silico evaluation of haemodynamics and pulse wave indices,”** AJP Hear. Circ. Physiol., 317(5), pp.H1062-H1085, 2019. https://doi.org/10.1152/ajpheart.00218.2019](https://doi.org/10.1152/ajpheart.00218.2019)\r\n\r\nPlease cite this publication when using the database.\r\n\r\n# Further Information\r\n\r\nFurther information on the Pulse Wave Database project can be found at [the project homepage](https://peterhcharlton.github.io/pwdb/). In particular, an accompanying [Wiki](https://github.com/peterhcharlton/pwdb/wiki) provides:\r\n\r\n- An introduction to the dataset [here](https://github.com/peterhcharlton/pwdb/wiki)\r\n- The methods used to create and analyse the dataset [here](https://github.com/peterhcharlton/pwdb/wiki/Reproducing-the-Pulse-Wave-Database)\r\n- An explanation of each of the variables in the dataset [here](https://github.com/peterhcharlton/pwdb/wiki)\r\n- Case studies of analyses conducted on the dataset in Matlab [here](https://github.com/peterhcharlton/pwdb/wiki/Case-Studies)", "variants": ["PWDB"], "title": "Modeling arterial pulse waves in healthy aging: a database for in silico evaluation of hemodynamics and pulse wave indexes"}
{"id": "Materials Project", "contents": "The **Materials Project** is a collection of chemical compounds labelled with different attributes.", "variants": ["Materials Project"], "title": "Street Scene: A new dataset and evaluation protocol for video anomaly detection"}
{"id": "KeypointNet", "contents": "**KeypointNet** is a large-scale and diverse 3D keypoint dataset that contains 83,231 keypoints and 8,329 3D models from 16 object categories, by leveraging numerous human annotations, based on ShapeNet models.\n\nSource: [https://github.com/qq456cvb/KeypointNet](https://github.com/qq456cvb/KeypointNet)\nImage Source: [https://github.com/qq456cvb/KeypointNet](https://github.com/qq456cvb/KeypointNet)", "variants": ["KeypointNet"], "title": "KeypointNet: A Large-scale 3D Keypoint Dataset Aggregated from Numerous Human Annotations"}
{"id": "LRW-1000", "contents": "**LRW-1000** is a naturally-distributed large-scale benchmark for word-level lipreading in the wild, including 1000 classes with about 718,018 video samples from more than 2000 individual speakers. There are more than 1,000,000 Chinese character instances in total. Each class corresponds to the syllables of a Mandarin word which is composed by one or several Chinese characters. This dataset aims to cover a natural variability over different speech modes and imaging conditions to incorporate challenges encountered in practical applications.\n\nSource: [VIPL](https://vipl.ict.ac.cn/en/view_database.php?id=13)\nImage Source: [https://arxiv.org/pdf/1810.06990v6.pdf](https://arxiv.org/pdf/1810.06990v6.pdf)", "variants": ["LRW-1000"], "title": "LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild"}
{"id": "METU Trademark", "contents": "The METU Trademark Dataset is a large dataset (the largest publicly available logo dataset as of 2014, and the largest one not requiring any preprocessing as of 2017), which is composed of more than 900K real logos belonging to real companies worldwide. The dataset also includes query sets of varying difficulties, allowing Trademark Retrieval researchers to benchmark their methods against other methods to progress the field.", "variants": ["METU Trademark"], "title": "A Large-scale Dataset and Benchmark for Similar Trademark Retrieval"}
{"id": "StarData", "contents": "**StarData** is a StarCraft: Brood War replay dataset, with 65,646 games. The full dataset after compression is 365 GB, 1535 million frames, and 496 million player actions. The entire frame data was dumped out at 8 frames per second.\r\n\r\nSource: [https://github.com/TorchCraft/StarData](https://github.com/TorchCraft/StarData)\r\nImage Source: [https://www.youtube.com/watch?app=desktop&v=vBjgww8jDgw](https://www.youtube.com/watch?app=desktop&v=vBjgww8jDgw)", "variants": ["StarData"], "title": "STARDATA: A StarCraft AI Research Dataset"}
{"id": "MSSD", "contents": "The Spotify Music Streaming Sessions Dataset (MSSD) consists of 160 million streaming sessions with associated user interactions, audio features and metadata describing the tracks streamed during the sessions, and snapshots of the playlists listened to during the sessions. \r\n\r\nThis dataset enables research on important problems including how to model user listening and interaction behaviour in streaming, as well as Music Information Retrieval (MIR), and session-based sequential recommendations.", "variants": ["MSSD"], "title": "The Music Streaming Sessions Dataset"}
{"id": "CUHK-SYSU", "contents": "The CUKL-SYSY dataset is a large scale benchmark for person search, containing 18,184 images and 8,432 identities. Different from previous re-id benchmarks, matching query persons with manually cropped pedestrians, this dataset is much closer to real application scenarios by searching person from whole images in the gallery.\r\n\r\nSource: [http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html](http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html)\r\nImage Source: [http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html](http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html)", "variants": ["CUHK-SYSU"], "title": "Joint Detection and Identification Feature Learning for Person Search"}
{"id": "Amazon Product Data", "contents": "This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\r\n\r\nThis dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).", "variants": ["Amazon Product Data"], "title": "Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"}
{"id": "MINC", "contents": "MINC is a large-scale, open dataset of materials in the wild.\r\n\r\nSource: [Material Recognition in the Wild with the Materials in Context Database](https://arxiv.org/pdf/1412.0623v2.pdf)\r\nImage Source: [Bell et al](https://arxiv.org/pdf/1412.0623v2.pdf)", "variants": ["MINC"], "title": "Material recognition in the wild with the Materials in Context Database"}
{"id": "arXiv Summarization Dataset", "contents": "This is a dataset for evaluating summarisation methods for research papers.\r\n\r\nSource: [A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents](/paper/a-discourse-aware-attention-model-for)", "variants": ["arXiv Summarization Dataset"], "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents"}
{"id": "MedMentions", "contents": "MedMentions is a new manually annotated resource for the recognition of biomedical concepts. What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines. \r\n\r\nSource: [MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts](/paper/medmentions-a-large-biomedical-corpus)", "variants": ["MedMentions"], "title": "MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts"}
{"id": "Cervix93 Cytology Dataset", "contents": "The dataset has 93 image stacks and their corresponding Extended Depth of Field (EDF) image acquired from cases with grades Nagative, LSIL or HSIL (The Bethesda System):\n- Negative: 16\n- LSIL: 46\n- HSIL: 31\nThe ground truth includes the grade labels for each frame and manually marked points inside cervical cells in each frame. There are in total 2705 manually marked points inside all frames:\n- Negative: 238\n- LSIL: 1536\n- HSIL: 931\n\nSource: [https://github.com/parham-ap/cytology_dataset](https://github.com/parham-ap/cytology_dataset)", "variants": ["Cervix93 Cytology Dataset"], "title": "A New Cervical Cytology Dataset for Nucleus Detection and Image Classification (Cervix93) and Methods for Cervical Nucleus Detection"}
{"id": "WebCaricature Dataset", "contents": "Aims to facilitate research in caricature recognition. All the caricatures and face images were collected from the Web. Compared with two existing datasets, this dataset is much more challenging, with a much greater number of available images, artistic styles and larger intra-personal variations. \r\n\r\nSource: [WebCaricature: a benchmark for caricature recognition](https://arxiv.org/pdf/1703.03230v4.pdf)", "variants": ["WebCaricature Dataset"], "title": "WebCaricature: a benchmark for caricature recognition"}
{"id": "VidSTG", "contents": "The **VidSTG** dataset is a spatio-temporal video grounding dataset constructed based on the video relation dataset VidOR. VidOR contains 7,000, 835 and 2,165 videos for training, validation and testing, respectively. The goal of the Spatio-Temporal Video Grounding task (STVG) is to localize the spatio-temporal section of an untrimmed video that matches a given sentence depicting an object.\n\nSource: [https://github.com/Guaranteer/VidSTG-Dataset](https://github.com/Guaranteer/VidSTG-Dataset)\nImage Source: [https://github.com/Guaranteer/VidSTG-Dataset](https://github.com/Guaranteer/VidSTG-Dataset)", "variants": ["VidSTG"], "title": "Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences"}
{"id": "X-WikiRE", "contents": "X-WikiRE is a new, large-scale multilingual relation extraction dataset in which relation extraction is framed as a problem of reading comprehension to allow for generalization to unseen relations. \r\n\r\nSource: [X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension](https://arxiv.org/pdf/1908.05111v2.pdf)", "variants": ["X-WikiRE"], "title": "X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension"}
{"id": "AVA", "contents": "**AVA** is a project that provides audiovisual annotations of video for improving our understanding of human activity. Each of the video clips has been exhaustively annotated by human annotators, and together they represent a rich variety of scenes, recording conditions, and expressions of human activity. There are annotations for:\r\n\r\n- Kinetics (AVA-Kinetics) - a crossover between AVA and Kinetics. In order to provide localized action labels on a wider variety of visual scenes, authors provide AVA action labels on videos from Kinetics-700, nearly doubling the number of total annotations, and increasing the number of unique videos by over 500x. \r\n- Actions (AvA Actions) - the AVA dataset densely annotates 80 atomic visual actions in 430 15-minute movie clips, where actions are localized in space and time, resulting in 1.62M action labels with multiple labels per human occurring frequently. \r\n- Spoken Activity (AVA ActiveSpeaker, AVA Speech). AVA ActiveSpeaker: associates speaking activity with a visible face, on the AVA v1.0 videos, resulting in 3.65 million frames labeled across ~39K face tracks. AVA Speech densely annotates audio-based speech activity in AVA v1.0 videos, and explicitly labels 3 background noise conditions, resulting in ~46K labeled segments spanning 45 hours of data.\r\nImage Source: [https://www.researchgate.net/profile/Paolo_Napoletano/publication/309327222/figure/fig1/AS:419620126248965@1477056642346/Sample-images-from-the-Aesthetic-Visual-Analysis-AVA-database-sorted-by-their-aesthetic.png](https://www.researchgate.net/profile/Paolo_Napoletano/publication/309327222/figure/fig1/AS:419620126248965@1477056642346/Sample-images-from-the-Aesthetic-Visual-Analysis-AVA-database-sorted-by-their-aesthetic.png)", "variants": ["AVA v2.1", "AVA-ActiveSpeaker", "AVA-Speech"], "title": "AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions"}
{"id": "Touchdown Dataset", "contents": "Touchdown is a corpus for executing navigation instructions and resolving spatial descriptions in visual real-world environments. The task is to follow instruction to a goal position and there find a hidden object, Touchdown the bear.\n\nSource: [https://github.com/lil-lab/touchdown](https://github.com/lil-lab/touchdown)\nImage Source: [https://github.com/lil-lab/touchdown](https://github.com/lil-lab/touchdown)", "variants": ["Touchdown Dataset"], "title": "TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments"}
{"id": "FSDKaggle2019", "contents": "**FSDKaggle2019** is an audio dataset containing 29,266 audio files annotated with 80 labels of the AudioSet Ontology. FSDKaggle2019 has been used for the DCASE Challenge 2019 Task 2, which was run as a Kaggle competition titled Freesound Audio Tagging 2019. The dataset allows development and evaluation of machine listening methods in conditions of label noise, minimal supervision, and real-world acoustic mismatch. FSDKaggle2019 consists of two train sets and one test set. One train set and the test set consists of manually-labeled data from Freesound, while the other train set consists of noisily labeled web audio data from Flickr videos taken from the YFCC dataset.\nThe curated train set consists of manually labeled data from FSD: 4970 total clips with a total duration of 10.5 hours.  The noisy train set has 19,815 clips with a total duration of 80 hours. The test set has 4481 clips with a total duration of 12.9 hours.\n\nSource: [https://labs.freesound.org/datasets/](https://labs.freesound.org/datasets/)\nImage Source: [https://labs.freesound.org/datasets/](https://labs.freesound.org/datasets/)", "variants": ["FSDKaggle2019"], "title": "Audio tagging with noisy labels and minimal supervision"}
{"id": "A2D", "contents": "A2D (Actor-Action Dataset) is a dataset for simultaneously inferring actors and actions in videos. A2D has seven actor classes (adult, baby, ball, bird, car, cat, and dog) and eight action classes (climb, crawl, eat, fly, jump, roll, run, and walk) not including the no-action class, which we also consider. The A2D has 3,782 videos with at least 99 instances per valid actor-action tuple and videos are labeled with both pixel-level actors and actions for sampled frames. The A2D dataset serves as a large-scale testbed for various vision problems: video-level single- and multiple-label actor-action recognition, instance-level object segmentation/co-segmentation, as well as pixel-level actor-action semantic segmentation to name a few.", "variants": ["A2D"], "title": "Can humans fly? Action understanding with multiple classes of actors"}
{"id": "FAS100K", "contents": "**FAS100K** is a large-scale visual localization dataset. This dataset is comprised of two traverses of 238 and 130 kms respectively where the latter is a partial repeat of the former. The data was collected using stereo cameras in Australia under sunny day conditions. It covers a variety of road and environment types including urban and rural areas. The raw image data from one of the cameras streaming at 5 Hz constitutes 63,650 and 34,497 image frames for the two traverses respectively.\n\nSource: [https://arxiv.org/pdf/2001.08434.pdf](https://arxiv.org/pdf/2001.08434.pdf)", "variants": ["FAS100K"], "title": "Fast, Compact and Highly Scalable Visual Place Recognition through Sequence-based Matching of Overloaded Representations"}
{"id": "MegaAge", "contents": "MegaAge is a large dataset that consists of 41,941 faces annotated with age posterior distributions.", "variants": ["MegaAge"], "title": "Quantifying Facial Age by Posterior of Age Comparisons"}
{"id": "FSDKaggle2018", "contents": "**FSDKaggle2018** is an audio dataset containing 11,073 audio files annotated with 41 labels of the AudioSet Ontology. FSDKaggle2018 has been used for the DCASE Challenge 2018 Task 2. All audio samples are gathered from Freesound and are provided as uncompressed PCM 16 bit, 44.1 kHz mono audio files. The 41 categories of the AudioSet Ontology are:\n\"Acoustic_guitar\", \"Applause\", \"Bark\", \"Bass_drum\", \"Burping_or_eructation\", \"Bus\", \"Cello\", \"Chime\", \"Clarinet\", \"Computer_keyboard\", \"Cough\", \"Cowbell\", \"Double_bass\", \"Drawer_open_or_close\", \"Electric_piano\", \"Fart\", \"Finger_snapping\", \"Fireworks\", \"Flute\", \"Glockenspiel\", \"Gong\", \"Gunshot_or_gunfire\", \"Harmonica\", \"Hi-hat\", \"Keys_jangling\", \"Knock\", \"Laughter\", \"Meow\", \"Microwave_oven\", \"Oboe\", \"Saxophone\", \"Scissors\", \"Shatter\", \"Snare_drum\", \"Squeak\", \"Tambourine\", \"Tearing\", \"Telephone\", \"Trumpet\", \"Violin_or_fiddle\", \"Writing\".\n\nSource: [https://zenodo.org/record/2552860](https://zenodo.org/record/2552860)\nImage Source: [https://labs.freesound.org/datasets/](https://labs.freesound.org/datasets/)", "variants": ["FSDKaggle2018"], "title": "General-purpose Tagging of Freesound Audio with AudioSet Labels: Task Description, Dataset, and Baseline"}
{"id": "Something-Something V1", "contents": "The 20BN-SOMETHING-SOMETHING dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 108,499 videos, with 86,017 in the training set, 11,522 in the validation set and 10,960 in the test set. There are 174 labels.\r\n\r\n⚠️ Attention: This is the outdated V1 of the dataset. V2 is available [here](https://paperswithcode.com/dataset/something-something-v2).\r\n\r\nSource: [https://20bn.com/datasets/something-something/v1](https://20bn.com/datasets/something-something/v1)\r\nImage Source: [https://20bn.com/datasets/something-something/v1](https://20bn.com/datasets/something-something/v1)", "variants": ["Something-Something V1"], "title": "The “Something Something” Video Database for Learning and Evaluating Visual Common Sense"}
{"id": "Interview", "contents": "A large-scale (105K conversations) media dialog dataset collected from news interview transcripts.\r\n\r\nSource: [Interview: A Large-Scale Open-Source Corpus of Media Dialog](/paper/interview-a-large-scale-open-source-corpus-of)", "variants": ["Interview"], "title": "Interview: A Large-Scale Open-Source Corpus of Media Dialog"}
{"id": "FocusPath", "contents": "FocusPath is a dataset compiled from diverse Whole Slide Image (WSI) scans in different focus (z-) levels. Images are naturally blurred by out-of-focus lens provided with GT scores of focus levels. The dataset can be used for No-Reference Focus Quality assessment of Digital Pathology/Microscopy images.\r\n\r\nSource: [FocusPath](https://github.com/mahdihosseini/FoucsPath)", "variants": ["FocusPath"], "title": "Encoding Visual Sensitivity by MaxPol Convolution Filters for Image Sharpness Assessment"}
{"id": "ChineseFoodNet", "contents": "ChineseFoodNet aims to automatically recognizing pictured Chinese dishes. Most of the existing food image datasets collected food images either from recipe pictures or selfie. In the dataset, images of each food category of the dataset consists of not only web recipe and menu pictures but photos taken from real dishes, recipe and menu as well. ChineseFoodNet contains over 180,000 food photos of 208 categories, with each category covering a large variations in presentations of same Chinese food. \r\n\r\nSource: [ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition](/paper/chinesefoodnet-a-large-scale-image-dataset)\r\nImage Source: [https://sites.google.com/view/chinesefoodnet](https://sites.google.com/view/chinesefoodnet)", "variants": ["ChineseFoodNet"], "title": "ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition"}
{"id": "CASIA-WebFace", "contents": "The **CASIA-WebFace** dataset is used for face verification and face identification tasks. The dataset contains 494,414 face images of 10,575 real identities collected from the web.\r\n\r\nSource: [On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs](https://arxiv.org/abs/1811.07104)", "variants": ["WebFace", "WebFace - 8x upscaling", "CASIA-WebFace"], "title": "Learning Face Representation from Scratch"}
{"id": "COSTRA 1.0", "contents": "COSTRA 1.0 is a dataset of complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing. The first version of the dataset is limited to sentences in Czech but the construction method is universal and the authors plan to use it also for other languages. The dataset consist of 4,262 unique sentences with average length of 10 words, illustrating 15 types of modifications such as simplification, generalization, or formal and informal language variation.\r\n\r\nSource: [COSTRA 1.0: A Dataset of Complex Sentence Transformations](/paper/costra-10-a-dataset-of-complex-sentence)", "variants": ["COSTRA 1.0"], "title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations"}
{"id": "Middlebury 2014", "contents": "The **Middlebury 2014** dataset contains a set of 23 high resolution stereo pairs for which known camera calibration parameters and ground truth disparity maps obtained with a structured light scanner are available. The images in the Middlebury dataset all show static indoor scenes with varying difficulties including repetitive structures, occlusions, wiry objects as well as untextured areas.\r\n\r\nSource: [Using Self-Contradiction to Learn Confidence Measures in Stereo Vision](https://arxiv.org/abs/1604.05132)\r\nImage Source: [https://vision.middlebury.edu/stereo/data/scenes2014/](https://vision.middlebury.edu/stereo/data/scenes2014/)", "variants": ["Middlebury 2014"], "title": "High-Resolution Stereo Datasets with Subpixel-Accurate Ground Truth"}
{"id": "CUHK Face Alignment Database", "contents": "The CUHK Face Alignment Database is dataset with 13,466 face images, among which 5, 590 images are from LFW and the remaining 7, 876 images are downloaded from the web. Each face is labeled with the positions of five facial keypoints. 10,000 images are used for training and the remaining 3,466 images for validation.\r\n\r\n<paper>\r\n\r\nImage Source: [Deep Convolutional Network Cascade for Facial Point Detection](http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm)", "variants": ["CUHK Face Alignment Database"], "title": "Deep Convolutional Network Cascade for Facial Point Detection"}
{"id": "Mathematics Dataset", "contents": "This dataset code generates mathematical question and answer pairs, from a range of question types at roughly school-level difficulty. This is designed to test the mathematical learning and algebraic reasoning skills of learning models.\r\n\r\nSource: [DeepMind](https://github.com/deepmind/mathematics_dataset)", "variants": ["Mathematics Dataset"], "title": "Analysing Mathematical Reasoning Abilities of Neural Models"}
{"id": "CUHK-QA", "contents": "CUHK-QA is a dataset for natural language-based person search using iterative questioning. \r\n\r\nThe dataset consists of 400 images of 360 people, and  20 participants answered 5 specific questions about the appearance of each person. So, each person has labelled 20 images. Average length of combined description for each image is 39.15. All the images have been taken from the test set of CUHK-PEDES dataset.\r\n\r\nSource: [Interactive Natural Language-based Person Search](/paper/interactive-natural-language-based-person)", "variants": ["CUHK-QA"], "title": "Interactive Natural Language-based Person Search"}
{"id": "Tour20", "contents": "Contains 140 videos with multiple human created summaries, which were acquired in a controlled experiment. \r\n\r\nSource: [Diversity-aware Multi-Video Summarization](/paper/diversity-aware-multi-video-summarization)", "variants": ["Tour20"], "title": "Diversity-aware Multi-Video Summarization"}
{"id": "MegaFace", "contents": "**MegaFace** was a publicly available dataset which is used for evaluating the performance of face recognition algorithms with up to a million distractors (i.e., up to a million people who are not in the test set). MegaFace contains 1M images from 690K individuals with unconstrained pose, expression, lighting, and exposure. MegaFace captures many different subjects rather than many images of a small number of subjects. The gallery set of MegaFace is collected from a subset of Flickr. The probe set of MegaFace used in the challenge consists of two databases; Facescrub and FGNet. FGNet contains 975 images of 82 individuals, each with several images spanning ages from 0 to 69. Facescrub dataset contains more than 100K face images of 530 people. The MegaFace challenge evaluates performance of face recognition algorithms by increasing the numbers of “distractors” (going from 10 to 1M) in the gallery set. In order to evaluate the face recognition algorithms fairly, MegaFace challenge has two protocols including large or small training sets. If a training set has more than 0.5M images and 20K subjects, it is considered as large. Otherwise, it is considered as small.\r\n\r\n**NOTE**: This dataset [has been retired](https://exposing.ai/megaface/). \r\n\r\nSource: [A Deep Face Identification Network Enhanced by Facial Attributes Prediction](https://arxiv.org/abs/1805.00324)", "variants": ["MegaFace"], "title": "The MegaFace Benchmark: 1 Million Faces for Recognition at Scale"}
{"id": "COCO Captions", "contents": "COCO Captions contains over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions are be provided for each image.\r\n\r\nSource: [Microsoft COCO Captions: Data Collection and Evaluation Server](https://arxiv.org/abs/1504.00325)", "variants": ["COCO Captions", "COCO Captions test", "MSCOCO-1k"], "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}
{"id": "SBD", "contents": "The **Semantic Boundaries Dataset** (**SBD**) is a dataset for predicting pixels on the boundary of the object (as opposed to the inside of the object with semantic segmentation). The dataset consists of 11318 images from the trainval set of the PASCAL VOC2011 challenge, divided into 8498 training and 2820 test images. This dataset has object instance boundaries with accurate figure/ground masks that are also labeled with one of 20 Pascal VOC classes.\r\n\r\nSource: [Weakly Supervised Object Boundaries](https://arxiv.org/abs/1511.07803)\r\nImage Source: [http://home.bharathh.info/pubs/codes/SBD/download.html](http://home.bharathh.info/pubs/codes/SBD/download.html)", "variants": ["SBD", "Sbd val"], "title": "Semantic contours from inverse detectors"}
{"id": "APT-Malware", "contents": "The APT Malware dataset is used to train classifiers to predict if a given malware belongs to the “Advanced Persistent Threat” (APT) type or not. It contains 3131 samples spread over 24 different unique malware classes.\n\nSource: [https://arxiv.org/pdf/1810.07321.pdf](https://arxiv.org/pdf/1810.07321.pdf)", "variants": ["APT-Malware"], "title": "Malware triage for early identification of Advanced Persistent Threat activities"}
{"id": "AirSim", "contents": "**AirSim** is a simulator for drones, cars and more, built on Unreal Engine. It is open-source, cross platform, and supports software-in-the-loop simulation with popular flight controllers such as PX4 & ArduPilot and hardware-in-loop with PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin that can simply be dropped into any Unreal environment. Similarly, there exists an experimental version for a Unity plugin.\r\n\r\nSource: [AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles](https://arxiv.org/pdf/1705.05065v2.pdf)", "variants": ["AirSim"], "title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles"}
{"id": "AFLW2000-3D", "contents": "**AFLW2000-3D** is a dataset of 2000 images that have been annotated with image-level 68-point 3D facial landmarks. This dataset is used for evaluation of 3D facial landmark detection models. The head poses are very diverse and often hard to be detected by a CNN-based face detector.\r\n\r\nSource: [https://www.tensorflow.org/datasets/catalog/aflw2k3d](https://www.tensorflow.org/datasets/catalog/aflw2k3d)\r\nImage Source: [http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm)", "variants": ["AFLW2000", "AFLW2000-3D"], "title": "Face Alignment Across Large Poses: A 3D Solution"}
{"id": "MQR", "contents": "A multi-domain question rewriting dataset is constructed from human contributed Stack Exchange question edit histories. The dataset contains 427,719 question pairs which come from 303 domains.\r\n\r\nSource: [How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions](/paper/how-to-ask-better-questions-a-large-scale)", "variants": ["MQR"], "title": "How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions"}
{"id": "Animal-Pose Dataset", "contents": "**Animal-Pose Dataset** is an animal pose dataset to facilitate training and evaluation. This dataset provides animal pose annotations on five categories are provided: dog, cat, cow, horse, sheep, with in total 6,000+ instances in 4,000+ images. Besides, the dataset also contains bounding box annotations for other 7 animal categories.\r\n\r\nSource: [Cross-Domain Adaptation for Animal Pose Estimation](/paper/cross-domain-adaptation-for-animal-pose)\r\nImage Source: [https://sites.google.com/view/animal-pose/](https://sites.google.com/view/animal-pose/)", "variants": ["Animal-Pose Dataset"], "title": "Cross-Domain Adaptation for Animal Pose Estimation"}
{"id": "SumMe", "contents": "The **SumMe** dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).\r\n\r\nSource: [https://gyglim.github.io/me/vsum/index.html](https://gyglim.github.io/me/vsum/index.html)\r\nImage Source: [https://gyglim.github.io/me/vsum/index.html](https://gyglim.github.io/me/vsum/index.html)", "variants": ["SumMe"], "title": "Creating Summaries from User Videos"}
{"id": "IRS", "contents": "**IRS** is an open dataset for indoor robotics vision tasks, especially disparity and surface normal estimation. It contains totally 103,316 samples covering a wide range of indoor scenes, such as home, office, store and restaurant.\n\nSource: [https://github.com/HKBU-HPML/IRS](https://github.com/HKBU-HPML/IRS)\nImage Source: [https://github.com/HKBU-HPML/IRS](https://github.com/HKBU-HPML/IRS)", "variants": ["IRS"], "title": "IRS: A Large Synthetic Indoor Robotics Stereo Dataset for Disparity and Surface Normal Estimation"}
{"id": "SelQA", "contents": "SelQA is a dataset that consists of questions generated through crowdsourcing and sentence length answers that are drawn from the ten most prevalent topics in the English Wikipedia. \r\n\r\nSource: [SelQA: A New Benchmark for Selection-based Question Answering](/paper/selqa-a-new-benchmark-for-selection-based)", "variants": ["SelQA"], "title": "SelQA: A New Benchmark for Selection-Based Question Answering"}
{"id": "TopLogo-10", "contents": "Collected from top 10 most popular clothing/wearable brandname logos captured in rich visual context.\r\n\r\nSource: [Deep Learning Logo Detection with Data Expansion by Synthesising Context](/paper/deep-learning-logo-detection-with-data)", "variants": ["TopLogo-10"], "title": "Deep Learning Logo Detection with Data Expansion by Synthesising Context"}
{"id": "ArSentD-LEV", "contents": "The Arabic Sentiment Twitter Dataset for the Levantine dialect (ArSenTD-LEV) is a dataset of 4,000 tweets with the following annotations: the overall sentiment of the tweet, the target to which the sentiment was expressed, how the sentiment was expressed, and the topic of the tweet. \r\n\r\nSource: [ArSentD-LEV: A Multi-Topic Corpus for Target-based Sentiment Analysis in Arabic Levantine Tweets](/paper/190601830)", "variants": ["ArSentD-LEV"], "title": "ArSentD-LEV: A Multi-Topic Corpus for Target-based Sentiment Analysis in Arabic Levantine Tweets"}
{"id": "Anonymized Keystrokes Dataset", "contents": "Includes two datasets for this task, one for English-French (En-Fr) and another for English-German (En-De). For each dataset, the action sequences for full documents are provided, along with an editor identifier. The dataset contains document-level post-editing action sequences, including edit operations from keystrokes, mouse actions, and waiting times.\r\n\r\nSource: [Translator2Vec: Understanding and Representing Human Post-Editors](https://arxiv.org/pdf/1907.10362.pdf)", "variants": ["Anonymized Keystrokes Dataset"], "title": "Translator2Vec: Understanding and Representing Human Post-Editors"}
{"id": "Relational Strategies in Customer Service (RSiCS) Dataset", "contents": "Corpus for improving the quality and relational abilities of Intelligent Virtual Agents (IVAs).\r\n\r\nSource: [An Annotated Corpus of Relational Strategies in Customer Service](/paper/an-annotated-corpus-of-relational-strategies)", "variants": ["Relational Strategies in Customer Service (RSiCS) Dataset"], "title": "An Annotated Corpus of Relational Strategies in Customer Service"}
{"id": "Image网", "contents": "**Image网** (pronounced Imagewang; 网 means \"net\" in Chinese) is an image classification dataset combined from [Imagenette](/dataset/imagenette) and [Imagewoof](/dataset/imagewoof) datasets in a way to make it into a semi-supervised unbalanced classification problem:\r\n\r\n* the validation set is the same as the validation set of Imagewoof; there are no Imagenette images in the validation set (they're all in the training set),\r\n\r\n* only 10% of Imagewoof images are in the training set. The remaining images are in the \"unsupervised\" split.\r\n\r\nSource: [fast.ai](https://github.com/fastai/imagenette)", "variants": ["Image网"], "title": "fastai: A Layered API for Deep Learning"}
{"id": "SPair-71k", "contents": "SPair-71k contains 70,958 image pairs with diverse variations in viewpoint and scale. Compared to previous datasets, it is significantly larger in number and contains more accurate and richer annotations. \r\n\r\nSource: [SPair-71k: A Large-scale Benchmark for Semantic Correspondence](/paper/spair-71k-a-large-scale-benchmark-for)\r\nImage Source: [http://cvlab.postech.ac.kr/research/SPair-71k/](http://cvlab.postech.ac.kr/research/SPair-71k/)", "variants": ["SPair-71k"], "title": "SPair-71k: A Large-scale Benchmark for Semantic Correspondence"}
{"id": "MERL-RAV", "contents": "The MERL-RAV (MERL Reannotation of AFLW with Visibility) Dataset contains over 19,000 face images in a full range of head poses. Each face is manually labeled with the ground-truth locations of 68 landmarks, with the additional information of whether each landmark is unoccluded, self-occluded (due to extreme head poses), or externally occluded. The images were annotated by professional labelers, supervised by researchers at Mitsubishi Electric Research Laboratories (MERL).", "variants": ["MERL-RAV"], "title": "LUVLi Face Alignment: Estimating Landmarks' Location, Uncertainty, and Visibility Likelihood"}
{"id": "BIOSSES", "contents": "The BIOSSES data set comprises total 100 sentence pairs all of which were selected from the \"[TAC2 Biomedical Summarization Track Training Data Set](https://tac.nist.gov/2014/BiomedSumm/)\" .\r\n\r\nThe sentence pairs were evaluated by five different human experts that judged their similarity and gave scores in a range [0-4]. Our guideline was prepared based on SemEval 2012 Task 6 Guideline.\r\n\r\nImage source: [BIOSSES](https://tabilab.cmpe.boun.edu.tr/BIOSSES/DataSet.html)", "variants": ["BIOSSES"], "title": "BIOSSES: a semantic sentence similarity estimation system for the biomedical domain"}
{"id": "CUB-200-2011", "contents": "The **Caltech-UCSD Birds-200-2011** (**CUB-200-2011**) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from [Reed et al.]( https://paperswithcode.com/paper/learning-deep-representations-of-fine-grained). They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.\r\n\r\nSource: [Fine-grained Visual-textual Representation Learning](https://arxiv.org/abs/1709.00340)\r\nImage Source: [http://www.vision.caltech.edu/visipedia/CUB-200-2011.html](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)", "variants": ["CUB-200-2011", "CUB", "CUB 128 x 128", "CUB 200 5-way 1-shot", "CUB 200 5-way 5-shot", "CUB 200 50-way (0-shot)", "CUB Birds", "CUB-200 - 0-Shot Learning", "CUB-200-2011 - 0-Shot", "CUB-LT", "CUB-200-2011, 30 samples per class", "CUB-200-2011, 5 samples per class", "Imbalanced CUB-200-2011", "CUB-200-2011 5-way (1-shot)", "CUB-200-2011 5-way (5-shot)", "CUB-200-2011, 10 samples per class"], "title": "DISFA: A Spontaneous Facial Action Intensity Database"}
{"id": "PRED18", "contents": "Twenty DAVIS recordings with a total duration of about 1.25 hour were obtained by driving the two robots in the robot arena of the University of Ulster in Londonderry.\r\n\r\nSource: [Steering a Predator Robot using a Mixed Frame/Event-Driven Convolutional Neural Network](/paper/steering-a-predator-robot-using-a-mixed)", "variants": ["PRED18"], "title": "Steering a predator robot using a mixed frame/event-driven convolutional neural network"}
{"id": "BanglaLekhaImageCaptions", "contents": "This dataset consists of images and annotations in Bengali. The images are human annotated in Bengali by two adult native Bengali speakers. All popular image captioning datasets have a predominant western cultural bias with the annotations done in English. Using such datasets to train an image captioning system assumes that a good English to target language translation system exists and that the original dataset had elements of the target culture. Both these assumptions are false, leading to the need of a culturally relevant dataset in Bengali, to generate appropriate image captions of images relevant to the Bangladeshi and wider subcontinental context. The dataset presented consists of 9,154 images.", "variants": ["BanglaLekhaImageCaptions"], "title": "Chittron: An Automatic Bangla Image Captioning System"}
{"id": "LIDC-IDRI", "contents": "The **LIDC-IDRI** dataset contains lesion annotations from four experienced thoracic radiologists. LIDC-IDRI contains 1,018 low-dose lung CTs from 1010 lung patients.\r\n\r\nSource: [A 3D Probabilistic Deep Learning System for Detection and Diagnosis of Lung Cancer Using Low-Dose CT Scans](https://arxiv.org/abs/1902.03233)\r\nImage Source: [https://thesai.org/Publications/ViewPaper?Volume=8&Issue=10&Code=IJACSA&SerialNo=15](https://thesai.org/Publications/ViewPaper?Volume=8&Issue=10&Code=IJACSA&SerialNo=15)", "variants": ["LIDC-IDRI"], "title": "The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans."}
{"id": "OpenAI Gym", "contents": "**OpenAI Gym** is a toolkit for developing and comparing reinforcement learning algorithms. It includes environment such as Algorithmic, Atari, Box2D, Classic Control, MuJoCo, Robotics, and Toy Text.\r\n\r\nSource: [https://github.com/openai/gym](https://github.com/openai/gym)\r\nImage Source: [https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947)", "variants": ["OpenAI Gym", "Cart Pole (OpenAI Gym)", "Lunar Lander (OpenAI Gym)"], "title": "OpenAI Gym"}
{"id": "Market1501-Attributes", "contents": "The **Market1501-Attributes** dataset is built from the Market1501 dataset. Market1501 Attribute is an augmentation of this dataset with 28 hand annotated attributes, such as gender, age, sleeve length, flags for items carried as well as upper clothes colors and lower clothes colors.\n\nSource: [Color inference from semantic labeling for person search in videos](https://arxiv.org/abs/1911.13114)\nImage Source: [https://github.com/vana77/Market-1501_Attribute](https://github.com/vana77/Market-1501_Attribute)", "variants": ["Market1501-Attributes"], "title": "Improving Person Re-identification by Attribute and Identity Learning"}
{"id": "Decagon", "contents": "Bio-decagon is a dataset for polypharmacy side effect identification problem framed as a multirelational link prediction problem in a two-layer multimodal graph/network of two node types: drugs and proteins. Protein-protein interaction\r\nnetwork describes relationships between proteins. Drug-drug interaction network contains 964 different types of edges (one for each side effect type) and describes which drug pairs lead to which side effects. Lastly,\r\ndrug-protein links describe the proteins targeted by a given drug.\r\n\r\nThe final network after linking entity vocabularies used by different databases has 645 drug and 19,085 protein nodes connected by 715,612 protein-protein, 4,651,131 drug-drug, and 18,596 drug-protein edges.\r\n\r\nSource: [Modeling polypharmacy side effects with graph convolutional networks](https://academic.oup.com/bioinformatics/article/34/13/i457/5045770)\r\nImage Source: [Modeling polypharmacy side effects with graph convolutional networks](http://snap.stanford.edu/decagon/)", "variants": ["Decagon"], "title": "Modeling polypharmacy side effects with graph convolutional networks"}
{"id": "TORCS", "contents": "**TORCS** (**The Open Racing Car Simulator**) is a driving simulator. It is capable of simulating the essential elements of vehicular dynamics such as mass, rotational inertia, collision, mechanics of suspensions, links and differentials, friction and aerodynamics. Physics simulation is simplified and is carried out through Euler integration of differential equations at a temporal discretization level of 0.002 seconds. The rendering pipeline is lightweight and based on OpenGL that can be turned off for faster training. TORCS offers a large variety of tracks and cars as free assets. It also provides a number of programmed robot cars with different levels of performance that can be used to benchmark the performance of human players and software driving agents. TORCS was built with the goal of developing Artificial Intelligence for vehicular control and has been used extensively by the machine learning community ever since its inception.\r\n\r\nSource: [MADRaS : Multi Agent Driving Simulator](https://arxiv.org/abs/2010.00993)\r\nImage Source: [https://sourceforge.net/projects/torcs/](https://sourceforge.net/projects/torcs/)", "variants": ["TORCS"], "title": "AI2-THOR: An Interactive 3D Environment for Visual AI"}
{"id": "MultiNLI", "contents": "The **Multi-Genre Natural Language Inference** (**MultiNLI**) dataset has 433K sentence pairs. Its size and mode of collection are modeled closely like [SNLI](/dataset/snli). MultiNLI offers ten distinct genres (Face-to-face, Telephone, 9/11, Travel, Letters, Oxford University Press, Slate, Verbatim, Goverment and Fiction) of written and spoken English data. There are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time.\r\n\r\nSource: [Semantic Sentence Matching with Densely-connectedRecurrent and Co-attentive Information](https://arxiv.org/abs/1805.11360)", "variants": ["MultiNLI", "MultiNLI Dev"], "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}
{"id": "SQA", "contents": "The SQA dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total.\r\n\r\nSource: [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253)", "variants": ["SQA"], "title": "Search-based Neural Structured Learning for Sequential Question Answering"}
{"id": "Lost and Found", "contents": "**Lost and Found** is a novel lost-cargo image sequence dataset comprising more than two thousand frames with pixelwise annotations of obstacle and free-space and provide a thorough comparison to several stereo-based baseline methods. The dataset will be made available to the community to foster further research on this important topic.\r\n\r\nSource: [Lost and Found: Detecting Small Road Hazards for Self-Driving Vehicles](/paper/lost-and-found-detecting-small-road-hazards)\r\nImage Source: [http://www.6d-vision.com/lostandfounddataset](http://www.6d-vision.com/lostandfounddataset)", "variants": ["Lost and Found"], "title": "Lost and Found: detecting small road hazards for self-driving vehicles"}
{"id": "MobiFace", "contents": "MobiFace is the first dataset for single face tracking in mobile situations. It consists of 80 unedited live-streaming mobile videos captured by 70 different smartphone users in fully unconstrained environments. Over 95K bounding boxes are manually labelled. The videos are carefully selected to cover typical smartphone usage. The videos are also annotated with 14 attributes, including 6 newly proposed attributes and 8 commonly seen in object tracking.\r\n\r\nSource: [MobiFace](https://ibug.doc.ic.ac.uk/resources/mobiface/)", "variants": ["MobiFace"], "title": "MobiFace: A Novel Dataset for Mobile Face Tracking in the Wild"}
{"id": "SIP", "contents": "The **Salient Person** dataset (**SIP**) contains 929 salient person samples with different poses and illumination conditions.\n\nSource: [Accurate RGB-D Salient Object Detection via Collaborative Learning](https://arxiv.org/abs/2007.11782)\nImage Source: [https://arxiv.org/pdf/1907.06781.pdf](https://arxiv.org/pdf/1907.06781.pdf)", "variants": ["SIP"], "title": "Rethinking RGB-D Salient Object Detection: Models, Datasets, and Large-Scale Benchmarks"}
{"id": "GQA", "contents": "The **GQA** dataset is a large-scale visual question answering dataset with real images from the Visual Genome dataset and balanced question-answer pairs. Each training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images and question-answer pairs, the GQA dataset provides two types of pre-extracted visual features for each image – convolutional grid features of size 7×7×2048 extracted from a ResNet-101 network trained on ImageNet, and object detection features of size Ndet×2048 (where Ndet is the number of detected objects in each image with a maximum of 100 per image) from a Faster R-CNN detector.\r\n\r\nSource: [Language-Conditioned Graph Networks for Relational Reasoning](https://arxiv.org/abs/1905.04405)\r\nImage Source: [https://arxiv.org/pdf/1902.09506.pdf](https://arxiv.org/pdf/1902.09506.pdf)", "variants": ["GQA test-std", "GQA test-dev", "GQA Test2019", "GQA"], "title": "GQA: a new dataset for compositional question answering over real-world images"}
{"id": "DensePose", "contents": "DensePose-COCO is a large-scale ground-truth dataset with image-to-surface correspondences manually annotated\r\non 50K COCO images and train DensePose-RCNN, to densely regress part-specific UV coordinates within every human\r\nregion at multiple frames per second.\r\n\r\nSource: [DensePose: Dense Human Pose Estimation In The Wildc](https://arxiv.org/pdf/1802.00434v1.pdf)\r\nImage Source: [Guler et al](https://arxiv.org/pdf/1802.00434v1.pdf)", "variants": ["DensePose"], "title": "DensePose: Dense Human Pose Estimation in the Wild"}
{"id": "ANTIQUE", "contents": "ANTIQUE is a collection of 2,626 open-domain non-factoid questions from a diverse set of categories. The dataset  contains 34,011 manual relevance annotations. The questions were asked by real users in a community question answering service, i.e., Yahoo! Answers. Relevance judgments for all the answers to each question were collected through crowdsourcing.", "variants": ["ANTIQUE"], "title": "ANTIQUE: A Non-Factoid Question Answering Benchmark"}
{"id": "WOS", "contents": "Web of Science (WOS) is a document classification dataset that contains 46,985 documents with 134 categories which include 7 parents categories.\r\n\r\nSource: [HDLTex: Hierarchical Deep Learning for Text Classification](/paper/hdltex-hierarchical-deep-learning-for-text)", "variants": ["WOS-11967", "WOS-46985", "WOS-5736", "WOS"], "title": "HDLTex: Hierarchical Deep Learning for Text Classification"}
{"id": "AG News", "contents": "**AG News** (**AG’s News Corpus**) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.\r\n\r\nSource: [https://arxiv.org/pdf/1509.01626.pdf](https://arxiv.org/pdf/1509.01626.pdf)", "variants": ["AG News", "AG News (200 Labels)"], "title": "Character-level Convolutional Networks for Text Classification"}
{"id": "OpoSum", "contents": "OPOSUM is a dataset for the training and evaluation of Opinion Summarization models which contains Amazon reviews from six product domains: Laptop Bags, Bluetooth Headsets, Boots, Keyboards, Televisions, and Vacuums.\r\nThe six training collections were created by downsampling from the Amazon Product Dataset introduced in McAuley et al. (2015) and contain reviews and their respective ratings. \r\n\r\nA subset of the dataset has been manually annotated, specifically, for each domain, 10 different products were uniformly sampled (across ratings) with 10 reviews each, amounting to a total of 600 reviews, to be used only for development (300) and testing (300).\r\n\r\nSource: [Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised](https://arxiv.org/abs/1808.08858)", "variants": ["OpoSum"], "title": "Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised"}
{"id": "DesireDB", "contents": "Includes gold-standard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context.\r\n\r\nSource: [Modelling Protagonist Goals and Desires in First-Person Narrative](/paper/modelling-protagonist-goals-and-desires-in)", "variants": ["DesireDB"], "title": "Modelling Protagonist Goals and Desires in First-Person Narrative"}
{"id": "SURREAL", "contents": "**SURREAL** (Synthetic hUmans foR REAL tasks) is a large-scale person dataset that generates photorealistic synthetic images with labeling for human part segmentation and depth estimation, producing 6.5M frames in 67.5K short clips (about 100 frames each) of 2.6K action sequences with 145 different synthetic subjects. To ensure realism, the synthetic bodies are created using the SMPL body model, whose parameters are fit by the MoSh method given raw 3D MoCap marker data.\r\n\r\nSource: [Synthetic Data for Deep Learning](https://arxiv.org/abs/1909.11512)\r\nImage Source: [https://www.di.ens.fr/willow/research/surreal/data/](https://www.di.ens.fr/willow/research/surreal/data/)", "variants": ["Surreal", "SURREAL"], "title": "Learning from Synthetic Humans"}
{"id": "Kinship", "contents": "This relational database consists of 24 unique names in two families (they have equivalent structures).\n\nSource: [https://archive.ics.uci.edu/ml/datasets/kinship](https://archive.ics.uci.edu/ml/datasets/kinship)", "variants": ["Kinship"], "title": "A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications"}
{"id": "ASTD", "contents": "Arabic Sentiment Tweets Dataset (ASTD) is an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classified as objective, subjective positive, subjective negative, and subjective mixed.\r\n\r\nSource: [ASTD: Arabic Sentiment Tweets Dataset](/paper/astd-arabic-sentiment-tweets-dataset)", "variants": ["ASTD"], "title": "ASTD: Arabic Sentiment Tweets Dataset"}
{"id": "MSU-MFSD", "contents": "The **MSU-MFSD** dataset contains 280 video recordings of genuine and attack faces. 35 individuals have participated in the development of this database with a total of 280 videos. Two kinds of cameras with different resolutions (720×480 and 640×480) were used to record the videos from the 35 individuals. For the real accesses, each individual has two video recordings captured with the Laptop cameras and Android, respectively. For the video attacks, two types of cameras, the iPhone and Canon cameras were used to capture high definition videos on each of the subject. The videos taken with Canon camera were then replayed on iPad Air screen to generate the HD replay attacks while the videos recorded by the iPhone mobile were replayed itself to generate the mobile replay attacks. Photo attacks were produced by printing the 35 subjects’ photos on A3 papers using HP colour printer. The recording videos with respect to the 35 individuals were divided into training (15 subjects with 120 videos) and testing (40 subjects with 160 videos) datasets, respectively.\r\n\r\nSource: [Enhance the Motion Cues for Face Anti-Spoofing using CNN-LSTM Architecture](https://arxiv.org/abs/1901.05635)\r\n\r\nImage Source: [face anti-spoofing based on color texture analysis](https://arxiv.org/abs/1511.06316)", "variants": ["MSU-MFSD"], "title": "Face Spoof Detection With Image Distortion Analysis"}
{"id": "UKP", "contents": "The **UKP** Argument Annotated Essays corpus consists of argument annotated persuasive essays including annotations of argument components and argumentative relations.\n\nSource: [https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/argument_annotated_essays/index.en.jsp](https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/argument_annotated_essays/index.en.jsp)\nImage Source: [https://www.aclweb.org/anthology/C14-1142.pdf](https://www.aclweb.org/anthology/C14-1142.pdf)", "variants": ["UKP"], "title": "Annotating Argument Components and Relations in Persuasive Essays"}
{"id": "LastFM Asia", "contents": "A social network of LastFM users which was collected from the public API in March 2020. Nodes are LastFM users from Asian countries and edges are mutual follower relationships between them. The vertex features are extracted based on the artists liked by the users. The task related to the graph is multinomial node classification - one has to predict the location of users. This target feature was derived from the country field for each user.\n\nSource: [https://snap.stanford.edu/data/feather-lastfm-social.html](https://snap.stanford.edu/data/feather-lastfm-social.html)", "variants": ["LastFM Asia"], "title": "Deep Graph Kernels"}
{"id": "TextCaps", "contents": "Contains 145k captions for 28k images. The dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. \r\n\r\nSource: [TextCaps: a Dataset for Image Captioning with Reading Comprehension](/paper/textcaps-a-dataset-for-image-captioning-with)", "variants": ["TextCaps 2020", "TextCaps"], "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"}
{"id": "COUNTER", "contents": "The **COUNTER** (COrpus of Urdu News TExt Reuse) corpus contains 600 source-derived document pairs collected from the field of journalism. It can be used to evaluate mono-lingual text reuse detection systems in general and specifically for Urdu language.\r\n\r\nThe corpus has 600 source and 600 derived documents. It contains in total 275,387 words (tokens), 21,426 unique words and 10,841 sentences. It has been manually annotated at document level with three levels of reuse: wholly derived (135), partially derived (288) and non derived (177).\r\n\r\nSource: [COUNTER](http://ucrel.lancs.ac.uk/textreuse/counter.php)", "variants": ["COUNTER"], "title": "COUNTER: corpus of Urdu news text reuse"}
{"id": "Set12", "contents": "**Set12** is a collection of 12 grayscale images of different scenes that are widely used for evaluation of image denoising methods. The size of each image is 256×256.\r\n\r\nSource: [Designing and Training of A Dual CNN for Image Denoising](https://arxiv.org/abs/2007.03951)\r\nImage Source: [https://www.researchgate.net/figure/12-images-from-Set12-dataset_fig11_338424598](https://www.researchgate.net/figure/12-images-from-Set12-dataset_fig11_338424598)", "variants": ["Set12 sigma15", "Set12 sigma25", "Set12 sigma30", "Set12 sigma50", "Set12 sigma70", "Set12"], "title": "Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising"}
{"id": "Dayton", "contents": "The **Dayton** dataset is a dataset for ground-to-aerial (or aerial-to-ground) image translation, or cross-view image synthesis. It contains images of road views and aerial views of roads. There are 76,048 images in total and the train/test split is 55,000/21,048. The images in the original dataset have 354×354 resolution.\n\nSource: [Multi-Channel Attention Selection GANs for Guided Image-to-Image Translation](https://arxiv.org/abs/2002.01048)\nImage Source: [https://arxiv.org/abs/1912.06112](https://arxiv.org/abs/1912.06112)", "variants": ["Dayton (256×256) - aerial-to-ground", "Dayton (256×256) - ground-to-aerial", "Dayton (64x64) - ground-to-aerial", "Dayton (64×64) - aerial-to-ground", "Dayton"], "title": "Localizing and Orienting Street Views Using Overhead Imagery"}
{"id": "IWSLT 2019", "contents": "The **IWSLT 2019** dataset contains source, Machine Translated, reference and Post-Edited text, which can be used to quantify and evaluate Post-editing effort after automatic MT.\n\nSource: [https://arxiv.org/abs/1910.06204](https://arxiv.org/abs/1910.06204)", "variants": ["IWSLT 2019"], "title": "Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of MT quality"}
{"id": "CDCP", "contents": "The Cornell eRulemaking Corpus – CDCP is an argument mining corpus annotated with argumentative structure information capturing the evaluability of arguments. The corpus consists of 731 user comments on Consumer Debt Collection Practices (CDCP) rule by the Consumer Financial Protection Bureau (CFPB); the resulting dataset contains 4931 elementary unit and 1221 support relation annotations. It is a resource for building argument mining systems that can not only extract arguments from unstructured text, but also identify what additional information is necessary\r\nfor readers to understand and evaluate a given argument. Immediate applications include providing real-time feedback to commenters, specifying which types of support for which propositions can be added to construct better-formed arguments.", "variants": ["CDCP"], "title": "A Corpus of eRulemaking User Comments for Measuring Evaluability of Arguments"}
{"id": "VQG", "contents": "**VQG** is a collection of datasets for visual question generation. VQG questions were collected by crowdsourcing the task on Amazon Mechanical Turk (AMT). The authors provided details on the prompt and the specific instructions for all the crowdsourcing tasks in this paper in the supplementary material. The prompt was successful at capturing nonliteral questions. Images were taken from the MSCOCO dataset.\r\n\r\nSource: [What BERT Sees: Cross-Modal Transfer for Visual Question Generation](https://arxiv.org/pdf/2002.10832v3.pdf)", "variants": ["Visual Question Generation", "VQG"], "title": "Generating Natural Questions About an Image"}
{"id": "D2City", "contents": "A large-scale comprehensive collection of dashcam videos collected by vehicles on DiDi's platform. D2-City contains more than 10000 video clips which deeply reflect the diversity and complexity of real-world traffic scenarios in China.\r\n\r\nSource: [D$^2$-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios](/paper/d2-city-a-large-scale-dashcam-video-dataset)", "variants": ["D2City"], "title": "D$^2$-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios"}
{"id": "JIGSAWS", "contents": "The **JHU-ISI Gesture and Skill Assessment Working Set** (**JIGSAWS**) is a surgical activity dataset for human motion modeling. The data was collected through a collaboration between The Johns Hopkins University (JHU) and Intuitive Surgical, Inc. (Sunnyvale, CA. ISI) within an IRB-approved study. The release of this dataset has been approved by the Johns Hopkins University IRB.   The dataset was captured using the da Vinci Surgical System from eight surgeons with different levels of skill performing five repetitions of three elementary surgical tasks on a bench-top model: suturing, knot-tying and needle-passing, which are standard components of most surgical skills training curricula. The JIGSAWS dataset consists of three components:\r\n\r\n* kinematic data: Cartesian positions, orientations, velocities, angular velocities and gripper angle describing the motion of the manipulators.\r\n* video data: stereo video captured from the endoscopic camera. Sample videos of the JIGSAWS tasks can be downloaded from the official webpage.\r\n* manual annotations including:\r\n* gesture (atomic surgical activity segment labels).\r\n* skill (global rating score using modified objective structured assessments of technical skills).\r\n* experimental setup: a standardized cross-validation experimental setup that can be used to evaluate automatic surgical gesture recognition and skill assessment methods.\r\n\r\nSource: [https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release](https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release)\r\nImage Source: [https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release](https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release)", "variants": ["JIGSAWS"], "title": "Cross-Age Reference Coding for Age-Invariant Face Recognition and Retrieval"}
{"id": "MineRL", "contents": "**MineRL**is an imitation learning dataset with over 60 million frames of recorded human player data. The dataset includes a set of tasks which highlights many of the hardest problems in modern-day Reinforcement Learning: sparse rewards and hierarchical policies.\r\n\r\nSource: [MineRL](https://minerl.io/)", "variants": ["MineRL"], "title": "The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors"}
{"id": "MSC", "contents": "**MSC** is a dataset for Macro-Management in StarCraft 2 based on the platfrom SC2LE. It consists of well-designed feature vectors, pre-defined high-level actions and final result of each match. It contains 36,619 high quality replays, which are unbroken and played by relatively professional players.\n\nSource: [https://github.com/wuhuikai/MSC](https://github.com/wuhuikai/MSC)", "variants": ["MSC"], "title": "MSC: A Dataset for Macro-Management in StarCraft II"}
{"id": "CrowdPose", "contents": "The **CrowdPose** dataset contains about 20,000 images and a total of 80,000 human poses with 14 labeled keypoints. The test set includes 8,000 images. The crowded images containing homes are extracted from MSCOCO, MPII and AI Challenger.\r\n\r\nSource: [Human Pose Estimation for Real-World Crowded Scenarios](https://arxiv.org/abs/1907.06922)\r\nImage Source: [https://github.com/Jeff-sjtu/CrowdPose](https://github.com/Jeff-sjtu/CrowdPose)", "variants": ["CrowdPose"], "title": "CrowdPose: Efficient Crowded Scenes Pose Estimation and a New Benchmark"}
{"id": "SIDD", "contents": "SIDD is an image denoising dataset containing 30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras. Ground truth images are provided along with the noisy images.\r\n\r\nSource: [A High-Quality Denoising Dataset for Smartphone Cameras](/paper/a-high-quality-denoising-dataset-for)", "variants": ["SIDD"], "title": "A High-Quality Denoising Dataset for Smartphone Cameras"}
{"id": "ExtremeWeather", "contents": "Encourages machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change.\r\n\r\nSource: [ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events](https://arxiv.org/pdf/1612.02095v2.pdf)", "variants": ["ExtremeWeather"], "title": "Semi-Supervised Detection of Extreme Weather Events in Large Climate Datasets"}
{"id": "FM-IQA", "contents": "**FM-IQA** is a question-answering dataset containing over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations.\r\n\r\nSource: [Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering](/paper/are-you-talking-to-a-machine-dataset-and)", "variants": ["FM-IQA"], "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering"}
{"id": "ActivityNet-QA", "contents": "The ActivityNet-QA dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benchmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning.\r\n\r\nSource: [ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering](/paper/activitynet-qa-a-dataset-for-understanding)", "variants": ["ActivityNet-QA"], "title": "ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering"}
{"id": "TupleInf Open IE Dataset", "contents": "The TupleInf Open IE dataset contains Open IE tuples extracted from 263K sentences that were used by the solver in “Answering Complex Questions Using Open Information Extraction” (referred as Tuple KB, T). These sentences were collected from a large Web corpus using training questions from 4th and 8th grade as queries. This dataset contains 156K sentences collected for 4th grade questions and 107K sentences for 8th grade questions. Each sentence is followed by the Open IE v4 tuples using their simple format.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/tuple-ie)", "variants": ["TupleInf Open IE Dataset"], "title": "Answering Complex Questions Using Open Information Extraction"}
{"id": "RAF-DB", "contents": "The **Real-world Affective Faces** Database (**RAF-DB**) is a dataset for facial expression. It contains 29672 facial images tagged with basic or compound expressions by 40 independent taggers. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc.\r\n\r\nSource: [Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition](https://arxiv.org/abs/2007.10298)\r\nImage Source: [http://www.whdeng.cn/raf/model1.html](http://www.whdeng.cn/raf/model1.html)", "variants": ["Real-World Affective Faces", "RAF-DB"], "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild"}
{"id": "CapGaze", "contents": "Consists of eye movements and verbal descriptions recorded synchronously over images.\r\n\r\nSource: [Human Attention in Image Captioning: Dataset and Analysis](/paper/a-synchronized-multi-modal-attention-caption)", "variants": ["CapGaze"], "title": "Human Attention in Image Captioning: Dataset and Analysis"}
{"id": "LitBank", "contents": "LitBank is an annotated dataset of 100 works of English-language fiction to support tasks in natural language processing and the computational humanities, described in more detail in the following publications:\r\n\r\n- David Bamman, Sejal Popat and Sheng Shen (2019), \"An Annotated Dataset of Literary Entities,\" NAACL 2019.\r\n- Matthew Sims, Jong Ho Park and David Bamman (2019), \"Literary Event Detection,\" ACL 2019.\r\n- David Bamman, Olivia Lewke and Anya Mansoor (2020), \"An Annotated Dataset of Coreference in English Literature\", LREC.\r\n\r\nLitBank currently contains annotations for entities, events, entity coreference, and quotation attribution in a sample of ~2,000 words from each of those texts, totaling 210,532 tokens.\r\n\r\nLitBank is licensed under a Creative Commons Attribution 4.0 International License.", "variants": ["LitBank"], "title": "An Annotated Dataset of Literary Entities"}
{"id": "FAKBAT", "contents": "The Freebase Annotations of TREC KBA 2014 Stream Corpus with Timestamps (**FAKBAT**) is an extension of the FAKBA1 dataset that contains entity age and entity timestamp. It comprises roughly 1.2 billion timestamped documents from global public news wires, blogs, forums, and shortened links shared on social media. It spans 572 days (October 7, 2011–May 1, 2013).\n\nSource: [https://arxiv.org/pdf/1701.04039.pdf](https://arxiv.org/pdf/1701.04039.pdf)", "variants": ["FAKBAT"], "title": "The Birth of Collective Memories: Analyzing Emerging Entities in Text Streams"}
{"id": "UIEB", "contents": "Includes 950 real-world underwater images, 890 of which have the corresponding reference images.\r\n\r\nSource: [An Underwater Image Enhancement Benchmark Dataset and Beyond](/paper/an-underwater-image-enhancement-benchmark)", "variants": ["UIEB"], "title": "An Underwater Image Enhancement Benchmark Dataset and Beyond"}
{"id": "HoME", "contents": "HoME (Household Multimodal Environment) is a multimodal environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more.", "variants": ["HoME"], "title": "HoME: a Household Multimodal Environment"}
{"id": "FLIC", "contents": "The **FLIC** dataset contains 5003 images from popular Hollywood movies. The images were obtained by running a state-of-the-art person detector on every tenth frame of 30 movies. People detected with high confidence (roughly 20K candidates) were then sent to the crowdsourcing marketplace Amazon Mechanical Turk to obtain ground truth labelling. Each image was annotated by five Turkers to label 10 upper body joints. The median-of-five labelling was taken in each image to be robust to outlier annotation. Finally, images were rejected manually by if the person was occluded or severely non-frontal.\r\n\r\nSource: [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html)\r\nImage Source: [https://www.tensorflow.org/datasets/catalog/flic](https://www.tensorflow.org/datasets/catalog/flic)", "variants": ["FLIC Elbows", "FLIC Wrists", "FLIC"], "title": "MODEC: Multimodal Decomposable Models for Human Pose Estimation"}
{"id": "BuzzFeed-Webis Fake News Corpus 2016", "contents": "The BuzzFeed-Webis Fake News Corpus 16 comprises the output of 9 publishers in a week close to the US elections. Among the selected publishers are 6 prolific hyperpartisan ones (three left-wing and three right-wing), and three mainstream publishers (see Table 1). All publishers earned Facebook’s blue checkmark, indicating authenticity and an elevated status within the network. For seven weekdays (September 19 to 23 and September 26 and 27), every post and linked news article of the 9 publishers was fact-checked by professional journalists at BuzzFeed. In total, 1,627 articles were checked, 826 mainstream, 256 left-wing and 545 right-wing. The imbalance between categories results from differing publication frequencies.", "variants": ["BuzzFeed-Webis Fake News Corpus 2016"], "title": "A Stylometric Inquiry into Hyperpartisan and Fake News"}
{"id": "MLQA", "contents": "MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance. MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between 4 different languages on average.\r\n\r\nSource: [Facebook Research](https://github.com/facebookresearch/mlqa)\r\nImage Source: [https://github.com/facebookresearch/mlqa](https://github.com/facebookresearch/mlqa)", "variants": ["MLQA"], "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering"}
{"id": "TLP", "contents": "A new long video dataset and benchmark for single object tracking. The dataset consists of 50 HD videos from real world scenarios, encompassing a duration of over 400 minutes (676K frames), making it more than 20 folds larger in average duration per sequence and more than 8 folds larger in terms of total covered duration, as compared to existing generic datasets for visual tracking.\r\n\r\nSource: [Long-Term Visual Object Tracking Benchmark](/paper/long-term-visual-object-tracking-benchmark)", "variants": ["TLP"], "title": "Long-Term Visual Object Tracking Benchmark"}
{"id": "OpenDialKG", "contents": "OpenDialKG contains utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. \r\n\r\nSource: [OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs](/paper/opendialkg-explainable-conversational)", "variants": ["OpenDialKG"], "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs"}
{"id": "OSTD", "contents": "This dataset consists of 18 movies with duration range between 10 and 104 minutes leveraged from the OVSD dataset (Rotman et al., 2016). For these videos, the summary length limit is set to be the minimum between 4 minutes and 10% of the video length.\r\n\r\nSource: [ILS-SUMM: ITERATED LOCAL SEARCH FOR UNSUPERVISED VIDEO SUMMARIZATION](https://arxiv.org/pdf/1912.03650.pdf)", "variants": ["OSTD"], "title": "ILS-SUMM: Iterated Local Search for Unsupervised Video Summarization"}
{"id": "INTERACTION Dataset", "contents": "The INTERACTION dataset contains naturalistic motions of various traffic participants in a variety of highly interactive driving scenarios from different countries. The dataset can serve for many behavior-related research areas, such as \r\n\r\n- 1) intention/behavior/motion prediction, \r\n- 2) behavior cloning and imitation learning,\r\n- 3) behavior analysis and modeling,\r\n- 4) motion pattern and representation learning,\r\n- 5) interactive behavior extraction and categorization,\r\n- 6) social and human-like behavior generation,\r\n- 7) decision-making and planning algorithm development and verification,\r\n- 8) driving scenario/case generation, etc. \r\n\r\nSource: [https://interaction-dataset.com/](https://interaction-dataset.com/)\r\nImage Source: [https://interaction-dataset.com/](https://interaction-dataset.com/)", "variants": ["INTERACTION Dataset"], "title": "INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps"}
{"id": "HolStep", "contents": "HolStep is a dataset based on higher-order logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. \r\n\r\nSource: [HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving](/paper/holstep-a-machine-learning-dataset-for-higher)", "variants": ["HolStep (Conditional)", "HolStep (Unconditional)", "HolStep"], "title": "HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving"}
{"id": "FigureQA", "contents": "FigureQA is a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. \r\n\r\nSource: [FigureQA: An Annotated Figure Dataset for Visual Reasoning](/paper/figureqa-an-annotated-figure-dataset-for)", "variants": ["FigureQA"], "title": "FigureQA: An Annotated Figure Dataset for Visual Reasoning"}
{"id": "PersonalDialog", "contents": "PersonalDialog is a large-scale multi-turn dialogue dataset containing various traits from a large number of speakers. The dataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers. Each utterance is associated with a speaker who is marked with traits like Age, Gender, Location, Interest Tags, etc. Several anonymization schemes are designed to protect the privacy of each speaker. \r\n\r\nSource: [Personalized Dialogue Generation with Diversified Traits](https://arxiv.org/abs/1901.09672)\r\nImage Source: [https://arxiv.org/pdf/1901.09672v2.pdf](https://arxiv.org/pdf/1901.09672v2.pdf)", "variants": ["PersonalDialog"], "title": "Personalized Dialogue Generation with Diversified Traits"}
{"id": "Holopix50k", "contents": "An in-the-wild stereo image dataset, comprising 49,368 image pairs contributed by users of the Holopix mobile social platform.\r\n\r\nSource: [Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset](/paper/holopix50k-a-large-scale-in-the-wild-stereo-1)", "variants": ["Holopix50k"], "title": "Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset"}
{"id": "SUN09", "contents": "The **SUN09** dataset consists of 12,000 annotated images with more than 200 object categories. It consists of natural, indoor and outdoor images. Each image contains an average of 7 different annotated objects and the average occupancy of each object is 5% of image size. The frequencies of object categories follow a power law distribution.\r\n\r\nSource: [A Pooling Approach to Modelling Spatial Relations forImage Retrieval and Annotation](https://arxiv.org/abs/1411.5190)\r\nImage Source: [http://people.csail.mit.edu/myungjin/HContext.html](http://people.csail.mit.edu/myungjin/HContext.html)", "variants": ["SUN09"], "title": "Exploiting hierarchical context on a large database of object categories"}
{"id": "ELAS", "contents": "ELAS is a dataset for lane detection. It contains more than 20 different scenes (in more than 15,000 frames) and considers a variety of scenarios (urban road, highways, traffic, shadows, etc.). The dataset was manually annotated for several events that are of interest for the research community (i.e., lane estimation, change, and centering; road markings; intersections; LMTs; crosswalks and adjacent lanes).\r\n\r\nSource: [Ego-Lane Analysis System (ELAS): Dataset and Algorithms](https://arxiv.org/abs/1806.05984)", "variants": ["ELAS"], "title": "Ego-Lane Analysis System (ELAS): Dataset and Algorithms"}
{"id": "3DPW", "contents": "The **3D Poses in the Wild dataset** is the first dataset in the wild with accurate 3D poses for evaluation. While other datasets outdoors exist, they are all restricted to a small recording volume. 3DPW is the first one that includes video footage taken from a moving phone camera.\r\n\r\nThe dataset includes:\r\n\r\n* 60 video sequences.\r\n* 2D pose annotations.\r\n* 3D poses obtained with the method introduced in the paper.\r\n* Camera poses for every frame in the sequences.\r\n* 3D body scans and 3D people models (re-poseable and re-shapeable). Each sequence contains its corresponding models.\r\n* 18 3D models in different clothing variations.\r\n\r\nSource: [https://virtualhumans.mpi](http://virtualhumans.mpi-inf.mpg.de/3DPW)\r\nImage Source: [https://virtualhumans.mpi](http://virtualhumans.mpi-inf.mpg.de/3DPW)", "variants": ["3DPW"], "title": "Recovering Accurate 3D Human Pose in the Wild Using IMUs and a Moving Camera"}
{"id": "Stanford Cars", "contents": "The **Stanford Cars** dataset consists of 196 classes of cars with a total of 16,185 images, taken from the rear. The data is divided into almost a 50-50 train/test split with 8,144 training images and 8,041 testing images. Categories are typically at the level of Make, Model, Year. The images are 360×240.\r\n\r\nSource: [View Independent Vehicle Make, Model and Color Recognition Using Convolutional Neural Network](https://arxiv.org/abs/1702.01721)\r\nImage Source: [https://ai.stanford.edu/~jkrause/cars/car_dataset.html](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)", "variants": ["Stanford Cars", "Stanford Cars 5-way (1-shot)", "Stanford Cars 5-way (5-shot)", "Stanford Cars (Fine-grained 6 Tasks)"], "title": "3D Object Representations for Fine-Grained Categorization"}
{"id": "MatterportLayout", "contents": "MatterportLayout extends the Matterport3D dataset with general Manhattan layout annotations. It has 2,295 RGBD panoramic images from Matterport3D which are extended with ground truth 3D layouts.\r\n\r\nSource: [https://github.com/ericsujw/Matterport3DLayoutAnnotation](https://github.com/ericsujw/Matterport3DLayoutAnnotation)", "variants": ["MatterportLayout"], "title": "3D Manhattan Room Layout Reconstruction from a Single 360 Image"}
{"id": "Oxford RobotCar Dataset", "contents": "The Oxford RobotCar Dataset contains over 100 repetitions of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different combinations of weather, traffic and pedestrians, along with longer term changes such as construction and roadworks.\r\n\r\nSource: [Real-time Kinematic Ground Truth for the Oxford RobotCar Dataset](/paper/real-time-kinematic-ground-truth-for-the)", "variants": ["Oxford RobotCar Dataset"], "title": "Real-time Kinematic Ground Truth for the Oxford RobotCar Dataset"}
{"id": "Flickr1024", "contents": "Contains 1024 pairs of high-quality images and covers diverse scenarios.\r\n\r\nSource: [Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution](/paper/flickr1024-a-dataset-for-stereo-image-super)", "variants": ["Flickr1024"], "title": "Flickr1024: A Dataset for Stereo Image Super-Resolution"}
{"id": "DEMAND", "contents": "The **DEMAND** (Diverse Environments Multichannel Acoustic Noise Database) provides a set of recordings that allow testing of algorithms using real-world noise in a variety of settings. This version provides 15 recordings. All recordings are made with a 16-channel array, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm.\n\nSource: [DEMAND: a collection of multi-channel recordings of acoustic noise in diverse environments](https://zenodo.org/record/1227121)\nImage Source: [https://asa.scitation.org/doi/pdf/10.1121/1.4799597](https://asa.scitation.org/doi/pdf/10.1121/1.4799597)", "variants": ["DEMAND"], "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks"}
{"id": "RMFD", "contents": "**Real-World Masked Face Dataset** (**RMFD**) is a large dataset for masked face detection.\n\nSource: [https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset](https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset)\nImage Source: [https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset](https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset)", "variants": ["RMFD"], "title": "Masked Face Recognition Dataset and Application"}
{"id": "PASCAL-5i", "contents": "**PASCAL-5i** is a dataset used to evaluate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples from 5 classes that are used for evaluating the few-shot learning method. The rest 15 classes are used for training.\r\n\r\nSource: [AMP: Adaptive Masked Proxies for Few-Shot Segmentation](https://arxiv.org/abs/1902.11123)\r\nImage Source: [https://arxiv.org/pdf/1709.03410.pdf](https://arxiv.org/pdf/1709.03410.pdf)", "variants": ["Pascal5i", "PASCAL-5i"], "title": "One-Shot Learning for Semantic Segmentation"}
{"id": "KIT Motion-Language", "contents": "The KIT Motion-Language is a dataset linking human motion and natural language.\r\n\r\nSource: [The KIT Motion-Language Dataset](/paper/the-kit-motion-language-dataset)", "variants": ["KIT Motion-Language"], "title": "The KIT Motion-Language Dataset"}
{"id": "CHiME-5", "contents": "The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing , and machine learning.\r\n\r\nSource: [The fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, task and baselines](https://arxiv.org/pdf/1803.10609v1.pdf)", "variants": ["CHiME-5"], "title": "The fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, task and baselines"}
{"id": "UCF-CC-50", "contents": "**UCF-CC-50** is a dataset for crowd counting and consists of images of extremely dense crowds. It has 50 images with 63,974 head center annotations in total. The head counts range between 94 and 4,543 per image. The small dataset size and large variance make this a very challenging counting dataset.\r\n\r\nSource: [Active Crowd Counting with Limited Supervision](https://arxiv.org/abs/2007.06334)", "variants": ["UCF-CC-50"], "title": "Multi-source Multi-scale Counting in Extremely Dense Crowd Images"}
{"id": "GAP Coreference Dataset", "contents": "GAP is a gender-balanced dataset containing 8,908 coreference-labeled pairs of (ambiguous pronoun, antecedent name), sampled from Wikipedia and released by Google AI Language for the evaluation of coreference resolution in practical applications.\r\n\r\nSource: [GAP Coreference Dataset](https://github.com/google-research-datasets/gap-coreference)\r\nImage Source: [https://arxiv.org/pdf/1810.05201.pdf](https://arxiv.org/pdf/1810.05201.pdf)", "variants": ["GAP", "GAP Coreference Dataset"], "title": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns"}
{"id": "Synscapes", "contents": "Synscapes is a synthetic dataset for street scene parsing created using photorealistic rendering techniques, and show state-of-the-art results for training and validation as well as new types of analysis. \r\n\r\nSource: [Synscapes: A Photorealistic Synthetic Dataset for Street Scene Parsing](https://arxiv.org/pdf/1810.08705v1.pdf)\r\nImage Source: [https://7dlabs.com/synscapes-overview](https://7dlabs.com/synscapes-overview)", "variants": ["Synscapes to BDD100K", "Synscapes"], "title": "Synscapes: A Photorealistic Synthetic Dataset for Street Scene Parsing"}
{"id": "Metaphorical Connections", "contents": "The **Metaphorical Connections** dataset is a poetry dataset that contains annotations between metaphorical prompts and short poems. Each poem is annotated whether or not it successfully communicates the idea of the metaphorical prompt.\n\nSource: [https://github.com/kgero/metaphorical-connections](https://github.com/kgero/metaphorical-connections)", "variants": ["Metaphorical Connections"], "title": "Challenges in Finding Metaphorical Connections"}
{"id": "Egyptian Arabic Segmentation Dataset", "contents": "Contains 350 tweets with more than 8,000 words including 3,000 unique words written in Egyptian dialect. The tweets have much dialectal content covering most of dialectal Egyptian phonological, morphological, and syntactic phenomena. It also includes Twitter-specific aspects of the text, such as #hashtags, @mentions, emoticons and URLs.\r\n\r\nSource: [A Neural Architecture for Dialectal Arabic Segmentation](/paper/a-neural-architecture-for-dialectal-arabic)", "variants": ["Egyptian Arabic Segmentation Dataset"], "title": "A Neural Architecture for Dialectal Arabic Segmentation"}
{"id": "TutorialBank", "contents": "TutorialBank is a publicly available dataset which aims to facilitate NLP education and research. The dataset consists of links to over 6,300 high-quality resources on NLP and related fields. The corpus’s magnitude, manual collection and focus on annotation for education in addition to research differentiates it from other corpora.\r\n\r\nSource: [TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation](/paper/tutorialbank-a-manually-collected-corpus-for)\r\nImage Source: [http://aan.how/browse/resources/](http://aan.how/browse/resources/)", "variants": ["TutorialBank"], "title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation"}
{"id": "NES-MDB", "contents": "The **Nintendo Entertainment System Music Database** (**NES-MDB**) is a dataset intended for building automatic music composition systems for the NES audio synthesizer. It consists of  5278 songs from the soundtracks of 397 NES games. The dataset represents 296 unique composers, and the songs contain more than two million notes combined. It has file format options for MIDI, score and NLM (NES Language Modeling).\r\n\r\nSource: [https://github.com/chrisdonahue/nesmdb](https://github.com/chrisdonahue/nesmdb)\r\nImage Source: [https://github.com/chrisdonahue/nesmdb](https://github.com/chrisdonahue/nesmdb)\r\nAudio Source: [https://github.com/chrisdonahue/nesmdb](https://github.com/chrisdonahue/nesmdb)", "variants": ["NES-MDB"], "title": "The NES Music Database: A multi-instrumental dataset with expressive performance attributes"}
{"id": "DOTmark", "contents": "DOTmark is a benchmark for discrete optimal transport, which is designed to serve as a neutral collection of problems, where discrete optimal transport methods can be tested, compared to one another, and brought to their limits on large-scale instances. It consists of a variety of grayscale images, in various resolutions and classes, such as several types of randomly generated images, classical test images and real data from microscopy.\r\n\r\nSource: [DOTmark - A Benchmark for Discrete Optimal Transport](/paper/dotmark-a-benchmark-for-discrete-optimal)", "variants": ["DOTmark"], "title": "DOTmark - A Benchmark for Discrete Optimal Transport"}
{"id": "EuroCity Persons", "contents": "The EuroCity Persons dataset provides a large number of highly diverse, accurate and detailed annotations of pedestrians, cyclists and other riders in urban traffic scenes. The images for this dataset were collected on-board a moving vehicle in 31 cities of 12 European countries. With over 238,200 person instances manually labeled in over 47,300 images, EuroCity Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. The dataset furthermore contains a large number of person orientation annotations (over 211,200).", "variants": ["EuroCity Persons"], "title": "The EuroCity Persons Dataset: A Novel Benchmark for Object Detection"}
{"id": "DispScenes", "contents": "The **DispScenes** dataset was created to address the specific problem of disparate image matching. The image pairs in all the datasets exhibit high levels of variation in illumination and viewpoint and also contain instances of occlusion. The DispScenes dataset provides manual ground truth keypoint correspondences for all images.\n\nSource: [Matching Disparate Image Pairs Using Shape-Aware ConvNets](https://arxiv.org/abs/1811.09889)", "variants": ["DispScenes"], "title": "Deep Spectral Correspondence for Matching Disparate Image Pairs"}
{"id": "ETHZ-Shape", "contents": "The ETHZ Shape dataset contains images of five diverse shape-based classes, collected from Flickr and Google Images. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The authors deliberately selected several images where the object comprises only a rather small portion of the image, and made an effort to include objects appearing at a wide range of scales. The objects are mostly unoccluded and are all taken from approximately the same viewpoint (the side).\n\nSource: [http://calvin-vision.net/datasets/ethz-shape-classes/](http://calvin-vision.net/datasets/ethz-shape-classes/)\nImage Source: [http://calvin-vision.net/datasets/ethz-shape-classes/](http://calvin-vision.net/datasets/ethz-shape-classes/)", "variants": ["ETHZ-Shape"], "title": "Groups of Adjacent Contour Segments for Object Detection"}
{"id": "MC-AFP", "contents": "A dataset of around 2 million examples for machine reading-comprehension.\r\n\r\nSource: [Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors](/paper/building-large-machine-reading-comprehension)", "variants": ["MC-AFP"], "title": "Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors"}
{"id": "Chinese Text in the Wild", "contents": "Chinese Text in the Wild is a dataset of Chinese text with about 1 million Chinese characters from 3850 unique ones annotated by experts in over 30000 street view images. This is a challenging dataset with good diversity containing planar text, raised text, text under poor illumination, distant text, partially occluded text, etc.", "variants": ["Chinese Text in the Wild"], "title": "Chinese Text in the Wild"}
{"id": "TRN", "contents": "The Toulouse Road Network dataset describes patches of road maps from the city of Toulouse, represented both as spatial graphs G = (A, X) and as grayscale segmentation images. \r\n\r\nThe TRN dataset contains 111,034 data points (map tiles), of which: 80,357 are in the training set (around 72.4%), 11,679 are in the validation set (around 10.5%), 18,998 are in the test set (around 17.1%). \r\n  \r\nEach tile represents a squared region of side 0.001 degrees of latitude and longitude on the map, which corresponds to a square of around 110 meters. The semantic segmentation of each patch is represented as a 64 × 64 grayscale image. \r\n\r\nThe dataset is generated starting from publicly available data from OpenStreetMap. More details on the dataset characteristic and generation methods are available in our [blogpost](https://davide-belli.github.io/toulouse-road-network.html).\r\n\r\nSource: [https://github.com/davide-belli/toulouse-road-network-dataset](https://github.com/davide-belli/toulouse-road-network-dataset)\r\nImage Source: [https://arxiv.org/pdf/1910.14388.pdf](https://arxiv.org/pdf/1910.14388.pdf)", "variants": ["Toulouse Road Network", "TRN"], "title": "Image-Conditioned Graph Generation for Road Network Extraction"}
{"id": "VQA-HAT", "contents": "VQA-HAT (Human ATtention) is a dataset to evaluate the informative regions of an image depending on the question being asked about it. The dataset consists of human visual attention maps over the images in the original VQA dataset. It contains more than 60k attention maps.\r\n\r\nSource: [Human Attention in Visual Question Answering:](https://computing.ece.vt.edu/~abhshkdz/vqa-hat/)", "variants": ["VQA-HAT"], "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?"}
{"id": "CARD-660", "contents": "An expert-annotated word similarity dataset which provides a highly reliable, yet challenging, benchmark for rare word representation techniques. \r\n\r\nSource: [Card-660: Cambridge Rare Word Dataset - a Reliable Benchmark for Infrequent Word Representation Models](/paper/card-660-cambridge-rare-word-dataset-a)", "variants": ["CARD-660"], "title": "Card-660: Cambridge Rare Word Dataset - a Reliable Benchmark for Infrequent Word Representation Models"}
{"id": "ISI_Bengali_Character", "contents": "The **ISI_Bengali_Character** dataset contains 158 classes of Bengali numerals, characters or their parts. 19,530 Bengali character samples are available. Most of the images in the dataset are synthesized.\n\nSource: [Boosting Scene Character Recognition by Learning Canonical Forms of Glyphs](https://arxiv.org/abs/1907.05577)\nImage Source: [https://www.isical.ac.in/~ujjwal/download/SegmentedSceneCharacter.html](https://www.isical.ac.in/~ujjwal/download/SegmentedSceneCharacter.html)", "variants": ["ISI_Bengali_Character"], "title": "Multilingual scene character recognition with co-occurrence of histogram of oriented gradients"}
{"id": "Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison", "contents": "The Evaluation framework of Raganato et al. 2017 includes two training sets (SemCor-Miller et al., 1993- and OMSTI-Taghipour and Ng, 2015-) and five test sets from the Senseval/SemEval series (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2013; Moro and Navigli, 2015), standardized to the same format and sense inventory (i.e. WordNet 3.0).\r\n\r\nTypically, there are two kinds of approach for WSD: supervised (which make use of sense-annotated training data) and knowledge-based (which make use of the properties of lexical resources).\r\n\r\n**Supervised:** The most widely used training corpus used is SemCor, with 226,036 sense annotations from 352 documents manually annotated. All supervised systems in the evaluation table are trained on SemCor. Some supervised methods, particularly neural architectures, usually employ the SemEval 2007 dataset as development set (marked by *). The most usual baseline is the Most Frequent Sense (MFS) heuristic, which selects for each target word the most frequent sense in the training data.\r\n\r\n**Knowledge-based:** Knowledge-based systems usually exploit WordNet or BabelNet as semantic network. The first sense given by the underlying sense inventory (i.e. WordNet 3.0) is included as a baseline.\r\n\r\nDescription from [NLP Progress](http://nlpprogress.com/english/word_sense_disambiguation.html)", "variants": ["Knowledge-based:", "Supervised:", "Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison"], "title": "Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison"}
{"id": "Curated AFD", "contents": "The **Curated AFD** dataset is a curated version of the Asian Face Dataset (AFD) for face recognition research. The original AFD dataset has been curated to remove wrong identity labels, duplicate images and duplicate subjects.\n\nSource: [https://arxiv.org/abs/2004.03074](https://arxiv.org/abs/2004.03074)", "variants": ["Curated AFD"], "title": "A Method for Curation of Web-Scraped Face Image Datasets"}
{"id": "Gumar Corpus", "contents": "A large-scale corpus of Gulf Arabic consisting of 110 million words from 1,200 forum novels.\r\n\r\nSource: [A Large Scale Corpus of Gulf Arabic](/paper/a-large-scale-corpus-of-gulf-arabic)", "variants": ["Gumar Corpus"], "title": "A Large Scale Corpus of Gulf Arabic"}
{"id": "CosmosQA", "contents": "CosmosQA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people’s everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context.\r\n\r\nSource: [Teaching Pretrained Models with Commonsense Reasoning: A Preliminary KB-Based Approach](https://arxiv.org/abs/1909.09743)\r\nImage Source: [https://arxiv.org/pdf/1909.00277.pdf](https://arxiv.org/pdf/1909.00277.pdf)", "variants": ["CosmosQA"], "title": "Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning"}
{"id": "LIP", "contents": "The **LIP** (**Look into Person**) dataset is a large-scale dataset focusing on semantic understanding of a person. It contains 50,00 images with elaborated pixel-wise annotations of 19 semantic human part labels and 2D human poses with 16 key points. The images are collected from real-world scenarios and the subjects appear with challenging poses and view, heavy occlusions, various appearances and low resolution.\r\n\r\nSource: [http://sysu-hcp.net/lip/](http://sysu-hcp.net/lip/)\r\nImage Source: [http://sysu-hcp.net/lip/](http://sysu-hcp.net/lip/)", "variants": ["LIP val", "LIP"], "title": "Look into Person: Self-Supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing"}
{"id": "KorQuAD", "contents": "KorQuAD is a large-scale question-and-answer dataset constructed for Korean machine reading comprehension, and investigate the dataset to understand the distribution of answers and the types of reasoning required to answer the question. This dataset benchmarks the data generating process of SQuAD to meet the standard.", "variants": ["KorQuAD"], "title": "KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension"}
{"id": "IDD", "contents": "IDD is a dataset for road scene understanding in unstructured environments used for semantic segmentation and object detection for autonomous driving. It consists of 10,004 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads. \r\n\r\nSource: [IDD: A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments](/paper/idd-a-dataset-for-exploring-problems-of)\r\nImage Source: [Varma et al](https://arxiv.org/pdf/1811.10200v1.pdf)", "variants": ["IDD"], "title": "IDD: A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments"}
{"id": "GOZ", "contents": "The Generix Object Zero-shot Learning (**GOZ**) dataset is a benchmark dataset for zero-shot learning.\n\nSource: [https://github.com/TristHas/GOZ](https://github.com/TristHas/GOZ)", "variants": ["GOZ"], "title": "On Zero-Shot Recognition of Generic Objects"}
{"id": "StereoMSI", "contents": "StereoMSI comprises of 350 registered colour-spectral image pairs. The dataset has been used for the two tracks of the PIRM2018 challenge.\r\n\r\nSource: [PIRM2018 Challenge on Spectral Image Super-Resolution: Dataset and Study](/paper/pirm2018-challenge-on-spectral-image-super)\r\nImage Source: [Shoeiby et al](https://arxiv.org/pdf/1904.00540v2.pdf)", "variants": ["StereoMSI"], "title": "PIRM2018 Challenge on Spectral Image Super-Resolution: Dataset and Study"}
{"id": "EgoShots", "contents": "Egoshots is a 2-month Ego-vision Dataset with Autographer Wearable Camera annotated \"for free\" with transfer learning. Three state of the art pre-trained image captioning models are used. The dataset represents the life of 2 interns while working at Philips Research (Netherlands) (May-July 2015) generously donating their data.\n\nSource: [https://github.com/NataliaDiaz/Egoshots](https://github.com/NataliaDiaz/Egoshots)", "variants": ["EgoShots"], "title": "Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models"}
{"id": "BC5CDR", "contents": "**BC5CDR** corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions.\r\n\r\nSource: [https://www.ncbi.nlm.nih.gov/research/bionlp/Data/](https://www.ncbi.nlm.nih.gov/research/bionlp/Data/)\r\nImage Source: [https://arxiv.org/pdf/1805.10586.pdf](https://arxiv.org/pdf/1805.10586.pdf)", "variants": ["BC5CDR", "BC5CDR-disease", "BC5CDR-chemical"], "title": "BioCreative V CDR task corpus: a resource for chemical disease relation extraction"}
{"id": "BASIL", "contents": "300 news articles annotated with 1,727 bias spans and find evidence that informational bias appears in news articles more frequently than lexical bias.\r\n\r\nSource: [In Plain Sight: Media Bias Through the Lens of Factual Reporting](/paper/in-plain-sight-media-bias-through-the-lens-of)", "variants": ["BASIL"], "title": "In Plain Sight: Media Bias Through the Lens of Factual Reporting"}
{"id": "PubMed RCT", "contents": "**PubMed 200k RCT** is new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: the authors hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field.\r\n\r\nSource: [GitHub](https://github.com/Franck-Dernoncourt/pubmed-rct)\nImage Source: [https://arxiv.org/pdf/1710.06071.pdf](https://arxiv.org/pdf/1710.06071.pdf)", "variants": ["PubMed RCT"], "title": "PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts"}
{"id": "Oxford-IIIT Pets", "contents": "The Oxford-IIIT Pet Dataset has 37 categories with roughly 200 images for each class. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation.\r\n\r\nSource: [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)", "variants": ["Oxford-IIIT Pets"], "title": "Cats and dogs"}
{"id": "Salient Closed Boundary Tracking", "contents": "This dataset contains nine video sequences captured by a webcam for salient closed boundary tracking evaluation. Each sequence is about 30 sec (30 fps) and the frame size is 640×480 (width×height). There are 9598 frames in total. In each sequence, different motion styles such as translation, rotation and viewpoint changing are all performed.\n\nSource: [https://github.com/NathanUA/SalientClosedBoundaryTrackingDataset](https://github.com/NathanUA/SalientClosedBoundaryTrackingDataset)\nImage Source: [https://github.com/NathanUA/SalientClosedBoundaryTrackingDataset](https://github.com/NathanUA/SalientClosedBoundaryTrackingDataset)", "variants": ["Salient Closed Boundary Tracking"], "title": "Real-time salient closed boundary tracking via line segments perceptual grouping"}
{"id": "UFPR-Eyeglasses", "contents": "The UFPR-Eyeglasses dataset has 1,135 images of both eyes (2,270 cropped images of each eye) from 83 subjects (166 classes). The dataset is used to evaluate the effect of the occlusion caused by eyeglasses in periocular recognition.\r\n\r\nSource: [Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks for Attribute Normalization](https://arxiv.org/abs/2002.03985)", "variants": ["UFPR-Eyeglasses"], "title": "Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks for Attribute Normalization"}
{"id": "Multimodal Opinionlevel Sentiment Intensity", "contents": "Multimodal Opinionlevel Sentiment Intensity (MOSI) contains: (1) multimodal observations including transcribed speech and visual gestures as well as automatic audio and visual features, (2) opinion-level subjectivity segmentation, (3) sentiment intensity annotations with high coder agreement, and (4) alignment between words, visual and acoustic features.\r\n\r\nSource: [Zadeh et al](https://arxiv.org/pdf/1606.06259.pdf)\r\n\r\nImage source: [Zadeh et al](https://arxiv.org/pdf/1606.06259.pdf)", "variants": ["MOSI", "Multimodal Opinionlevel Sentiment Intensity"], "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos"}
{"id": "Bach Doodle", "contents": "The **Bach Doodle** Dataset is composed of 21.6 million harmonizations submitted from the Bach Doodle. The dataset contains both metadata about the composition (such as the country of origin and feedback), as well as a MIDI of the user-entered melody and a MIDI of the generated harmonization. The dataset contains about 6 years of user entered music.\n\nSource: [https://magenta.tensorflow.org/datasets/bach-doodle](https://magenta.tensorflow.org/datasets/bach-doodle)\nImage Source: [https://magenta.tensorflow.org/datasets/bach-doodle](https://magenta.tensorflow.org/datasets/bach-doodle)", "variants": ["Bach Doodle"], "title": "The Bach Doodle: Approachable music composition with machine learning at scale"}
{"id": "SatStereo", "contents": "Provides a set of stereo-rectified images and the associated groundtruthed disparities for 10 AOIs (Area of Interest) drawn from two sources: 8 AOIs from IARPA's MVS Challenge dataset and 2 AOIs from the CORE3D-Public dataset.\r\n\r\nSource: [A New Stereo Benchmarking Dataset for Satellite Images](/paper/a-new-stereo-benchmarking-dataset-for)", "variants": ["SatStereo"], "title": "A New Stereo Benchmarking Dataset for Satellite Images"}
{"id": "Taskmaster-1", "contents": "**Taskmaster-1** is a dialog dataset consisting of 13,215 task-based dialogs in English, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.\r\n\r\nImage Source: [https://arxiv.org/pdf/1909.05358v1.pdf](https://arxiv.org/pdf/1909.05358v1.pdf)", "variants": ["Taskmaster-1"], "title": "Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset"}
{"id": "HICO-DET", "contents": "**HICO-DET** is a dataset for detecting human-object interactions (HOI) in images. It contains 47,776 images (38,118 in train set and 9,658 in test set), 600 HOI categories constructed by 80 object categories and 117 verb classes. HICO-DET provides more than 150k annotated human-object pairs. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person has annotations for 29 action categories and there are no interaction labels including objects.\r\n\r\nSource: [Visual Compositional Learning for Human-Object Interaction Detection](https://arxiv.org/abs/2007.12407)\r\nImage Source: [http://www-personal.umich.edu/~ywchao/hico/](http://www-personal.umich.edu/~ywchao/hico/)", "variants": ["HICO-DET"], "title": "Learning to Detect Human-Object Interactions"}
{"id": "MINOS", "contents": "**MINOS** is a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. MINOS leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites.", "variants": ["MINOS"], "title": "MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments"}
{"id": "TextVQA", "contents": "TextVQA is a dataset to benchmark visual reasoning based on text in images.\r\nTextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions.\r\n\r\nStatistics\r\n* 28,408 images from OpenImages\r\n* 45,336 questions\r\n* 453,360 ground truth answers", "variants": ["TextVQA Val", "TextVQA Test", "TextVQA test-standard", "TextVQA"], "title": "Towards VQA Models That Can Read"}
{"id": "MobiBits", "contents": "A novel database comprising representations of five different biometric characteristics, collected in a mobile, unconstrained or semi-constrained setting with three different mobile devices, including characteristics previously unavailable in existing datasets, namely hand images, thermal hand images, and thermal face images, all acquired with a mobile, off-the-shelf device.\r\n\r\nSource: [MobiBits: Multimodal Mobile Biometric Database](/paper/mobibits-multimodal-mobile-biometric-database)", "variants": ["MobiBits"], "title": "MobiBits: Multimodal Mobile Biometric Database"}
{"id": "InLoc", "contents": "InLoc is a dataset with reference 6DoF poses for large-scale indoor localization. Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario.\r\n\r\nSource: [InLoc: Indoor Visual Localization with Dense Matching and View Synthesis](/paper/inloc-indoor-visual-localization-with-dense)", "variants": ["InLoc"], "title": "InLoc: Indoor Visual Localization with Dense Matching and View Synthesis"}
{"id": "PartNet", "contents": "PartNet is a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. The dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others.\r\n\r\nSource: [PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding](/paper/partnet-a-large-scale-benchmark-for-fine)\r\nImage Source: [https://cs.stanford.edu/~kaichun/partnet/](https://cs.stanford.edu/~kaichun/partnet/)", "variants": ["PartNet"], "title": "PartNet: A Large-Scale Benchmark for Fine-Grained and Hierarchical Part-Level 3D Object Understanding"}
{"id": "DAWT", "contents": "The DAWT dataset consists of Densely Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of the entity. The data set contains total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally present in the Wikipedia markup. Moreover, it spans several languages including English, Spanish, Italian, German, French and Arabic. \r\n\r\nSource: [DAWT: Densely Annotated Wikipedia Texts across multiple languages](/paper/dawt-densely-annotated-wikipedia-texts-across)", "variants": ["DAWT"], "title": "DAWT: Densely Annotated Wikipedia Texts across multiple languages"}
{"id": "KenyanFood13", "contents": "The Kenyan Food Type Dataset (**KenyanFood13**) is an image classification dataset for Kenyan food. The images are categorized into 13 different labels.\n\nSource: [https://github.com/monajalal/Kenyan-Food](https://github.com/monajalal/Kenyan-Food)\nImage Source: [https://github.com/monajalal/Kenyan-Food](https://github.com/monajalal/Kenyan-Food)", "variants": ["KenyanFood13"], "title": "Scraping Social Media Photos Posted in Kenya and Elsewhere to Detect and Analyze Food Types"}
{"id": "Event2Mind", "contents": "Event2Mind is a corpus of 25,000 event phrases covering a diverse range of everyday events and situations.\r\n\r\nSource: [Event2Mind: Commonsense Inference on Events, Intents, and Reactions](/paper/event2mind-commonsense-inference-on-events)", "variants": ["Event2Mind", "Event2Mind dev", "Event2Mind test"], "title": "Event2Mind: Commonsense Inference on Events, Intents, and Reactions"}
{"id": "Lani", "contents": "LANI is a 3D navigation environment and corpus, where an agent navigates between landmarks. **Lani** contains 27,965 crowd-sourced instructions for navigation in an open environment. Each datapoint includes an instruction, a human-annotated ground-truth demonstration trajectory, and an environment with various landmarks and lakes. The dataset train/dev/test split is 19,758/4,135/4,072. Each environment specification defines placement of 6–13 landmarks within a square grass field of size 50m×50m.\n\nSource: [Mapping Navigation Instructions to Continuous Control Actions with Position-Visitation Prediction](https://arxiv.org/abs/1811.04179)\nImage Source: [https://arxiv.org/pdf/1809.00786.pdf](https://arxiv.org/pdf/1809.00786.pdf)", "variants": ["Lani"], "title": "Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction"}
{"id": "PhotoSynth", "contents": "The **PhotoSynth** (PS) dataset for patch matching consists of a total of 30 scenes with 25 scenes for training and 5 scenes for validation. The different image pairs are captured in different illumination conditions, at different scales and with different viewpoints.\n\nSource: [https://arxiv.org/abs/1801.01466](https://arxiv.org/abs/1801.01466)\nImage Source: [https://github.com/rmitra/PS-Dataset](https://github.com/rmitra/PS-Dataset)", "variants": ["PhotoSynth"], "title": "A Large Dataset for Improving Patch Matching"}
{"id": "Atlas", "contents": "**Atlas** is a dataset for e-commerce clothing product categorization. The Atlas dataset consists of a high-quality product taxonomy dataset focusing on clothing products which contain 186,150 images under clothing category with 3 levels and 52 leaf nodes in the taxonomy.\r\n\r\nSource: [https://github.com/vumaasha/atlas](https://github.com/vumaasha/atlas)\r\nImage Source: [Umaashankaret et al](https://arxiv.org/pdf/1908.08984.pdf)", "variants": ["Atlas"], "title": "Atlas: A Dataset and Benchmark for E-commerce Clothing Product Categorization"}
{"id": "VRAI", "contents": "VRAI is a large-scale vehicle ReID dataset for UAV-based intelligent applications. The dataset consists of 137, 613 images of 13, 022 vehicle instances. The images of each vehicle instance are captured by cameras of two DJI consumer UAVs at different locations, with a variety of view angles and flight-altitudes (15m to 80m).\r\n\r\nSource: [Vehicle Re-identification in Aerial Imagery: Dataset and Approach](https://arxiv.org/pdf/1904.01400)", "variants": ["VRAI test", "VRAI test-dev", "VRAI"], "title": "Vehicle Re-Identification in Aerial Imagery: Dataset and Approach"}
{"id": "CoNLL-2012", "contents": "The CoNLL-2012 shared task involved predicting coreference in English, Chinese, and Arabic, using the final version, v5.0, of the OntoNotes corpus. It was a follow-on to the English-only task organized in 2011.\r\n\r\nSource: [Pradhan et al.](https://www.aclweb.org/anthology/W12-4501.pdf)", "variants": ["CoNLL 2012", "CoNLL-2012"], "title": "CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes"}
{"id": "Finer", "contents": "Finnish News Corpus for Named Entity Recognition (Finer) is a corpus that consists of 953 articles (193,742 word tokens) with six named entity classes (organization, location, person, product, event,and date). The articles are extracted from the archives of Digitoday, a Finnish online technology news source.", "variants": ["Finer"], "title": "A Finnish News Corpus for Named Entity Recognition"}
{"id": "WISDOM", "contents": "Synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. \r\n\r\nSource: [Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data](/paper/segmenting-unknown-3d-objects-from-real-depth)", "variants": ["WISDOM"], "title": "Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data"}
{"id": "UW IOM", "contents": "Comprises twenty individuals picking up and placing objects of varying weights to and from cabinet and table locations at various heights.\r\n\r\nSource: [Toward Ergonomic Risk Prediction via Segmentation of Indoor Object Manipulation Actions Using Spatiotemporal Convolutional Networks](/paper/predicting-ergonomic-risks-during-indoor)", "variants": ["UW IOM"], "title": "Predicting Ergonomic Risks During Indoor Object Manipulation Using Spatiotemporal Convolutional Networks"}
{"id": "WikiSection", "contents": "A publicly available dataset with 242k labeled sections in English and German from two distinct domains: diseases and cities.\r\n\r\nSource: [SECTOR: A Neural Model for Coherent Topic Segmentation and Classification](/paper/sector-a-neural-model-for-coherent-topic)", "variants": ["WikiSection"], "title": "SECTOR: A Neural Model for Coherent Topic Segmentation and Classification"}
{"id": "CMU-MOSEI", "contents": "CMU Multimodal Opinion Sentiment and Emotion Intensity (**CMU-MOSEI**) is the largest dataset of sentence level sentiment analysis and emotion recognition in online videos. CMU-MOSEI contains more than 65 hours of annotated video from more than 1000 speakers and 250 topics.\r\n\r\nSource: [https://www.amir-zadeh.com/datasets](https://www.amir-zadeh.com/datasets)\r\nImage Source: [https://www.amir-zadeh.com/datasets](https://www.amir-zadeh.com/datasets)", "variants": ["CMU-MOSEI"], "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph"}
{"id": "MSRC-12", "contents": "The Microsoft Research Cambridge-12 Kinect gesture data set consists of sequences of human movements, represented as body-part locations, and the associated gesture to be recognized by the system. The data set includes 594 sequences and 719,359 frames—approximately six hours and 40 minutes—collected from 30 people performing 12 gestures. In total, there are 6,244 gesture instances. The motion files contain tracks of 20 joints estimated using the Kinect Pose Estimation pipeline. The body poses are captured at a sample rate of 30Hz with an accuracy of about two centimeters in joint positions.\r\n\r\nSource: [https://pgram.com/dataset/msrc-12-kinect-gesture-data-set/](https://pgram.com/dataset/msrc-12-kinect-gesture-data-set/)", "variants": ["MSRC-12"], "title": "Instructing people for training gestural interactive systems"}
{"id": "Wiki-Flickr Event Dataset", "contents": "The Wiki-Flick Event dataset for cross-modal event retrieval is a well-labelled but weakly-aligned dataset collected for cross-modality event retrieval. The dataset consists of 28,825 images on Flickr and 11,960 text articles from hundreds of social media, belonging to 82 categories of events.\n\nSource: [https://github.com/zhengyang5/Wiki-Flickr-Event-Dataset](https://github.com/zhengyang5/Wiki-Flickr-Event-Dataset)", "variants": ["Wiki-Flickr Event Dataset"], "title": "Learning Shared Semantic Space with Correlation Alignment for Cross-Modal Event Retrieval"}
{"id": "DDD17", "contents": "DDD17 has over 12 h of a 346x260 pixel DAVIS sensor recording highway and city driving in daytime, evening, night, dry and wet weather conditions, along with vehicle speed, GPS position, driver steering, throttle, and brake captured from the car's on-board diagnostics interface. \r\n\r\nSource: [DDD17: End-To-End DAVIS Driving Dataset](https://arxiv.org/pdf/1711.01458v1.pdf)\r\nImage Source: [DVS Driving Dataset 2017 (DDD17) README](https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub)", "variants": ["DDD17"], "title": "DDD17: End-To-End DAVIS Driving Dataset"}
{"id": "ChestX-ray8", "contents": "**ChestX-ray8** is a medical imaging dataset which comprises 108,948 frontal-view X-ray images of 32,717 (collected from the year of 1992 to 2015) unique patients with the text-mined eight common disease labels, mined from the text radiological reports via NLP techniques.\r\n\r\nSource: [https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610](https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610)\r\nImage Source: [https://nihcc.app.box.com/v/ChestXray-NIHCC](https://nihcc.app.box.com/v/ChestXray-NIHCC)", "variants": ["ChestX-ray8"], "title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases"}
{"id": "WinoGrande", "contents": "WinoGrande is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations.\r\n\r\nSource: [WinoGrande: An Adversarial Winograd Schema Challenge at Scale](/paper/winogrande-an-adversarial-winograd-schema)\r\nImage Source: [https://winogrande.allenai.org/](https://winogrande.allenai.org/)", "variants": ["WinoGrande"], "title": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale"}
{"id": "HellaSwag", "contents": "HellaSwag is a challenge dataset for evaluating commonsense NLI that is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy).", "variants": ["HellaSwag"], "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}
{"id": "Matterport3D", "contents": "The **Matterport3D** dataset is a large RGB-D dataset for scene understanding in indoor environments. It contains 10,800 panoramic views inside 90 real building-scale scenes, constructed from 194,400 RGB-D images. Each scene is a residential building consisting of multiple rooms and floor levels, and is annotated with surface construction, camera poses, and semantic segmentation.\r\n\r\nSource: [Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention](https://arxiv.org/abs/1812.04155)", "variants": ["Matterport3D"], "title": "Matterport3D: Learning from RGB-D Data in Indoor Environments"}
{"id": "CSPubSum", "contents": "CSPubSum is a dataset for summarisation of computer science publications, created by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. \r\n\r\nSource: [A Supervised Approach to Extractive Summarisation of Scientific Papers](/paper/a-supervised-approach-to-extractive)", "variants": ["CSPubSum"], "title": "A Supervised Approach to Extractive Summarisation of Scientific Papers"}
{"id": "aPY", "contents": "**aPY** is a coarse-grained dataset composed of 15339 images from 3 broad categories (animals, objects and vehicles), further divided into a total of 32 subcategories (aeroplane, …, zebra).\r\n\r\nSource: [From Classical to Generalized Zero-Shot Learning: a Simple Adaptation Process](https://arxiv.org/abs/1809.10120)\r\nImage Source: [https://www.cs.cmu.edu/~afarhadi/papers/Attributes.pdf](https://www.cs.cmu.edu/~afarhadi/papers/Attributes.pdf)", "variants": ["aPY - 0-Shot", "aPY"], "title": "Describing objects by their attributes"}
{"id": "Perceptual Similarity", "contents": "Perceptual Similarity is a dataset of human perceptual similarity judgments.\r\n\r\nSource: [Perceptual Similarity](https://arxiv.org/pdf/1801.03924.pdf)\r\nImage Source: [https://github.com/richzhang/PerceptualSimilarity](https://github.com/richzhang/PerceptualSimilarity)", "variants": ["Perceptual Similarity"], "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric"}
{"id": "XQA", "contents": "XQA is a data which consists of a total amount of 90k question-answer pairs in nine languages for cross-lingual open-domain question answering.\r\n\r\nSource: [XQA: A Cross-lingual Open-domain Question Answering Dataset](https://www.aclweb.org/anthology/P19-1227.pdf)", "variants": ["XQA"], "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset"}
{"id": "SpaceNet MVOI", "contents": "An open source Multi-View Overhead Imagery dataset with 27 unique looks from a broad range of viewing angles (-32.5 degrees to 54.0 degrees). Each of these images cover the same 665 square km geographic extent and are annotated with 126,747 building footprint labels, enabling direct assessment of the impact of viewpoint perturbation on model performance.\r\n\r\nSource: [SpaceNet MVOI: a Multi-View Overhead Imagery Dataset](/paper/spacenet-mvoi-a-multi-view-overhead-imagery)", "variants": ["SpaceNet MVOI"], "title": "SpaceNet MVOI: A Multi-View Overhead Imagery Dataset"}
{"id": "BigEarthNet", "contents": "BigEarthNet consists of 590,326 Sentinel-2 image patches, each of which is a section of i) 120x120 pixels for 10m bands; ii) 60x60 pixels for 20m bands; and iii) 20x20 pixels for 60m bands. \r\n\r\nSource: [BigEarthNet: A Large-Scale Benchmark Archive For Remote Sensing Image Understanding](/paper/bigearthnet-a-large-scale-benchmark-archive)", "variants": ["BigEarthNet"], "title": "Bigearthnet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding"}
{"id": "Some Like it Hoax", "contents": "**Some Like it Hoax** is a fake news detection dataset consisting of 15,500 Facebook posts and 909,236 users.\r\n\r\nSource: [Some Like it Hoax: Automated Fake News Detection in Social Networks](/paper/some-like-it-hoax-automated-fake-news)", "variants": ["Some Like it Hoax"], "title": "Some Like it Hoax: Automated Fake News Detection in Social Networks"}
{"id": "Sleep-EDF", "contents": "The sleep-edf database contains 197 whole-night PolySomnoGraphic sleep recordings, containing EEG, EOG, chin EMG, and event markers. Some records also contain respiration and body temperature. Corresponding hypnograms (sleep patterns) were manually scored by well-trained technicians according to the Rechtschaffen and Kales manual, and are also available.\r\n\r\nSource: [https://www.physionet.org/content/sleep-edfx/1.0.0/](https://www.physionet.org/content/sleep-edfx/1.0.0/)", "variants": ["Sleep-EDF-SC", "Sleep-EDF-ST", "Sleep-EDF"], "title": "nuScenes: A multimodal dataset for autonomous driving"}
{"id": "Negotiation Dialogues Dataset", "contents": "This dataset consists of 5808 dialogues, based on 2236 unique scenarios. Each dialogue is converted into two training examples in the dataset, showing the complete conversation from the perspective of each agent. The perspectives differ on their input goals, output choice, and in special tokens marking whether a statement was read or written.\r\n\r\nSource: [Negotiation Dialogues Dataset](https://github.com/facebookresearch/end-to-end-negotiator)", "variants": ["Negotiation Dialogues Dataset"], "title": "Hierarchical Text Generation and Planning for Strategic Dialogue"}
{"id": "Image Paragraph Captioning", "contents": "The Image Paragraph Captioning dataset allows researchers to benchmark their progress in generating paragraphs that tell a story about an image. The dataset contains 19,561 images from the [Visual Genome dataset](https://paperswithcode.com/dataset/visual-genome). Each image contains one paragraph. The training/val/test sets contains 14,575/2,487/2,489 images.\r\n\r\nSince all the images are also part of the Visual Genome dataset, each image also contains 50 region descriptions (short phrases describing parts of an image), 35 objects, 26 attributes and 21 relationships and 17 question-answer pairs.\r\n\r\nSource: [A Hierarchical Approach for Generating Descriptive Image Paragraphs](/paper/a-hierarchical-approach-for-generating)\r\nImage Source: [https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html](https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html)", "variants": ["Image Paragraph Captioning"], "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs"}
{"id": "ETH", "contents": "**ETH** is a dataset for pedestrian detection. The testing set contains 1,804 images in three video clips. The dataset is captured from a stereo rig mounted on car, with a resolution of 640 x 480 (bayered), and a framerate of 13--14 FPS.\r\n\r\nSource: [Scale-aware Fast R-CNN for Pedestrian Detection](https://arxiv.org/abs/1510.08160)\r\nImage Source: [https://medium.com/@zhenqinghu/pedestrian-detection-on-eth-data-set-with-faster-r-cnn-19d0a906f1d3](https://medium.com/@zhenqinghu/pedestrian-detection-on-eth-data-set-with-faster-r-cnn-19d0a906f1d3)", "variants": ["ETH BIWI Walking Pedestrians dataset", "ETH/UCY", "ETH"], "title": "Depth and Appearance for Mobile Scene Analysis"}
{"id": "FSOD", "contents": "Few-Shot Object Detection Dataset (FSOD) is a high-diverse dataset specifically designed for few-shot object detection and intrinsically designed to evaluate thegenerality of a model on novel categories.\r\n\r\nSource: [FSOD](https://github.com/fanq15/Few-Shot-Object-Detection-Dataset)", "variants": ["FSOD"], "title": "Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector"}
{"id": "DoMSEV", "contents": "The Dataset of Multimodal Semantic Egocentric Video (DoMSEV) contains 80-hours of multimodal (RGB-D, IMU, and GPS) data related to First-Person Videos with annotations for recorder profile, frame scene, activities, interaction, and attention.\r\n\r\nSource: [A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos](/paper/a-weighted-sparse-sampling-and-smoothing-1)", "variants": ["DoMSEV"], "title": "A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos"}
{"id": "MNIST-M", "contents": "**MNIST-M** is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background. It contains 59,001 training and 90,001 test images.\r\n\r\nSource: [A Review of Single-Source Deep Unsupervised Visual Domain Adaptation](https://arxiv.org/abs/2009.00155)\r\nImage Source: [https://arxiv.org/pdf/1505.07818v4.pdf](https://arxiv.org/pdf/1505.07818v4.pdf)", "variants": ["MNIST-to-MNIST-M", "MNIST-M"], "title": "Domain-Adversarial Training of Neural Networks"}
{"id": "Pavia University", "contents": "The **Pavia University** dataset is a hyperspectral image dataset which gathered by a sensor known as the reflective optics system imaging spectrometer (ROSIS-3) over the city of Pavia, Italy. The image consists of 610×340 pixels with 115 spectral bands. The image is divided into 9 classes with a total of 42,776 labelled samples, including the asphalt, meadows, gravel, trees, metal sheet, bare soil, bitumen, brick, and shadow.\n\nSource: [Diversity in Machine Learning](https://arxiv.org/abs/1807.01477)\nImage Source: [http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University)", "variants": ["Pavia University"], "title": "DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images"}
{"id": "XNLI", "contents": "The **Cross-lingual Natural Language Inference** (**XNLI**) corpus is the extension of the Multi-Genre NLI (MultiNLI) corpus to 15 languages. The dataset was created by manually translating the validation and test sets of MultiNLI into each of those 15 languages. The English training set was machine translated for all languages. The dataset is composed of 122k train, 2490 validation and 5010 test examples.\r\n\r\nSource: [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)\r\nImage Source: [https://github.com/facebookresearch/XNLI](https://github.com/facebookresearch/XNLI)", "variants": ["XNLI Chinese", "XNLI Chinese Dev", "XNLI French", "XNLI Zero-Shot English-to-French", "XNLI Zero-Shot English-to-German", "XNLI Zero-Shot English-to-Spanish", "XNLI", "XNLI Dev"], "title": "XNLI: Evaluating Cross-lingual Sentence Representations"}
{"id": "Relative Size", "contents": "The Relative Size dataset contains 486 object pairs between 41 physical objects. Size comparisons are not available for all pairs of objects (e.g. bird and watermelon) because for some pairs humans cannot determine which object is bigger.\r\n\r\nDataset contains only object pairs that people have consistently agreed which one is bigger. Objects appear in 24 pairs on average, window with 13 pairs has the least, and eye with 35 pairs has the most number of comparisons.\r\n\r\nSource: [Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects](/paper/are-elephants-bigger-than-butterflies)", "variants": ["Relative Size"], "title": "Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects"}
{"id": "CelebA", "contents": "CelebFaces Attributes dataset contains 202,599 face images of the size 178×218 from 10,177 celebrities, each annotated with 40 binary labels indicating facial attributes like hair color, gender and age.\r\n\r\nSource: [Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image Translation with Visual Attention](https://arxiv.org/abs/1811.07483)\r\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)", "variants": ["CelebA", "CelebA 128x128", "CelebA + AFLW Unaligned", "CelebA 128 x 128", "CelebA 256x256", "CelebA 64x64", "CelebA Aligned"], "title": "Deep Learning Face Attributes in the Wild"}
{"id": "DSBI", "contents": "The **Double-Sided Braille Image** dataset (**DSBI**) is a large-scale dataset for Braille image recognition. It has detailed Braille recto dots, verso dots and Braille cells annotation.\n\nSource: [https://arxiv.org/abs/1811.10893](https://arxiv.org/abs/1811.10893)\nImage Source: [https://github.com/yeluo1994/DSBI](https://github.com/yeluo1994/DSBI)", "variants": ["DSBI"], "title": "DSBI: Double-Sided Braille Image Dataset and Algorithm Evaluation for Braille Dots Detection"}
{"id": "Who-did-What", "contents": "**Who-did-What** collects its corpus from news and provides options for questions similar to CBT. Each question is formed from two independent articles: an article is treated as context to be read and a separate article on the same event is used to form the query.\r\n\r\nSource: [ChID: A Large-scale Chinese IDiom Dataset for Cloze Test](https://arxiv.org/abs/1906.01265)\r\nImage Source: [https://tticnlp.github.io/who_did_what/sample.html](https://tticnlp.github.io/who_did_what/sample.html)", "variants": ["Who-did-What"], "title": "Who did What: A Large-Scale Person-Centered Cloze Dataset"}
{"id": "WGISD", "contents": "Embrapa Wine Grape Instance Segmentation Dataset (WGISD) contains grape clusters properly annotated in 300 images and a novel annotation methodology for segmentation of complex objects in natural images.\r\n\r\nSource: [Grape detection, segmentation and tracking using deep neural networks and three-dimensional association](/paper/grape-detection-segmentation-and-tracking)", "variants": ["WGISD"], "title": "Grape detection, segmentation and tracking using deep neural networks and three-dimensional association"}
{"id": "Detection of Traffic Anomaly", "contents": "Contains 4,677 videos with temporal, spatial, and categorical annotations.\r\n\r\nSource: [When, Where, and What? A New Dataset for Anomaly Detection in Driving Videos](/paper/when-where-and-what-a-new-dataset-for-anomaly)", "variants": ["Detection of Traffic Anomaly"], "title": "When, Where, and What? A New Dataset for Anomaly Detection in Driving Videos"}
{"id": "Surveillance Camera Fight Dataset", "contents": "The dataset is collected from the Youtube videos that contains fight instances in it. Also, some non-fight sequences from regular surveillance camera videos are included.\n* There are 300 videos in total as 150 fight + 150 non-fight\n* Videos are 2-second long\n* Only the fight related parts are included in the samples\n\nSource: [https://github.com/sayibet/fight-detection-surv-dataset](https://github.com/sayibet/fight-detection-surv-dataset)\nImage Source: [https://github.com/sayibet/fight-detection-surv-dataset](https://github.com/sayibet/fight-detection-surv-dataset)", "variants": ["Surveillance Camera Fight Dataset"], "title": "Vision-based Fight Detection from Surveillance Cameras"}
{"id": "Tilde MODEL Corpus", "contents": "Tilde MODEL Corpus is a multilingual corpora for European languages – particularly focused on the smaller languages. The collected resources have been cleaned, aligned, and formatted into a corpora standard TMX format useable for developing new Language technology products and services.\r\n\r\nIt contains over 10M segments of multilingual open data.\r\n\r\nThe data has been collected from sites allowing free use and reuse of its content, as well as from Public Sector web sites.\r\n\r\nSource: [Tilde MODEL - Multilingual Open Data for EU Languages](/paper/tilde-model-multilingual-open-data-for-eu)", "variants": ["Tilde MODEL Corpus"], "title": "Tilde MODEL - Multilingual Open Data for EU Languages"}
{"id": "WikiSem500", "contents": "The **WikiSem500** dataset contains around 500 per-language cluster groups for English, Spanish, German, Chinese, and Japanese (a total of 13,314 test cases).\n\nSource: [https://arxiv.org/abs/1611.01547](https://arxiv.org/abs/1611.01547)", "variants": ["WikiSem500"], "title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations"}
{"id": "Polyglot-NER", "contents": "Polyglot-NER builds massive multilingual annotators with minimal human expertise and intervention.\r\n\r\nSource: [POLYGLOT-NER: Massive Multilingual Named Entity Recognition](/paper/polyglot-ner-massive-multilingual-named)", "variants": ["Polyglot-NER"], "title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition"}
{"id": "PANDA", "contents": "PANDA is the first gigaPixel-level humAN-centric viDeo dAtaset, for large-scale, long-term, and multi-object visual analysis. The videos in PANDA were captured by a gigapixel camera and cover real-world scenes with both wide field-of-view (~1 square kilometer area) and high-resolution details (~gigapixel-level/frame). The scenes may contain 4k head counts with over 100x scale variation. PANDA provides enriched and hierarchical ground-truth annotations, including 15,974.6k bounding boxes, 111.8k fine-grained attribute labels, 12.7k trajectories, 2.2k groups and 2.9k interactions.\r\n\r\nSource: [PANDA: A Gigapixel-level Human-centric Video Dataset](/paper/panda-a-gigapixel-level-human-centric-video)", "variants": ["PANDA"], "title": "PANDA: A Gigapixel-level Human-centric Video Dataset"}
{"id": "WMT 2014 Medical", "contents": "The Medical Translation Task of WMT 2014 addresses the problem of domain-specific and genre-specific machine translation. The task is split into two subtasks: summary translation, focused on translation of sentences from summaries of medical articles, and query translation, focused on translation of queries entered by users into medical information search engines. Both subtasks included translation between English and Czech, German, and French, in both directions.\n\nSource: [https://www.aclweb.org/anthology/W14-3302.pdf](https://www.aclweb.org/anthology/W14-3302.pdf)", "variants": ["WMT 2014 Medical"], "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}
{"id": "Retrieval-SfM", "contents": "The Retrieval-SFM dataset is used for instance image retrieval. The dataset contains 28559 images from 713 locations in the world. Each image has a label indicating the location it belongs to. Most locations are famous man-made architectures such as palaces and towers, which are relatively static and positively contribute to visual place recognition. The training dataset contains various perceptual changes including variations in viewing angles, occlusions and illumination conditions, etc.\n\nSource: [Localizing Discriminative Visual Landmarks for Place Recognition](https://arxiv.org/abs/1904.06635)", "variants": ["Retrieval-SfM"], "title": "CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples"}
{"id": "SemanticUSL", "contents": "SemanticUSL is a dataset for domain adaptation for LiDAR point cloud semantic segmentation. The dataset has the same data format and ontology as SemanticKITTI. \r\n\r\nSource: [LiDARNet: A Boundary-Aware Domain Adaptation Model for Point Cloud Semantic Segmentation](https://arxiv.org/pdf/2003.01174)\r\nImage Source: [https://unmannedlab.github.io/semanticusl](https://unmannedlab.github.io/semanticusl)", "variants": ["SemanticUSL"], "title": "LiDARNet: A Boundary-Aware Domain Adaptation Model for Lidar Point Cloud Semantic Segmentation"}
{"id": "CLUECorpus2020", "contents": "CLUECorpus2020 is a large-scale corpus that can be used directly for self-supervised learning such as pre-training of a language model, or language generation. It has 100G raw corpus with 35 billion Chinese characters, which is retrieved from Common Crawl. \r\n\r\nSource: [CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model](/paper/cluecorpus2020-a-large-scale-chinese-corpus)", "variants": ["CLUECorpus2020"], "title": "CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model"}
{"id": "Video2GIF", "contents": "The **Video2GIF** dataset contains over 100,000 pairs of GIFs and their source videos. The GIFs were collected from two popular GIF websites (makeagif.com, gifsoup.com) and the corresponding source videos were collected from YouTube in Summer 2015. IDs and URLs of the GIFs and the videos are provided, along with temporal alignment of GIF segments to their source videos. The dataset shall be used to evaluate GIF creation and video highlight techniques.\r\n\r\nIn addition to the 100K GIF-video pairs, the dataset contains 357 pairs of GIFs and their source videos as the test set. The 357 videos come with a Creative Commons CC-BY license, which allowed the authors to redistribute the material with appropriate credit to make the results on test set reproducible even when some of the videos become unavailable.\r\n\r\nSource: [Video2GIF](https://github.com/gifs/personalized-highlights-dataset)", "variants": ["Video2GIF"], "title": "Video2GIF: Automatic Generation of Animated GIFs from Video"}
{"id": "VG-Depth", "contents": "Enable visual relation detection and serves as an extension to Visual Genome (VG).\r\n\r\nSource: [Improving Visual Relation Detection using Depth Maps](/paper/improving-visual-relation-detection-using)", "variants": ["VG-Depth"], "title": "Improving Visual Relation Detection using Depth Maps"}
{"id": "Stacked MNIST", "contents": "The **Stacked MNIST** dataset is derived from the standard MNIST dataset with an increased number of discrete modes. 240,000 RGB images in the size of 32×32 are synthesized by stacking three random digit images from MNIST along the color channel, resulting in 1,000 explicit modes in a uniform distribution corresponding to the number of possible triples of digits.\r\n\r\nSource: [Inclusive GAN: Improving Data and Minority Coverage in Generative Models](https://arxiv.org/abs/2004.03355)\r\nImage Source: [https://arxiv.org/abs/1705.07761](https://arxiv.org/abs/1705.07761)", "variants": ["Stacked MNIST"], "title": "Unrolled Generative Adversarial Networks"}
{"id": "Brain US", "contents": "This brain anatomy segmentation dataset has 1300 2D US scans for training and 329 for testing. A total of 1629 in vivo B-mode US images were obtained from 20 different subjects (age<1 years old) who were treated between 2010 and 2016. The dataset contained subjects with IVH and without (healthy subjects but in risk of developing IVH). The US scans were collected using a Philips US machine with a C8-5 broadband curved array transducer using coronal and sagittal scan planes. For every collected image ventricles and septum pellecudi are manually segmented by an expert ultrasonographer. We split these images randomly into 1300 Training images and 329 Testing images for experiments. Note that these images are of size 512 × 512. \r\n\r\nSource: [Learning to Segment Brain Anatomy from 2D Ultrasound with Less Data](https://arxiv.org/pdf/1912.08364.pdf)\r\n\r\nImage source: [Learning to Segment Brain Anatomy from 2D Ultrasound with Less Data](https://arxiv.org/pdf/1912.08364.pdf)", "variants": ["Brain US"], "title": "Learning to Segment Brain Anatomy from 2D Ultrasound with Less Data"}
{"id": "CrowdHuman", "contents": "**CrowdHuman** is a large and rich-annotated human detection dataset, which contains 15,000, 4,370 and 5,000 images collected from the Internet for training, validation and testing respectively. The number is more than 10× boosted compared with previous challenging pedestrian detection dataset like CityPersons. The total number of persons is also noticeably larger than the others with ∼340k person and ∼99k ignore region annotations in the CrowdHuman training subset.\r\n\r\nSource: [SADet: Learning An Efficient and Accurate Pedestrian Detector](https://arxiv.org/abs/2007.13119)\r\nImage Source: [http://www.crowdhuman.org/](http://www.crowdhuman.org/)", "variants": ["CrowdHuman (full body)", "CrowdHuman"], "title": "CrowdHuman: A Benchmark for Detecting Human in a Crowd"}
{"id": "GraspNet", "contents": "A large-scale grasp pose detection dataset with a unified evaluation system. The dataset contains 87,040 RGBD images with over 370 million grasp poses. \r\n\r\nSource: [GraspNet: A Large-Scale Clustered and Densely Annotated Dataset for Object Grasping](/paper/graspnet-a-large-scale-clustered-and-densely)", "variants": ["GraspNet"], "title": "GraspNet: A Large-Scale Clustered and Densely Annotated Dataset for Object Grasping"}
{"id": "SWAX", "contents": "Comprised of real human and wax figure images and videos that endorse the problem of face spoofing detection. The dataset consists of more than 1800 face images and 110 videos of 55 people/waxworks, arranged in training, validation and test sets with a large range in expression, illumination and pose variations. \r\n\r\nSource: [The SWAX Benchmark: Attacking Biometric Systems with Wax Figures](/paper/the-swax-benchmark-attacking-biometric)", "variants": ["SWAX"], "title": "The SWAX Benchmark: Attacking Biometric Systems with Wax Figures"}
{"id": "Kvasir-SEG", "contents": "Kvasir-SEG is an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist. \r\n\r\nSource: [Kvasir-SEG: A Segmented Polyp Dataset](/paper/kvasir-seg-a-segmented-polyp-dataset)\r\nImage Source: [https://datasets.simula.no/kvasir-seg/](https://datasets.simula.no/kvasir-seg/)", "variants": ["Kvasir-SEG"], "title": "Kvasir-SEG: A Segmented Polyp Dataset"}
{"id": "iHarmony4", "contents": "**iHarmony4** is a synthesized dataset for Image Harmonization. It contains 4 sub-datasets: HCOCO, HAdobe5k, HFlickr, and Hday2night (based on COCO, Adobe5k, Flickr, day2night datasets respectively), each of which contains synthesized composite images, foreground masks of composite images and corresponding real images. \r\n\r\nSource: [Image_Harmonization_Datasets](https://github.com/bcmi/Image_Harmonization_Datasets)", "variants": ["iHarmony4"], "title": "DoveNet: Deep Image Harmonization via Domain Verification"}
{"id": "Synthetic Rain Datasets", "contents": "The Synthetic Rain Datasets consists of 13,712 clean-rain image pairs gathered from multiple datasets (Rain14000, Rain1800, Rain800, Rain12). With a single trained model, evaluation could be performed on various test sets, including Rain100H, Rain100L, Test100, Test2800, and Test1200.\r\n\r\nPSNR and SSIM are computed on Y-channel in YCbCr color space.", "variants": ["Synthetic Rain Datasets", "Rain100H", "Rain100L", "Test100", "Test1200", "Test2800"], "title": "Multi-Scale Progressive Fusion Network for Single Image Deraining"}
{"id": "OCTAGON", "contents": "The OCTAGON dataset is a set of Angiography by Octical Coherence Tomography images (OCT-A) used to the segmentation of the Foveal Avascular Zone (FAZ). The dataset includes 144 healthy OCT-A images and 69 diabetic OCT-A images, divided into four groups, each one with 36 and about 17 OCT-A images, respectively. These groups are: 3x3 superficial, 3x3 deep, 6x6 superficial and 6x6 deep, where 3x3 and 6x6 are the zoom of the image and superficial/deep are the depth level of the extracted image. The healthy dataset includes OCT-A images from people classified in 6 age ranges: 10-19 years, 20-29 years, 30-39 years, 40-49 years, 50-59 years and 60-69 years. Each age range includes 3 different patients with information of left and right eyes for each one. Finally, for each eye, there are four different images: one 3x3 superficial image, one 3x3 deep image, one 6x6 superficial image and one 6x6 deep image. Each image have two manual labelled of expert clinicians of the FAZ and their quantification in the healthy OCT-A images, and one manual labelled in the diabetic OCT-A images.\r\n\r\nSource: [OCTAGON dataset](http://www.varpa.es/research/ophtalmology.html#octagon)", "variants": ["OCTAGON"], "title": "Automatic segmentation of the Foveal Avascular Zone in ophthalmological OCT-A images"}
{"id": "Oxford Radar RobotCar Dataset", "contents": "The Oxford Radar RobotCar Dataset is a radar extension to The Oxford RobotCar Dataset. It has been extended with data from a Navtech CTS350-X Millimetre-Wave FMCW radar and Dual Velodyne HDL-32E LIDARs with optimised ground truth radar odometry for 280 km of driving around Oxford, UK (in addition to all sensors in the original Oxford RobotCar Dataset).\r\n\r\nSource: [The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset](/paper/the-oxford-radar-robotcar-dataset-a-radar)", "variants": ["Oxford Radar RobotCar Dataset"], "title": "The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset"}
{"id": "KIT Whole-Body Human Motion", "contents": "The **KIT Whole-Body Human Motion** Database is a large-scale dataset of whole-body human motion with methods and tools, which allows a unifying representation of captured human motion, and efficient search in the database, as well as the transfer of subject-specific motions to robots with different embodiments. Captured subject-specific motion is normalized regarding the subject’s height and weight by using a reference kinematics and dynamics model of the human body, the master motor map (MMM). In contrast with previous approaches and human motion databases, the motion data in this database consider not only the motions of the human subject but the position and motion of objects with which the subject is interacting as well. In addition to the description of the MMM reference model, See the paper for procedures and techniques used for the systematic recording, labeling, and organization of human motion capture data, object motions as well as the subject–object relations.\r\n\r\nSource: [https://motion-database.humanoids.kit.edu/](https://motion-database.humanoids.kit.edu/)\r\nImage Source: [https://motion-database.humanoids.kit.edu/](https://motion-database.humanoids.kit.edu/)", "variants": ["KIT Whole-Body Human Motion"], "title": "The KIT whole-body human motion database"}
{"id": "InstaFake", "contents": "Includes two datasets published for the detection of fake and automated accounts. \r\n\r\nSource: [Instagram Fake and Automated Account Detection](/paper/instagram-fake-and-automated-account)", "variants": ["InstaFake"], "title": "Instagram Fake and Automated Account Detection"}
{"id": "3DSeg-8", "contents": "The 3DSeg-8 is a collection of several publicly available 3D segmentation datasets from different medical imaging modalities, e.g. magnetic resonance imaging (MRI) and computed tomography (CT), with various scan regions, target organs and pathologies.", "variants": ["3DSeg-8"], "title": "Med3D: Transfer Learning for 3D Medical Image Analysis"}
{"id": "Billion Word Benchmark", "contents": "The **One Billion Word** dataset is a dataset for language modeling. The training/held-out data was produced from the WMT 2011 News Crawl data using a combination of Bash shell and Perl scripts.", "variants": ["One Billion Word", "Billion Word Benchmark"], "title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling"}
{"id": "SESIV", "contents": "SEmantic Salient Instance Video (SESIV) dataset is obtained by augmenting the DAVIS-2017 benchmark dataset by assigning semantic ground-truth for salient instance labels. The SESIV dataset consists of 84 high-quality video sequences with pixel-wisely per-frame ground-truth labels.\r\n\r\nSource: [Semantic Instance Meets Salient Object: Study on Video Semantic Salient Instance Segmentation](https://arxiv.org/pdf/1807.01452)", "variants": ["SESIV"], "title": "Semantic Instance Meets Salient Object: Study on Video Semantic Salient Instance Segmentation"}
{"id": "GYAFC", "contents": "Grammarly’s Yahoo Answers Formality Corpus (GYAFC) is the largest dataset for any style containing a total of 110K informal / formal sentence pairs.\r\n\r\nYahoo Answers is a question answering forum, contains a large number of informal sentences and allows redistribution of data. The authors used the Yahoo Answers L6 corpus to create the GYAFC dataset of informal and formal sentence pairs. In order to ensure a uniform distribution of data, they removed sentences that are questions, contain URLs, and are shorter than 5 words or longer than 25. After these preprocessing steps, 40 million sentences remain. \r\n\r\nThe Yahoo Answers corpus consists of several different domains like Business, Entertainment & Music, Travel, Food, etc. Pavlick and Tetreault formality classifier (PT16) shows that the formality level varies significantly\r\nacross different genres. In order to control for this variation, the authors work with two specific domains that contain the most informal sentences and show results on training and testing within those categories. The authors use the formality classifier from PT16 to identify informal sentences and train this classifier on the Answers genre of the PT16 corpus\r\nwhich consists of nearly 5,000 randomly selected sentences from Yahoo Answers manually annotated on a scale of -3 (very informal) to 3 (very formal). They find that the domains of Entertainment & Music and Family & Relationships contain the most informal sentences and create the GYAFC dataset using these domains.\r\n\r\nSource: [Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer](https://arxiv.org/pdf/1803.06535v2.pdf)", "variants": ["GYAFC"], "title": "Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer"}
{"id": "PubMedQA", "contents": "The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts.\r\n\r\nPubMedQA has 1k expert labeled, 61.2k unlabeled and 211.3k artificially generated QA instances.\r\n\r\nSource: [PubMedQA](https://pubmedqa.github.io/)\r\nImage Source: [https://arxiv.org/pdf/1909.06146v1.pdf](https://arxiv.org/pdf/1909.06146v1.pdf)", "variants": ["PubMedQA"], "title": "PubMedQA: A Dataset for Biomedical Research Question Answering"}
{"id": "VizWiz-QualityIssues", "contents": "A large-scale dataset that links the assessment of image quality issues to two practical vision tasks: image captioning and visual question answering.\r\n\r\nSource: [Assessing Image Quality Issues for Real-World Problems](/paper/assessing-image-quality-issues-for-real-world)", "variants": ["VizWiz-QualityIssues"], "title": "Assessing Image Quality Issues for Real-World Problems"}
{"id": "NAS-Bench-1Shot1", "contents": "NAS-Bench-1Shot1 draws on the recent large-scale tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS methods. \r\n\r\nSource: [NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search](https://arxiv.org/pdf/2001.10422v2.pdf)", "variants": ["NAS-Bench-1Shot1"], "title": "NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search"}
{"id": "tieredImageNet", "contents": "The **tieredImageNet** dataset is a larger subset of ILSVRC-12 with 608 classes (779,165 images) grouped into 34 higher-level nodes in the ImageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of training, validation, and testing nodes, and the corresponding classes form the respective meta-sets. As argued in Ren et al. (2018), this split near the root of the ImageNet hierarchy results in a more challenging, yet realistic regime with test classes that are less similar to training classes.\r\n\r\nSource: [tieredImageNet](https://github.com/yaoyao-liu/tiered-imagenet-tools)\r\nImage Source: [https://arxiv.org/pdf/1803.00676.pdf](https://arxiv.org/pdf/1803.00676.pdf)", "variants": ["Tiered ImageNet 5-way (1-shot)", "Tiered ImageNet 5-way (5-shot)", "tieredImageNet"], "title": "Meta-Learning for Semi-Supervised Few-Shot Classification"}
{"id": "SYSU-MM01", "contents": "The **SYSU-MM01** is a dataset collected for the Visible-Infrared Re-identification problem. The images in the dataset were obtained from 491 different persons by recording them using 4 RGB and 2 infrared cameras. Within the dataset, the persons are divided into 3 fixed splits to create training, validation and test sets. In the training set, there are 20284 RGB and 9929 infrared images of 296 persons. The validation set contains 1974 RGB and 1980 infrared images of 99 persons. The testing set consists of the images of 96 persons where 3803 infrared images are used as query and 301 randomly selected RGB images are used as gallery.\r\n\r\nSource: [An Efficient Framework for Visible-Infrared Cross Modality Person Re-Identification](https://arxiv.org/abs/1907.06498)\r\nImage Source: [https://github.com/wuancong/SYSU-MM01](https://github.com/wuancong/SYSU-MM01)", "variants": ["SYSU-MM01"], "title": "RGB-Infrared Cross-Modality Person Re-identification"}
{"id": "CHASE_DB1", "contents": "**CHASE_DB1** is a dataset for retinal vessel segmentation which contains 28 color retina images with the size of 999×960 pixels which are collected from both left and right eyes of 14 school children. Each image is annotated by two independent human experts.\r\n\r\nSource: [MixModule: Mixed CNN Kernel Module for Medical Image Segmentation](https://arxiv.org/abs/1910.08728)\r\nImage Source: [https://www.mdpi.com/2073-8994/9/11/276](https://www.mdpi.com/2073-8994/9/11/276)", "variants": ["CHASE_DB1"], "title": "An Ensemble Classification-Based Approach Applied to Retinal Blood Vessel Segmentation"}
{"id": "ROPES", "contents": "ROPES is a QA dataset which tests a system's ability to apply knowledge from a passage of text to a new situation. A system is presented a background passage containing a causal or qualitative relation(s), a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the back-ground passage in the context of the situation.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/ropes)", "variants": ["ROPES"], "title": "Reasoning Over Paragraph Effects in Situations"}
{"id": "IMDb-Face", "contents": "IMDb-Face is  large-scale noise-controlled dataset for face recognition research. The dataset contains about 1.7 million faces, 59k identities, which is manually cleaned from 2.0 million raw images. All images are obtained from the IMDb website. \r\n\r\nSource: [The Devil of Face Recognition is in the Noise](/paper/the-devil-of-face-recognition-is-in-the-noise)", "variants": ["IMDb-Face"], "title": "The Devil of Face Recognition is in the Noise"}
{"id": "CMCNC", "contents": "The **Coherent Multiple Choice Narrative Cloze** (**CMCNC**) dataset is an evaluation dataset for the multi-choice narrative cloze task, where the goal is to distinguish which event has been held out from a document from a small set of randomly drawn events.\n\nSource: [https://arxiv.org/pdf/1711.07611.pdf](https://arxiv.org/pdf/1711.07611.pdf)", "variants": ["CMCNC"], "title": "Event Representations with Tensor-based Compositions"}
{"id": "Birdsnap", "contents": "**Birdsnap** is a large bird dataset consisting of 49,829 images from 500 bird species with 47,386 images used for training and 2,443 images used for testing.\r\n\r\nSource: [Fine-Grained Classification via Mixture of Deep Convolutional Neural Networks](https://arxiv.org/abs/1511.09209)\r\nImage Source: [http://thomasberg.org/](http://thomasberg.org/)", "variants": ["Birdsnap"], "title": "Birdsnap: Large-Scale Fine-Grained Visual Categorization of Birds"}
{"id": "MRQA 2019", "contents": "The MRQA (Machine Reading for Question Answering) dataset is a dataset for evaluating the generalization capabilities of reading comprehension systems.\r\n\r\nSource: [MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension](/paper/mrqa-2019-shared-task-evaluating)", "variants": ["MRQA 2019"], "title": "MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension"}
{"id": "VIPeR", "contents": "The **Viewpoint Invariant Pedestrian Recognition** (**VIPeR**) dataset includes 632 people and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128×48 pixels. It provides the pose angle of each person as 0° (front), 45°, 90° (right), 135°, and 180° (back).\r\n\r\nSource: [PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification](https://arxiv.org/abs/1705.06011)\r\n\r\nImage Source: [Qin et al](https://www.researchgate.net/figure/Some-examples-from-the-VIPeR-dataset-Each-column-is-one-of-632-same-person-example_fig8_331482460)", "variants": ["VIPeR"], "title": "Scalable Person Re-identification: A Benchmark"}
{"id": "SuperGLUE", "contents": "**SuperGLUE** is a benchmark dataset designed to pose a more rigorous test of language understanding than GLUE. SuperGLUE has the same high-level motivation as GLUE: to provide a simple, hard-to-game measure of progress toward general-purpose language understanding technologies for English. SuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around eight language understanding tasks, drawing on existing data, accompanied by a single-number\r\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\r\n\r\n- More challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks were identified from those submitted to an open call for task proposals and were selected based on difficulty for current NLP approaches.\r\n- More diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair classification. The authors expand the set of task formats in SuperGLUE to include\r\ncoreference resolution and question answering (QA).\r\n- Comprehensive human baselines: the authors include human performance estimates for all benchmark tasks, which verify that substantial headroom exists between a strong BERT-based baseline and human performance.\r\n- Improved code support: SuperGLUE is distributed with a new, modular toolkit for work on pretraining, multi-task learning, and transfer learning in NLP, built around standard tools including PyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\r\n- Refined usage rules: The conditions for inclusion on the SuperGLUE leaderboard were revamped to ensure fair competition, an informative leaderboard, and full credit\r\nassignment to data and task creators.", "variants": ["SuperGLUE", "BoolQ", "COPA", "CommitmentBank", "MultiRC", "RTE", "WSC", "Words in Context", "ReCoRD"], "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}
{"id": "MovieLens", "contents": "The **MovieLens** datasets, first released in 1998, describe people’s expressed preferences for movies. These preferences take the form of tuples, each the result of a person expressing a preference (a 0-5 star rating) for a movie at a particular time. These preferences were entered by way of the MovieLens web site1 — a recommender system that asks its users to give movie ratings in order to receive personalized movie recommendations.\r\n\r\nSource: [The MovieLens Datasets: History and Context](http://files.grouplens.org/papers/harper-tiis2015.pdf)\r\nImage Source: [http://files.grouplens.org/papers/harper-tiis2015.pdf](http://files.grouplens.org/papers/harper-tiis2015.pdf)", "variants": ["MovieLens 100K", "MovieLens 10M", "MovieLens 1M", "MovieLens 20M", "MovieLens-Latest", "MovieLens"], "title": "The MovieLens Datasets: History and Context"}
{"id": "Parkinson's Pose Estimation Dataset", "contents": "The data includes all movement trajectories extracted from the videos of Parkinson's assessments using Convolutional Pose Machines (CPM) as well as the confidence values from CPM. The dataset also includes ground truth ratings of parkinsonism and dyskinesia severity using the UDysRS, UPDRS, and CAPSIT.\n\nSource: [https://github.com/limi44/Parkinson-s-Pose-Estimation-Dataset](https://github.com/limi44/Parkinson-s-Pose-Estimation-Dataset)", "variants": ["Parkinson's Pose Estimation Dataset"], "title": "Vision-Based Assessment of Parkinsonism and Levodopa-Induced Dyskinesia with Deep Learning Pose Estimation"}
{"id": "BanglaLekha-Isolated", "contents": "This dataset contains Bangla handwritten numerals, basic characters and compound characters. This dataset was collected from multiple geographical location within Bangladesh and includes sample collected from a variety of aged groups. This dataset can also be used for other classification problems i.e: gender, age, district. \r\n\r\nSource: [BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character Dataset](/paper/banglalekha-isolated-a-comprehensive-bangla)", "variants": ["BanglaLekha Isolated Dataset", "BanglaLekha-Isolated"], "title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character Dataset"}
{"id": "PanoContext", "contents": "The **PanoContext** dataset contains 500 annotated cuboid layouts of indoor environments such as bedrooms and living rooms.\r\n\r\nSource: [LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image](https://arxiv.org/abs/1803.08999)\r\nImage Source: [https://panocontext.cs.princeton.edu/paper.pdf](https://panocontext.cs.princeton.edu/paper.pdf)", "variants": ["PanoContext"], "title": "PanoContext: A Whole-Room 3D Context Model for Panoramic Scene Understanding"}
{"id": "CINIC-10", "contents": "**CINIC-10** is a dataset for image classification. It has a total of 270,000 images, 4.5 times that of CIFAR-10. It is constructed from two different sources: ImageNet and CIFAR-10. Specifically, it was compiled as a bridge between CIFAR-10 and ImageNet. It is split into three equal subsets - train, validation, and test - each of which contain 90,000 images.\r\n\r\nSource: [Group Knowledge Transfer:Collaborative Training of Large CNNs on the Edge](https://arxiv.org/abs/2007.14513)\r\nImage Source: [https://arxiv.org/abs/1810.03505](https://arxiv.org/abs/1810.03505)", "variants": ["CINIC-10"], "title": "CINIC-10 is not ImageNet or CIFAR-10"}
{"id": "Wizard-of-Oz", "contents": "The WoZ 2.0 dataset is a newer dialogue state tracking dataset whose evaluation is detached from the noisy output of speech recognition systems. Similar to DSTC2, it covers the restaurant search domain and has identical evaluation.\r\n\r\nDescription from [NLP Progress](http://nlpprogress.com/english/dialogue.html)\r\n\r\nImage source: [Mrkšić et al.](https://arxiv.org/pdf/1606.03777.pdf)", "variants": ["Wizard-of-Oz"], "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking"}
{"id": "CoNLL-2003", "contents": "**CoNLL-2003** is a named entity recognition dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition.\r\nThe data consists of eight files covering two languages: English and German.\r\nFor each of the languages there is a training file, a development file, a test file and a large file with unannotated data.\r\n\r\nThe English data was taken from the Reuters Corpus. This corpus consists of Reuters news stories between August 1996 and August 1997.\r\nFor the training and development set, ten days worth of data were taken from the files representing the end of August 1996.\r\nFor the test set, the texts were from December 1996. The preprocessed raw data covers the month of September 1996.\r\n\r\nThe text for the German data was taken from the ECI Multilingual Text Corpus. This corpus consists of texts in many languages. The portion of data that\r\nwas used for this task, was extracted from the German newspaper Frankfurter Rundshau. All three of the training, development and test sets were taken\r\nfrom articles written in one week at the end of August 1992.\r\nThe raw data were taken from the months of September to December 1992.\r\n\r\n\r\n| English      data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |\r\n|-------------------|----------|-----------|---------|------|------|------|------|\r\n| Training     set  | 946      | 14,987    | 203,621 | 7140 | 3438 | 6321 | 6600 |\r\n| Development  set  | 216      | 3,466     | 51,362  | 1837 | 922  | 1341 | 1842 |\r\n| Test         set  | 231      | 3,684     | 46,435  | 1668 | 702  | 1661 | 1617 |\r\n\r\nNumber of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in English data files.\r\n\r\n\r\n\r\n| German       data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |\r\n|-------------------|----------|-----------|---------|------|------|------|------|\r\n| Training     set  | 553      | 12,705    | 206,931 | 4363 | 2288 | 2427 | 2773 |\r\n| Development  set  | 201      | 3,068     | 51,444  | 1181 | 1010 | 1241 | 1401 |\r\n| Test         set  | 155      | 3,160     | 51,943  | 1035 | 670  | 773  | 1195 |\r\n\r\nNumber of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in German data files.", "variants": ["CoNLL-2003", "CoNLL03", "CoNLL 2003  English ", "CoNLL 2003 (English)", "CoNLL 2003 (German)", "CONLL 2003 German", "CoNLL 2003 (German) Revised", "CoNLL 2003 NER dev"], "title": "Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"}
{"id": "Middlebury 2001", "contents": "The **Middlebury 2001** is a stereo dataset of indoor scenes with multiple handcrafted layouts.\n\nSource: [https://vision.middlebury.edu/stereo/data/scenes2001/](https://vision.middlebury.edu/stereo/data/scenes2001/)\nImage Source: [https://vision.middlebury.edu/stereo/data/scenes2001/](https://vision.middlebury.edu/stereo/data/scenes2001/)", "variants": ["Middlebury 2001"], "title": "A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms"}
{"id": "NUS-WIDE", "contents": "The **NUS-WIDE** dataset contains 269,648 images with a total of 5,018 tags collected from Flickr. These images are manually annotated with 81 concepts, including objects and scenes.\r\n\r\nSource: [Parallel Grid Pooling for Data Augmentation](https://arxiv.org/abs/1803.11370)\r\n\r\nImage Source: [Li et al](https://www.researchgate.net/publication/273063258_Data_Clustering_Using_Side_Information_Dependent_Chinese_Restaurant_Processes)", "variants": ["NUS-WIDE"], "title": "NUS-WIDE: a real-world web image database from National University of Singapore"}
{"id": "JHMDB", "contents": "**JHMDB** is an action recognition dataset that consists of 960 video sequences belonging to 21 actions. It is a subset of the larger HMDB51 dataset collected from digitized movies and YouTube videos. The dataset contains video and annotation for puppet flow per frame (approximated optimal flow on the person), puppet mask per frame, joint positions per frame, action label per clip and meta label per clip (camera motion, visible body parts, camera viewpoint, number of people, video quality).\r\n\r\nSource: [Unsupervised Deep Metric Learning via Orthogonality based Probabilistic Loss](https://arxiv.org/abs/2008.09880)\r\nImage Source: [https://arxiv.org/pdf/1712.06316.pdf](https://arxiv.org/pdf/1712.06316.pdf)", "variants": ["J-HMDB-21", "J-HMDB", "JHMDB (2D poses only)", "JHMDB Pose Tracking", "J-HMBD Early Action", "JHMDB"], "title": "Towards Understanding Action Recognition"}
{"id": "GOT-10k", "contents": "The **GOT-10k** dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labelled bounding boxes. The dataset contains more than 560 classes of real-world moving objects and 80+ classes of motion patterns.\r\n\r\nSource: [http://got-10k.aitestunion.com/](http://got-10k.aitestunion.com/)\r\nImage Source: [https://arxiv.org/pdf/1810.11981.pdf](https://arxiv.org/pdf/1810.11981.pdf)", "variants": ["GOT-10k"], "title": "GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild"}
{"id": "UCF101", "contents": "**UCF101** dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 × 240.\r\n\r\nSource: [Two-stream Collaborative Learning with Spatial-Temporal Attention for Video Classification](https://arxiv.org/abs/1711.03273)\r\nImage Source: [https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)", "variants": ["UCF-101", "UCF-101 16 frames, 128x128, Unconditional", "UCF-101 16 frames, 64x64, Unconditional", "UCF-101 16 frames, Unconditional, Single GPU", "UCF101", "UCF101-24"], "title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"}
{"id": "Flickr1024", "contents": "Contains 1024 pairs of high-quality images and covers diverse scenarios.\r\n\r\nSource: [Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution](/paper/flickr1024-a-dataset-for-stereo-image-super)", "variants": ["Flickr1024"], "title": "Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution"}
{"id": "MVSEC", "contents": "The Multi Vehicle Stereo Event Camera (MVSEC) dataset is a collection of data designed for the development of novel 3D perception algorithms for event based cameras. Stereo event data is collected from car, motorbike, hexacopter and handheld data, and fused with lidar, IMU, motion capture and GPS to provide ground truth pose and depth images. \r\n\r\nSource: [EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras](/paper/ev-flownet-self-supervised-optical-flow)", "variants": ["MVSEC"], "title": "EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras"}
{"id": "BillSum", "contents": "BillSum is the first dataset for summarization of US Congressional and California state bills.\r\n\r\nThe BillSum dataset consists of three parts: US training bills, US test bills and California test bills. The US bills were collected from the Govinfo service provided by the United States Government Publishing Office (GPO). The corpus consists of bills from the 103rd-115th (1993-2018) sessions of Congress. The data was split into 18,949 train bills and 3,269 test bills. For California, bills from the 2015-2016 session were scraped directly from the legislature’s website; the summaries were written by their Legislative Counsel.\r\n\r\nThe BillSum corpus focuses on mid-length legislation from 5,000 to 20,000 character in length. The authors chose to measure the text length in characters, instead of words or sentences, because the texts have complex structure that makes it difficult to consistently measure words. The range was chosen because on one side, short bills introduce minor changes and do not require summaries. While the CRS produces summaries for them, they often contain most of the text of the bill. On the\r\nother side, very long legislation is often composed of several large sections.\r\n\r\nSource: [BillSum: A Corpus for Automatic Summarization of US Legislation](https://arxiv.org/pdf/1910.00523v2.pdf)", "variants": ["BillSum"], "title": "BillSum: A Corpus for Automatic Summarization of US Legislation"}
{"id": "2D-3D-S", "contents": "The **2D-3D-S** dataset provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. It covers over 6,000 m2 collected in 6 large-scale indoor areas that originate from 3 different buildings. It contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360° equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces.\r\n\r\nSource: [https://github.com/alexsax/2D-3D-Semantics](https://github.com/alexsax/2D-3D-Semantics)\r\nImage Source: [https://github.com/alexsax/2D-3D-Semantics](https://github.com/alexsax/2D-3D-Semantics)", "variants": ["2D-3D-S"], "title": "Joint 2D-3D-Semantic Data for Indoor Scene Understanding"}
{"id": "TUM Visual-Inertial Dataset", "contents": "A novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024x1024 resolution at 20 Hz, high dynamic range and photometric calibration. \r\n\r\nSource: [The TUM VI Benchmark for Evaluating Visual-Inertial Odometry](/paper/the-tum-vi-benchmark-for-evaluating-visual)", "variants": ["TUM Visual-Inertial Dataset"], "title": "The TUM VI Benchmark for Evaluating Visual-Inertial Odometry"}
{"id": "Indoor and outdoor DFD dataset", "contents": "The dfd_indoor dataset contains 110 images for training and 29 images for testing. The dfd_outdoor dataset contains 34 images for tests; no ground truth was given for this dataset, as the depth sensor only works on indoor scenes.\r\n\r\nSource: [DFD dataset](https://github.com/marcelampc/d3net_depth_estimation)", "variants": ["Indoor and outdoor DFD dataset"], "title": "Deep Depth from Defocus: how can defocus blur improve 3D estimation using dense neural networks?"}
{"id": "STAIR Actions Captions", "contents": "A large-scale Japanese video caption dataset consisting of 79,822 videos and 399,233 captions. Each caption in the dataset describes a video in the form of \"who does what and where.\"\r\n\r\nSource: [Video Caption Dataset for Describing Human Actions in Japanese](/paper/video-caption-dataset-for-describing-human)", "variants": ["STAIR Actions Captions"], "title": "Video Caption Dataset for Describing Human Actions in Japanese"}
{"id": "House3D Environment", "contents": "A rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et.al.)\r\n\r\nSource: [Building Generalizable Agents with a Realistic and Rich 3D Environment](/paper/building-generalizable-agents-with-a)", "variants": ["House3D Environment"], "title": "Building Generalizable Agents with a Realistic and Rich 3D Environment"}
{"id": "QUASAR", "contents": "The Question Answering by Search And Reading (**QUASAR**) is a large-scale dataset consisting of [QUASAR-S](quasar-s) and [QUASAR-T](quasar-t). Each of these datasets is built to focus on evaluating systems devised to understand a natural language query, a large corpus of texts and to extract an answer to the question from the corpus. Specifically, QUASAR-S comprises 37,012 fill-in-the-gaps questions that are collected from the popular website Stack Overflow using entity tags. The QUASAR-T dataset contains 43,012 open-domain questions collected from various internet sources. The candidate documents for each question in this dataset are retrieved from an Apache Lucene based search engine built on top of the ClueWeb09 dataset.\r\n\r\nSource: [MRNN: A Multi-Resolution Neural Network with Duplex Attention for Document Retrieval in the Context of Question Answering](https://arxiv.org/abs/1911.00964)\r\nImage Source: [https://arxiv.org/pdf/1707.03904.pdf](https://arxiv.org/pdf/1707.03904.pdf)", "variants": ["Quasar", "QUASAR"], "title": "Quasar: Datasets for Question Answering by Search and Reading"}
{"id": "Polyvore", "contents": "This dataset contains 21,889 outfits from polyvore.com, in which 17,316 are for training, 1,497 for validation and 3,076 for testing.\r\n\r\nSource: [GitHub](https://github.com/xthan/polyvore-dataset)\r\nImage Source: [https://arxiv.org/pdf/1707.05691.pdf](https://arxiv.org/pdf/1707.05691.pdf)", "variants": ["Polyvore"], "title": "Learning Fashion Compatibility with Bidirectional LSTMs"}
{"id": "FB1.5M", "contents": "The **FB1.5M** dataset is a benchmark for Knowledge Graph Completion. It is based on Freebase and it contains 30 relations with less than 500 triplets as low-resource relations.\n\nSource: [https://arxiv.org/pdf/1911.03091.pdf](https://arxiv.org/pdf/1911.03091.pdf)", "variants": ["FB1.5M"], "title": "Relation Adversarial Network for Low Resource Knowledge Graph Completion"}
{"id": "OxUva", "contents": "OxUva is a dataset and benchmark for evaluating single-object tracking algorithms.\r\n\r\nSource: [Long-term Tracking in the Wild: A Benchmark](/paper/long-term-tracking-in-the-wild-a-benchmark)", "variants": ["OxUva"], "title": "Long-term Tracking in the Wild: A Benchmark"}
{"id": "CityFlow", "contents": "CityFlow is a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. \r\n\r\nCamera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). \r\n\r\nSource: [CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification](/paper/cityflow-a-city-scale-benchmark-for-multi)", "variants": ["CityFlow"], "title": "CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification"}
{"id": "CAL500", "contents": "**CAL500** (**Computer Audition Lab 500**) is a dataset aimed for evaluation of music information retrieval systems. It consists of 502 songs picked from western popular music. The audio is represented as a time series of the first 13 Mel-frequency cepstral coefficients (and their first and second derivatives) extracted by sliding a 12 ms half-overlapping short-time window over the waveform of each song. Each song has been annotated by at least 3 people with 135 musically-relevant concepts spanning six semantic categories:\r\n\r\n* 29 instruments were annotated as present in the song or not,\r\n* 22 vocal characteristics were annotated as relevant to the singer or not,\r\n* 36 genres,\r\n* 18 emotions were rated on a scale from one to three (e.g., ``not happy\", ``neutral\", ``happy\"),\r\n* 15 song concepts describing the acoustic qualities of the song, artist and recording (e.g., tempo, energy, sound quality),\r\n* 15 usage terms (e.g., \"I would listen to this song while driving, sleeping, etc.\").\r\n\r\nSource: [http://calab1.ucsd.edu/~datasets/cal500/details_cal500.txt](http://calab1.ucsd.edu/~datasets/cal500/details_cal500.txt)\r\nAudio Source: [http://calab1.ucsd.edu/~datasets/cal500/cal500data/](http://calab1.ucsd.edu/~datasets/cal500/cal500data/)", "variants": ["CAL500"], "title": "Semantic Annotation and Retrieval of Music and Sound Effects"}
{"id": "STARE", "contents": "The **STARE** (**Structured Analysis of the Retina**) dataset is a dataset for retinal vessel segmentation. It contains 20 equal-sized (700×605) color fundus images. For each image, two groups of annotations are provided..\r\n\r\nSource: [DPN: Detail-Preserving Network with High Resolution Representation for Efficient Segmentation of Retinal Vessels](https://arxiv.org/abs/2009.12053)\r\nImage Source: [https://www.researchgate.net/figure/Results-of-the-different-methods-applied-to-the-STARE-dataset-a-original-image-b_fig4_279215756](https://www.researchgate.net/figure/Results-of-the-different-methods-applied-to-the-STARE-dataset-a-original-image-b_fig4_279215756)", "variants": ["STARE"], "title": "Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response"}
{"id": "Driving Event Camera Dataset", "contents": "This dataset consists of a number of sequences that were recorded with a VGA (640x480) event camera (Samsung DVS Gen3) and a conventional RGB camera (Huawei P20 Pro) placed on the windshield of a car driving through Zurich.\r\n\r\nSource: [Driving Event Camera Dataset (Samsung DVS Gen3)](http://rpg.ifi.uzh.ch/event_driving_datasets.html)\r\n\r\nImage Source: [Driving Event Camera Dataset (Samsung DVS Gen3)](http://rpg.ifi.uzh.ch/event_driving_datasets.html)", "variants": ["Driving Event Camera Dataset"], "title": "High Speed and High Dynamic Range Video with an Event Camera"}
{"id": "Dialogue State Tracking Challenge", "contents": "The Dialog State Tracking Challenges 2 & 3 (DSTC2&3) were research challenge focused on improving the state of the art in tracking the state of spoken dialog systems. State tracking, sometimes called belief tracking, refers to accurately estimating the user's goal as a dialog progresses. Accurate state tracking is desirable because it provides robustness to errors in speech recognition, and helps reduce ambiguity inherent in language within a temporal process like dialog.\r\nIn these challenges, participants were given labelled corpora of dialogs to develop state tracking algorithms. The trackers were then evaluated on a common set of held-out dialogs, which were released, un-labelled, during a one week period.\r\n\r\nThe corpus was collected using Amazon Mechanical Turk, and consists of dialogs in two domains: restaurant information, and tourist information. Tourist information subsumes restaurant information, and includes bars, cafés etc. as well as multiple new slots. There were two rounds of evaluation using this data:\r\n\r\nDSTC 2 released a large number of training dialogs related to restaurant search. Compared to DSTC (which was in the bus timetables domain), DSTC 2 introduces changing user goals, tracking 'requested slots' as well as the new restaurants domain. Results from DSTC 2 were presented at SIGDIAL 2014.\r\nDSTC 3 addressed the problem of adaption to a new domain - tourist information. DSTC 3 releases a small amount of labelled data in the tourist information domain; participants will use this data plus the restaurant data from DSTC 2 for training.\r\nDialogs used for training are fully labelled; user transcriptions, user dialog-act semantics and dialog state are all annotated. (This corpus therefore is also suitable for studies in Spoken Language Understanding.)\r\n\r\nSource: [https://github.com/matthen/dstc](https://github.com/matthen/dstc)\r\nImage Source: [https://www.aclweb.org/anthology/W13-4065.pdf](https://www.aclweb.org/anthology/W13-4065.pdf)", "variants": ["Second dialogue state tracking challenge", "Dialogue State Tracking Challenge"], "title": "The Dialog State Tracking Challenge"}
{"id": "MedHop", "contents": "With the same format as WikiHop, the MedHop dataset is based on research paper abstracts from PubMed, and the queries are about interactions between pairs of drugs. The correct answer has to be inferred by combining information from a chain of reactions of drugs and proteins.\r\n\r\nSource: [QAngaroo](http://qangaroo.cs.ucl.ac.uk/)", "variants": ["MedHop"], "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents"}
{"id": "CelebAMask-HQ", "contents": "**CelebAMask-HQ** is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has segmentation mask of facial attributes corresponding to CelebA.\r\n\r\nSource: [https://github.com/switchablenorms/CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ)\r\nImage Source: [https://github.com/switchablenorms/CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ)", "variants": ["CelebAMask-HQ"], "title": "MaskGAN: Towards Diverse and Interactive Facial Image Manipulation"}
{"id": "SCUT-HEAD", "contents": "Includes 4405 images with 111251 heads annotated.\r\n\r\nSource: [Detecting Heads using Feature Refine Net and Cascaded Multi-Scale Architecture](/paper/detecting-heads-using-feature-refine-net-and)", "variants": ["SCUT-HEAD"], "title": "Detecting Heads using Feature Refine Net and Cascaded Multi-scale Architecture"}
{"id": "CRVD", "contents": "The CRVD dataset consists of 55 groups of noisy-clean videos with ISO values ranging from 1600 to 25600.\r\n\r\nSource: [Supervised Raw Video Denoising with a Benchmark Dataset on Dynamic Scenes](/paper/supervised-raw-video-denoising-with-a)", "variants": ["CRVD"], "title": "Supervised Raw Video Denoising with a Benchmark Dataset on Dynamic Scenes"}
{"id": "GQA", "contents": "The **GQA** dataset is a large-scale visual question answering dataset with real images from the Visual Genome dataset and balanced question-answer pairs. Each training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images and question-answer pairs, the GQA dataset provides two types of pre-extracted visual features for each image – convolutional grid features of size 7×7×2048 extracted from a ResNet-101 network trained on ImageNet, and object detection features of size Ndet×2048 (where Ndet is the number of detected objects in each image with a maximum of 100 per image) from a Faster R-CNN detector.\r\n\r\nSource: [Language-Conditioned Graph Networks for Relational Reasoning](https://arxiv.org/abs/1905.04405)\r\nImage Source: [https://arxiv.org/pdf/1902.09506.pdf](https://arxiv.org/pdf/1902.09506.pdf)", "variants": ["GQA test-std", "GQA test-dev", "GQA Test2019", "GQA"], "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}
{"id": "Oxford 102 Flower", "contents": "**Oxford 102 Flower** is an image classification dataset consisting of 102 flower categories. The flowers chosen to be flower commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images.\r\n\r\nThe images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories.", "variants": ["Oxford 102 Flower", "Flowers", "Flowers-102", "Flowers-102 - 0-Shot", "Flowers (Fine-grained 6 Tasks)", "Oxford 102 Flowers", "Oxford 102 Flowers 256 x 256"], "title": "Automated Flower Classification over a Large Number of Classes"}
{"id": "PASCAL VOC 2011", "contents": "**PASCAL VOC 2011** is an image segmentation dataset. It contains around 2,223 images for training, consisting of 5,034 objects. Testing consists of 1,111 images with 2,028 objects. In total there are over 5,000 precisely segmented objects for training.\r\n\r\nSource: [Scene Parsing with Integration of Parametric and Non-parametric Models](https://arxiv.org/abs/1604.05848)\r\nImage Source: [http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2011/index.html](http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2011/index.html)", "variants": ["PASCAL VOC 2011 test", "PASCAL VOC 2011"], "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}
{"id": "irc-disentanglement", "contents": "This is a dataset for disentangling conversations on IRC, which is the task of identifying separate conversations in a single stream of messages. It contains disentanglement information for 77,563 messages or IRC.\n\nSource: [https://github.com/jkkummerfeld/irc-disentanglement](https://github.com/jkkummerfeld/irc-disentanglement)\nImage Source: [https://github.com/jkkummerfeld/irc-disentanglement](https://github.com/jkkummerfeld/irc-disentanglement)", "variants": ["irc-disentanglement", "Linux IRC (Ch2 Kummerfeld)", "Linux IRC (Ch2 Elsner)"], "title": "Analyzing Assumptions in Conversation Disentanglement Research Through the Lens of a New Dataset and Model"}
{"id": "ActivityNet", "contents": "The **ActivityNet** dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.\r\n\r\nSource: [Dynamic Temporal Pyramid Network: A Closer Look at Multi-Scale Modeling for Activity Detection](https://arxiv.org/abs/1808.02536)", "variants": ["ActivityNet", "ActivityNet-Entities", "ActivityNet-1.3", "ActivityNet-1.2"], "title": "ActivityNet: A large-scale video benchmark for human activity understanding"}
{"id": "Immediacy Dataset", "contents": "Consists of 10,000 images is constructed, in which all the immediacy measures and the human poses are annotated.\r\n\r\nSource: [Multi-Task Recurrent Neural Network for Immediacy Prediction](/paper/multi-task-recurrent-neural-network-for)", "variants": ["Immediacy Dataset"], "title": "Multi-task Recurrent Neural Network for Immediacy Prediction"}
{"id": "BLiMP", "contents": "BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars. Aggregate human agreement with the labels is 96.4%.\r\n\r\nSource: [BLiMP ](https://github.com/alexwarstadt/blimp)\r\nImage Source: [https://arxiv.org/pdf/1912.00582v3.pdf](https://arxiv.org/pdf/1912.00582v3.pdf)", "variants": ["BLiMP"], "title": "BLiMP: A Benchmark of Linguistic Minimal Pairs for English"}
{"id": "Talk2Nav", "contents": "Talk2Nav is a large-scale dataset with verbal navigation instructions.\r\n\r\nSource: [Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory](/paper/talk2nav-long-range-vision-and-language)\r\nImage Source: [https://www.trace.ethz.ch/publications/2019/talk2nav/index.html](https://www.trace.ethz.ch/publications/2019/talk2nav/index.html)", "variants": ["Talk2Nav"], "title": "Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory"}
{"id": "ClipShots", "contents": "**ClipShots** is a large-scale dataset for shot boundary detection collected from Youtube and Weibo covering more than 20 categories, including sports, TV shows, animals, etc. In contrast to previous shot boundary detection datasets, e.g. TRECVID and RAI, which only consist of documentaries or talk shows where the frames are relatively static, ClipShots contains moslty short videos from Youtube and Weibo. Many short videos are home-made, with more challenges, e.g. hand-held vibrations and large occlusion. The types of these videos are various, including movie spotlights, competition highlights, family videos recorded by mobile phones etc. Each video has a length of 1-20 minutes. The gradual transitions in the dataset include dissolve, fade in fade out, and sliding in sliding out.\n\nSource: [https://github.com/Tangshitao/ClipShots](https://github.com/Tangshitao/ClipShots)", "variants": ["ClipShots"], "title": "Fast Video Shot Transition Localization with Deep Structured Models"}
{"id": "Middlebury 2005", "contents": "**Middlebury 2005** is a stereo dataset of indoor scenes.\n\nSource: [https://vision.middlebury.edu/stereo/data/scenes2005/](https://vision.middlebury.edu/stereo/data/scenes2005/)\nImage Source: [https://vision.middlebury.edu/stereo/data/scenes2005/](https://vision.middlebury.edu/stereo/data/scenes2005/)", "variants": ["Middlebury 2005"], "title": "Learning Conditional Random Fields for Stereo"}
{"id": "Kitchen Scenes", "contents": "Kitchen Scenes is a multi-view RGB-D dataset of nine kitchen scenes, each containing several objects in realistic cluttered environments including a subset of objects from the BigBird dataset. The viewpoints of the scenes are densely sampled and objects in the scenes are annotated with bounding boxes and in the 3D point cloud. \r\n\r\nSource: [Multiview RGB-D Dataset for Object Instance Detection](/paper/multiview-rgb-d-dataset-for-object-instance)", "variants": ["Kitchen Scenes"], "title": "Multiview RGB-D Dataset for Object Instance Detection"}
{"id": "OpenMIC-2018", "contents": "**OpenMIC-2018** is an instrument recognition dataset containing 20,000 examples of Creative Commons-licensed music available on the [Free Music Archive](http://freemusicarchive.org/). Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform.\n\nSource: [OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition](http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf)\nImage Source: [OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition](http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf)\nAudio Source: [https://zenodo.org/record/1432913](https://zenodo.org/record/1432913)", "variants": ["OpenMIC-2018"], "title": "OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition"}
{"id": "LasVR", "contents": "A large-scale video database for rain removal (LasVR), which consists of 316 rain videos.\r\n\r\nSource: [Removing Rain in Videos: A Large-scale Database and A Two-stream ConvLSTM Approach](/paper/removing-rain-in-videos-a-large-scale)", "variants": ["LasVR"], "title": "Removing Rain in Videos: A Large-Scale Database and a Two-Stream ConvLSTM Approach"}
{"id": "Topology Optimization Dataset", "contents": "TOP is a synthetic dataset for topology optimization generated using Topy. The generated dataset has 10,000 objects which consist on 100 iterations of the optimization process for the problem defined on a regular 40 x 40 grid.\n\nSource: [https://arxiv.org/pdf/1709.09578.pdf](https://arxiv.org/pdf/1709.09578.pdf)\nImage Source: [https://github.com/ISosnovik/top](https://github.com/ISosnovik/top)", "variants": ["Topology Optimization Dataset"], "title": "Neural networks for topology optimization"}
{"id": "HACS", "contents": "HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.\r\n\r\nAuthors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled\r\nfrom 492K, 6K and 6K videos, respectively.", "variants": ["HACS"], "title": "HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization"}
{"id": "QuaRel", "contents": "QuaRel is a crowdsourced dataset of 2771 multiple-choice story questions, including their logical forms.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/quarel)", "variants": ["QuaRel"], "title": "QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships"}
{"id": "MemexQA", "contents": "A large, realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers.\r\n\r\nSource: [MemexQA: Visual Memex Question Answering](/paper/memexqa-visual-memex-question-answering)", "variants": ["MemexQA"], "title": "MemexQA: Visual Memex Question Answering"}
{"id": "Plant Seedlings Dataset", "contents": "A database of images of approximately 960 unique plants belonging to 12 species at several growth stages is made publicly available. It comprises annotated RGB images with a physical resolution of roughly 10 pixels per mm.\r\n\r\nSource: [A Public Image Database for Benchmark of Plant Seedling Classification Algorithms](/paper/a-public-image-database-for-benchmark-of)", "variants": ["Plant Seedlings Dataset"], "title": "A Public Image Database for Benchmark of Plant Seedling Classification Algorithms"}
{"id": "Cell", "contents": "The CELL benchmark is made of fluorescence microscopy images of cell. \r\n\r\nSource: [Multi-Domain Adversarial Learning](/paper/multi-domain-adversarial-learning-1)\r\n\r\nImage Source: [https://arxiv.org/pdf/1903.09239v1.pdf](https://arxiv.org/pdf/1903.09239v1.pdf)", "variants": ["Cell17", "Cell", "CellNet"], "title": "Multi-Domain Adversarial Learning"}
{"id": "LIAR", "contents": "LIAR is a publicly available dataset for fake news detection. A decade-long of 12.8K manually labeled short statements were collected in various contexts from POLITIFACT.COM, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. The LIAR dataset4 includes 12.8K human labeled short statements from POLITIFACT.COM’s API, and each statement is evaluated by a POLITIFACT.COM editor for its truthfulness. \r\n\r\nSource: [“Liar, Liar Pants on Fire”: A New Benchmark Dataset for Fake News Detection](https://www.aclweb.org/anthology/P17-2067.pdf)", "variants": ["LIAR"], "title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection"}
{"id": "ECUSTFD", "contents": "The **ECUST Food Dataset** is a food recognition dataset that contains 2978 images\n\nSource: [https://github.com/Liang-yc/ECUSTFD-resized-](https://github.com/Liang-yc/ECUSTFD-resized-)\nImage Source: [https://github.com/Liang-yc/ECUSTFD-resized-](https://github.com/Liang-yc/ECUSTFD-resized-)", "variants": ["ECUSTFD"], "title": "Computer vision-based food calorie estimation: dataset, method, and experiment"}
{"id": "LISA Gaze Dataset", "contents": "LISA Gaze is a dataset for driver gaze estimation comprising of 11 long drives, driven by 10 subjects in two different cars. \r\n\r\nSource: [Driver Gaze Zone Estimation using Convolutional Neural Networks: A General Framework and Ablative Analysis](/paper/driver-gaze-zone-estimation-using)", "variants": ["LISA Gaze Dataset"], "title": "Driver Gaze Zone Estimation Using Convolutional Neural Networks: A General Framework and Ablative Analysis"}
{"id": "VizWiz-Captions", "contents": "Consists of over 39,000 images originating from people who are blind that are each paired with five captions.\r\n\r\nSource: [Captioning Images Taken by People Who Are Blind](/paper/captioning-images-taken-by-people-who-are)", "variants": ["VizWiz-Captions"], "title": "Captioning Images Taken by People Who Are Blind"}
{"id": "Photographic Defect Severity", "contents": "A large-scale dataset of user annotations on seven common photographic defects.\r\n\r\nSource: [Learning to Detect Multiple Photographic Defects](/paper/learning-to-detect-multiple-photographic)", "variants": ["Photographic Defect Severity"], "title": "Learning to Detect Multiple Photographic Defects"}
{"id": "LSUN", "contents": "The Large-scale Scene Understanding (**LSUN**) challenge aims to provide a different benchmark for large-scale scene classification and understanding. The LSUN classification dataset contains 10 scene categories, such as dining room, bedroom, chicken, outdoor church, and so on. For training data, each category contains a huge number of images, ranging from around 120,000 to 3,000,000. The validation data includes 300 images, and the test data has 1000 images for each category.\r\n\r\nSource: [Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs](https://arxiv.org/abs/1610.01119)\r\nImage Source: [https://www.yf.io/p/lsun](https://www.yf.io/p/lsun)", "variants": ["LSUN Bedroom 256 x 256", "LSUN Bedroom 64 x 64", "LSUN Car 256 x 256", "LSUN Car 512 x 384", "LSUN Cat 256 x 256", "LSUN Churches 256 x 256", "LSUN Horse 256 x 256", "LSUN", "LSUN Bedroom", "LSUN Bedroom 128 x 128", "LSUN Roon Layout"], "title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop"}
{"id": "WNLaMPro", "contents": "The **WordNet Language Model Probing** (**WNLaMPro**) dataset consists of relations between keywords and words. It contains 4 different kinds of relations: Antonym, Hypernym, Cohyponym and Corruption.\n\nSource: [https://arxiv.org/pdf/1904.06707.pdf](https://arxiv.org/pdf/1904.06707.pdf)", "variants": ["WNLaMPro"], "title": "Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"}
{"id": "New York Times Annotated Corpus", "contents": "The **New York Times Annotated Corpus** contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. The corpus includes:\r\n\r\n- Over 1.8 million articles (excluding wire services articles that appeared during the covered period).\r\n- Over 650,000 article summaries written by library scientists.\r\n- Over 1,500,000 articles manually tagged by library scientists with tags drawn from a normalized indexing vocabulary of people, organizations, locations and topic descriptors.\r\n- Over 275,000 algorithmically-tagged articles that have been hand verified by the online production staff at nytimes.com.\r\nAs part of the New York Times' indexing procedures, most articles are manually summarized and tagged by a staff of library scientists. This collection contains over 650,000 article-summary pairs which may prove to be useful in the development and evaluation of algorithms for automated document summarization. Also, over 1.5 million documents have at least one tag. Articles are tagged for persons, places, organizations, titles and topics using a controlled vocabulary that is applied consistently across articles. For instance if one article mentions \"Bill Clinton\" and another refers to \"President William Jefferson Clinton\", both articles will be tagged with \"CLINTON, BILL\".\r\n\r\nSource: [https://catalog.ldc.upenn.edu/LDC2008T19](https://catalog.ldc.upenn.edu/LDC2008T19)", "variants": ["New York Times Annotated Corpus"], "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"}
{"id": "DDI-100", "contents": "The DDI-100 dataset is a synthetic dataset for text detection and recognition based on 7000 real unique document pages and consists of more than 100000 augmented images. The ground truth comprises text and stamp masks, text and characters bounding boxes with relevant annotations.\r\n\r\nSource: [DDI-100](https://arxiv.org/pdf/1912.11658v1.pdf)", "variants": ["DDI-100"], "title": "DDI-100: Dataset for Text Detection and Recognition"}
{"id": "FMD", "contents": "The **Fluorescence Microscopy Denoising** (**FMD**) dataset is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. Image averaging is used to effectively obtain ground truth images and 60,000 noisy images with different noise levels.\n\nSource: [https://arxiv.org/abs/1812.10366](https://arxiv.org/abs/1812.10366)\nImage Source: [https://github.com/bmmi/denoising-fluorescence](https://github.com/bmmi/denoising-fluorescence)", "variants": ["FMD"], "title": "A Poisson-Gaussian Denoising Dataset With Real Fluorescence Microscopy Images"}
{"id": "TGIF-QA", "contents": "The TGIF-QA dataset contains 165K QA pairs for the animated GIFs from the TGIF dataset [Li et al. CVPR 2016]. The question & answer pairs are collected via crowdsourcing with a carefully designed user interface to ensure quality. The dataset can be used to evaluate video-based Visual Question Answering techniques.\r\n\r\nSource: [GitHub](https://github.com/YunseokJANG/tgif-qa)\r\nImage Source: [https://github.com/YunseokJANG/tgif-qa](https://github.com/YunseokJANG/tgif-qa)", "variants": ["TGIF-QA"], "title": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"}
{"id": "Linux", "contents": "The LINUX dataset consists of 48,747 Program Dependence Graphs (PDG) generated from the **Linux** kernel. Each graph represents a function, where a node represents one statement and an edge represents the dependency between the two statements\n\nSource: [Convolutional Set Matching for Graph Similarity](https://arxiv.org/abs/1810.10866)", "variants": ["Linux"], "title": "An Efficient Graph Indexing Method"}
{"id": "WikiSRS", "contents": "**WikiSRS** is a novel dataset of similarity and relatedness judgments of paired Wikipedia entities (people, places, and organizations), as assigned by Amazon Mechanical Turk workers.\n\nSource: [https://github.com/OSU-slatelab/WikiSRS](https://github.com/OSU-slatelab/WikiSRS)", "variants": ["WikiSRS"], "title": "Jointly Embedding Entities and Text with Distant Supervision"}
{"id": "Reddit Corpus", "contents": "**Reddit Corpus** is part of a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using '1-of-100 accuracy'. The Reddit Corpus contains 726 million multi-turn dialogues from the Reddit board. \r\n\r\nSource: [conversational-datasets](https://github.com/PolyAI-LDN/conversational-datasets)\r\n\r\nImage source: [Henderson et al.](ttps://arxiv.org/pdf/1904.06472.pdf)", "variants": ["PolyAI Reddit", "Reddit Corpus"], "title": "A Repository of Conversational Datasets"}
{"id": "WLD", "contents": "**WildLife Documentary** is an animal object detection dataset. It contains 15 documentary films that are downloaded from YouTube. The videos vary between 9 minutes to as long as 50 minutes, with resolution ranging from 360p\r\nto 1080p. A unique property of this dataset is that all videos are accompanied with subtitles that are automatically generated from speech by YouTube. The subtitles are revised manually to correct obvious spelling mistakes. All the animals in the videos are annotated, resulting in more than 4098 object tracklets of 60 different visual\r\nconcepts, e.g., ‘tiger’, ‘koala’, ‘langur’, and ‘ostrich’.", "variants": ["WLD"], "title": "Discover and Learn New Objects from Documentaries"}
{"id": "AffectNet", "contents": "**AffectNet** is a large facial expression dataset with around 0.4 million images manually labeled for the presence of eight (neutral, happy, angry, sad, fear, surprise, disgust, contempt) facial expressions along with the intensity of valence and arousal.\r\n\r\nSource: [Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition](https://arxiv.org/abs/2007.10298)\r\nImage Source: [http://mohammadmahoor.com/affectnet/](http://mohammadmahoor.com/affectnet/)", "variants": ["AffectNet"], "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild"}
{"id": "UDC", "contents": "**Ubuntu Dialogue Corpus** (**UDC**) is a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. \r\n\r\nSource: [The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems](https://arxiv.org/pdf/1506.08909v3.pdf)", "variants": ["UDC", "Ubuntu Dialogue (Activity)", "Ubuntu Dialogue (Cmd)", "Ubuntu Dialogue (Entity)", "Ubuntu Dialogue (Tense)", "Ubuntu Dialogue (v1, Ranking)", "Ubuntu Dialogue (v2, Ranking)"], "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems"}
{"id": "EYTH", "contents": "Includes egocentric videos containing hands in the wild.\r\n\r\nSource: [Analysis of Hand Segmentation in the Wild](/paper/analysis-of-hand-segmentation-in-the-wild)", "variants": ["EYTH"], "title": "Analysis of Hand Segmentation in the Wild"}
{"id": "MultiFC", "contents": "Publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. \r\n\r\nSource: [MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims](/paper/multifc-a-real-world-multi-domain-dataset-for)", "variants": ["MultiFC"], "title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims"}
{"id": "SUNCG", "contents": "**SUNCG** is a large-scale dataset of synthetic 3D scenes with dense volumetric annotations.\r\n\r\nThe dataset is currently not available.\r\n\r\nSource: [https://sscnet.cs.princeton.edu/](https://sscnet.cs.princeton.edu/)\r\nImage Source: [https://sscnet.cs.princeton.edu/](https://sscnet.cs.princeton.edu/)", "variants": ["SUNCG"], "title": "Semantic Scene Completion from a Single Depth Image"}
{"id": "Conceptual Captions", "contents": "Automatic image captioning is the task of producing a natural-language utterance (usually a sentence) that correctly reflects the visual content of an image. Up to this point, the resource most used for this task was the MS-COCO dataset, containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).\r\n\r\nGoogle's Conceptual Captions dataset has more than 3 million images, paired with natural-language captions. In contrast with the curated style of the MS-COCO images, Conceptual Captions images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. The raw descriptions are harvested from the Alt-text HTML attribute associated with web images. The authors developed an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.\r\n\r\nSource: [Conceptual Captions](https://github.com/google-research-datasets/conceptual-captions)\r\nImage Source: [Sharma et al](https://www.aclweb.org/anthology/P18-1238)", "variants": ["Conceptual Captions"], "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}
{"id": "Simitate", "contents": "**Simitate** is a hybrid benchmarking suite targeting the evaluation of approaches for imitation learning. It consists on a dataset containing 1938 sequences where humans perform daily activities in a realistic environment. The dataset is strongly coupled with an integration into a simulator. RGB and depth streams with a resolution of 960×540 at 30Hz and accurate ground truth poses for the demonstrator's hand, as well as the object in 6 DOF at 120Hz are provided. Along with the dataset the 3D model of the used environment and labelled object images are also provided.\n\nSource: [https://arxiv.org/abs/1905.06002](https://arxiv.org/abs/1905.06002)\nImage Source: [https://github.com/raphaelmemmesheimer/simitate](https://github.com/raphaelmemmesheimer/simitate)", "variants": ["Simitate"], "title": "Simitate: A Hybrid Imitation Learning Benchmark"}
{"id": "Spoken-SQuAD", "contents": "In SpokenSQuAD, the document is in spoken form, the input question is in the form of text and the answer to each question is always a span in the document. The following procedures were used to generate spoken documents from the original SQuAD dataset. First, the Google text-to-speech system was used to generate the spoken version of the articles in SQuAD. Then CMU Sphinx was sued to generate the corresponding ASR transcriptions. The SQuAD training set was used to generate the training set of Spoken SQuAD, and SQuAD development set was used to generate the testing set for Spoken SQuAD. If the answer of a question did not exist in the ASR transcriptions of the associated article, the question-answer pair was removed from the dataset because these examples are too difficult for listening comprehension machine at this stage.\n\nSource: [https://github.com/chiahsuan156/Spoken-SQuAD](https://github.com/chiahsuan156/Spoken-SQuAD)\nImage Source: [https://github.com/chiahsuan156/Spoken-SQuAD](https://github.com/chiahsuan156/Spoken-SQuAD)", "variants": ["Spoken-SQuAD"], "title": "Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension"}
{"id": "LC25000", "contents": "The **LC25000** dataset contains 25,000 color images with 5 classes of 5,000 images each. All images are 768 x 768 pixels in size and are in jpeg file format. The 5 classes are: colon adenocarcinomas, benign colonic tissues, lung adenocarcinomas, lung squamous cell carcinomas and bening lung tissues.\n\nSource: [https://github.com/tampapath/lung_colon_image_set](https://github.com/tampapath/lung_colon_image_set)", "variants": ["LC25000"], "title": "Lung and Colon Cancer Histopathological Image Dataset (LC25000)"}
{"id": "Semantic Scholar", "contents": "The **Semantic Scholar** corpus (S2) is composed of titles from scientific papers published in machine learning conferences and journals from 1985 to 2017, split by year (33 timesteps).\r\n\r\nSource: [Learning Dynamic Author Representations with Temporal Language Models](https://arxiv.org/abs/1909.04985)\r\nImage Source: [http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/](http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/)", "variants": ["Semantic Scholar"], "title": "Construction of the Literature Graph in Semantic Scholar"}
{"id": "Visual Genome", "contents": "**Visual Genome** contains Visual Question Answering data in a multi-choice setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs, 17 questions per image on average. Compared to the Visual Question Answering dataset, Visual Genome represents a more balanced distribution over 6 question types: What, Where, When, Who, Why and How. The Visual Genome dataset also presents 108K images with densely annotated objects, attributes and relationships.\r\n\r\nSource: [RaAM: A Relation-aware Attention Model for Visual Question Answering](https://arxiv.org/abs/1903.12314)\r\nImage Source: [Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://paperswithcode.com/paper/visual-genome-connecting-language-and-vision/)", "variants": ["Visual Genome", "Visual Genome (pairs)", "Visual Genome (subjects)", "Visual Genome 128x128", "Visual Genome 64x64", "Visual Genome 256x256"], "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}
{"id": "MED", "contents": "**MED** is a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications. The dataset was constructed by collecting naturally-occurring examples by crowdsourcing and well-designed ones from linguistics publications.\r\nIt consists of 5,382 examples.\r\n\r\nSource: [https://github.com/verypluming/MED](https://github.com/verypluming/MED)\r\nImage Source: [https://www.aclweb.org/anthology/W19-4804v2.pdf](https://www.aclweb.org/anthology/W19-4804v2.pdf)", "variants": ["MED"], "title": "Can neural networks understand monotonicity reasoning?"}
{"id": "Jamendo Lyrics", "contents": "Dataset for lyrics alignment and transcription evaluation. It contains 20 music pieces under CC license from the Jamendo website along with their lyrics, with:\r\n\r\n* Manual annotations indicating the start time of each word in the audio file\r\n* Predictions of start and end times for each word from both of the models presented in the paper", "variants": ["Jamendo Lyrics"], "title": "End-to-end Lyrics Alignment for Polyphonic Music Using an Audio-to-character Recognition Model"}
{"id": "Chairs", "contents": "The **Chairs** dataset contains rendered images of around 1000 different three-dimensional chair models.\r\n\r\nSource: [Adversarial Disentanglement with Grouped Observations](https://arxiv.org/abs/2001.04761)\r\nImage Source: [https://www.di.ens.fr/willow/research/seeing3Dchairs/](https://www.di.ens.fr/willow/research/seeing3Dchairs/)", "variants": ["Chairs"], "title": "Seeing 3D Chairs: Exemplar Part-Based 2D-3D Alignment Using a Large Dataset of CAD Models"}
{"id": "ABC Dataset", "contents": "The **ABC Dataset** is a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms.", "variants": ["ABC Dataset"], "title": "ABC: A Big CAD Model Dataset for Geometric Deep Learning"}
{"id": "ATOMIC", "contents": "**ATOMIC** is an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., \"if X pays Y a compliment, then Y will likely return the compliment\").\r\n\r\nSource: [ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning](/paper/atomic-an-atlas-of-machine-commonsense-for-if)\r\nImage Source: [https://homes.cs.washington.edu/~msap/atomic/](https://homes.cs.washington.edu/~msap/atomic/)", "variants": ["ATOMIC"], "title": "ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning"}
{"id": "RONEC", "contents": "Romanian Named Entity Corpus is a named entity corpus for the Romanian language. The corpus contains over 26000 entities in ~5000 annotated sentences, belonging to 16 distinct classes. The sentences have been extracted from a copy-right free newspaper, covering several styles. This corpus represents the first initiative in the Romanian language space specifically targeted for named entity recognition. \r\n\r\nSource: [Introducing RONEC -- the Romanian Named Entity Corpus](/paper/introducing-ronec-the-romanian-named-entity)", "variants": ["RONEC"], "title": "Introducing RONEC -- the Romanian Named Entity Corpus"}
{"id": "CoQA", "contents": "**CoQA** is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation.\r\n\r\nCoQA contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains. CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.\r\n\r\nSource: [https://stanfordnlp.github.io/coqa/](https://stanfordnlp.github.io/coqa/)\r\nImage Source: [https://stanfordnlp.github.io/coqa/](https://stanfordnlp.github.io/coqa/)", "variants": ["CoQA"], "title": "CoQA: A Conversational Question Answering Challenge"}
{"id": "WN18RR", "contents": "**WN18RR** is a link prediction dataset created from WN18, which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples are obtained by inverting triples from the training set. Thus the WN18RR dataset is created to ensure that the evaluation dataset does not have inverse relation test leakage. In summary, WN18RR dataset contains 93,003 triples with 40,943 entities and 11 relation types.\r\n\r\nSource: [End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion](https://arxiv.org/abs/1811.04441)", "variants": ["WN18RR"], "title": "Convolutional 2D Knowledge Graph Embeddings"}
{"id": "EV-IMO", "contents": "Includes accurate pixel-wise motion masks, egomotion and ground truth depth. \r\n\r\nSource: [EV-IMO: Motion Segmentation Dataset and Learning Pipeline for Event Cameras](/paper/ev-imo-motion-segmentation-dataset-and)", "variants": ["EV-IMO"], "title": "EV-IMO: Motion Segmentation Dataset and Learning Pipeline for Event Cameras"}
{"id": "HouseExpo", "contents": "A large-scale indoor layout dataset containing 35,357 2D floor plans including 252,550 rooms in total.\r\n\r\nSource: [HouseExpo: A Large-scale 2D Indoor Layout Dataset for Learning-based Algorithms on Mobile Robots](/paper/houseexpo-a-large-scale-2d-indoor-layout)", "variants": ["HouseExpo"], "title": "HouseExpo: A Large-scale 2D Indoor Layout Dataset for Learning-based Algorithms on Mobile Robots"}
{"id": "METU-VIREF Dataset", "contents": "**METU-VIREF** is a video referring expression dataset comprising of videos from VIRAT Ground and ILSVRC2015 VID datasets. VIRAT is a surveillance dataset and contains mainly people and vehicles. To line up with this and restrict the domain, only videos that contain vehicles from the ILSVRC dataset are used. The METU-VIREF dataset does not contain whole videos from these datasets (the videos need to be downloaded from the respective sources) but just referring expressions for video sequences containing an object pair. For this, object pairs are chosen which had a relation that a meaningful referring expression could be written for.\r\n\r\nSource: [VIREF](https://github.com/hazananayurt/viref)", "variants": ["METU-VIREF Dataset"], "title": "Searching for Ambiguous Objects in Videos using Relational Referring Expressions"}
{"id": "MIDAS-KIKI", "contents": "Consists of manually annotated dangerous and non-dangerous Kiki challenge videos. \r\n\r\nSource: [Kiki Kills: Identifying Dangerous Challenge Videos from Social Media](/paper/kiki-kills-identifying-dangerous-challenge)", "variants": ["MIDAS-KIKI"], "title": "Kiki Kills: Identifying Dangerous Challenge Videos from Social Media"}
{"id": "irc-disentanglement", "contents": "This is a dataset for disentangling conversations on IRC, which is the task of identifying separate conversations in a single stream of messages. It contains disentanglement information for 77,563 messages or IRC.\n\nSource: [https://github.com/jkkummerfeld/irc-disentanglement](https://github.com/jkkummerfeld/irc-disentanglement)\nImage Source: [https://github.com/jkkummerfeld/irc-disentanglement](https://github.com/jkkummerfeld/irc-disentanglement)", "variants": ["irc-disentanglement", "Linux IRC (Ch2 Kummerfeld)", "Linux IRC (Ch2 Elsner)"], "title": "A Large-Scale Corpus for Conversation Disentanglement"}
{"id": "CIFAR-FS", "contents": "**CIFAR100 few-shots** (**CIFAR-FS**) is randomly sampled from CIFAR-100 (Krizhevsky & Hinton, 2009) by using the same criteria with which miniImageNet has been generated. The average inter-class similarity is sufficiently high to represent a challenge for the current state of the art. Moreover, the limited original resolution of 32×32 makes the task harder and at the same time allows fast prototyping.\r\n\r\nSource: [Bertinetto et al.](https://arxiv.org/pdf/1805.08136.pdf)\r\nImage source: [Bertinetto et al.](https://www.robots.ox.ac.uk/~luca/r2d2.html)", "variants": ["CIFAR-FS 5-way (1-shot)", "CIFAR-FS 5-way (5-shot)", "CIFAR-FS"], "title": "Meta-learning with differentiable closed-form solvers"}
{"id": "MMD", "contents": "The MMD (MultiModal Dialongs) dataset is a dataset for multimodal domain-aware conversations. It consists of over 150K conversation sessions between shoppers and sales agents, annotated by a group of in-house annotators using a semi-automated manually intense iterative process. \r\n\r\nSource: [Towards Building Large Scale Multimodal Domain-Aware Conversation Systems](/paper/towards-building-large-scale-multimodal)", "variants": ["MMD"], "title": "Multimodal Dialogs (MMD): A large-scale dataset for studying multimodal domain-aware conversations"}
{"id": "HELP", "contents": "The **HELP** dataset is an automatically created natural language inference (NLI) dataset that embodies the combination of lexical and logical inferences focusing on monotonicity (i.e., phrase replacement-based reasoning). The HELP (Ver.1.0) has 36K inference pairs consisting of upward monotone, downward monotone, non-monotone, conjunction, and disjunction.\r\n\r\nSource: [HELP](https://github.com/verypluming/HELP)", "variants": ["Help, Anna! (HANNA)", "HELP"], "title": "HELP: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning"}
{"id": "JuICe", "contents": "JuICe is a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data.\r\n\r\nSource: [JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation](https://arxiv.org/pdf/1910.02216v2.pdf)", "variants": ["JuICe"], "title": "JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"}
{"id": "KdConv", "contents": "KdConv is a Chinese multi-domain Knowledge-driven Conversation dataset, grounding the topics in multi-turn conversations to knowledge graphs. KdConv contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics, while the corpus can also used for exploration of transfer learning and domain adaptation.\r\n\r\nSource: [KdConv](https://github.com/thu-coai/KdConv)", "variants": ["KdConv"], "title": "KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation"}
{"id": "MultiMNIST", "contents": "The **MultiMNIST** dataset is generated from MNIST. The training and tests are generated by overlaying a digit on top of another digit from the same set (training or test) but different class. Each digit is shifted up to 4 pixels in each direction resulting in a 36×36 image. Considering a digit in a 28×28 image is bounded in a 20×20 box, two digits bounding boxes on average have 80% overlap. For each digit in the MNIST dataset 1,000 MultiMNIST examples are generated, so the training set size is 60M and the test set size is 10M.\r\n\r\nSource: [https://arxiv.org/pdf/1710.09829.pdf](https://arxiv.org/pdf/1710.09829.pdf)\r\nImage Source: [Sabour et al](https://arxiv.org/pdf/1710.09829v2.pdf)", "variants": ["MultiMNIST"], "title": "Dynamic Routing Between Capsules"}
{"id": "Composable activities dataset", "contents": "The Composable activities dataset consists of 693 videos that contain activities in 16 classes performed by 14 actors. Each activity is composed of 3 to 11 atomic actions. RGB-D data for each sequence is captured using a Microsoft Kinect sensor and estimate position of relevant body joints.\r\n\r\nThe dataset provides annotations of the activity for each video and the actions for each of the four human parts (left/right arm and leg) for each frame in every video.\r\n\r\nSource: [Discriminative Hierarchical Modeling of Spatio-Temporally Composable Human Activities](/paper/discriminative-hierarchical-modeling-of)\r\nImage Source: [https://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/](https://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/)", "variants": ["Composable activities dataset"], "title": "Discriminative Hierarchical Modeling of Spatio-temporally Composable Human Activities"}
{"id": "ITG", "contents": "**In The Groove** (**ITG**) is an audio dataset where given a raw audio track, the goal is to produce a choreography step chart, similar to those used in the Dance Dance Revolution video game. It contains 133 songs choreographed by a three different authors, with 652 charts for the 133 songs.\n\nSource: [https://arxiv.org/pdf/1703.06891.pdf](https://arxiv.org/pdf/1703.06891.pdf)\nImage Source: [https://github.com/chrisdonahue/ddc](https://github.com/chrisdonahue/ddc)", "variants": ["ITG"], "title": "Dance Dance Convolution"}
{"id": "FVI", "contents": "The Free-Form Video Inpainting dataset is a dataset used for training and evaluation video inpainting models. It consists of 1940 videos from the YouTube-VOS dataset and 12,600 videos from the YouTube-BoundingBoxes.\n\nSource: [https://arxiv.org/abs/1904.10247](https://arxiv.org/abs/1904.10247)\nImage Source: [https://github.com/amjltc295/Free-Form-Video-Inpainting](https://github.com/amjltc295/Free-Form-Video-Inpainting)", "variants": ["FVI"], "title": "Free-Form Video Inpainting With 3D Gated Convolution and Temporal PatchGAN"}
{"id": "Multi Task Crowd", "contents": "Multi Task Crowd is a new 100 image dataset fully annotated for crowd counting, violent behaviour detection and density level classification.\r\n\r\nSource: [ResnetCrowd: A Residual Deep Learning Architecture for Crowd Counting, Violent Behaviour Detection and Crowd Density Level Classification](https://arxiv.org/pdf/1705.10698)", "variants": ["Multi Task Crowd"], "title": "ResnetCrowd: A residual deep learning architecture for crowd counting, violent behaviour detection and crowd density level classification"}
{"id": "TUM Kitchen", "contents": "The **TUM Kitchen** dataset is an action recognition dataset that contains 20 video sequences captured by 4 cameras with overlapping views. The camera network captures the scene from four viewpoints with 25 fps, and every RGB frame is of the resolution 384×288 by pixels. The action labels are frame-wise, and provided for the left arm, the right arm and the torso separately.\n\nSource: [Temporal Human Action Segmentation via Dynamic Clustering](https://arxiv.org/abs/1803.05790)\nImage Source: [https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data](https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data)", "variants": ["TUM Kitchen"], "title": "The TUM Kitchen Data Set of everyday manipulation activities for motion tracking and action recognition"}
{"id": "LRS2", "contents": "The Oxford-BBC **Lip Reading Sentences 2** (**LRS2**) dataset is one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists of mainly news and talk shows from BBC programs. Each sentence is up to 100 characters in length. The training, validation and test sets are divided according to broadcast date. It is a challenging set since it contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances and the test set contains 1,242 utterances.\r\n\r\nSource: [Audio-visual Recognition of Overlapped speech for the LRS2 dataset](https://arxiv.org/abs/2001.01656)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)", "variants": ["LRS2"], "title": "Lip Reading Sentences in the Wild"}
{"id": "BigHand2.2M Benchmark", "contents": "A large-scale hand pose dataset, collected using a novel capture method.\r\n\r\nSource: [BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis](/paper/bighand22m-benchmark-hand-pose-dataset-and)", "variants": ["BigHand2.2M Benchmark"], "title": "BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis"}
{"id": "Charades", "contents": "The **Charades** dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\r\n\r\nSource: [Temporal Reasoning Graph for Activity Recognition](https://arxiv.org/abs/1908.09995)", "variants": ["Charades"], "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding"}
{"id": "Cops-Ref", "contents": "Cops-Ref is a dataset for visual reasoning in context of referring expression comprehension with two main features.\r\n\r\nSource: [Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension](/paper/cops-ref-a-new-dataset-and-task-on)\r\nImage Source: [https://github.com/zfchenUnique/Cops-Ref](https://github.com/zfchenUnique/Cops-Ref)", "variants": ["Cops-Ref"], "title": "Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension"}
{"id": "Toronto-3D", "contents": "**Toronto-3D** is a large-scale urban outdoor point cloud dataset acquired by an MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of road and consists of about 78.3 million points. Point clouds has 10 attributes and classified in 8 labelled object classes.\n\nSource: [https://github.com/WeikaiTan/Toronto-3D](https://github.com/WeikaiTan/Toronto-3D)\nImage Source: [https://github.com/WeikaiTan/Toronto-3D](https://github.com/WeikaiTan/Toronto-3D)", "variants": ["Toronto-3D"], "title": "Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways"}
{"id": "ASNQ", "contents": "A large scale dataset to enable the transfer step, exploiting the Natural Questions dataset. \r\n\r\nSource: [TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection](/paper/tanda-transfer-and-adapt-pre-trained)", "variants": ["ASNQ"], "title": "TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection"}
{"id": "MutualFriends", "contents": "In MutualFriends, two agents, A and B, each have a private knowledge base, which contains a list of friends with multiple attributes (e.g., name, school, major, etc.). The agents must chat with each other to find their unique mutual friend.\r\n\r\nSource: [Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings](/paper/learning-symmetric-collaborative-dialogue)", "variants": ["MutualFriends"], "title": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings"}
{"id": "MEDIA", "contents": "The **MEDIA** French corpus is dedicated to semantic extraction from speech in a context of human/machine dialogues. The corpus has manual transcription and conceptual annotation of dialogues from 250 speakers. It is split into the following three parts : (1) the training set (720 dialogues, 12K sentences), (2) the development set (79 dialogues, 1.3K sentences, and (3) the test set (200 dialogues, 3K sentences).\n\nSource: [Dialogue history integration into end-to-end signal-to-concept spoken language understanding systems](https://arxiv.org/abs/2002.06012)\nImage Source: [http://www.lrec-conf.org/proceedings/lrec2004/pdf/356.pdf](http://www.lrec-conf.org/proceedings/lrec2004/pdf/356.pdf)", "variants": ["MEDIA"], "title": "SemEval-2014 Task 10: Multilingual Semantic Textual Similarity"}
{"id": "FER2013", "contents": "Fer2013 contains approximately 30,000 facial RGB images of different expressions with size restricted to 48×48, and the main labels of it can be divided into 7 types: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral. The Disgust expression has the minimal number of images – 600, while other labels have nearly 5,000 samples each.\r\n\r\nSource: [Eavesdrop the Composition Proportion of Training Labels in Federated Learning](https://arxiv.org/abs/1910.06044)\r\nImage Source: [https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280](https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280)", "variants": ["FER2013"], "title": "Challenges in Representation Learning: A report on three machine learning contests"}
{"id": "KnowledgeNet", "contents": "KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction).\r\n\r\nFor instance, the dataset contains text expressing the fact (Gennaro Basile; RESIDENCE; Moravia), in the passage: \"Gennaro Basile was an Italian painter, born in Naples but active in the German-speaking countries. He settled at Brünn, in Moravia, and lived about 1756...\"\r\n\r\nSource: [KnowledgeNet](https://github.com/diffbot/knowledge-net)", "variants": ["KnowledgeNet"], "title": "KnowledgeNet: A Benchmark Dataset for Knowledge Base Population"}
{"id": "CDNET", "contents": "A video database for testing change detection algorithms. \r\n\r\nSource: [CDNET](http://www.changedetection.net/)", "variants": ["CDNET"], "title": "Targeted change detection in remote sensing images"}
{"id": "TabFact", "contents": "**TabFact** is a large-scale dataset which consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. TabFact is the first dataset to evaluate language inference on structured data, which involves mixed reasoning skills in both symbolic and linguistic aspects. \r\n\r\nSource: [GitHub](https://github.com/wenhuchen/Table-Fact-Checking)", "variants": ["TabFact"], "title": "TabFact: A Large-scale Dataset for Table-based Fact Verification"}
{"id": "UAV-GESTURE", "contents": "UAV-GESTURE is a dataset for UAV control and gesture recognition. It is an outdoor recorded video dataset for UAV commanding signals with 13 gestures suitable for basic UAV navigation and command from general aircraft handling and\r\nhelicopter handling signals. It contains 119 high-definition video clips\r\nconsisting of 37,151 frames. \r\n\r\nSource: [UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition](/paper/uav-gesture-a-dataset-for-uav-control-and)", "variants": ["UAV-GESTURE"], "title": "UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition"}
{"id": "SlowFlow", "contents": "**SlowFlow** is an optical flow dataset collected by applying Slow Flow technique on data from a high-speed camera and analyzing the performance of the state-of-the-art in optical flow under various levels of motion blur.\r\n\r\nSource: [Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data](/paper/slow-flow-exploiting-high-speed-cameras-for)", "variants": ["SlowFlow"], "title": "Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data"}
{"id": "CAIL2019-SCM", "contents": "Chinese AI and Law 2019 Similar Case Matching dataset. CAIL2019-SCM contains 8,964 triplets of cases published by the Supreme People's Court of China. CAIL2019-SCM focuses on detecting similar cases, and the participants are required to check which two cases are more similar in the triplets.\r\n\r\nSource: [CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain](/paper/cail2019-scm-a-dataset-of-similar-case)", "variants": ["CAIL2019-SCM"], "title": "CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain"}
{"id": "Occ-Traj120", "contents": "**Occ-Traj120** is a trajectory dataset that contains occupancy representations of different local-maps with associated trajectories. This dataset contains 400 locally-structured maps with occupancy representation and roughly around 120K trajectories in total.\n\nSource: [https://github.com/soraxas/Occ-Traj120](https://github.com/soraxas/Occ-Traj120)\nImage Source: [https://github.com/soraxas/Occ-Traj120](https://github.com/soraxas/Occ-Traj120)", "variants": ["Occ-Traj120"], "title": "Occ-Traj120: Occupancy Maps with Associated Trajectories"}
{"id": "Hate Speech and Offensive Language", "contents": "HSOL is a dataset for hate speech detection. The authors begun with a hate speech lexicon containing words and\r\nphrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API they searched\r\nfor tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. They extracted\r\nthe time-line for each user, resulting in a set of 85.4 million tweets. From this corpus they took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech.\r\n\r\nSource: [Automated Hate Speech Detection and the Problem of Offensive Language](/paper/automated-hate-speech-detection-and-the)", "variants": ["Hate Speech and Offensive Language"], "title": "Automated Hate Speech Detection and the Problem of Offensive Language"}
{"id": "SentiCap", "contents": "The **SentiCap dataset** contains several thousand images with captions with positive and negative sentiments. These sentimental captions are constructed by the authors by re-writing factual descriptions. In total there are 2000+ sentimental captions.\r\n\r\nSource: [SentiCap: Generating Image Descriptions with Sentiments](https://arxiv.org/pdf/1510.01431v2.pdf)", "variants": ["SentiCap"], "title": "SentiCap: Generating Image Descriptions with Sentiments"}
{"id": "ROSTD", "contents": "A dataset of 4K out-of-domain (OOD) examples for the publicly available dataset from (Schuster et al. 2019). In contrast to existing settings which synthesize OOD examples by holding out a subset of classes, the examples were authored by annotators with apriori instructions to be out-of-domain with respect to the sentences in an existing dataset. \r\n\r\nSource: [Likelihood Ratios and Generative Classifiers for Unsupervised Out-of-Domain Detection In Task Oriented Dialog](/paper/likelihood-ratios-and-generative-classifiers)", "variants": ["ROSTD"], "title": "Likelihood Ratios and Generative Classifiers for Unsupervised Out-of-Domain Detection In Task Oriented Dialog"}
{"id": "ETH Py150 Open", "contents": "A massive, deduplicated corpus of 7.4M Python files from GitHub.\r\n\r\nSource: [Learning and Evaluating Contextual Embedding of Source Code](/paper/pre-trained-contextual-embedding-of-source-1)", "variants": ["ETH Py150 Open"], "title": "Pre-trained Contextual Embedding of Source Code"}
{"id": "Hindi Visual Genome", "contents": "Hindi Visual Genome is a multimodal dataset consisting of text and images suitable for English-Hindi multimodal machine translation task and multimodal research.\r\n\r\nSource: [Hindi Visual Genome: A Dataset for Multimodal English-to-Hindi Machine Translation](/paper/hindi-visual-genome-a-dataset-for-multimodal)\r\nImage Source: [https://ufal.mff.cuni.cz/hindi-visual-genome](https://ufal.mff.cuni.cz/hindi-visual-genome)", "variants": ["Hindi Visual Genome"], "title": "Hindi Visual Genome: A Dataset for Multimodal English-to-Hindi Machine Translation"}
{"id": "UCLA Protest Image", "contents": "40,764 images (11,659 protest images and hard negatives) with various annotations of visual attributes and sentiments.\r\n\r\nSource: [Protest Activity Detection and Perceived Violence Estimation from Social Media Images](/paper/protest-activity-detection-and-perceived)", "variants": ["UCLA Protest Image"], "title": "Protest Activity Detection and Perceived Violence Estimation from Social Media Images"}
{"id": "DukeMTMC-reID", "contents": "The **DukeMTMC-reID** (Duke Multi-Tracking Multi-Camera ReIDentification) dataset is a subset of the DukeMTMC for image-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian image datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images.\r\n\r\n**NOTE**: This dataset [has been retracted](https://exposing.ai/duke_mtmc/).\r\n\r\nSource: [Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification](https://arxiv.org/abs/1804.11027)", "variants": ["DukeMTMC-reID", "DukeMTMCreID", "Market-1501->DukeMTMC-reID", "MSMT17->DukeMTMC-reID"], "title": "Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking"}
{"id": "MultiTHUMOS", "contents": "The **MultiTHUMOS** dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.\r\n\r\nSource: [http://ai.stanford.edu/~syyeung/everymoment.html](http://ai.stanford.edu/~syyeung/everymoment.html)\r\nImage Source: [http://ai.stanford.edu/~syyeung/everymoment.html](http://ai.stanford.edu/~syyeung/everymoment.html)", "variants": ["Multi-THUMOS", "MultiTHUMOS"], "title": "Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos"}
{"id": "AmazonQA", "contents": "AmazonQA consists of 923k questions, 3.6M answers and 14M reviews across 156k products. Building on the well-known Amazon dataset, additional annotations are collected, marking each question as either answerable or unanswerable based on the available reviews. \r\n\r\nSource: [AmazonQA: A Review-Based Question Answering Task](https://arxiv.org/pdf/1908.04364.pdf)", "variants": ["AmazonQA"], "title": "AmazonQA: A Review-Based Question Answering Task"}
{"id": "QMNIST", "contents": "The exact pre-processing steps used to construct the MNIST dataset have long been lost. This leaves us with no reliable way to associate its characters with the ID of the writer and little hope to recover the full MNIST testing set that had 60K images but was never released. The official MNIST testing set only contains 10K randomly sampled images and is often considered too small to provide meaningful confidence intervals.\nThe **QMNIST** dataset was generated from the original data found in the NIST Special Database 19 with the goal to match the MNIST preprocessing as closely as possible.\nQMNIST is licensed under the BSD-style license.\n\nSource: [https://github.com/facebookresearch/qmnist](https://github.com/facebookresearch/qmnist)\nImage Source: [https://github.com/facebookresearch/qmnist](https://github.com/facebookresearch/qmnist)", "variants": ["QMNIST"], "title": "Cold Case: The Lost MNIST Digits"}
{"id": "GamePad Environment", "contents": "GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant.\r\n\r\nSource: [GamePad: A Learning Environment for Theorem Proving](/paper/gamepad-a-learning-environment-for-theorem)", "variants": ["GamePad Environment"], "title": "GamePad: A Learning Environment for Theorem Proving"}
{"id": "PolyU Dataset", "contents": "PolyU Dataset is a large dataset of real-world noisy images with reasonably obtained corresponding “ground truth” images. The basic idea is to capture the same and unchanged scene for many (e.g., 500) times and compute their mean image, which can be roughly taken as the “ground truth” image for the real-world noisy images. The rational of this strategy is that for each pixel, the noise is generated randomly larger or smaller than 0. Sampling the same pixel many times and computing the average value will approximate the truth pixel value and alleviate significantly the noise.\r\n\r\nSource: [Real-world Noisy Image Denoising: A New Benchmark](https://arxiv.org/pdf/1804.02603.pdf)", "variants": ["PolyU Dataset"], "title": "Real-world Noisy Image Denoising: A New Benchmark"}
{"id": "ST-VQA", "contents": "ST-VQA aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the VQA process. \r\n\r\nSource: [Scene Text Visual Question Answering](/paper/scene-text-visual-question-answering)", "variants": ["ST-VQA"], "title": "Scene Text Visual Question Answering"}
{"id": "TQA", "contents": "The TextbookQuestionAnswering (TQA) dataset is drawn from middle school science curricula. It consists of 1,076 lessons from Life Science, Earth Science and Physical Science textbooks. This includes 26,260 questions, including 12,567 that have an accompanying diagram.\r\n\r\nThe TQA dataset encourages work on the task of Multi-Modal Machine Comprehension (M3C) task. The M3C task builds on the popular Visual Question Answering (VQA) and Machine Comprehension (MC) paradigms by framing question answering as a machine comprehension task, where the context needed to answer questions is provided and composed of both text and images. The dataset constructed to showcase this task has been built from a middle school science curriculum that pairs a given question to a limited span of knowledge needed to answer it.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/tqa)", "variants": ["TQA"], "title": "Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension"}
{"id": "Zooniverse", "contents": "The Humbug Zooinverse dataset is a dataset of mosquito audio recordings. With over a thousand contributors, it contains 195,434 labels of two second duration, of which approximately 10 percent signify mosquito events.\n\nSource: [https://github.com/HumBug-Mosquito/ZooniverseData](https://github.com/HumBug-Mosquito/ZooniverseData)", "variants": ["Zooniverse"], "title": "HumBug Zooniverse: a crowd-sourced acoustic mosquito dataset"}
{"id": "SearchQA", "contents": "SearchQA was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis. \r\n\r\nSource: [SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine](https://arxiv.org/pdf/1704.05179.pdf)", "variants": ["SearchQA"], "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine"}
{"id": "3DMatch", "contents": "The 3DMATCH benchmark evaluates how well descriptors (both 2D and 3D) can establish correspondences between RGB-D frames of different views. The dataset contains 2D RGB-D patches and 3D patches (local TDF voxel grid volumes) of wide-baselined correspondences. \r\n\r\nThe pixel size of each 2D patch is determined by the projection of the 0.3m3 local 3D patch around the interest point onto the image plane. \r\n\r\nSource: [3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions](/paper/3dmatch-learning-local-geometric-descriptors)", "variants": ["3DMatch Benchmark", "3DMatch"], "title": "3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions"}
{"id": "Opusparcus", "contents": "Opusparcus is a paraphrase corpus for six European languages: German, English, Finnish, French, Russian, and Swedish. The paraphrases are extracted from the OpenSubtitles2016 corpus, which contains subtitles from movies and TV shows.\r\n\r\nFor each target language, the Opusparcus data have been partitioned into three types of data sets: training, development and test sets. The training sets are large, consisting of millions of sentence pairs, and have been compiled automatically, with the help of probabilistic ranking functions. The development and test sets consist of sentence pairs that have been annotated manually; each set contains approximately 1000 sentence pairs that have been verified to be acceptable paraphrases by two annotators.\r\n\r\nSource: [Opusparcus](http://urn.fi/urn:nbn:fi:lb-2018021221)", "variants": ["Opusparcus"], "title": "Open Subtitles Paraphrase Corpus for Six Languages"}
{"id": "Fashion 144K", "contents": "**Fashion 144K** is a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information.\r\n\r\nSource: [Neuroaesthetics in Fashion: Modeling the Perception of Fashionability](/paper/neuroaesthetics-in-fashion-modeling-the-1)", "variants": ["Fashion 144K"], "title": "Neuroaesthetics in fashion: Modeling the perception of fashionability"}
{"id": "2D-3D Match Dataset", "contents": "2D-3D Match Dataset is a new dataset of 2D-3D correspondences by leveraging the availability of several 3D datasets from RGB-D scans. Specifically, the data from SceneNN and 3DMatch are used. The training dataset consists of 110 RGB-D scans, of which 56 scenes are from SceneNN and 54 scenes are from 3DMatch. The 2D-3D correspondence data is generated as follows. Given a 3D point which is randomly sampled from a 3D point cloud, a set of 3D patches from different scanning views are extracted. To find a 2D-3D correspondence, for each 3D patch, its 3D position is re-projected into all RGB-D frames for which the point lies in the camera frustum, taking occlusion into account. The corresponding local 2D patches around the re-projected point are extracted. In total, around 1.4 millions 2D-3D correspondences are collected.\r\n\r\nSource: [2D-3D Match Dataset](https://github.com/hkust-vgd/lcd)", "variants": ["2D-3D Match Dataset"], "title": "LCD: Learned Cross-Domain Descriptors for 2D-3D Matching"}
{"id": "PubLayNet", "contents": "PubLayNet is a dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated.\r\n\r\nSource: [PubLayNet: largest dataset ever for document layout analysis](/paper/190807836)", "variants": ["PubLayNet"], "title": "PubLayNet: Largest Dataset Ever for Document Layout Analysis"}
{"id": "Arabic Text Diacritization", "contents": "Extracted from the Tashkeela Corpus, the dataset consists of 55K lines containing about 2.3M words.\n\nSource: [https://github.com/AliOsm/arabic-text-diacritization](https://github.com/AliOsm/arabic-text-diacritization)", "variants": ["Tashkeela", "Arabic Text Diacritization"], "title": "Arabic Text Diacritization Using Deep Neural Networks"}
{"id": "iSUN", "contents": "**iSUN** is a ground truth of gaze traces on images from the SUN dataset. The collection is partitioned into 6,000 images for training, 926 for validation and 2,000 for test.\r\n\r\nSource: [End-to-end Convolutional Network for Saliency Prediction](https://arxiv.org/abs/1507.01422)\r\nImage Source: [http://turkergaze.cs.princeton.edu/](http://turkergaze.cs.princeton.edu/)", "variants": ["iSUN"], "title": "TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking"}
{"id": "Raider", "contents": "The **Raider** dataset collects fMRI recordings of 1000 voxels from the ventral temporal cortex, for 10 healthy adult participants passively watching the full-length movie “Raiders of the Lost Ark”.\n\nSource: [Time-Resolved fMRI Shared Response Model using Gaussian Process Factor Analysis](https://arxiv.org/abs/2006.05572)\nImage Source: [https://arxiv.org/abs/1909.12537](https://arxiv.org/abs/1909.12537)", "variants": ["Raider"], "title": "A Common, High-Dimensional Model of the Representational Space in Human Ventral Temporal Cortex"}
{"id": "InsuranceQA", "contents": "**InsuranceQA** is a question answering dataset for the insurance domain, the data stemming from the website Insurance Library. There are 12,889 questions and 21,325 answers in the training set. There are 2,000 questions and 3,354 answers in the validation set. There are 2,000 questions and 3,308 answers in the test set.\r\n\r\nSource: [APPLYING DEEP LEARNING TO ANSWER SELECTION: A STUDY AND AN OPEN TASK](https://arxiv.org/pdf/1508.01585v2.pdf)", "variants": ["InsuranceQA"], "title": "Applying deep learning to answer selection: A study and an open task"}
{"id": "UTKFace", "contents": "The **UTKFace** dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc.\r\n\r\nSource: [https://susanqq.github.io/UTKFace/](https://susanqq.github.io/UTKFace/)\r\nImage Source: [https://susanqq.github.io/UTKFace/](https://susanqq.github.io/UTKFace/)", "variants": ["UTKFace"], "title": "Age Progression/Regression by Conditional Adversarial Autoencoder"}
{"id": "RWF-2000", "contents": "A database with 2,000 videos captured by surveillance cameras in real-world scenes. \r\n\r\nSource: [RWF-2000: An Open Large Scale Video Database for Violence Detection](/paper/rwf-2000-an-open-large-scale-video-database)", "variants": ["RWF-2000"], "title": "RWF-2000: An Open Large Scale Video Database for Violence Detection"}
{"id": "LFPW", "contents": "The **Labeled Face Parts in-the-Wild** (**LFPW**) consists of 1,432 faces from images downloaded from the web using simple text queries on sites such as google.com, flickr.com, and yahoo.com.   Each image was labeled by three MTurk workers, and 29 fiducial points, shown below, are included in dataset.\r\n\r\nSource: [https://neerajkumar.org/databases/lfpw/](https://neerajkumar.org/databases/lfpw/)\r\nImage Source: [https://neerajkumar.org/databases/lfpw/](https://neerajkumar.org/databases/lfpw/)", "variants": ["LFPW"], "title": "Localizing Parts of Faces Using a Consensus of Exemplars"}
{"id": "Roll4Real", "contents": "Consists of real objects rolling on complex terrains (pool table, elliptical bowl, and random height-field). \r\n\r\nSource: [Unsupervised Intuitive Physics from Visual Observations](/paper/unsupervised-intuitive-physics-from-visual)", "variants": ["Roll4Real"], "title": "Unsupervised Intuitive Physics from Visual Observations"}
{"id": "CraigslistBargains", "contents": "A richer dataset based on real items on Craigslist.\r\n\r\nSource: [Decoupling Strategy and Generation in Negotiation Dialogues](/paper/decoupling-strategy-and-generation-in)", "variants": ["CraigslistBargains"], "title": "Decoupling Strategy and Generation in Negotiation Dialogues"}
{"id": "Taskonomy", "contents": "Taskonomy provides a large and high-quality dataset of varied indoor scenes.\r\n\r\n- Complete pixel-level geometric information via aligned meshes.\r\n- Semantic information via knowledge distillation from ImageNet, MS COCO, and MIT Places.\r\n- Globally consistent camera poses. Complete camera intrinsics.\r\n- High-definition images.\r\n- 3x times big as ImageNet.\r\n\r\nSource: [Taskonomy](http://taskonomy.stanford.edu/)\r\nImage Source: [http://taskonomy.stanford.edu/](http://taskonomy.stanford.edu/)", "variants": ["Taskonomy"], "title": "Taskonomy: Disentangling Task Transfer Learning"}
{"id": "ShapeStacks", "contents": "A simulation-based dataset featuring 20,000 stack configurations composed of a variety of elementary geometric primitives richly annotated regarding semantics and structural stability. \r\n\r\nSource: [ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking](https://arxiv.org/pdf/1804.08018v2.pdf)", "variants": ["ShapeStacks"], "title": "ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking"}
{"id": "IQUAD", "contents": "IQUAD is a dataset for Visual Question Answering in interactive environments. It is built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive object. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration.\r\n\r\nSource: [IQA: Visual Question Answering in Interactive Environments](https://arxiv.org/abs/1712.03316)", "variants": ["IQUAD"], "title": "IQA: Visual Question Answering in Interactive Environments"}
{"id": "COCO-Stuff", "contents": "The **Common Objects in COntext-stuff** (COCO-stuff) dataset is a dataset for scene understanding tasks like semantic segmentation, object detection and image captioning. It is constructed by annotating the original COCO dataset, which originally annotated things while neglecting stuff annotations. There are 164k images in COCO-stuff dataset that span over 172 categories including 80 things, 91 stuff, and 1 unlabeled class.\r\n\r\nSource: [Image Colorization: A Survey and Dataset](https://arxiv.org/abs/2008.10774)\r\nImage Source: [https://github.com/nightrome/cocostuff](https://github.com/nightrome/cocostuff)", "variants": ["COCO-Stuff", "COCO-Stuff 128x128", "COCO-Stuff 64x64", "COCO-Stuff Labels-to-Photos", "COCO-Stuff test", "COCO-Stuff-15", "COCO-Stuff-3"], "title": "COCO-Stuff: Thing and Stuff Classes in Context"}
{"id": "JEC-QA", "contents": "JEC-QA is a LQA (Legal Question Answering) dataset collected from the National Judicial Examination of China. It contains 26,365 multiple-choice and multiple-answer questions in total. The task of the dataset is to predict the answer using the questions and relevant articles. To do well on JEC-QA, both retrieving and answering are important.\r\n\r\nSource: [JEC-QA Website](https://jecqa.thunlp.org/)", "variants": ["JEC-QA"], "title": "JEC-QA: A Legal-Domain Question Answering Dataset"}
{"id": "VoxCeleb2", "contents": "**VoxCeleb2** is a large scale speaker recognition dataset obtained automatically from open-source media. VoxCeleb2 consists of over a million utterances from over 6k speakers. Since the dataset is collected ‘in the wild’, the speech segments are corrupted with real world noise including laughter, cross-talk, channel effects, music and other sounds. The dataset is also multilingual, with speech from speakers of 145 different nationalities, covering a wide range of accents, ages, ethnicities and languages. The dataset is audio-visual, so is also useful for a number of other applications, for example – visual speech synthesis, speech separation, cross-modal transfer from face to voice or vice versa and training face recognition from video to complement existing face recognition datasets.\r\n\r\nSource: [VoxCeleb2: Deep Speaker Recognition](https://arxiv.org/pdf/1806.05622v2.pdf)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/voxceleb/](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)", "variants": ["VoxCeleb2 - 1-shot learning", "VoxCeleb2 - 8-shot learning", "VoxCeleb2 - 32-shot learning", "VoxCeleb2"], "title": "VoxCeleb2: Deep Speaker Recognition"}
{"id": "Visual Question Answering v2.0", "contents": "Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. It is the second version of the [VQA](https://www.paperswithcode.com/dataset/vqa) dataset.\r\n\r\n- 265,016 images (COCO and abstract scenes)\r\n- At least 3 questions (5.4 questions on average) per image\r\n- 10 ground truth answers per question\r\n- 3 plausible (but likely incorrect) answers per question\r\n- Automatic evaluation metric\r\n\r\nThe [first version of the dataset](/dataset/visual-question-answering) was released in October 2015.", "variants": ["Visual Question Answering v2.0", "VQA 2.0", "VQA v2", "VQA v2 test-dev", "VQA v2 test-std"], "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}
{"id": "TIM-Tremor", "contents": "Contains static tasks as well as a multitude of more dynamic tasks, involving larger motion of the hands. The dataset has 55 tremor patient recordings together with: associated ground truth accelerometer data from the most affected hand, RGB video data, and aligned depth data.\r\n\r\nSource: [Hand-tremor frequency estimation in videos](/paper/hand-tremor-frequency-estimation-in-videos)", "variants": ["TIM-Tremor"], "title": "Hand-tremor frequency estimation in videos"}
{"id": "Semantic3D", "contents": "**Semantic3D** is a point cloud dataset of scanned outdoor scenes with over 3 billion points. It contains 15 training and 15 test scenes annotated with 8 class labels. This large labelled 3D point cloud data set of natural covers a range of diverse urban scenes: churches, streets, railroad tracks, squares, villages, soccer fields, castles to name just a few. The point clouds provided are scanned statically with state-of-the-art equipment and contain very fine details.\r\n\r\nSource: [Tangent Convolutions for Dense Prediction in 3D](https://arxiv.org/abs/1807.02443)\r\nImage Source: [http://www.semantic3d.net/](http://www.semantic3d.net/)", "variants": ["Semantic3D"], "title": "Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark"}
{"id": "IBM-Rank-30k", "contents": "The IBM-Rank-30k is a dataset for the task of argument quality ranking. It is a corpus of 30,497 arguments carefully annotated for point-wise quality.\r\n\r\nImage Source: [A Large-scale Dataset for Argument Quality Ranking: Construction and Analysis](https://arxiv.org/pdf/1911.11408v1.pdf)", "variants": ["IBM-Rank-30k"], "title": "A Large-scale Dataset for Argument Quality Ranking: Construction and Analysis"}
{"id": "P-DESTRE", "contents": "Provides consistent ID annotations across multiple days, making it suitable for the extremely challenging problem of person search, i.e., where no clothing information can be reliably used. Apart this feature, the P-DESTRE annotations enable the research on UAV-based pedestrian detection, tracking, re-identification and soft biometric solutions.\r\n\r\nSource: [The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection, Tracking, Re-Identification and Search from Aerial Devices](/paper/the-p-destre-a-fully-annotated-dataset-for)", "variants": ["P-DESTRE"], "title": "The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection, Tracking, Re-Identification and Search from Aerial Devices"}
{"id": "DREAM", "contents": "DREAM is a multiple-choice Dialogue-based REAding comprehension exaMination dataset. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding.\r\n\r\nDREAM contains 10,197 multiple choice questions for 6,444 dialogues, collected from English-as-a-foreign-language examinations designed by human experts. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge.\r\n\r\nSource: [DREAM](https://dataset.org/dream/)", "variants": ["DREAMS sleep spindles", "DREAM"], "title": "DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension"}
{"id": "Tatoeba", "contents": "The **Tatoeba** dataset consists of up to 1,000 English-aligned sentence pairs covering 122 languages.\r\n\r\nImage Source: [https://arxiv.org/pdf/1812.10464v2.pdf](https://arxiv.org/pdf/1812.10464v2.pdf)", "variants": ["Tatoeba"], "title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond"}
{"id": "Edina-DR", "contents": "Edina-DR is a novel corpus of discourse relation pairs; the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse. \r\n\r\nSource: [Implicit Discourse Relation Identification for Open-domain Dialogues](/paper/implicit-discourse-relation-identification)", "variants": ["Edina-DR"], "title": "Implicit Discourse Relation Identification for Open-domain Dialogues"}
{"id": "MNIST-MIX", "contents": "**MNIST-MIX** is a multi-language handwritten digit recognition dataset. It contains digits from 10 different languages.\n\nSource: [https://github.com/jwwthu/MNIST-MIX](https://github.com/jwwthu/MNIST-MIX)", "variants": ["MNIST-MIX"], "title": "MNIST-MIX: A Multi-language Handwritten Digit Recognition Dataset"}
{"id": "Hate Speech", "contents": "Dataset of hate speech annotated on Internet forum posts in English at sentence-level. The source forum in Stormfront, a large online community of white nacionalists. A total of 10,568 sentence have been been extracted from Stormfront and classified as conveying hate speech or not.\r\n\r\nSource: [https://arxiv.org/pdf/1809.04444.pdf](https://arxiv.org/pdf/1809.04444.pdf)", "variants": ["Hate Speech"], "title": "Hate Speech Dataset from a White Supremacy Forum"}
{"id": "FIGR-8", "contents": "The FIGR-8 database is a dataset containing 17,375 classes of 1,548,256 images representing pictograms, ideograms, icons, emoticons or object or conception depictions. Its aim is to set a benchmark for Few-shot Image Generation tasks, albeit not being limited to it. Each image is represented by 192x192 pixels with grayscale value of 0-255. Classes are not balanced (they do not all contain the same number of elements), but they all do contain at the very least 8 images.\r\n\r\nSource: [https://github.com/marcdemers/FIGR-8](https://github.com/marcdemers/FIGR-8)\r\nImage Source: [https://github.com/marcdemers/FIGR-8](https://github.com/marcdemers/FIGR-8)", "variants": ["FIGR-8"], "title": "FIGR: Few-shot Image Generation with Reptile"}
{"id": "MSeg", "contents": "A composite dataset that unifies se- mantic segmentation datasets from different domains. \r\n\r\nSource: [MSeg: A Composite Dataset for Multi-Domain Semantic Segmentation](/paper/mseg-a-composite-dataset-for-multi-domain)", "variants": ["MSeg"], "title": "MSeg: A Composite Dataset for Multi-domain Semantic Segmentation"}
{"id": "YouCook", "contents": "This data set was prepared from 88 open-source YouTube cooking videos. The YouCook dataset contains videos of people cooking various recipes. The videos were downloaded from YouTube and are all in the third-person viewpoint; they represent a significantly more challenging visual problem than existing cooking and kitchen datasets (the background kitchen/scene is different for many and most videos have dynamic camera changes). In addition, frame-by-frame object and action annotations are provided for training data (as well as a number of precomputed low-level features). Finally, each video has a number of human provided natural language descriptions (on average, there are eight different descriptions per video). This dataset has been created to serve as a benchmark in describing complex real-world videos with natural language descriptions.\r\n\r\nSource: [A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching](/paper/a-thousand-frames-in-just-a-few-words-lingual)", "variants": ["YouCook"], "title": "A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching"}
{"id": "UFDD", "contents": "Unconstrained Face Detection Dataset (UFDD) aims to fuel further research in unconstrained face detection.\r\n\r\nSource: [Pushing the Limits of Unconstrained Face Detection: a Challenge Dataset and Baseline Results](/paper/pushing-the-limits-of-unconstrained-face)\r\nImage Source: [https://ufdd.info/](https://ufdd.info/)", "variants": ["UFDD"], "title": "Pushing the Limits of Unconstrained Face Detection: a Challenge Dataset and Baseline Results"}
{"id": "HIC", "contents": "The Hands in action dataset (**HIC**) dataset has RGB-D sequences of hands interacting with objects.\n\nSource: [Learning joint reconstruction of hands and manipulated objects](https://arxiv.org/abs/1904.05767)\nImage Source: [http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/](http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/)", "variants": ["HIC"], "title": "Capturing Hands in Action Using Discriminative Salient Points and Physics Simulation"}
{"id": "VOCASET", "contents": "**VOCASET** is a 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio. The dataset has 12 subjects and 480 sequences of about 3-4 seconds each with sentences chosen from an array of standard protocols that maximize phonetic diversity.\r\n\r\nSource: [timzhang642](https://github.com/timzhang642/3D-Machine-Learning#datasets)", "variants": ["VOCASET"], "title": "Capture, Learning, and Synthesis of 3D Speaking Styles"}
{"id": "PoMo", "contents": "PoMo consists of more than 231K sentences with post-modifiers and associated facts extracted from Wikidata for around 57K unique entities.\r\n\r\nSource: [PoMo: Generating Entity-Specific Post-Modifiers in Context](/paper/pomo-generating-entity-specific-post)", "variants": ["PoMo"], "title": "PoMo: Generating Entity-Specific Post-Modifiers in Context"}
{"id": "FCE", "contents": "The Cambridge Learner Corpus **First Certificate in English** (CLC **FCE**) dataset consists of short texts, written by learners of English as an additional language in response to exam prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. The full dataset consists of 323,192 sentences. The publicly released subset of the dataset, named FCE-public, consists of 33,673 sentences split into test and training sets of 2,720 and 30,953 sentences, respectively.\r\n\r\nSource: [Compositional Sequence Labeling Models for Error Detection in Learner Writing](https://arxiv.org/abs/1607.06153)", "variants": ["FCE"], "title": "Chinese NER Using Lattice LSTM"}
{"id": "DECADE", "contents": "DECADE is a large-scale dataset of ego-centric videos from a dog's perspective as well as her corresponding movements.\r\n\r\nSource: [Who Let The Dogs Out? Modeling Dog Behavior From Visual Data](/paper/who-let-the-dogs-out-modeling-dog-behavior)", "variants": ["DECADE"], "title": "Who Let the Dogs Out? Modeling Dog Behavior from Visual Data"}
{"id": "Social-IQ", "contents": "Social-IQ is an unconstrained benchmark specifically designed to train and evaluate socially intelligent technologies. By providing a rich source of open-ended questions and answers, Social-IQ opens the door to explainable social intelligence. The dataset contains rigorously annotated and validated videos, questions and answers, as well as annotations for the complexity level of each question and answer. Social-IQ contains 1,250 natural in-the-wild social situations, 7,500 questions and 52,500 correct and incorrect answers. \r\n\r\nSource: [Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence](/paper/social-iq-a-question-answering-benchmark-for)", "variants": ["Social-IQ"], "title": "Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence"}
{"id": "CADP", "contents": "A novel dataset for traffic accidents analysis. \r\n\r\nSource: [CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis](/paper/cadp-a-novel-dataset-for-cctv-traffic-camera)", "variants": ["CADP"], "title": "CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis"}
{"id": "TE141K", "contents": "A new text effects dataset with 141,081 text effect/glyph pairs in total. The dataset consists of 152 professionally designed text effects rendered on glyphs, including English letters, Chinese characters, and Arabic numerals. \r\n\r\nSource: [TE141K: Artistic Text Benchmark for Text Effect Transfer](/paper/190503646)", "variants": ["TE141K"], "title": "TE141K: Artistic Text Benchmark for Text Effect Transfer"}
{"id": "arXiv Astro-Ph", "contents": "Arxiv ASTRO-PH (Astro Physics) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to Astro Physics category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.\n\nSource: [https://snap.stanford.edu/data/ca-AstroPh.html](https://snap.stanford.edu/data/ca-AstroPh.html)", "variants": ["arXiv-AstroPh 2-clique", "arXiv-AstroPh 4-clique", "arXiv Astro-Ph"], "title": "Special Report: NCBI disease corpus: A resource for disease name recognition and concept normalization"}
{"id": "CAT2000", "contents": "Includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings.\r\n\r\nSource: [CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research](https://arxiv.org/pdf/1505.03581v1.pdf)", "variants": ["CAT2000"], "title": "CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research"}
{"id": "AESLC", "contents": "To study the task of email subject line generation: automatically generating an email subject line from the email body. \r\n\r\nSource: [This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation](/paper/this-email-could-save-your-life-introducing)", "variants": ["AESLC"], "title": "This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation"}
{"id": "WikiCatSum", "contents": "**WikiCatSum** is a domain specific Multi-Document Summarisation (MDS) dataset. It assumes the summarisation task of generating Wikipedia lead sections for Wikipedia entities of a certain domain (e.g. Companies) from the set of documents cited in Wikipedia articles or returned by Google (using article titles as queries). The dataset includes three domains: Companies, Films, and Animals.\n\nSource: [https://datashare.ed.ac.uk/handle/10283/3368](https://datashare.ed.ac.uk/handle/10283/3368)", "variants": ["WikiCatSum"], "title": "Generating Summaries with Topic Templates and Structured Convolutional Decoders"}
{"id": "Visual Choice of Plausible Alternatives", "contents": "Visual Choice of Plausible Alternatives (VCOPA) is an evaluation dataset containing 380 VCOPA questions and over 1K images with various topics, which is amenable to automatic evaluation, and present the performance of baseline reasoning approaches as initial benchmarks for future systems.\r\n\r\nSource: [Visual Choice of Plausible Alternatives: An Evaluation of Image-based Commonsense Causal Reasoning](/paper/visual-choice-of-plausible-alternatives-an)", "variants": ["Visual Choice of Plausible Alternatives"], "title": "Visual choice of plausible alternatives: An Evaluation of Image-based Commonsense Causal Reasoning"}
{"id": "SemanticKITTI", "contents": "**SemanticKITTI** is a large-scale outdoor-scene dataset for point cloud semantic segmentation. It is derived from the KITTI Vision Odometry Benchmark which it extends with dense point-wise annotations for the complete 360 field-of-view of the employed automotive LiDAR. The dataset consists of 22 sequences. Overall, the dataset provides 23201 point clouds for training and 20351 for testing.\r\n\r\nSource: [Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation](https://arxiv.org/abs/2008.01550)\r\nImage Source: [https://github.com/PRBonn/semantic-kitti-api](https://github.com/PRBonn/semantic-kitti-api)", "variants": ["SemanticKITTI"], "title": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences"}
{"id": "Permuted bAbI dialog task", "contents": "The Permuted bAbi dialog task is an adaptation of the \"Dialog bAbI tasks data\" dataset released by Facebook. It is used for evaluating end-to-end dialog systems in the restaurant domain. This dataset introduces multiple valid next utterances to the original-bAbI dialog tasks, which allows evaluation of end-to-end goal-oriented dialog systems in a more realistic setting.\n\nSource: [https://github.com/IBM/permuted-bAbI-dialog-tasks](https://github.com/IBM/permuted-bAbI-dialog-tasks)", "variants": ["Permuted bAbI dialog task"], "title": "Learning End-to-End Goal-Oriented Dialog with Multiple Answers"}
{"id": "YouTube-8M", "contents": "The **YouTube-8M** dataset is a large scale video dataset, which includes more than 7 million videos with 4716 classes labeled by the annotation system. The dataset consists of three parts: training set, validate set, and test set. In the training set, each class contains at least 100 training videos. Features of these videos are extracted by the state-of-the-art popular pre-trained models and released for public use. Each video contains audio and visual modality. Based on the visual information, videos are divided into 24 topics, such as sports, game, arts & entertainment, etc\r\n\r\nSource: [Audio-Visual Embedding for Cross-Modal Music Video Retrieval through Supervised Deep CCA](https://arxiv.org/abs/1908.03744)", "variants": ["YouTube-8M"], "title": "YouTube-8M: A Large-Scale Video Classification Benchmark"}
{"id": "IG-1B-Targeted", "contents": "**IG-1B-Targeted** is an internal Facebook AI Research dataset that consists of 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets.", "variants": ["IG-1B-Targeted"], "title": "Billion-scale semi-supervised learning for image classification"}
{"id": "Qulac", "contents": "A dataset on asking Questions for Lack of Clarity in open-domain information-seeking conversations. **Qulac** presents the first dataset and offline evaluation framework for studying clarifying questions in open-domain information-seeking conversational search systems.\n\nSource: [https://github.com/aliannejadi/qulac](https://github.com/aliannejadi/qulac)\nImage Source: [https://github.com/aliannejadi/qulac](https://github.com/aliannejadi/qulac)", "variants": [], "title": "Asking Clarifying Questions in Open-Domain Information-Seeking Conversations"}
{"id": "Celeb-DF", "contents": "**Celeb-DF** is a large-scale challenging dataset for deepfake forensics. It includes 590 original videos collected from YouTube with subjects of different ages, ethnic groups and genders, and 5639 corresponding DeepFake videos.\r\n\r\nSource: [https://github.com/danmohaha/celeb-deepfakeforensics](https://github.com/danmohaha/celeb-deepfakeforensics)\r\nImage Source: [https://github.com/danmohaha/celeb-deepfakeforensics](https://github.com/danmohaha/celeb-deepfakeforensics)", "variants": ["Celeb-DF"], "title": "Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics"}
{"id": "EmoBank", "contents": "**EmoBank** is a corpus of 10k English sentences balancing multiple genres, annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design. \r\n\r\nSource: [EmoBank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis](https://www.aclweb.org/anthology/E17-2092)", "variants": ["EmoBank"], "title": "EmoBank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis"}
{"id": "MCScript", "contents": "**MCScript** is used as the official dataset of SemEval2018 Task11. This dataset constructs a collection of text passages about daily life activities and a series of questions referring to each passage, and each question is equipped with two answer choices. The MCScript comprises 9731, 1411, and 2797 questions in training, development, and test set respectively.\r\n\r\nSource: [Multi-Perspective Fusion Network for Commonsense Reading Comprehension](https://arxiv.org/abs/1901.02257)\r\nImage Source: [https://arxiv.org/pdf/1803.05223.pdf](https://arxiv.org/pdf/1803.05223.pdf)", "variants": ["MCScript"], "title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge"}
{"id": "CHALET", "contents": "**CHALET** is a 3D house simulator with support for navigation and manipulation. Unlike existing systems, CHALET supports both a wide range of object manipulation, as well as supporting complex environemnt layouts consisting of multiple rooms. The range of object manipulations includes the ability to pick up and place objects, toggle the state of objects like taps or televesions, open or close containers, and insert or remove objects from these containers. In addition, the simulator comes with 58 rooms that can be combined to create houses, including 10 default house layouts. CHALET is therefore suitable for setting up challenging environments for various AI tasks that require complex language understanding and planning, such as navigation, manipulation, instruction following, and interactive question answering.\r\n\r\nSource: [https://github.com/lil-lab/chalet](https://github.com/lil-lab/chalet)\r\nImage Source: [https://arxiv.org/pdf/1809.00786.pdf](https://arxiv.org/pdf/1809.00786.pdf)", "variants": ["CHALET"], "title": "CHALET: Cornell House Agent Learning Environment"}
{"id": "COQE", "contents": "Contains more than 5,000 images of 10,000 liquid containers in context labelled with volume, amount of content, bounding box annotation, and corresponding similar 3D CAD models.\r\n\r\nSource: [See the Glass Half Full: Reasoning about Liquid Containers, their Volume and Content](/paper/see-the-glass-half-full-reasoning-about)", "variants": ["COQE"], "title": "See the Glass Half Full: Reasoning About Liquid Containers, Their Volume and Content"}
{"id": "V2C", "contents": "Contains ~9K videos of human agents performing various actions, annotated with 3 types of commonsense descriptions.", "variants": ["V2C"], "title": "Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning"}
{"id": "TartanAir", "contents": "A dataset for robot navigation task and more. The data is collected in photo-realistic simulation environments in the presence of various light conditions, weather and moving objects.\r\n\r\nSource: [TartanAir: A Dataset to Push the Limits of Visual SLAM](/paper/tartanair-a-dataset-to-push-the-limits-of)", "variants": ["TartanAir"], "title": "TartanAir: A Dataset to Push the Limits of Visual SLAM"}
{"id": "NQuAD", "contents": "NQuAD is a Nuclear Question Answering Dataset, which contains 700+ nuclear Question Answer pairs developed and verified by expert nuclear researchers.\r\n\r\nSource: [NukeBERT: A Pre-trained language model for Low Resource Nuclear Domain](https://arxiv.org/pdf/2003.13821.pdf)", "variants": ["NQuAD"], "title": "NukeBERT: A Pre-trained language model for Low Resource Nuclear Domain"}
{"id": "TaoDescribe", "contents": "The **TaoDescribe** dataset contains 2,129,187 product titles and descriptions in Chinese.\n\nSource: [https://github.com/qibinc/KOBE](https://github.com/qibinc/KOBE)", "variants": ["TaoDescribe"], "title": "Towards Knowledge-Based Personalized Product Description Generation in E-commerce"}
{"id": "NTU RGB+D 120", "contents": "NTU RGB+D 120 is a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. \r\n\r\nSource: [NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding](https://arxiv.org/pdf/1905.04757v2.pdf)", "variants": ["NTU RGB+D 120"], "title": "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding"}
{"id": "METU-ALET", "contents": "**METU-ALET** is an image dataset for the detection of the tools in the wild. The dataset has annotations for tools that belongs to the categories such as farming, gardening, office, stonemasonry, vehicle, woodworking and workshop. The images in the dataset contains a total of 22,841 bounding boxes and 49 different tool categories.\n\nSource: [https://github.com/metu-kovan/METU-ALET](https://github.com/metu-kovan/METU-ALET)\nImage Source: [https://github.com/metu-kovan/METU-ALET](https://github.com/metu-kovan/METU-ALET)", "variants": ["METU-ALET"], "title": "ALET (Automated Labeling of Equipment and Tools): A Dataset, a Baseline and a Usecase for Tool Detection in the Wild"}
{"id": "New College", "contents": "The **New College** Data is a freely available dataset collected from a robot completing several loops outdoors around the New College campus in Oxford. The data includes odometry, laser scan, and visual information. The dataset URL is not working anymore.\r\n\r\nSource: [https://www.ros.org/news/2010/07/new-college-dataset-parser-for-ros.html](https://www.ros.org/news/2010/07/new-college-dataset-parser-for-ros.html)", "variants": ["New College"], "title": "The New College Vision and Laser Data Set"}
{"id": "Crowd Dataset", "contents": "A dense crowd dataset with manually annotated groundtruth, collected from different public datasets. This dataset comprises 20 videos that exhibit a multitude of motion behaviors that cover both the obvious and subtle instabilities.\r\n\r\nSource: [Crowd Dataset](http://cs-chan.com/project4.htm)", "variants": ["Crowd Dataset"], "title": "Detection of Salient Regions in Crowded Scenes"}
{"id": "Replay-Attack", "contents": "The **Replay-Attack** Database for face spoofing consists of 1300 video clips of photo and video attack attempts to 50 clients, under different lighting conditions. All videos are generated by either having a (real) client trying to access a laptop through a built-in webcam or by displaying a photo or a video recording of the same client for at least 9 seconds.\r\n\r\nSource: [https://www.idiap.ch/dataset/replayattack](https://www.idiap.ch/dataset/replayattack)\r\nImage Source: [https://www.researchgate.net/figure/Sample-images-from-the-PRINT-ATTACK-1-and-REPLAY-ATTACK-12-databases-Top-and-bottom_fig1_330400888](https://www.researchgate.net/figure/Sample-images-from-the-PRINT-ATTACK-1-and-REPLAY-ATTACK-12-databases-Top-and-bottom_fig1_330400888)", "variants": ["Replay-Attack"], "title": "On the effectiveness of local binary patterns in face anti-spoofing"}
{"id": "LSICC", "contents": "Large Scale Informal Chinese Corpus (LSICC) is a large-scale corpus of informal Chinese. This corpus contains around 37 million book reviews and 50 thousand netizen's comments to the news.\r\n\r\nSource: [LSICC: A Large Scale Informal Chinese Corpus](/paper/lsicc-a-large-scale-informal-chinese-corpus)", "variants": ["LSICC"], "title": "LSICC: A Large Scale Informal Chinese Corpus"}
{"id": "ICubWorld", "contents": "iCubWorld datasets are collections of images recording the visual experience of iCub while observing objects in its typical environment, a laboratory or an office. The acquisition setting is devised to allow a natural human-robot interaction, where a teacher verbally provides the label of the object of interest and shows it to the robot, by holding it in the hand; the iCub can either track the object while the teacher moves it, or take it in its hand.", "variants": ["ICubWorld"], "title": "iCub World: Friendly Robots Help Building Good Vision Data-Sets"}
{"id": "VehicleX", "contents": "**VehicleX** is a large-scale synthetic dataset. Created in Unity, it contains 1,362 vehicles of various 3D models with fully editable attributes.\n\nSource: [https://github.com/yorkeyao/VehicleX](https://github.com/yorkeyao/VehicleX)\nImage Source: [https://arxiv.org/pdf/1912.08855.pdf](https://arxiv.org/pdf/1912.08855.pdf)", "variants": ["VehicleX"], "title": "Simulating Content Consistent Vehicle Datasets with Attribute Descent"}
{"id": "PHOENIX14T", "contents": "Over a period of three years (2009 - 2011) the daily news and weather forecast airings of the German public tv-station PHOENIX featuring sign language interpretation have been recorded and the weather forecasts of a subset of 386 editions have been transcribed using gloss notation. Furthermore, we used automatic speech recognition with manual cleaning to transcribe the original German speech. As such, this corpus allows to train end-to-end sign language translation systems from sign language video input to spoken language.\r\n\r\nThe signing is recorded by a stationary color camera placed in front of the sign language interpreters. Interpreters wear dark clothes in front of an artificial grey background with color transition. All recorded videos are at 25 frames per second and the size of the frames is 210 by 260 pixels. Each frame shows the interpreter box only.", "variants": ["PHOENIX14T"], "title": "Neural Sign Language Translation"}
{"id": "Fakeddit", "contents": "**Fakeddit** is a novel multimodal dataset for fake news detection consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision.\n\nSource: [https://fakeddit.netlify.app/](https://fakeddit.netlify.app/)", "variants": ["Fakeddit"], "title": "r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection"}
{"id": "OmniArt", "contents": "Presents half a million samples and structured meta-data to encourage further research and societal engagement.\r\n\r\nSource: [OmniArt: Multi-task Deep Learning for Artistic Data Analysis](/paper/omniart-multi-task-deep-learning-for-artistic)", "variants": ["OmniArt"], "title": "OmniArt: Multi-task Deep Learning for Artistic Data Analysis"}
{"id": "Liu et al. Corpus", "contents": "The **Liu et al. Corpus** is a pretraining dataset for large language models. It consists of 160Gb of news, books, stories, and web text.", "variants": ["Liu et al. Corpus"], "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}
{"id": "Objects365", "contents": "Objects365 is a large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. \r\n\r\nSource: [Objects365: A Large-Scale, High-Quality Dataset for Object Detection](https://paperswithcode.com/paper/objects365-a-large-scale-high-quality-dataset)\r\nImage Source: [https://www.objects365.org/overview.html](https://www.objects365.org/overview.html)", "variants": ["Objects365"], "title": "Objects365: A Large-Scale, High-Quality Dataset for Object Detection"}
{"id": "SIXray", "contents": "The **SIXray** dataset is constructed by the Pattern Recognition and Intelligent System Development Laboratory, University of Chinese Academy of Sciences. It contains 1,059,231 X-ray images which are collected from some several subway stations. There are six common categories of prohibited items, namely, gun, knife, wrench, pliers, scissors and hammer. It has three subsets called SIXray10, SIXray100 and SIXray1000, There are image-level annotations provided by human security inspectors for the whole dataset. In addition the images in the test set are annotated with a bounding-box for each prohibited item to evaluate the performance of object localization.\n\nSource: [https://github.com/MeioJane/SIXray](https://github.com/MeioJane/SIXray)\nImage Source: [https://github.com/MeioJane/SIXray](https://github.com/MeioJane/SIXray)", "variants": ["SIXray"], "title": "SIXray: A Large-Scale Security Inspection X-Ray Benchmark for Prohibited Item Discovery in Overlapping Images"}
{"id": "SGD", "contents": "The Schema-Guided Dialogue (SGD) dataset consists of over 20k annotated multi-domain, task-oriented conversations between a human and a virtual assistant. These conversations involve interactions with services and APIs spanning 20 domains, ranging from banks and events to media, calendar, travel, and weather. For most of these domains, the dataset contains multiple different APIs, many of which have overlapping functionalities but different interfaces, which reflects common real-world scenarios. The wide range of available annotations can be used for intent prediction, slot filling, dialogue state tracking, policy imitation learning, language generation, user simulation learning, among other tasks in large-scale virtual assistants. Besides these, the dataset has unseen domains and services in the evaluation set to quantify the performance in zero-shot or few shot settings.\r\n\r\nSource: [The Schema-Guided Dialogue Dataset](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue)", "variants": ["SGD"], "title": "Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset"}
{"id": "KAIST Urban", "contents": "This data set provides Light Detection and Ranging (LiDAR) data and stereo image with various position sensors targeting a highly complex urban environment. The presented data set captures features in urban environments (e.g. metropolis areas, complex buildings and residential areas). The data of 2D and 3D LiDAR are provided, which are typical types of LiDAR sensors. Raw sensor data for vehicle navigation is presented in a file format. For convenience, development tools are provided in the Robot Operating System (ROS) environment.\n\nSource: [https://irap.kaist.ac.kr/dataset/](https://irap.kaist.ac.kr/dataset/)\nImage Source: [https://irap.kaist.ac.kr/dataset/](https://irap.kaist.ac.kr/dataset/)", "variants": ["KAIST Urban"], "title": "Complex urban dataset with multi-level sensors from highly diverse urban environments"}
{"id": "Winogender Schemas", "contents": "Winogender Schemas is a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender.\r\n\r\nSource: [Gender Bias in Coreference Resolution](/paper/gender-bias-in-coreference-resolution)", "variants": ["Winogender Schemas"], "title": "Gender Bias in Coreference Resolution"}
{"id": "3DPeople Dataset", "contents": "A large-scale synthetic dataset with 2.5 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits.\r\n\r\nSource: [3DPeople: Modeling the Geometry of Dressed Humans](/paper/3dpeople-modeling-the-geometry-of-dressed)", "variants": ["3DPeople Dataset"], "title": "3DPeople: Modeling the Geometry of Dressed Humans"}
{"id": "EgoGesture", "contents": "The **EgoGesture** dataset contains 2,081 RGB-D videos, 24,161 gesture samples and 2,953,224 frames from 50 distinct subjects.\r\n\r\nSource: [http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html](http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html)\r\nImage Source: [http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html](http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html)", "variants": ["EgoGesture"], "title": "EgoGesture: A New Dataset and Benchmark for Egocentric Hand Gesture Recognition"}
{"id": "NYU Symmetry Database", "contents": "The **NYU Symmetry** database contains 176 single-symmetry and 63 multiple-symmetry images (.png files) with accompanying ground-truth annotations (.mat files). Also included are a .m file to visualize the annotations on top of the images, and a .txt file with instructions on how to interpret the .mat annotations.", "variants": ["NYU Symmetry Database"], "title": "A convolutional approach to reflection symmetry"}
{"id": "MedQuAD", "contents": "MedQuAD includes 47,457 medical question-answer pairs created from 12 NIH websites (e.g. cancer.gov, niddk.nih.gov, GARD, MedlinePlus Health Topics). The collection covers 37 question types (e.g. Treatment, Diagnosis, Side Effects) associated with diseases, drugs and other medical entities such as tests.  \r\n\r\nSource: [A Question-Entailment Approach to Question Answering](/paper/a-question-entailment-approach-to-question)", "variants": ["MedQuAD"], "title": "A question-entailment approach to question answering"}
{"id": "Visual7W", "contents": "**Visual7W** is a large-scale visual question answering (QA) dataset, with object-level groundings and multimodal answers. Each question starts with one of the seven Ws, what, where, when, who, why, how and which. It is collected from 47,300 COCO iamges and it has 327,929 QA pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories.\r\n\r\nSource: [https://github.com/yukezhu/visual7w-toolkit](https://github.com/yukezhu/visual7w-toolkit)\r\nImage Source: [http://ai.stanford.edu/~yukez/visual7w/](http://ai.stanford.edu/~yukez/visual7w/)", "variants": ["Visual7W"], "title": "Visual7W: Grounded Question Answering in Images"}
{"id": "LS3D-W", "contents": "A 3D facial landmark dataset of around 230,000 images.\r\n\r\nSource: [How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)](/paper/how-far-are-we-from-solving-the-2d-3d-face)", "variants": ["LS3D-W Balanced", "LS3D-W"], "title": "How far are we from solving the 2D&3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)"}
{"id": "AIDER", "contents": "Dataset aimed to do automated aerial scene classification of disaster events from on-board a UAV.\r\n\r\nSource: [Deep-Learning-Based Aerial Image Classification for Emergency Response Applications Using Unmanned Aerial Vehicles](/paper/deep-learning-based-aerial-image)", "variants": ["AIDER"], "title": "Deep-Learning-Based Aerial Image Classification for Emergency Response Applications Using Unmanned Aerial Vehicles"}
{"id": "ACFR Orchard Fruit Dataset", "contents": "ACFR Orchard Fruit Dataset is an agricultural dataset containing images and annotations for different fruits, collected at different farms across Australia. The dataset was gathered by the agriculture team at the Australian Centre for Field Robotics, The University of Sydney, Australia.\r\n\r\nSource: [ACFR Orchard Fruit Dataset](https://data.acfr.usyd.edu.au/ag/treecrops/2016-multifruit/)", "variants": ["ACFR Orchard Fruit Dataset"], "title": "Deep fruit detection in orchards"}
{"id": "MERL Shopping", "contents": "MERL Shopping is a dataset for training and testing action detection algorithms. The MERL Shopping Dataset consists of 106 videos, each of which is a sequence about 2 minutes long. The videos are from a fixed overhead camera looking down at people shopping in a grocery store setting. Each video contains several instances of the following 5 actions: \"Reach To Shelf\" (reach hand into shelf), \"Retract From Shelf \" (retract hand from shelf), \"Hand In Shelf\" (extended period with hand in the shelf), \"Inspect Product\" (inspect product while holding it in hand), and \"Inspect Shelf\" (look at shelf while not touching or reaching for the shelf).\r\n\r\nSource: [Action Detection in Videos](https://www.merl.com/demos/merl-shopping-dataset)\r\n\r\nImage Source: [Action Detection in Videos](https://www.merl.com/demos/merl-shopping-dataset)", "variants": ["MERL Shopping"], "title": "A Multi-stream Bi-directional Recurrent Neural Network for Fine-Grained Action Detection"}
{"id": "NEEQ Annual Reports", "contents": "Business taxonomies automatically constructed from the content of corporate annual reports.\r\n\r\nSource: [Business Taxonomy Construction Using Concept-Level Hierarchical Clustering](/paper/business-taxonomy-construction-using-concept)", "variants": ["NEEQ Annual Reports"], "title": "Business Taxonomy Construction Using Concept-Level Hierarchical Clustering"}
{"id": "PKU-MMD", "contents": "The **PKU-MMD** dataset is a large skeleton-based action detection dataset. It contains 1076 long untrimmed video sequences performed by 66 subjects in three camera views. 51 action categories are annotated, resulting almost 20,000 action instances and 5.4 million frames in total. Similar to NTU RGB+D, there are also two recommended evaluate protocols, i.e. cross-subject and cross-view.\r\n\r\nSource: [Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation](https://arxiv.org/abs/1804.06055)\r\nImage Source: [https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html](https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html)", "variants": ["PKU-MMD"], "title": "PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding"}
{"id": "Arxiv GR-QC", "contents": "**Arxiv GR-QC** (General Relativity and Quantum Cosmology) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to General Relativity and Quantum Cosmology category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.\n\nSource: [https://snap.stanford.edu/data/ca-GrQc.html](https://snap.stanford.edu/data/ca-GrQc.html)", "variants": ["arXiv-GrQc 2-clique", "arXiv-GrQc 3-clique", "Arxiv GR-QC"], "title": "Graph evolution: Densification and shrinking diameters"}
{"id": "Salient Object Subitizing Dataset", "contents": "A salient object subitizing image dataset of about 14K everyday images which are annotated using an online crowdsourcing marketplace. \r\n\r\nSource: [Salient Object Subitizing](/paper/salient-object-subitizing)", "variants": ["Salient Object Subitizing Dataset"], "title": "Salient Object Subitizing"}
{"id": "H3D", "contents": "The H3D is a large scale full-surround 3D multi-object detection and tracking dataset. It is gathered from HDD dataset, a large scale naturalistic driving dataset collected in San Francisco Bay Area. H3D consists of following features:\r\n\r\n* Full 360 degree LiDAR dataset (dense pointcloud from Velodyne-64)\r\n* 160 crowded and highly interactive traffic scenes\r\n* 1,071,302 3D bounding box labels\r\n* 8 common classes of traffic participants (Manually annotated every 2Hz and linearly propagated for 10 Hz data)\r\n* Benchmarked on state-of-the art algorithms for 3D only detection and tracking algorithms.", "variants": ["H3D"], "title": "The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes"}
{"id": "Twitch-FIFA", "contents": "**Twitch-FIFA** is video-context, many-speaker dialogue dataset based on live-broadcast soccer game videos and chats from Twitch.tv. This dataset can be used to train visually-grounded dialogue models that generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history.\n\nSource: [https://github.com/ramakanth-pasunuru/video-dialogue](https://github.com/ramakanth-pasunuru/video-dialogue)", "variants": ["Twitch-FIFA"], "title": "Game-Based Video-Context Dialogue"}
{"id": "HDD", "contents": "Honda Research Institute Driving Dataset (HDD) is a dataset to enable research on learning driver behavior in real-life environments. The dataset includes 104 hours of real human driving in the San Francisco Bay Area collected using an instrumented vehicle equipped with different sensors.\r\n\r\nSource: [Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning](https://arxiv.org/pdf/1811.02307v1.pdf)\r\nImage Source: [https://usa.honda-ri.com/hdd](https://usa.honda-ri.com/hdd)", "variants": ["HDD"], "title": "Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning"}
{"id": "CubiCasa5K", "contents": "**CubiCasa5K** is a large-scale floorplan image dataset containing 5000 samples annotated into over 80 floorplan object categories. The dataset annotations are performed in a dense and versatile manner by using polygons for separating the different objects.\n\nSource: [https://github.com/CubiCasa/CubiCasa5k](https://github.com/CubiCasa/CubiCasa5k)", "variants": ["CubiCasa5K"], "title": "CubiCasa5K: A Dataset and an Improved Multi-Task Model for Floorplan Image Analysis"}
{"id": "HotelRec", "contents": "Publicly available dataset in the hotel domain (50M versus 0.9M) and additionally, the largest recommendation dataset in a single domain and with textual reviews (50M versus 22M).\r\n\r\nSource: [HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset](/paper/hotelrec-a-novel-very-large-scale-hotel)", "variants": ["HotelRec"], "title": "HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset"}
{"id": "WikiLarge", "contents": "**WikiLarge** comprise 359 test sentences, 2000 development sentences and 300k training sentences. Each source sentences in test set has 8 simplified references\r\n\r\nSource: [Semi-Supervised Text Simplification with Back-Translation and Asymmetric Denoising Autoencoders](https://arxiv.org/abs/2004.14693)\r\nImage Source: [https://arxiv.org/pdf/1904.02767.pdf](https://arxiv.org/pdf/1904.02767.pdf)", "variants": ["WikiLarge"], "title": "Sentence Simplification with Deep Reinforcement Learning"}
{"id": "Urban Dict spelling variant", "contents": "**Urban Dict spelling variant** is a variant spelling dataset for use of NLP research in the informal domain. It consists of around 25k variant spelling pairs form UrbanDictionary.\r\n\r\nSource: [https://arxiv.org/abs/1911.04669](https://arxiv.org/abs/1911.04669)\r\nImage Source: [https://arxiv.org/pdf/1911.04669.pdf](https://arxiv.org/pdf/1911.04669.pdf)", "variants": ["Urban Dict spelling variant"], "title": "How to Evaluate Word Representations of Informal Domain?"}
{"id": "Wikipedia Title", "contents": "**Wikipedia Title** is a dataset for learning character-level compositionality from the character visual characteristics. It consists of a collection of Wikipedia titles in Chinese, Japanese or Korean labelled with the category to which the article belongs.\n\nSource: [https://arxiv.org/abs/1704.04859](https://arxiv.org/abs/1704.04859)", "variants": ["Wikipedia Title"], "title": "Learning Character-level Compositionality with Visual Features"}
{"id": "RoboNet", "contents": "An open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation.\r\n\r\nSource: [RoboNet: Large-Scale Multi-Robot Learning](/paper/robonet-large-scale-multi-robot-learning)", "variants": ["RoboNet"], "title": "RoboNet: Large-Scale Multi-Robot Learning"}
{"id": "ExpW", "contents": "The **Expression in-the-Wild (ExpW)** dataset is for facial expression recognition and contains 91,793 faces manually labeled with expressions. Each of the face images is annotated as one of the seven basic expression categories: “angry”, “disgust”, “fear”, “happy”, “sad”, “surprise”, or “neutral”.", "variants": ["ExpW"], "title": "From Facial Expression Recognition to Interpersonal Relation Prediction"}
{"id": "FDDB", "contents": "The **Face Detection Dataset and Benchmark** (**FDDB**) dataset is a collection of labeled faces from Faces in the Wild dataset. It contains a total of 5171 face annotations, where images are also of various resolution, e.g. 363x450 and 229x410. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Both greyscale and color images are included.\r\n\r\nSource: [A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications](https://arxiv.org/abs/1809.03336)", "variants": ["FDDB"], "title": "Lip Reading in the Wild"}
{"id": "Multi-Domain Sentiment Dataset v2.0", "contents": "The Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains). Some domains (books and dvds) have hundreds of thousands of reviews. Others (musical instruments) have only a few hundred. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed.\r\n\r\nSource: [Multi-Domain Sentiment Dataset (version 2.0)](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)", "variants": ["Multi-Domain Sentiment Dataset", "Multi-Domain Sentiment Dataset v2.0"], "title": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification"}
{"id": "DTD", "contents": "The **Describable Textures Dataset** (**DTD**) contains 5640 texture images in the wild. They are annotated with human-centric attributes inspired by the perceptual properties of textures.\r\n\r\nSource: [Where is the Fake? Patch-Wise Supervised GANs for Texture Inpainting](https://arxiv.org/abs/1911.02274)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/dtd/](https://www.robots.ox.ac.uk/~vgg/data/dtd/)", "variants": ["DTD"], "title": "Describing Textures in the Wild"}
{"id": "3D60", "contents": "Collects high quality 360 datasets with ground truth depth annotations, by re-using recently released large scale 3D datasets and re-purposing them to 360 via rendering. \r\n\r\nSource: [OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas](/paper/omnidepth-dense-depth-estimation-for-indoors)", "variants": ["3D60"], "title": "OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas"}
{"id": "VideoNavQA", "contents": "The VideoNavQA dataset contains pairs of questions and videos generated in the House3D environment. The goal of this dataset is to assess question-answering performance from nearly-ideal navigation paths, while considering a much more complete variety of questions than current instantiations of the Embodied Question Answering (EQA) task.\r\n\r\nVideoNavQA contains approximately 101,000 pairs of videos and questions, 28 types of questions belonging to 8 categories, with 70 possible answers. Each question type is\r\nassociated with a template that facilitates programmatic generation using ground truth information extracted from the video. The complexity of the questions in the dataset is far beyond that of other similar tasks using this generation method (such as CLEVR): the questions involve single or multiple object/room existence, object/room counting, object color recognition and localization, spatial reasoning, object/room size comparison and equality of object attributes (color, room location).\r\n\r\nSource: [VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering](https://arxiv.org/abs/1908.04950)", "variants": ["VideoNavQA"], "title": "VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering"}
{"id": "NLPR", "contents": "The **NLPR** dataset for salient object detection consists of 1,000 image pairs captured by a standard Microsoft Kinect with a resolution of 640×480. The images include indoor and outdoor scenes (e.g., offices, campuses, streets and supermarkets).\r\n\r\nSource: [Bifurcated Backbone Strategy for RGB-D Salient Object Detection](https://arxiv.org/abs/2007.02713)\r\nImage Source: [https://sites.google.com/site/rgbdsaliency/dataset](https://sites.google.com/site/rgbdsaliency/dataset)", "variants": ["NLPR"], "title": "RGBD salient object detection: A benchmark and algorithms"}
{"id": "T-REx", "contents": "A dataset of large scale alignments between Wikipedia abstracts and Wikidata triples. T-REx consists of 11 million triples aligned with 3.09 million Wikipedia abstracts (6.2 million sentences).\r\n\r\nSource: [T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples](/paper/t-rex-a-large-scale-alignment-of-natural)", "variants": ["T-REx"], "title": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"}
{"id": "YAGO", "contents": "**Yet Another Great Ontology** (**YAGO**) is a Knowledge Graph that augments WordNet with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base. YAGO originally consisted of more than 1 million entities and 5 million facts describing relationships between these entities. YAGO2 grounded entities, facts, and events in time and space, contained 446 million facts about 9.8 million entities, while YAGO3 added about 1 million more entities from non-English Wikipedia articles. YAGO3-10 a subset of YAGO3, containing entities which have a minimum of 10 relations each.\r\n\r\nSource: [Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)\r\nImage Source: [https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/)", "variants": ["YAGO37", "YAGO39K", "YAGO15k", "Yago11k", "YAGO", "YAGO3-10"], "title": "YAGO : A Core of Semantic Knowledge Unifying WordNet and Wikipedia"}
{"id": "Fine-grained 3D Pose", "contents": "A new large-scale dataset that consists of 409 fine-grained categories and 31,881 images with accurate 3D pose annotation.\r\n\r\nSource: [Improving Annotation for 3D Pose Dataset of Fine-Grained Object Categories](/paper/improving-annotation-for-3d-pose-dataset-of)", "variants": ["Fine-grained 3D Pose"], "title": "Improving Annotation for 3D Pose Dataset of Fine-Grained Object Categories"}
{"id": "AFLW2000-3D", "contents": "**AFLW2000-3D** is a dataset of 2000 images that have been annotated with image-level 68-point 3D facial landmarks. This dataset is used for evaluation of 3D facial landmark detection models. The head poses are very diverse and often hard to be detected by a CNN-based face detector.\r\n\r\nSource: [https://www.tensorflow.org/datasets/catalog/aflw2k3d](https://www.tensorflow.org/datasets/catalog/aflw2k3d)\r\nImage Source: [http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm)", "variants": ["AFLW2000", "AFLW2000-3D"], "title": "Face Alignment Across Large Poses: A 3D Solution"}
{"id": "Visual Wake Words", "contents": "Visual Wake Words represents a common microcontroller vision use-case of identifying whether a person is present in the image or not, and provides a realistic benchmark for tiny vision models.\r\n\r\nSource: [Visual Wake Words Dataset](https://arxiv.org/pdf/1906.05721.pdf)\r\nImage Source: [Chowdhery et al](https://arxiv.org/pdf/1906.05721.pdf)", "variants": ["Visual Wake Words"], "title": "Visual Wake Words Dataset"}
{"id": "HHOI", "contents": "A new RGB-D video dataset, i.e., UCLA Human-Human-Object Interaction (HHOI) dataset, which includes 3 types of human-human interactions, i.e., shake hands, high-five, pull up, and 2 types of human-object-human interactions, i.e., throw and catch, and hand over a cup. On average, there are 23.6 instances per interaction performed by totally 8 actors recorded from various views. Each interaction lasts 2-7 seconds presented at 10-15 fps.\r\n\r\nSource: [Learning Social Affordance for Human-Robot Interaction](/paper/learning-social-affordance-for-human-robot)", "variants": ["HHOI"], "title": "Learning Social Affordance for Human-Robot Interaction"}
{"id": "WikiAnn", "contents": "WikiAnn is a dataset for cross-lingual name tagging and linking based on Wikipedia articles in 295 languages.\r\n\r\nSource: [Cross-lingual Name Tagging and Linking for 282 Languages](/paper/cross-lingual-name-tagging-and-linking-for)", "variants": ["WikiAnn"], "title": "Cross-lingual Name Tagging and Linking for 282 Languages"}
{"id": "CASR", "contents": "CASR is a dataset for cyclist arm signal recognition in videos. It contains 219 annotated arm signal actions on videos of approximately 10 seconds each, containing one or two actions per video.\r\n\r\nSource: [Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation](https://arxiv.org/pdf/1910.03858.pdf)\r\n\r\nImage Source: [Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation](https://arxiv.org/pdf/1910.03858.pdf)", "variants": ["CASR"], "title": "Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation"}
{"id": "WebText", "contents": "**WebText** is an internal OpenAI corpus created by scraping web pages with emphasis on\r\ndocument quality. The authors scraped all outbound links from\r\nReddit which received at least 3\r\nkarma. The authors used the approach as a heuristic indicator for\r\nwhether other users found the link interesting, educational,\r\nor just funny.\r\n\r\nWebText contains the text subset of these 45 million links. It consists of over 8 million documents\r\nfor a total of 40 GB of text. All Wikipedia\r\ndocuments were removed from WebText since it is a common data source\r\nfor other datasets.", "variants": ["WebText"], "title": "Language Models are Unsupervised Multitask Learners"}
{"id": "GlaS", "contents": "The dataset used in this challenge consists of 165 images derived from 16 H&E stained histological sections of stage T3 or T42 colorectal adenocarcinoma. Each section belongs to a different patient, and sections were processed in the laboratory on different occasions. Thus, the dataset exhibits high inter-subject variability in both stain distribution and tissue architecture. The digitization of these histological sections into whole-slide images (WSIs) was accomplished using a Zeiss MIRAX MIDI Slide Scanner with a pixel resolution of 0.465µm.\r\n\r\nSource: [Sirinukunwattana et al.](https://arxiv.org/pdf/1603.00275.pdf)\r\n\r\nImage source: [Sirinukunwattana et al.](https://arxiv.org/pdf/1603.00275.pdf)", "variants": ["GlaS"], "title": "Gland segmentation in colon histology images: The glas challenge contest"}
{"id": "RUN", "contents": "The RUN dataset  is based on OpenStreetMap (OSM). The map contains rich layers and an abundance of entities of different types. Each entity is complex and can contain (at least) four labels: name, type, is building=y/n, and house number. An entity can spread over several tiles. As the maps do not overlap, only very few entities are shared among them. The RUN dataset aligns NL navigation instructions to coordinates of their corresponding route on the OSM map.\r\n\r\nSource: [RUN](https://github.com/OnlpLab/RUN)", "variants": ["RUN"], "title": "RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation"}
{"id": "SNAP", "contents": "**SNAP** is a collection of large network datasets. It includes graphs representing social networks, citation networks, web graphs, online communities, online reviews and more.\r\nImage Source: [https://snap.stanford.edu/data/](https://snap.stanford.edu/data/)", "variants": ["SNAP"], "title": "Recognizing Actions from Depth Cameras as Weakly Aligned Multi-part Bag-of-Poses"}
{"id": "Flickr Cropping Dataset", "contents": "The Flick Cropping Dataset consists of high quality cropping and pairwise ranking annotations used to evaluate the performance of automatic image cropping approaches.\n\nSource: [https://arxiv.org/abs/1701.01480](https://arxiv.org/abs/1701.01480)", "variants": ["Flickr Cropping Dataset"], "title": "Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset and Comparative Study"}
{"id": "MMID", "contents": "A large-scale multilingual corpus of images, each labeled with the word it represents. The dataset includes approximately 10,000 words in each of 100 languages.\r\n\r\nSource: [Learning Translations via Images with a Massively Multilingual Image Dataset](/paper/learning-translations-via-images-with-a)", "variants": ["MMID"], "title": "Learning Translations via Images with a Massively Multilingual Image Dataset"}
{"id": "FewRel", "contents": "The **FewRel** (**Few-Shot Relation Classification Dataset**) contains 100 relations and 70,000 instances from Wikipedia. The dataset is divided into three subsets: training set (64 relations), validation set (16 relations) and test set (20 relations).\r\n\r\nSource: [Neural Snowball for Few-Shot Relation Learning](https://arxiv.org/abs/1908.11007)\r\nImage Source: [https://www.aclweb.org/anthology/D18-1514.pdf](https://www.aclweb.org/anthology/D18-1514.pdf)", "variants": ["FewRel"], "title": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation"}
{"id": "InteriorNet", "contents": "**InteriorNet** is a RGB-D for large scale interior scene understanding and mapping. The dataset contains 20M images created by pipeline:\r\n\r\n* (A) the authors collected around 1 million CAD models provided by world-leading furniture manufacturers.\r\n* (B) based on those models, around 1,100 professional designers create around 22 million interior layouts. Most of such layouts have been used in real-world decorations.\r\n* (C) For each layout, authors generate a number of configurations to represent different random lightings and simulation of scene change over time in daily life.\r\n* (D) Authors provide an interactive simulator (ViSim) to help for creating ground truth IMU, events, as well as monocular or stereo camera trajectories including hand-drawn, random walking and neural network based realistic trajectory.\r\n* (E) All supported image sequences and ground truth.\r\n\r\nSource: [InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset](/paper/interiornet-mega-scale-multi-sensor-photo)\r\nImage Source: [InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset](/paper/interiornet-mega-scale-multi-sensor-photo)", "variants": ["InteriorNet"], "title": "InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset"}
{"id": "IN2LAAMA", "contents": "IN2LAAMA is a set of lidar-inertial datasets collected with a Velodyne VLP-16 lidar and a Xsens MTi-3 IMU.\r\n\r\nSource: [IN2LAAMA: INertial Lidar Localisation Autocalibration And MApping](/paper/in2laama-inertial-lidar-localisation)", "variants": ["IN2LAAMA"], "title": "IN2LAAMA: INertial Lidar Localisation Autocalibration And MApping"}
{"id": "Groove", "contents": "The **Groove MIDI Dataset (GMD)** is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming. The dataset contains 1,150 MIDI files and over 22,000 measures of drumming.", "variants": ["Groove"], "title": "Learning to Groove with Inverse Sequence Transformations"}
{"id": "RefCoco", "contents": "This referring expression generation (REG) dataset was collected using the ReferitGame. In this two-player game, the first player is shown an image with a segmented target object and asked to write a natural language expression referring to the target object. The second player is shown only the image and the referring expression and asked to click on the corresponding object. If the players do their job correctly, they receive points and swap roles. If not, they are presented with a new object and image for description. Images in these collections were selected to contain two or more objects of the same object category. In the RefCOCO dataset, no restrictions are placed on the type of language used in the referring expressions. In a version of this dataset called RefCOCO+ players are disallowed from using location words in their referring expressions by adding “taboo” words to the ReferItGame. This dataset was collected to obtain a referring expression dataset focsed on purely appearance based description, e.g., “the man in the yellow polka-dotted shirt” rather than “the second man from the left”, which tend to be more interesting from a computer vision based perspective and are independent of viewer perspective. RefCOCO consists of 142,209 refer expressions for 50,000 objects in 19,994 images, and RefCOCO+ has 141,564 expressions for 49,856 objects in 19,992 images.\r\n\r\nSource: [https://arxiv.org/pdf/1608.00272.pdf](https://arxiv.org/pdf/1608.00272.pdf)\r\nImage Source: [https://github.com/lichengunc/refer](https://github.com/lichengunc/refer)", "variants": ["RefCoco"], "title": "Modeling Context in Referring Expressions"}
{"id": "DEMAND", "contents": "The **DEMAND** (Diverse Environments Multichannel Acoustic Noise Database) provides a set of recordings that allow testing of algorithms using real-world noise in a variety of settings. This version provides 15 recordings. All recordings are made with a 16-channel array, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm.\n\nSource: [DEMAND: a collection of multi-channel recordings of acoustic noise in diverse environments](https://zenodo.org/record/1227121)\nImage Source: [https://asa.scitation.org/doi/pdf/10.1121/1.4799597](https://asa.scitation.org/doi/pdf/10.1121/1.4799597)", "variants": ["DEMAND"], "title": "The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings"}
{"id": "VLOG Dataset", "contents": "A large collection of interaction-rich video data which are annotated and analyzed.\r\n\r\nSource: [From Lifestyle Vlogs to Everyday Interactions](/paper/from-lifestyle-vlogs-to-everyday-interactions)", "variants": ["VLOG Dataset"], "title": "From Lifestyle Vlogs to Everyday Interactions"}
{"id": "Paris-Lille-3D", "contents": "The **Paris-Lille-3D** is a Benchmark on Point Cloud Classification. The Point Cloud has been labeled entirely by hand with 50 different classes. The dataset consists of around 2km of Mobile Laser System point cloud acquired in two cities in France (Paris and Lille).\r\n\r\nSource: [Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification](https://arxiv.org/pdf/1712.00032v2.pdf)\r\nImage Source: [https://npm3d.fr/paris-lille-3d](https://npm3d.fr/paris-lille-3d)", "variants": ["Paris-Lille-3D"], "title": "Paris-Lille-3D: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification"}
{"id": "CROSS", "contents": "Cross-Reference Omnidirectional Stitching IQA is a novel omnidirectional image dataset containing stitched images as well as dual-fisheye images captured from standard quarters of 0◦, 90◦ , 180◦ and 270◦. In this manner, when evaluating the quality of an image stitched from a pair of fisheye images (e.g., 0◦ and 180◦), the other pair of fisheye images (e.g., 90◦ and 270◦) can be used as the cross-reference to provide ground-truth observations of the stitching regions.\r\n\r\nSource: [Image Quality Assessment for Omnidirectional Cross-reference Stitching](https://arxiv.org/pdf/1904.04960.pdf)", "variants": ["CROSS"], "title": "Image Quality Assessment for Omnidirectional Cross-reference Stitching"}
{"id": "Places205", "contents": "The **Places205** dataset is a large-scale scene-centric dataset with 205 common scene categories. The training dataset contains around 2,500,000 images from these categories. In the training set, each scene category has the minimum 5,000 and maximum 15,000 images. The validation set contains 100 images per category (a total of 20,500 images), and the testing set includes 200 images per category (a total of 41,000 images).\r\n\r\nSource: [Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs](https://arxiv.org/abs/1610.01119)\r\nImage Source: [http://places.csail.mit.edu/browser.html](http://places.csail.mit.edu/browser.html)", "variants": ["Places205", "Places"], "title": "Learning Deep Features for Scene Recognition using Places Database"}
{"id": "DEFT Corpus", "contents": "A SemEval shared task in which participants must extract definitions from free text using a term-definition pair corpus that reflects the complex reality of definitions in natural language. \r\n\r\nSource: [SemEval-2020 Task 6: Definition extraction from free text with the DEFT corpus](/paper/semeval-2020-task-6-definition-extraction)", "variants": ["DEFT Corpus"], "title": "DEFT: A corpus for definition extraction in free- and semi-structured text"}
{"id": "WiLI-2018", "contents": "WiLI-2018 is a benchmark dataset for monolingual written natural language identification. WiLI-2018 is a publicly available, free of charge dataset of short text extracts from Wikipedia. It contains 1000 paragraphs of 235 languages, totaling in 23500 paragraphs. WiLI is a classification dataset: Given an unknown paragraph written in one dominant language, it has to be decided which language it is.\r\n\r\nSource: [The WiLI benchmark dataset for written language identification](/paper/the-wili-benchmark-dataset-for-written)", "variants": ["WiLI-2018"], "title": "The WiLI benchmark dataset for written language identification"}
{"id": "URMP", "contents": "**URMP** (**University of Rochester Multi-Modal Musical Performance**) is a dataset for facilitating audio-visual analysis of musical performances. The dataset comprises 44 simple multi-instrument musical pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece the dataset provided the musical score in MIDI format, the high-quality individual instrument audio recordings and the videos of the assembled pieces.\n\nSource: [http://www2.ece.rochester.edu/projects/air/projects/URMP.html](http://www2.ece.rochester.edu/projects/air/projects/URMP.html)\nImage Source: [http://www2.ece.rochester.edu/projects/air/projects/URMP.html](http://www2.ece.rochester.edu/projects/air/projects/URMP.html)\nAudio Source: [http://www2.ece.rochester.edu/projects/air/projects/URMP.html](http://www2.ece.rochester.edu/projects/air/projects/URMP.html)", "variants": ["URMP"], "title": "Creating A Multi-track Classical Musical Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications"}
{"id": "SimpleDBpediaQA", "contents": "A new benchmark dataset for simple question answering over knowledge graphs that was created by mapping SimpleQuestions entities and predicates from Freebase to DBpedia. \r\n\r\nSource: [Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia](/paper/farewell-freebase-migrating-the)", "variants": ["SimpleDBpediaQA"], "title": "Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia"}
{"id": "TSAC", "contents": "Tunisian Sentiment Analysis Corpus (TSAC) is a Tunisian Dialect corpus of 17.000 comments from Facebook. \r\n\r\nSource: [Sentiment Analysis of Tunisian Dialects: Linguistic Ressources and Experiments](/paper/sentiment-analysis-of-tunisian-dialects)", "variants": ["TSAC"], "title": "Sentiment Analysis of Tunisian Dialects: Linguistic Ressources and Experiments"}
{"id": "WikiAtomicEdits", "contents": "WikiAtomicEdits is a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. \r\n\r\nSource: [WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse](/paper/wikiatomicedits-a-multilingual-corpus-of)\r\nImage Source: [https://arxiv.org/pdf/1808.09422v1.pdf](https://arxiv.org/pdf/1808.09422v1.pdf)", "variants": ["WikiAtomicEdits"], "title": "WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse"}
{"id": "Letter", "contents": "Letter Recognition Data Set is a handwritten digit dataset. The task is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.\r\n\r\nSource: [UCL Machine Learning Repository Letter Recognition](https://archive.ics.uci.edu/ml/datasets/Letter+Recognition)\r\nImage Source: [http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf](http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf)", "variants": ["LetterA-J", "Letter"], "title": "Letter recognition using Holland-style adaptive classifiers"}
{"id": "VIPL-HR", "contents": "VIPL-HR database is a database for remote heart rate (HR) estimation from face videos under less-constrained situations. It contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Nine different conditions, including various head movements and illumination conditions are taken into consideration. All the videos are recorded using Logitech C310, RealSense F200 and the front camera of HUAWEI P9 smartphone, and the ground-truth HR is recorded using a CONTEC CMS60C BVP sensor (a FDA approved device). \r\n\r\nSource: [VIPL-HR: A Multi-modal Database for Pulse Estimation from Less-constrained Face Video](/paper/vipl-hr-a-multi-modal-database-for-pulse)", "variants": ["VIPL-HR"], "title": "VIPL-HR: A Multi-modal Database for Pulse Estimation from Less-constrained Face Video"}
{"id": "KnowIT VQA", "contents": "KnowIT VQA is a video dataset with 24,282 human-generated question-answer pairs about The Big Bang Theory. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered.\r\n\r\nSource: [KnowIT VQA: Answering Knowledge-Based Questions about Videos](/paper/knowit-vqa-answering-knowledge-based)", "variants": ["KnowIT VQA"], "title": "KnowIT VQA: Answering Knowledge-Based Questions about Videos"}
{"id": "TUM RGB-D", "contents": "**TUM RGB-D** is an RGB-D dataset. It contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640x480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz).\r\n\r\nSource: [https://vision.in.tum.de/data/datasets/rgbd-dataset](https://vision.in.tum.de/data/datasets/rgbd-dataset)\r\nImage Source: [https://vision.in.tum.de/research/rgb-d_sensors_kinect](https://vision.in.tum.de/research/rgb-d_sensors_kinect)", "variants": ["TUM RGB-D"], "title": "A benchmark for the evaluation of RGB-D SLAM systems"}
{"id": "ObjectNet", "contents": "**ObjectNet** is a test set of images collected directly using crowd-sourcing. ObjectNet is unique as the objects are captured at unusual poses in cluttered, natural scenes, which can severely degrade recognition performance. There are 50,000 images in the test set which controls for rotation, background and viewpoint. There are 313 object classes with 113 overlapping ImageNet.\r\n\r\nSource: [On Robustness and Transferability of Convolutional Neural Networks](https://arxiv.org/abs/2007.08558)\r\nImage Source: [https://objectnet.dev/](https://objectnet.dev/)", "variants": ["ObjectNet", "ObjectNet (Bounding Box)"], "title": "ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models"}
{"id": "Grocery Store", "contents": "**Grocery Store** is a dataset of natural images of grocery items. All natural images were taken with a smartphone camera in different grocery stores. It contains 5,125 natural images from 81 different classes of fruits, vegetables, and carton items (e.g. juice, milk, yoghurt). The 81 classes are divided into 42 coarse-grained classes, where e.g. the fine-grained classes 'Royal Gala' and 'Granny Smith' belong to the same coarse-grained class 'Apple'. Additionally, each fine-grained class has an associated iconic image and a product description of the item.\n\nSource: [https://github.com/marcusklasson/GroceryStoreDataset](https://github.com/marcusklasson/GroceryStoreDataset)\nImage Source: [https://github.com/marcusklasson/GroceryStoreDataset](https://github.com/marcusklasson/GroceryStoreDataset)", "variants": ["Grocery Store"], "title": "A Hierarchical Grocery Store Image Dataset With Visual and Semantic Labels"}
{"id": "IU ShareView", "contents": "IU ShareView dataset consists of 9 sets of two 5-10 minute first-person videos. Each set contains 3-4 participants performing a variety of everyday activities (shaking hands, chatting, eating, etc.) in one of six indoor environments. \r\n\r\nSource: [IU ShareView](http://vision.soic.indiana.edu/firstthird-eccv2018/)", "variants": ["IU ShareView"], "title": "Joint Person Segmentation and Identification in Synchronized First- and Third-person Videos"}
{"id": "PedX", "contents": "PedX is a large-scale multi-modal collection of pedestrians at complex urban intersections. The dataset provides high-resolution stereo images and LiDAR data with manual 2D and automatic 3D annotations. The data was captured using two pairs of stereo cameras and four Velodyne LiDAR sensors.\r\n\r\nSource: [pedx.io](http://pedx.io/)", "variants": ["PedX"], "title": "PedX: Benchmark Dataset for Metric 3-D Pose Estimation of Pedestrians in Complex Urban Intersections"}
{"id": "MLB Dataset", "contents": "A new dataset on the baseball domain.\r\n\r\nSource: [Data-to-text Generation with Entity Modeling](/paper/data-to-text-generation-with-entity-modeling)", "variants": ["MLB Dataset"], "title": "Data-to-text Generation with Entity Modeling"}
{"id": "Synthinel-1", "contents": "**Synthinel-1** is a collection of synthetic overhead imagery with full pixel-wise building segmentation labels.\n\nSource: [https://github.com/timqqt/Synthinel](https://github.com/timqqt/Synthinel)\nImage Source: [https://github.com/timqqt/Synthinel](https://github.com/timqqt/Synthinel)", "variants": ["Synthinel-1"], "title": "The Synthinel-1 dataset: a collection of high resolution synthetic overhead imagery for building segmentation"}
{"id": "DAVIS", "contents": "The Densely Annotation Video Segmentation dataset (**DAVIS**) is a high quality and high resolution densely annotated video segmentation dataset under two resolutions, 480p and 1080p. There are 50 video sequences with 3455 densely annotated frames in pixel level. 30 videos with 2079 frames are for training and 20 videos with 1376 frames are for validation.\r\n\r\nSource: [TENet: Triple Excitation Network for Video Salient Object Detection](https://arxiv.org/abs/2007.09943)", "variants": ["DAVIS", "DAVIS 2017", "DAVIS sigma10", "DAVIS sigma20", "DAVIS sigma30", "DAVIS sigma40", "DAVIS sigma50"], "title": "A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation"}
{"id": "DHF1K", "contents": "**DHF1K** is a video saliency dataset which contains a ground-truth map of binary pixel-wise gaze fixation points and a continuous map of the fixation points after being blurred by a gaussian filter. DHF1K contains 1000 videos in total. 700 of the videos are annotated, 600 of which are used for training and 100 for validation. The remaining 300 are the testing set which are to be evaluated on a public server.\r\n\r\nSource: [ViP: Video Platform for PyTorch](https://arxiv.org/abs/1910.02793)\r\nImage Source: [https://arxiv.org/pdf/1801.07424.pdf](https://arxiv.org/pdf/1801.07424.pdf)", "variants": ["DHF1K"], "title": "Revisiting Video Saliency: A Large-Scale Benchmark and a New Model"}
{"id": "MIZAN", "contents": "Persian-English parallel corpus with more than one million sentence pairs collected from masterpieces of literature.\r\n\r\nSource: [MIZAN: A Large Persian-English Parallel Corpus](/paper/mizan-a-large-persian-english-parallel-corpus)", "variants": ["MIZAN"], "title": "MIZAN: A Large Persian-English Parallel Corpus"}
{"id": "STAIR Captions", "contents": "STAIR Captions is a large-scale dataset containing 820,310 Japanese captions.\r\nThis dataset can be used for caption generation, multimodal retrieval, and image generation.\r\n\r\nSource: [STAIR Captions](http://captions.stair.center/)", "variants": ["STAIR Captions"], "title": "STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset"}
{"id": "PFN-PIC", "contents": "This dataset is a collection of spoken language instructions for a robotic system to pick and place common objects. Text instructions and corresponding object images are provided.\nThe dataset consists of situations where the robot is instructed by the operator to pick up a specific object and move it to another location: for example, Move the blue and white tissue box to the top right bin.\nThis dataset consists of RGBD images, bounding box annotations, destination box annotations, and text instructions.\n\nSource: [https://github.com/pfnet-research/picking-instruction](https://github.com/pfnet-research/picking-instruction)\nImage Source: [https://github.com/pfnet-research/picking-instruction](https://github.com/pfnet-research/picking-instruction)", "variants": ["PFN-PIC"], "title": "Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions"}
{"id": "Total-Text", "contents": "**Total-Text** is a text detection dataset that consists of 1,555 images with a variety of text types including horizontal, multi-oriented, and curved text instances. The training split and testing split have 1,255 images and 300 images, respectively.\r\n\r\nSource: [Convolutional Character Networks](https://arxiv.org/abs/1910.07954)\r\nImage Source: [https://github.com/cs-chan/Total-Text-Dataset](https://github.com/cs-chan/Total-Text-Dataset)", "variants": ["Total-Text"], "title": "Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition"}
{"id": "Hotels-50K", "contents": "The Hotels-50K dataset consists of over 1 million images from 50,000 different hotels around the world. These images come from both travel websites, as well as the TraffickCam mobile application, which allows every day travelers to submit images of their hotel room in order to help combat trafficking. The TraffickCam images are more visually similar to images from trafficking investigations than the images from travel websites.\r\n\r\nThe training dataset includes 1,027,871 images from 50,000 hotels, and 92 major hotel chains. Of the 50,000 hotels, 13,900 include user contributed images from the TraffickCam application (a total of 55,061 TraffickCam images are included in the training set).\r\n\r\nThe test dataset includes 17,954 TraffickCam images from 5,000 different hotels (as well as versions of the test images that have medium and large occlusions to replicate the occlusions seen in real world trafficking victim photographs).\r\n\r\nSource: [Hotels-50K: A Global Hotel Recognition Dataset](/paper/hotels-50k-a-global-hotel-recognition-dataset)", "variants": ["Hotels-50K"], "title": "Hotels-50K: A Global Hotel Recognition Dataset"}
{"id": "WiC", "contents": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise.\r\n\r\nSource: [WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations](/paper/wic-10000-example-pairs-for-evaluating)", "variants": ["WiC"], "title": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations"}
{"id": "Comic2k", "contents": "**Comic2k** is a dataset used for cross-domain object detection which contains 2k comic images with image and instance-level annotations.\nImage Source: [https://naoto0804.github.io/cross_domain_detection/](https://naoto0804.github.io/cross_domain_detection/)", "variants": ["Comic2k"], "title": "Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation"}
{"id": "CCPE-M", "contents": "A dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language.\r\n\r\nThe corpus was constructed from dialogues between two paid crowd-workers using a Wizard-of-Oz methodology. One worker plays the role of an \"assistant\", while the other plays the role of a \"user\". The \"assistant\" is tasked with eliciting the \"user\" preferences about movies following a Coached Conversational Preference Elicitation (CCPE) methodology. In particular, the assistant is required to ask questions designed so as to minimize the bias in the terminology the \"user\" employs to convey his or her preferences, and obtain these in as natural language as possible. Each dialog is annotated with entity mentions, preferences expressed about entities, descriptions of entities provided, and other statements of entities.\r\n\r\nSource: [CCPE-M: Coached Conversational Preference Elicitation dataset for Movies](https://github.com/google-research-datasets/ccpe)", "variants": ["CCPE-M"], "title": "Coached Conversational Preference Elicitation: A Case Study in Understanding Movie Preferences"}
{"id": "VQA-E", "contents": "VQA-E is a dataset for Visual Question Answering with Explanation, where the models are required to generate and explanation with the predicted answer. The VQA-E dataset is automatically derived from the VQA v2 dataset by synthesizing a textual explanation for each image-question-answer triple.\r\n\r\nImage Source: [VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions](https://arxiv.org/abs/1803.07464)", "variants": ["VQA-E"], "title": "VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions"}
{"id": "DpgMedia2019", "contents": "**DpgMedia2019** is a Dutch news dataset for partisanship detection. It contains more than 100K articles that are labelled on the publisher level and 776 articles that were crowdsourced using an internal survey platform and labelled on the article level.\n\nSource: [https://github.com/dpgmedia/partisan-news2019](https://github.com/dpgmedia/partisan-news2019)", "variants": ["DpgMedia2019"], "title": "DpgMedia2019: A Dutch News Dataset for Partisanship Detection"}
{"id": "PixelShift200", "contents": "Advanced pixel shift technology is employed to perform a full color sampling of the image. Pixel shift technology takes four samples of the same image at nearly the same time, and physically controls the camera sensor to move one pixel horizontally or vertically at each sampling to capture all color information at each pixel. The pixel shift technology ensures that the sampled images follow the distribution of natural images sampled by the camera, and the full information of the color (R, Gr, Gb, B channel) is completely obtained without any need of interpolation. In this way, the collected RGB images are artifacts-free, which leads to better training results for demosaicing related tasks.\r\n\r\nPixelShift200 Dataset contains 210 high quality 4K images.\r\n\r\n- Training: 200 images\r\n- Testing: 10 images\r\n- Key Features: fully colored, demosiacing artifacts free\r\n- Camera: SONY α7R III\r\n\r\nSource: [PixelShift200](https://www.gcqian.com/project/pixelshift200/#overview)", "variants": ["PixelShift200"], "title": "Trinity of Pixel Enhancement: a Joint Solution for Demosaicking, Denoising and Super-Resolution"}
{"id": "Proposal Flow Datasets", "contents": "Dataset that can be used to evaluate both general semantic flow techniques and region-based approaches such as proposal flow. \r\n\r\nSource: [Proposal Flow](/paper/proposal-flow)", "variants": ["Proposal Flow Datasets"], "title": "Proposal Flow"}
{"id": "LAMA", "contents": "Consists of a set of knowledge sources, each comprised of a set of facts.\r\n\r\nSource: [Language Models as Knowledge Bases?](https://arxiv.org/pdf/1909.01066v2.pdf)", "variants": ["LAMA"], "title": "Language Models as Knowledge Bases?"}
{"id": "MinneApple", "contents": "**MinneApple** is a benchmark dataset for apple detection and segmentation. The fruits are labelled using polygonal masks for each object instance to aid in precise object detection, localization, and segmentation. Additionally, the dataset also contains data for patch-based counting of clustered fruits. The dataset contains over 41, 000 annotated object instances in 1000 images.\n\nSource: [https://github.com/nicolaihaeni/MinneApple](https://github.com/nicolaihaeni/MinneApple)\nImage Source: [https://github.com/nicolaihaeni/MinneApple](https://github.com/nicolaihaeni/MinneApple)", "variants": ["MinneApple"], "title": "MinneApple: A Benchmark Dataset for Apple Detection and Segmentation"}
{"id": "Urban Environments", "contents": "The Urban Environments dataset is a dataset of 20 land use classes across 300 European cities paired with satellite imagery data.\r\n\r\nSource: [https://arxiv.org/abs/1704.02965](https://arxiv.org/abs/1704.02965)\r\nImage Source: [https://github.com/adrianalbert/urban-environments](https://github.com/adrianalbert/urban-environments)", "variants": ["Urban Environments"], "title": "Using Convolutional Networks and Satellite Imagery to Identify Patterns in Urban Environments at a Large Scale"}
{"id": "Rendered Handpose Dataset", "contents": "Rendered Handpose Dataset contains 41258 training and 2728 testing samples. Each sample provides:\r\n\r\n-\tRGB image (320x320 pixels)\r\n-\tDepth map (320x320 pixels)\r\n-\tSegmentation masks (320x320 pixels) for the classes: background, person, three classes for each finger and one for each palm\r\n-\t21 Keypoints for each hand with their uv coordinates in the image frame, xyz coordinates in the world frame and a visibility indicator\r\n-\tIntrinsic Camera Matrix K\r\n\r\nSource: [Learning to Estimate 3D Hand Pose from Single RGB Images](/paper/learning-to-estimate-3d-hand-pose-from-single)\r\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)", "variants": ["Rendered Handpose Dataset"], "title": "Learning to Estimate 3D Hand Pose from Single RGB Images"}
{"id": "GoPro", "contents": "The **GoPro** dataset for deblurring consists of 3,214 blurred images with the size of 1,280×720 that are divided into 2,103 training images and 1,111 test images. The dataset consists of pairs of a realistic blurry image and the corresponding ground truth shapr image that are obtained by a high-speed camera.\r\n\r\nSource: [Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networksfor Non-Uniform Single Image Deblurring](https://arxiv.org/abs/1903.10157)\r\nImage Source: [Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring](https://openaccess.thecvf.com/content_cvpr_2017/papers/Nah_Deep_Multi-Scale_Convolutional_CVPR_2017_paper.pdf)", "variants": ["GoPro linear subset", "GoPro"], "title": "Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring"}
{"id": "CUHK-PEDES", "contents": "The **CUHK-PEDES** dataset is a caption-annotated pedestrian dataset. It contains 40,206 images over 13,003 persons. Images are collected from five existing person re-identification datasets, CUHK03, Market-1501, SSM, VIPER, and CUHK01 while each image is annotated with 2 text descriptions by crowd-sourcing workers. Sentences incorporate rich details about person appearances, actions, poses.\r\n\r\nSource: [MGD-GAN: Text-to-Pedestrian generation through Multi-Grained Discrimination](https://arxiv.org/abs/2010.00947)\r\nImage Source: [https://www.researchgate.net/figure/Image-samples-in-three-datasets-For-MSCOCO-and-Flickr30k-dataset-we-view-every-image_fig2_321095980](https://www.researchgate.net/figure/Image-samples-in-three-datasets-For-MSCOCO-and-Flickr30k-dataset-we-view-every-image_fig2_321095980)", "variants": ["CUHK-PEDES"], "title": "Person Search with Natural Language Description"}
{"id": "DR(eye)VE", "contents": "DR(eye)VE is a large dataset of driving scenes for which eye-tracking annotations are available. This dataset features more than 500,000 registered frames, matching ego-centric views (from glasses worn by drivers) and car-centric views (from roof-mounted camera), further enriched by other sensors measurements.\r\n\r\nSource: [Predicting the Driver's Focus of Attention: the DR(eye)VE Project](/paper/predicting-the-drivers-focus-of-attention-the)\r\nImage Source: [http://imagelab.ing.unimore.it/dreyeve](http://imagelab.ing.unimore.it/dreyeve)", "variants": ["DR(eye)VE"], "title": "Predicting the Driver's Focus of Attention: The DR(eye)VE Project"}
{"id": "Dataset of Structured Queries and Spatial Relations", "contents": "Provides 450, 000 relevance annotations and 53 structured queries.\r\n\r\nSource: [A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation](/paper/a-pooling-approach-to-modelling-spatial)", "variants": ["Dataset of Structured Queries and Spatial Relations"], "title": "A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation"}
{"id": "COCO-Text", "contents": "The **COCO-Text** dataset is a dataset for text detection and recognition. It is based on the MS COCO dataset, which contains images of complex everyday scenes. The COCO-Text dataset contains non-text images, legible text images and illegible text images. In total there are 22184 training images and 7026 validation images with at least one instance of legible text.\r\n\r\nSource: [Improving Text Proposals for Scene Images with Fully Convolutional Networks](https://arxiv.org/abs/1702.05089)\r\nImage Source: [https://vision.cornell.edu/se3/coco-text-2/](https://vision.cornell.edu/se3/coco-text-2/)", "variants": ["COCO-Text"], "title": "COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images"}
{"id": "Isolet", "contents": "**Isolet** is an audio dataset for predicting which letter-name was spoken. The dataset was generated as follows. 150 subjects spoke the name of each letter of the alphabet twice, resulting in 52 training examples from each speaker. The speakers are grouped into sets of 30 speakers each, and are referred to as isolet1, isolet2, isolet3, isolet4, and isolet5. The data appears in isolet1+2+3+4.data in sequential order, first the speakers from isolet1, then isolet2, and so on. The test set, isolet5, is a separate file.\r\n\r\nSource: [UCI Machine Learning Repository Isolet](https://archive.ics.uci.edu/ml/datasets/isolet)", "variants": ["ISOLET", "Isolet"], "title": "Spoken Letter Recognition"}
{"id": "RACE", "contents": "The **ReAding Comprehension dataset from Examinations** (**RACE**) dataset is a machine reading comprehension dataset consisting of 27,933 passages and 97,867 questions from English exams, targeting Chinese students aged 12-18. RACE consists of two subsets, RACE-M and RACE-H, from middle school and high school exams, respectively. RACE-M has 28,293 questions and RACE-H has 69,574. Each question is associated with 4 candidate answers, one of which is correct. The data generation process of RACE differs from most machine reading comprehension datasets - instead of generating questions and answers by heuristics or crowd-sourcing, questions in RACE are specifically designed for testing human reading skills, and are created by domain experts.\r\n\r\nSource: [Dynamic Fusion Networks for Machine Reading Comprehension](https://arxiv.org/abs/1711.04964)\r\nImage Source: [Lai et al](https://arxiv.org/pdf/1704.04683v5.pdf)", "variants": ["RACE"], "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}
{"id": "DeeperForensics-1.0", "contents": "**DeeperForensics-1.0** represents the largest face forgery detection dataset by far, with 60,000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. The full dataset includes 48,475 source videos and 11,000 manipulated videos. The source videos are collected on 100 paid and consented actors from 26 countries, and the manipulated videos are generated by a newly proposed many-to-many end-to-end face swapping method, DF-VAE. 7 types of real-world perturbations at 5 intensity levels are employed to ensure a larger scale and higher diversity.\nImage Source: [https://github.com/EndlessSora/DeeperForensics-1.0](https://github.com/EndlessSora/DeeperForensics-1.0)", "variants": ["DeeperForensics-1.0"], "title": "DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection"}
{"id": "Stanford Background", "contents": "The **Stanford Background** dataset contains 715 RGB images and the corresponding label images. Images are approximately 240×320 pixels in size and pixels are classified into eight different categories\r\n\r\nSource: [Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation](https://arxiv.org/abs/1605.01368)\r\nImage Source: [http://dags.stanford.edu/projects/scenedataset.html](http://dags.stanford.edu/projects/scenedataset.html)", "variants": ["Stanford Background"], "title": "Decomposing a scene into geometric and semantically consistent regions"}
{"id": "BUCC", "contents": "The **BUCC** mining task is a shared task on parallel sentence extraction from two monolingual corpora with a subset of them assumed to be parallel, and that has been available since 2016. For each language pair, the shared task provides a monolingual corpus for each language and a gold mapping list containing true translation pairs. These pairs are the ground truth. The task is to construct a list of translation pairs from the monolingual corpora. The constructed list is compared to the ground truth, and evaluated in terms of the F1 measure.\r\n\r\nSource: [Language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852)\r\nImage Source: [https://comparable.limsi.fr/bucc2017/](https://comparable.limsi.fr/bucc2017/)", "variants": ["BUCC Chinese-to-English", "BUCC French-to-English", "BUCC German-to-English", "BUCC Russian-to-English", "BUCC"], "title": "Overview of the Second BUCC Shared Task: Spotting Parallel Sentences in Comparable Corpora"}
{"id": "JNC", "contents": "The JNC data provides common supervision data for headline generation.\r\n\r\nSource: [A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation](/paper/a-large-scale-multi-length-headline-corpus)", "variants": ["JNC"], "title": "A Large-Scale Multi-Length Headline Corpus for Improving Length-Constrained Headline Generation Model Evaluation"}
{"id": "CMRC 2018", "contents": "**CMRC 2018** is a dataset for **Chinese Machine Reading Comprehension**. Specifically, it is a span-extraction reading comprehension dataset that is similar to SQuAD.\r\n\r\nSource: [http://ymcui.com/cmrc2018/](http://ymcui.com/cmrc2018/)", "variants": ["CMRC 2018 (Simplified Chinese) Dev", "CMRC 2018 (Simplified Chinese) Challenge", "CMRC 2018"], "title": "A Span-Extraction Dataset for Chinese Machine Reading Comprehension"}
{"id": "IMDb Movie Reviews", "contents": "The **IMDb Movie Reviews** dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.\r\n\r\nSource: [http://nlpprogress.com/english/sentiment_analysis.html](http://nlpprogress.com/english/sentiment_analysis.html)\r\nImage Source: [Maas et al](https://www.aclweb.org/anthology/P11-1015/)", "variants": ["IMDb", "IMDb Movie Reviews", "User and product information"], "title": "From Few to many: Illumination cone models for face recognition under variable lighting and pose"}
{"id": "Completion3D", "contents": "The Completion3D benchmark is a dataset for evaluating state-of-the-art 3D Object Point Cloud Completion methods. Ggiven a partial 3D object point cloud the goal is to infer a complete 3D point cloud for the object.\r\n\r\nSource: [TopNet: Structural Point Cloud Decoder](/paper/topnet-structural-point-cloud-decoder)", "variants": ["Completion3D"], "title": "TopNet: Structural Point Cloud Decoder"}
{"id": "CocoDoom", "contents": "CocoDoom is a collection of pre-recorded data extracted from Doom gaming sessions along with annotations in the MS Coco format.\r\n\r\nSource: [ResearchDoom and CocoDoom: Learning Computer Vision with Games](https://arxiv.org/abs/1610.02431)", "variants": ["CocoDoom"], "title": "ResearchDoom and CocoDoom: Learning Computer Vision with Games"}
{"id": "MCAD", "contents": "Designed to evaluate the open view classification problem under the surveillance environment. In total, MCAD contains 14,298 action samples from 18 action categories, which are performed by 20 subjects and independently recorded with 5 cameras.\r\n\r\nSource: [Multi-Camera Action Dataset for Cross-Camera Action Recognition Benchmarking](/paper/multi-camera-action-dataset-for-cross-camera)", "variants": ["MCAD"], "title": "Multi-Camera Action Dataset for Cross-Camera Action Recognition Benchmarking"}
{"id": "SMS-WSJ", "contents": "Spatialized Multi-Speaker Wall Street Journal (SMS-WSJ) consists of artificially mixed speech taken from the WSJ database, but unlike earlier databases this one considers all WSJ0+1 utterances and takes care of strictly separating the speaker sets present in the training, validation and test sets. \r\n\r\nSource: [SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition](/paper/sms-wsj-database-performance-measures-and)", "variants": ["SMS-WSJ"], "title": "SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition"}
{"id": "SYSU-30k", "contents": "**SYSU-30k** contains 30k categories of persons, which is about 20 times larger than CUHK03 (1.3k categories) and Market1501 (1.5k categories), and 30 times larger than ImageNet (1k categories). SYSU-30k contains 29,606,918 images. Moreover, SYSU-30k provides not only a large platform for the weakly supervised ReID problem but also a more challenging test set that is consistent with the realistic setting for standard evaluation.\n\nSource: [https://github.com/wanggrun/SYSU-30k](https://github.com/wanggrun/SYSU-30k)\nImage Source: [https://github.com/wanggrun/SYSU-30k](https://github.com/wanggrun/SYSU-30k)", "variants": ["SYSU-30k"], "title": "Weakly Supervised Person Re-ID: Differentiable Graphical Learning and A New Benchmark"}
{"id": "SOBA", "contents": "A new dataset called SOBA, named after Shadow-OBject Association, with 3,623 pairs of shadow and object instances in 1,000 photos, each with individual labeled masks.\r\n\r\nSource: [Instance Shadow Detection](/paper/instance-shadow-detection)", "variants": ["SOBA"], "title": "Instance Shadow Detection"}
{"id": "General-100", "contents": "The **General-100** dataset is a dataset for image super-resolution. It contains 100 bmp format images with no compression) The size of the 100 images ranges from 710 x 704 (large) to 131 x 112 (small).", "variants": ["General-100"], "title": "Accelerating the Super-Resolution Convolutional Neural Network"}
{"id": "ReVerb Challenge", "contents": "The REVERB (**REverberant Voice Enhancement and Recognition Benchmark**) challenge is a benchmark for evaluation of automatic speech recognition techniques. The challenge assumes the scenario of capturing utterances spoken by a single stationary distant-talking speaker with 1-channe, 2-channel or 8-channel microphone-arrays in reverberant meeting rooms. It features both real recordings and simulated data.\r\n\r\nThe challenge constis of speech enhancement and automatic speech recognition tasks in reverberant environments. The speech enhancement challenge task consists of enhancing noisy reverberant speech with single-/multi-channel speech enhancement techniques, and evaluating the enhanced data in terms of objective and subjective evaluation metrics. The automatic speech recognition challenge task consists of improving the recognition accuracy of the same reverberant speech. The background noise is mostly stationary and the signal-to-noise ratio is modest.\r\n\r\nSource: [https://reverb2014.dereverberation.com/index.html](https://reverb2014.dereverberation.com/index.html)", "variants": ["Reverb", "ReVerb Challenge"], "title": "MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling"}
{"id": "DNS Challenge", "contents": "The DNS Challenge at INTERSPEECH 2020 intended to promote collaborative research in single-channel Speech Enhancement aimed to maximize the perceptual quality and intelligibility of the enhanced speech. The challenge evaluated the speech quality using the online subjective evaluation framework ITU-T P.808. The challenge provides large datasets for training noise suppressors.\r\n\r\nSource: [Deep Noise Suppression Challenge – INTERSPEECH 2020](https://www.microsoft.com/en-us/research/academic-program/deep-noise-suppression-challenge-interspeech-2020/)", "variants": ["DNS Challenge", "ICASSP 2021 Deep Noise Suppression Challenge", "Interspeech 2021 Deep Noise Suppression Challenge"], "title": "The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Speech Quality and Testing Framework"}
{"id": "Twitter News URL Corpus", "contents": "Twitter News URL Corpus is a human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification.\r\n\r\nSource: [A Continuously Growing Dataset of Sentential Paraphrases](/paper/a-continuously-growing-dataset-of-sentential)\r\nImage Source: [https://arxiv.org/pdf/1708.00391v1.pdf](https://arxiv.org/pdf/1708.00391v1.pdf)", "variants": ["Twitter News URL Corpus"], "title": "A Continuously Growing Dataset of Sentential Paraphrases"}
{"id": "CLEVR", "contents": "**CLEVR** (**Compositional Language and Elementary Visual Reasoning**) is a synthetic Visual Question Answering dataset. It contains images of 3D-rendered objects; each image comes with a number of highly compositional questions that fall into different categories. Those categories fall into 5 classes of tasks: Exist, Count, Compare Integer, Query Attribute and Compare Attribute. The CLEVR dataset consists of: a training set of 70k images and 700k questions, a validation set of 15k images and 150k questions, A test set of 15k images and 150k questions about objects, answers, scene graphs and functional programs for all train and validation images and questions. Each object present in the scene, aside of position, is characterized by a set of four attributes: 2 sizes: large, small, 3 shapes: square, cylinder, sphere, 2 material types: rubber, metal, 8 color types: gray, blue, brown, yellow, red, green, purple, cyan, resulting in 96 unique combinations.\r\n\r\nSource: [On transfer learning using a MAC model variant](https://arxiv.org/abs/1811.06529)\r\nImage Source: [Johnson et al](https://arxiv.org/pdf/1612.06890v1.pdf)", "variants": ["CLEVR", "CLEVR-Dialog"], "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"}
{"id": "ANLI", "contents": "The Adversarial Natural Language Inference (**ANLI**, Nie et al.) is a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. Particular, the data is selected to be difficult to the state-of-the-art models, including BERT and RoBERTa.\r\n\r\nSource: [The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/abs/2002.07972)\r\nImage Source: [https://arxiv.org/pdf/1910.14599.pdf](https://arxiv.org/pdf/1910.14599.pdf)", "variants": ["ANLI test", "ANLI"], "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}
{"id": "HKU-IS", "contents": "**HKU-IS** is a visual saliency prediction dataset which contains 4447 challenging images, most of which have either low contrast or multiple salient objects.\r\n\r\nSource: [Deep Contrast Learning for Salient Object Detection](https://arxiv.org/abs/1603.01976)\r\nImage Source: [https://sites.google.com/site/ligb86/mdfsaliency/](https://sites.google.com/site/ligb86/mdfsaliency/)", "variants": ["HKU-IS"], "title": "Visual saliency based on multiscale deep features"}
{"id": "CAL10K", "contents": "The **CAL10K** dataset (introduced as Swat10k) contains 10,870 songs that are weakly-labelled using a tag vocabulary of 475 acoustic tags and 153 genre tags. The tags have all been harvested from [Pandora’s](https://www.pandora.com/) website and result from song annotations performed by expert musicologists involved with the Music Genome Project.\n\nSource: [Exploring automatic music annotation with “acoustically-objectiv” tags](http://modelai.gettysburg.edu/2012/music/docs/Tingle_Autotag_MIR10.pdf)", "variants": ["CAL10K"], "title": "Towards time-varying music auto-tagging based on CAL500 expansion"}
{"id": "FDF", "contents": "A diverse dataset of human faces, including unconventional poses, occluded faces, and a vast variability in backgrounds. \r\n\r\nSource: [DeepPrivacy: A Generative Adversarial Network for Face Anonymization](/paper/deepprivacy-a-generative-adversarial-network)", "variants": ["FDF"], "title": "DeepPrivacy: A Generative Adversarial Network for Face Anonymization"}
{"id": "Global Voices", "contents": "Global Voices is a multilingual dataset for evaluating cross-lingual summarization methods. It is extracted from social-network descriptions of Global Voices news articles to cheaply collect evaluation data for into-English and from-English summarization in 15 languages. \r\n\r\nSource: [Global Voices: Crossing Borders in Automatic News Summarization](/paper/global-voices-crossing-borders-in-automatic)", "variants": ["Global Voices"], "title": "Global Voices: Crossing Borders in Automatic News Summarization"}
{"id": "ScisummNet", "contents": "Large-scale manually-annotated corpus for 1,000 scientific papers (on computational linguistics) for automatic summarization. Summaries for each paper are constructed from the papers that cite that paper and from that paper's abstract.\r\nSource: [ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks](https://arxiv.org/pdf/1909.01716v3.pdf)", "variants": ["ScisummNet"], "title": "ScisummNet: A Large Annotated Dataset and Content-Impact Models for Scientific Paper Summarization with Citation Networks"}
{"id": "PARANMT-50M", "contents": "PARANMT-50M is a dataset for training paraphrastic sentence embeddings. It consists of more than 50 million English-English sentential paraphrase pairs. \r\n\r\nSource: [ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations](https://arxiv.org/pdf/1711.05732v2.pdf)", "variants": ["PARANMT-50M"], "title": "Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations"}
{"id": "ActivityNet Thumbnails", "contents": "Consists of 10,000+ video-sentence pairs with each accompanied by an annotated sentence specified video thumbnail. \r\n\r\nSource: [Sentence Specified Dynamic Video Thumbnail Generation](/paper/sentence-specified-dynamic-video-thumbnail)", "variants": ["ActivityNet Thumbnails"], "title": "Sentence Specified Dynamic Video Thumbnail Generation"}
{"id": "ECHR", "contents": "ECHR is an English legal judgment prediction dataset of cases from the European Court of Human Rights (ECHR). The dataset contains ~11.5k cases, including the raw text.\r\n\r\nFor each case, the dataset provides a list of facts extracted using regular expressions from the case description. Each case is also mapped to articles of the Convention that were violated (if any). An importance score is also assigned by ECHR.\r\n\r\nSource: [Neural Legal Judgment Prediction in English](https://arxiv.org/abs/1906.02059)", "variants": ["ECHR"], "title": "Neural Legal Judgment Prediction in English"}
{"id": "TextComplexityDE", "contents": "TextComplexityDE is a dataset consisting of 1000 sentences in German language taken from 23 Wikipedia articles in 3 different article-genres to be used for developing text-complexity predictor models and automatic text simplification in German language. The dataset includes subjective assessment of different text-complexity aspects provided by German learners in level A and B. In addition, it contains manual simplification of 250 of those sentences provided by native speakers and subjective assessment of the simplified sentences by participants from the target group. The subjective ratings were collected using both laboratory studies and crowdsourcing approach.\r\n\r\nSource: [Subjective Assessment of Text Complexity: A Dataset for German Language](/paper/subjective-assessment-of-text-complexity-a)", "variants": ["TextComplexityDE"], "title": "Subjective Assessment of Text Complexity: A Dataset for German Language"}
{"id": "VQA 360°", "contents": "VQA 360° is a dataset for visual question answering on 360° images containing around 17,000 real-world image-question-answer triplets for a variety of question types. \r\n\r\nSource: [Visual Question Answering on 360° Images](http://aliensunmin.github.io/project/360-VQA/)", "variants": ["VQA 360°"], "title": "Visual Question Answering on 360{\\deg} Images"}
{"id": "AFW", "contents": "**AFW** (**Annotated Faces in the Wild**) is a face detection dataset that contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box.\r\n\r\nSource: [Pose-Invariant Face Alignment with a Single CNN](https://arxiv.org/abs/1707.06286)", "variants": ["AFW", "Annotated Faces in the Wild"], "title": "Face detection, pose estimation, and landmark localization in the wild"}
{"id": "Fraunhofer IPA Bin-Picking", "contents": "The **Fraunhofer IPA Bin-Picking** dataset is a large-scale dataset comprising both simulated and real-world scenes for various objects (potentially having symmetries) and is fully annotated with 6D poses. A pyhsics simulation is used to create scenes of many parts in bulk by dropping objects in a random position and orientation above a bin. Additionally, this dataset extends the Siléane dataset by providing more samples. This allows to e.g. train deep neural networks and benchmark the performance on the public Siléane dataset\n\nSource: [https://www.bin-picking.ai/en/dataset.html](https://www.bin-picking.ai/en/dataset.html)\nImage Source: [https://arxiv.org/abs/1912.12125](https://arxiv.org/abs/1912.12125)", "variants": ["Fraunhofer IPA Bin-Picking"], "title": "Large-scale 6D Object Pose Estimation Dataset for Industrial Bin-Picking"}
{"id": "Deep Fashion3D", "contents": "A novel benchmark and dataset for the evaluation of image-based garment reconstruction systems. Deep Fashion3D contains 2078 models reconstructed from real garments, which covers 10 different categories and 563 garment instances. It provides rich annotations including 3D feature lines, 3D body pose and the corresponded multi-view real images. In addition, each garment is randomly posed to enhance the variety of real clothing deformations.\r\n\r\nSource: [Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images](/paper/deep-fashion3d-a-dataset-and-benchmark-for-3d)", "variants": ["Deep Fashion3D"], "title": "Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images"}
{"id": "Visual Madlibs", "contents": "Visual Madlibs is a dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context.\r\n\r\nSource: [Visual Madlibs: Fill in the blank Image Generation and Question Answering](/paper/visual-madlibs-fill-in-the-blank-image)\r\nImage Source: [Yu et al](https://arxiv.org/pdf/1506.00278v1.pdf)", "variants": ["Visual Madlibs"], "title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering"}
{"id": "CoNLL 2002", "contents": "The shared task of CoNLL-2002 concerns language-independent named entity recognition. The types of named entities include: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The participants of the shared task were offered training and test data for at least two languages. Information sources other than the training data might have been used in this shared task.\r\n\r\nSource: [CoNLL 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\r\nImage Source: [https://www.aclweb.org/anthology/W02-2024.pdf](https://www.aclweb.org/anthology/W02-2024.pdf)", "variants": ["CoNLL 2002 (Spanish)", "CoNLL 2002 (Dutch)", "CoNLL 2002"], "title": "Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition"}
{"id": "MPI-INF-3DHP", "contents": "**MPI-INF-3DHP** is a 3D human body pose estimation dataset consisting of both constrained indoor and complex outdoor scenes. It records 8 actors performing 8 activities from 14 camera views. It consists on >1.3M frames captured from the 14 cameras.\r\n\r\nSource: [Anatomy-aware 3D Human Pose Estimation in Videos](https://arxiv.org/abs/2002.10322)\r\nImage Source: [https://arxiv.org/abs/1611.09813](https://arxiv.org/abs/1611.09813)", "variants": ["MPI-INF-3DHP"], "title": "Monocular 3D Human Pose Estimation in the Wild Using Improved CNN Supervision"}
{"id": "Food.com Recipes and Interactions", "contents": "Food.com Recipes and Interactions consists of 270K recipes and 1.4M user-recipe interactions (reviews) scraped from Food.com, covering a period of 18 years (January 2000 to December 2018).\r\n\r\nSource: [Generating Personalized Recipes from Historical User Preferences](https://arxiv.org/pdf/1909.00105.pdf)", "variants": ["Food.com Recipes and Interactions"], "title": "Generating Personalized Recipes from Historical User Preferences"}
{"id": "FSS-1000", "contents": "**FSS-1000** is a 1000 class dataset for few-shot segmentation. The dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc.\n\nSource: [https://github.com/HKUSTCV/FSS-1000](https://github.com/HKUSTCV/FSS-1000)\nImage Source: [https://github.com/HKUSTCV/FSS-1000](https://github.com/HKUSTCV/FSS-1000)", "variants": ["FSS-1000"], "title": "FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation"}
{"id": "DialogueFairness", "contents": "The Dialogue Fairness dataset is used to evaluate and understand fairness in dialogue models, focusing on gender and racial biases.\n\nSource: [https://github.com/zgahhblhc/DialogueFairness](https://github.com/zgahhblhc/DialogueFairness)", "variants": ["DialogueFairness"], "title": "Does Gender Matter? Towards Fairness in Dialogue Systems"}
{"id": "CMU Panoptic Studio Dataset", "contents": "CMU Panoptic Studio is a large scale dataset capturing more than 3 hours of group interaction scenes using 521 heterogeneous sensors. 65 sequences (5.5 hours) and 1.5 millions of 3D skeletons are available.\r\n\r\nSource: [Panoptic Studio: A Massively Multiview System for Social Interaction Capture](/paper/panoptic-studio-a-massively-multiview-system)", "variants": ["CMU Panoptic Studio Dataset"], "title": "Panoptic Studio: A Massively Multiview System for Social Interaction Capture"}
{"id": "DPED", "contents": "A large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera.\r\n\r\nSource: [DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks](https://arxiv.org/pdf/1704.02470v2.pdf)", "variants": ["DPED"], "title": "DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks"}
{"id": "Image Caption Quality Dataset", "contents": "Image Caption Quality Dataset is a dataset of crowdsourced ratings for machine-generated image captions. It contains more than 600k ratings of image-caption pairs.\r\n\r\nSource: [https://arxiv.org/abs/1909.03396](https://arxiv.org/abs/1909.03396)", "variants": ["Image Caption Quality Dataset"], "title": "Quality Estimation for Image Captions Based on Large-scale Human Evaluations"}
{"id": "SemArt", "contents": "SemArt is a multi-modal dataset for semantic art understanding. SemArt is a collection of fine-art painting images in which each image is associated to a number of attributes and a textual artistic comment, such as those that appear in art catalogues or museum collections. It contains 21,384 samples that provides artistic comments along with fine-art paintings and their attributes for studying semantic art understanding.\r\n\r\nSource: [How to Read Paintings: Semantic Art Understanding with Multi-Modal Retrieval](/paper/how-to-read-paintings-semantic-art)", "variants": ["SemArt"], "title": "How to Read Paintings: Semantic Art Understanding with Multi-Modal Retrieval"}
{"id": "PERSONA-CHAT", "contents": "The **PERSONA-CHAT** dataset contains multi-turn dialogues conditioned on personas. The dataset consists of 8939 complete dialogues for training, 1000 for validation, and 968 for testing. Each dialogue was performed between two crowd-source workers assuming artificial personas (described by 3 to 5 profile sentences, such as “I like to ski”, “I am an artist”, “I eat sardines for breakfast daily”). There are 955 possible personas for training, 100 for validation, and 100 for testing. Additionally, a version of revised persona descriptions are also provided by rephrasing, generalizing, or specializing the original ones.\r\n\r\nSource: [Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots](https://arxiv.org/abs/1908.05859)\r\nImage Source: [https://arxiv.org/pdf/1801.07243.pdf](https://arxiv.org/pdf/1801.07243.pdf)", "variants": ["Persona-Chat", "PERSONA-CHAT"], "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?"}
{"id": "IndoSum", "contents": "The **IndoSum** dataset is a benchmark dataset for Indonesian text summarization. The dataset consists of news articles and manually constructed summaries.\n\nSource: [https://github.com/kata-ai/indosum](https://github.com/kata-ai/indosum)", "variants": ["IndoSum"], "title": "Indosum: A New Benchmark Dataset for Indonesian Text Summarization"}
{"id": "DAVANet", "contents": "A large-scale multi-scene dataset for stereo deblurring, containing 20,637 blurry-sharp stereo image pairs from 135 diverse sequences and their corresponding bidirectional disparities.\r\n\r\nSource: [DAVANet: Stereo Deblurring with View Aggregation](/paper/davanet-stereo-deblurring-with-view)", "variants": ["DAVANet"], "title": "DAVANet: Stereo Deblurring With View Aggregation"}
{"id": "MURA", "contents": "A large dataset of musculoskeletal radiographs containing 40,561 images from 14,863 studies, where each study is manually labeled by radiologists as either normal or abnormal. \r\n\r\nSource: [MURA: Large Dataset for Abnormality Detection in Musculoskeletal Radiographs](/paper/mura-large-dataset-for-abnormality-detection)", "variants": ["MURA"], "title": "MURA Dataset: Towards Radiologist-Level Abnormality Detection in Musculoskeletal Radiographs"}
{"id": "ScribbleSup", "contents": "The **PASCAL-Scribble Dataset** is an extension of the PASCAL dataset with scribble annotations for semantic segmentation. The annotations follow two different protocols. In the first protocol, the PASCAL VOC 2012 set is annotated, with 20 object categories (aeroplane, bicycle, ...) and one background category. There are 12,031 images annotated, including 10,582 images in the training set and 1,449 images in the validation set.\r\nIn the second protocol, the 59 object/stuff categories and one background category involved in the PASCAL-CONTEXT dataset are used. Besides the 20 object categories in the first protocol, there are 39 extra categories (snow, tree, ...) included. This protocol is followed to annotate the PASCAL-CONTEXT dataset. 4,998 images in the training set have been annotated.\r\n\r\nSource: [https://jifengdai.org/downloads/scribble_sup/](https://jifengdai.org/downloads/scribble_sup/)\r\nImage Source: [https://jifengdai.org/downloads/scribble_sup/](https://jifengdai.org/downloads/scribble_sup/)", "variants": ["ScribbleSup"], "title": "ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation"}
{"id": "OCID", "contents": "Developing robot perception systems for handling objects in the real-world requires computer vision algorithms to be carefully scrutinized with respect to the expected operating domain. This demands large quantities of ground truth data to rigorously evaluate the performance of algorithms.\r\n\r\nThe Object Cluttered Indoor Dataset is an RGBD-dataset containing point-wise labeled point-clouds for each object. The data was captured using two ASUS-PRO Xtion cameras that are positioned at different heights. It captures diverse settings of objects, background, context, sensor to scene distance, viewpoint angle and lighting conditions. The main purpose of OCID is to allow systematic comparison of existing object segmentation methods in scenes with increasing amount of clutter. In addition OCID does also provide ground-truth data for other vision tasks like object-classification and recognition.\r\n\r\nSource: [OCID](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/object-clutter-indoor-dataset/)", "variants": ["OCID"], "title": "EasyLabel: A Semi-Automatic Pixel-wise Object Annotation Tool for Creating Robotic RGB-D Datasets"}
{"id": "French Wikipedia", "contents": "**French Wikipedia** is a dataset used for pretraining the CamemBERT French language model. It uses the official 2019 French Wikipedia dumps", "variants": ["French Wikipedia"], "title": "CamemBERT: a Tasty French Language Model"}
{"id": "Drive&Act", "contents": "The Drive&Act dataset is a state of the art multi modal benchmark for driver behavior recognition. The dataset includes 3D skeletons in addition to frame-wise hierarchical labels of 9.6 Million frames captured by 6 different views and 3 modalities (RGB, IR and depth).\r\n\r\nIt offers following key features:\r\n\r\n* 12h of video data in 29 long sequences\r\n* Calibrated multi view camera system with 5 views\r\n* Multi modal videos: NIR, Depth and Color data\r\n* Markerless motion capture: 3D Body Pose and Head Pose\r\n* Model of the static interior of the car\r\n* 83 manually annotated hierarchical activity labels:\r\n    * Level 1: Long running tasks (12)\r\n    * Level 2: Semantic actions (34)\r\n    * Level 3: Object Interaction tripplets [action|object|location] (6|17|14)\r\n\r\nSource: [Drive&Act: A Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles](/paper/driveact-a-multi-modal-dataset-for-fine)", "variants": ["Drive&Act"], "title": "Drive&Act: A Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles"}
{"id": "Dreaddit", "contents": "Consists of 190K posts from five different categories of Reddit communities.\r\n\r\nSource: [Dreaddit: A Reddit Dataset for Stress Analysis in Social Media](/paper/dreaddit-a-reddit-dataset-for-stress-analysis-1)", "variants": ["Dreaddit"], "title": "Dreaddit: A Reddit Dataset for Stress Analysis in Social Media"}
{"id": "YFCC100M Fine-Grained Geolocation", "contents": "The **YFCC100M Fine-Grained Geolocation** dataset is a subset of 100 a set of 36,146 YFCC100M images that had Flickr tags that could be identified as corresponding to one of the labels in the iNaturalist 2017 dataset. The 36,146 images that were selected so have the following characteristics:\nthe image must have geolocation available,\nthe image must have at most one iNaturalist label,\nat most ten examples were retained for each label.\n\nSource: [https://github.com/visipedia/fg_geo](https://github.com/visipedia/fg_geo)\nImage Source: [https://github.com/visipedia/fg_geo](https://github.com/visipedia/fg_geo)", "variants": ["YFCC100M Fine-Grained Geolocation"], "title": "Geo-Aware Networks for Fine-Grained Recognition"}
{"id": "Moving Symbols", "contents": "A parameterized synthetic dataset called Moving Symbols to support the objective study of video prediction networks. \r\n\r\nSource: [A Dataset To Evaluate The Representations Learned By Video Prediction Models](/paper/a-dataset-to-evaluate-the-representations)", "variants": ["Moving Symbols"], "title": "A Dataset To Evaluate The Representations Learned By Video Prediction Models"}
{"id": "CLEVR-Ref+", "contents": "CLEVR-Ref+ is a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators. \r\n\r\nSource: [CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions](https://arxiv.org/pdf/1901.00850v2.pdf)", "variants": ["CLEVR-Ref+"], "title": "CLEVR-Ref+: Diagnosing Visual Reasoning With Referring Expressions"}
{"id": "CCAligned", "contents": "**CCAligned** consists of parallel or comparable web-document pairs in 137 languages aligned with English. These web-document pairs were constructed by performing language identification on raw web-documents, and ensuring corresponding language codes were corresponding in the URLs of web documents. This pattern matching approach yielded more than 100 million aligned documents paired with English. Recognizing that each English document was often aligned to multiple documents in different target language, it is possible to join on English documents to obtain aligned documents that directly pair two non-English documents (e.g., Arabic-French).\r\n\r\nSource: [CCAligned](http://www.statmt.org/cc-aligned/)", "variants": ["CCAligned"], "title": "A Massive Collection of Cross-Lingual Web-Document Pairs"}
{"id": "Freiburg Spatial Relations", "contents": "The **Freiburg Spatial Relations** dataset features 546 scenes each containing two out of 25 household objects. The depicted spatial relations can roughly be described as on top, on top on the corner, inside, inside and inclined, next to, and inclined. The dataset contains the 25 object models as textured .obj and .dae files, a low resolution .dae version for visualization in rviz, a scene description file containing the translation and rotation of the objects for each scene, a file with labels for each scene, the 15 splits used for cross validation, and a bash script to convert the models to pointclouds.\n\nSource: [http://spatialrelations.cs.uni-freiburg.de/](http://spatialrelations.cs.uni-freiburg.de/)\nImage Source: [http://spatialrelations.cs.uni-freiburg.de/](http://spatialrelations.cs.uni-freiburg.de/)", "variants": ["Freiburg Spatial Relations"], "title": "Metric learning for generalizing spatial relations to new objects"}
{"id": "NLI-PT", "contents": "The first Portuguese dataset compiled for Native Language Identification (NLI), the task of identifying an author's first language based on their second language writing. The dataset includes 1,868 student essays written by learners of European Portuguese, native speakers of the following L1s: Chinese, English, Spanish, German, Russian, French, Japanese, Italian, Dutch, Tetum, Arabic, Polish, Korean, Romanian, and Swedish. NLI-PT includes the original student text and four different types of annotation: POS, fine-grained POS, constituency parses, and dependency parses. NLI-PT can be used not only in NLI but also in research on several topics in the field of Second Language Acquisition and educational NLP. \r\n\r\nSource: [A Portuguese Native Language Identification Dataset](/paper/a-portuguese-native-language-identification)", "variants": ["NLI-PT"], "title": "A Portuguese Native Language Identification Dataset"}
{"id": "ECSSD", "contents": "The **Extended Complex Scene Saliency Dataset** (**ECSSD**) is comprised of complex scenes, presenting textures and structures common to real-world images. ECSSD contains 1,000 intricate images and respective ground-truth saliency maps, created as an average of the labeling of five human participants.\r\n\r\nSource: [SAD: Saliency-based Defenses Against Adversarial Examples](https://arxiv.org/abs/2003.04820)", "variants": ["ECSSD"], "title": "Hierarchical Saliency Detection"}
{"id": "MARS", "contents": "**MARS** (**Motion Analysis and Re-identification Set**) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\r\n\r\nSource: [Multi-Target Tracking in Multiple Non-Overlapping Cameras using Constrained Dominant Sets](https://arxiv.org/abs/1706.06196)", "variants": ["MARS"], "title": "MARS: A Video Benchmark for Large-Scale Person Re-Identification"}
{"id": "MuCo-3DHP", "contents": "MuCo-3DHP is a large scale training data set showing real images of sophisticated multi-person interactions and occlusions. \r\n\r\nSource: [Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB](https://arxiv.org/pdf/1712.03453v3.pdf)", "variants": ["MuCo-3DHP"], "title": "Single-Shot Multi-person 3D Pose Estimation from Monocular RGB"}
{"id": "Libri-Light", "contents": "Libri-Light is a collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. It contains over 60K hours of audio.\r\n\r\nSource: [Libri-Light: A Benchmark for ASR with Limited or No Supervision](https://arxiv.org/pdf/1912.07875v1.pdf)", "variants": ["Libri-Light", "Libri-Light 60k", "Libri-Light 60k, test-clean", "Libri-Light test-clean", "Libri-Light test-other"], "title": "Libri-Light: A Benchmark for ASR with Limited or No Supervision"}
{"id": "Youtubean", "contents": "Youtbean is a dataset created from closed captions of YouTube product review videos. It can be used for aspect extraction and sentiment classification.\n\nSource: [https://arxiv.org/pdf/1708.02420.pdf](https://arxiv.org/pdf/1708.02420.pdf)", "variants": ["Youtubean"], "title": "Mining fine-grained opinions on closed captions of YouTube videos with an attention-RNN"}
{"id": "TVC", "contents": "TV show Caption is a large-scale multimodal captioning dataset, containing 261,490 caption descriptions paired with 108,965 short video moments. **TVC** is unique as its captions may also describe dialogues/subtitles while the captions in the other datasets are only describing the visual content.\n\nSource: [https://tvr.cs.unc.edu/tvc.html](https://tvr.cs.unc.edu/tvc.html)\nImage Source: [https://github.com/jayleicn/TVCaption](https://github.com/jayleicn/TVCaption)", "variants": ["TVC"], "title": "TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval"}
{"id": "HOList", "contents": "The official **HOList** benchmark for automated theorem proving consists of all theorem statements in the core, complex, and flyspeck corpora. The goal of the benchmark is to prove as many theorems as possible in the HOList environment in the order they appear in the database. That is, only theorems that occur before the current theorem are supposed to be used as premises (lemmata) in its proof.\r\n\r\nSource: [HoList](https://sites.google.com/view/holist/home)\r\nImage Source: [https://sites.google.com/view/holist/home](https://sites.google.com/view/holist/home)", "variants": ["HOList benchmark", "HOList"], "title": "HOList: An Environment for Machine Learning of Higher-Order Theorem Proving"}
{"id": "Tasty Videos", "contents": "A collection of 2511 recipes for zero-shot learning, recognition and anticipation.\r\n\r\nSource: [Zero-Shot Anticipation for Instructional Activities](/paper/zero-shot-anticipation-for-instructional)", "variants": ["Tasty Videos"], "title": "Zero-Shot Anticipation for Instructional Activities"}
{"id": "RuStance", "contents": "Includes Russian tweets and news comments from multiple sources, covering multiple stories, as well as text classification approaches to stance detection as benchmarks over this data in this language.\r\n\r\nSource: [Stance Prediction for Russian: Data and Analysis](/paper/stance-prediction-for-russian-data-and)", "variants": ["RuStance"], "title": "Stance Prediction for Russian: Data and Analysis"}
{"id": "PART-OF", "contents": "The **PART-OF** dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are parts of the human body. The dataset has 16,894 nodes with 19,436 edges between them.\n\nSource: [https://arxiv.org/pdf/1906.05939.pdf](https://arxiv.org/pdf/1906.05939.pdf)", "variants": ["PART-OF"], "title": "Embedding Biomedical Ontologies by Jointly Encoding Network Structure and Textual Node Descriptors"}
{"id": "WSC", "contents": "The **Winograd Schema Challenge** was introduced both as an alternative to the Turing Test and as a test of a system’s ability to do commonsense reasoning. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences, that appears to require commonsense knowledge to be resolved correctly. The examples were designed to be easily solvable by humans but difficult for machines, in principle requiring a deep understanding of the content of the text and the situation it describes.\r\n\r\nThe original Winograd Schema Challenge dataset consisted of 100 Winograd schemas constructed manually by AI experts. As of 2020 there are 285 examples available; however, the last 12 examples were only added recently. To ensure consistency with earlier models, several authors often prefer to report the performance on the first 273 examples only. These datasets are usually referred to as **WSC**285 and WSC273, respectively.\r\n\r\nSource: [https://arxiv.org/pdf/2004.13831.pdf](https://arxiv.org/pdf/2004.13831.pdf)\r\nImage Source: [https://arxiv.org/pdf/1907.11983.pdf](https://arxiv.org/pdf/1907.11983.pdf)", "variants": ["Winograd Schema Challenge", "WSC"], "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}
{"id": "Reddit", "contents": "The **Reddit** dataset is a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or “subreddit”, that a post belongs to. 50 large communities have been sampled to build a post-to-post graph, connecting posts if the same user comments on both. In total this dataset contains 232,965 posts with an average degree of 492. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). For features, off-the-shelf 300-dimensional GloVe CommonCrawl word vectors are used.\r\n\r\nSource: [https://arxiv.org/pdf/1706.02216.pdf](https://arxiv.org/pdf/1706.02216.pdf)\r\nImage Source: [https://minimaxir.com/2016/05/reddit-graph/](https://minimaxir.com/2016/05/reddit-graph/)", "variants": ["Reddit", "Reddit TIFU", "REDDIT-B", "Reddit (multi-ref)"], "title": "Inductive Representation Learning on Large Graphs"}
{"id": "PolSF", "contents": "Collects five open polarimetric SAR images, which are images of the San Francisco area. These five images come from different satellites at different times, which has great scientific research value. \r\n\r\nSource: [PolSF: PolSAR image dataset on San Francisco](/paper/polsf-polsar-image-dataset-on-san-francisco)", "variants": ["PolSF"], "title": "PolSF: PolSAR image dataset on San Francisco"}
{"id": "TalkSumm", "contents": "The **TalkSumm** dataset contains 1705 automatically-generated summaries of scientific papers from ACL, NAACL, EMNLP, SIGDIAL (2015-2018), and ICML (2017-2018).\r\n\r\nThe dataset is provided as a list of titles and URLs and the corresponding summaries.\r\n\r\nSource: [GitHub](https://github.com/levguy/talksumm)", "variants": ["TalkSumm"], "title": "TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks"}
{"id": "MuseData", "contents": "**MuseData** is an electronic library of orchestral and piano classical music from CCARH. It consists of around 3MB of 783 files.\r\n\r\nSource: [https://arxiv.org/pdf/1206.6392v1.pdf](https://arxiv.org/pdf/1206.6392v1.pdf)\r\nImage Source: [https://arxiv.org/pdf/1206.6392v1.pdf](https://arxiv.org/pdf/1206.6392v1.pdf)", "variants": ["MuseData"], "title": "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription"}
{"id": "Blackbird", "contents": "The Blackbird unmanned aerial vehicle (UAV) dataset is a large-scale, aggressive indoor flight dataset collected using a custom-built quadrotor platform for use in evaluation of agile perception. The Blackbird dataset contains over 10 hours of flight data from 168 flights over 17 flight trajectories and 5 environments. Each flight includes sensor data from 120Hz stereo and downward-facing photorealistic virtual cameras, 100Hz IMU, motor speed sensors, and 360Hz millimeter-accurate motion capture ground truth. Camera images for each flight were photorealistically rendered using FlightGoggles across a variety of environments to facilitate easy experimentation of high performance perception algorithms. \r\n\r\nSource: [The Blackbird Dataset: A large-scale dataset for UAV perception in aggressive flight](/paper/the-blackbird-dataset-a-large-scale-dataset)", "variants": ["Blackbird"], "title": "The Blackbird Dataset: A large-scale dataset for UAV perception in aggressive flight"}
{"id": "MathQA", "contents": "MathQA significantly enhances the AQuA dataset with fully-specified operational programs. \r\n\r\nSource: [MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms](/paper/mathqa-towards-interpretable-math-word)", "variants": ["MathQA"], "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}
{"id": "CURE-TSR", "contents": "Includes more than two million traffic sign images that are based on real-world and simulator data. \r\n\r\nSource: [CURE-TSR: Challenging Unreal and Real Environments for Traffic Sign Recognition](/paper/cure-tsr-challenging-unreal-and-real)", "variants": ["CURE-TSR"], "title": "CURE-TSR: Challenging Unreal and Real Environments for Traffic Sign Recognition"}
{"id": "INRIA SLFD", "contents": "The INRIA Sprse Light Field Dataset (SLFD) is a dataset for testing depth estimation methods in a light field. SLFD contains 53 scenes with disparity range [-20,20] pixels. The light fields are of spatial resolution 512 x 512 and angular resolution 9 x 9.\n\nSource: [http://clim.inria.fr/Datasets/InriaSynLF/index.html](http://clim.inria.fr/Datasets/InriaSynLF/index.html)\nImage Source: [http://clim.inria.fr/Datasets/InriaSynLF/index.html](http://clim.inria.fr/Datasets/InriaSynLF/index.html)", "variants": ["INRIA SLFD"], "title": "A Framework for Learning Depth From a Flexible Subset of Dense and Sparse Light Field Views"}
{"id": "HPatches", "contents": "The **HPatches** is a recent dataset for local patch descriptor evaluation that consists of 116 sequences of 6 images with known homography. The dataset is split into two parts: viewpoint - 59 sequences with significant viewpoint change and illumination - 57 sequences with significant illumination change, both natural and artificial.\r\n\r\nSource: [RF-Net: An End-to-End Image Matching Network based on Receptive Field](https://arxiv.org/abs/1906.00604)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/publications/2017/Balntas17/balntas17.pdf](https://www.robots.ox.ac.uk/~vgg/publications/2017/Balntas17/balntas17.pdf)", "variants": ["HPatches"], "title": "HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors"}
{"id": "PathTrack", "contents": "PathTrack is a dataset for person tracking which contains more than 15,000 person trajectories in 720 sequences.\r\n\r\nSource: [PathTrack: Fast Trajectory Annotation with Path Supervision](/paper/pathtrack-fast-trajectory-annotation-with)", "variants": ["PathTrack"], "title": "PathTrack: Fast Trajectory Annotation with Path Supervision"}
{"id": "TurkQA", "contents": "TurkQA consists of a selection of sentences from English Wikipedia articles, with questions and answers crowdsourced from workers on Amazon Mechanical Turk.\r\n\r\nSource: [TurkQA](https://groupring.net/malon/turkqa/index.html)", "variants": ["TurkQA"], "title": "Answer Extraction by Recursive Parse Tree Descent"}
{"id": "NewSHead", "contents": "The **NewSHead** dataset contains 369,940 English stories with 932,571 unique URLs, among which there are 359,940 stories for training, 5,000 for validation, and 5,000 for testing, respectively. Each news story contains at least three (and up to five) articles.\r\n\r\nThe dataset is collected from news stories published between May 2018 and May 2019, where a proprietary clustering algorithm iteratively loads articles published in a time window and groups them based on content similarity. Up to five representative articles are picked from the cluster for generating the story headline. Curators from a crowd-sourcing platform are requested to provide a headline of up to 35 characters to describe the major information covered by the story.\r\n\r\nSource: [NewSHead](https://github.com/google-research-datasets/NewSHead)", "variants": ["NewSHead"], "title": "Generating Representative Headlines for News Stories"}
{"id": "SUN RGB-D", "contents": "The SUN RGBD dataset contains 10335 real RGB-D images of room scenes. Each RGB image has a corresponding depth and segmentation map. As many as 700 object categories are labeled. The training and testing sets contain 5285 and 5050 images, respectively.\r\n\r\nSource: [Mix and match networks: multi-domain alignment for unpaired image-to-image translation](https://arxiv.org/abs/1903.04294)\r\nImage Source: [https://rgbd.cs.princeton.edu/](https://rgbd.cs.princeton.edu/)", "variants": ["SUN-RGBD", "SUN-RGBD val", "SUN RGB-D"], "title": "SUN RGB-D: A RGB-D scene understanding benchmark suite"}
{"id": "Discovery Dataset", "contents": "The *Discovery* datasets consists of adjacent sentence pairs (s1,s2) with a discourse marker (y) that occurred at the beginning of s2. They were extracted from the depcc web corpus.\r\n\r\nMarkers prediction can be used in order to train a sentence encoders. Discourse markers can be considered as noisy labels for various semantic tasks, such as entailment (y=therefore), subjectivity analysis (y=personally) or sentiment analysis (y=sadly), similarity (y=similarly), typicality, (y=curiously) ...\r\n\r\nThe specificity of this dataset is the diversity of the markers, since previously used data used only ~10 imbalanced classes. The author of the dataset provide:\r\n\r\n- a list of the 174 discourse markers\r\n- a Base version of the dataset with 1.74 million pairs (10k examples per marker)\r\n- a Big version with 3.4 million pairs\r\n- a Hard version with 1.74 million pairs where the connective couldn't be predicted with a fastText linear model\r\n\r\nSource: [GitHub](https://github.com/synapse-developpement/Discovery)", "variants": ["Discovery Dataset"], "title": "Mining Discourse Markers for Unsupervised Sentence Representation Learning"}
{"id": "STL-10", "contents": "The **STL-10** is an image dataset derived from ImageNet and popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes (such as birds, cats, trucks), among which 5,000 images are partitioned for training while the remaining 8,000 images for testing. All the images are color images with 96×96 pixels in size.\r\n\r\nSource: [Unsupervised Feature Learning with C-SVDDNet](https://arxiv.org/abs/1412.7259)\r\nImage Source: [https://cs.stanford.edu/~acoates/stl10/](https://cs.stanford.edu/~acoates/stl10/)", "variants": ["STL-10", "STL-10, 1000 Labels", "STL-10, 5000 Labels"], "title": "The 2017 DAVIS Challenge on Video Object Segmentation"}
{"id": "WeChat", "contents": "The **WeChat** dataset for fake news detection contains more than 20k news labelled as fake news or not.", "variants": ["WeChat"], "title": "Weak Supervision for Fake News Detection via Reinforcement Learning"}
{"id": "stickerchart", "contents": "The Stickerchat dataset is a large-scale real-world dialog dataset with stickers which contains 340K multi-turn dialog and sticker pairs.\n\nSource: [https://arxiv.org/abs/2003.04679](https://arxiv.org/abs/2003.04679)", "variants": ["stickerchart"], "title": "Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog"}
{"id": "AVE", "contents": "To investigate three temporal localization tasks: supervised and weakly-supervised audio-visual event localization, and cross-modality localization.\r\n\r\nSource: [Audio-Visual Event Localization in Unconstrained Videos](/paper/audio-visual-event-localization-in)", "variants": ["AVE"], "title": "Audio-Visual Event Localization in Unconstrained Videos"}
{"id": "BdSLImset", "contents": "Bangladeshi Sign Language Image Dataset (BdSLImset) is a dataset that contains images of different Bangladeshi sign letters.\r\n\r\nSource: [https://github.com/imruljubair/BdSLImset](https://github.com/imruljubair/BdSLImset)\r\nImage Source: [https://arxiv.org/pdf/1811.12813v1.pdf](https://arxiv.org/pdf/1811.12813v1.pdf)", "variants": ["BdSLImset"], "title": "Real Time Bangladeshi Sign Language Detection using Faster R-CNN"}
{"id": "SRD", "contents": "SRD is a dataset for shadow removal that contains 3088 shadow and shadow-free image pairs.", "variants": ["SRD"], "title": "DeshadowNet: A Multi-context Embedding Deep Network for Shadow Removal"}
{"id": "SEMCAT", "contents": "Contains more than 6500 words semantically grouped under 110 categories.\r\n\r\nSource: [Semantic Structure and Interpretability of Word Embeddings](/paper/semantic-structure-and-interpretability-of)", "variants": ["SEMCAT"], "title": "Semantic Structure and Interpretability of Word Embeddings"}
{"id": "Store dataset", "contents": "The Store Dataset is a dataset for estimating 3D poses of multiple humans in real-time. It is captured inside two kinds of simulated stores with 12 and 28 cameras, respectively.\n\nSource: [https://arxiv.org/abs/2003.03972](https://arxiv.org/abs/2003.03972)", "variants": ["Store dataset"], "title": "Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS"}
{"id": "WMT 2015 News", "contents": "News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, French, Russian) and additional 1500 sentences from each of the 5 languages translated to English. The sentences are taken from newspaper articles for each language pair, except for French, where the test set was drawn from user-generated comments on the news articles (from Guardian and Le Monde). The translation was done by professional translators.\n\nThe training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.\nSome training corpora were identical from WMT 2014 (Europarl, United Nations, French-English 10⁹ corpus, CzEng, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (News Commentary, monolingual news data). Additionally, the Finnish Europarl and Finnish-English Wikipedia Headline corpus were added.\n\nSource: [https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/](https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/)\nImage Source: [httpshttps://www.aclweb.org/anthology/W15-3001.pdf](httpshttps://www.aclweb.org/anthology/W15-3001.pdf)", "variants": ["WMT 2015 News"], "title": "Findings of the 2015 Workshop on Statistical Machine Translation"}
{"id": "RealNews", "contents": "**RealNews** is a large corpus of news articles from Common Crawl. Data is scraped from Common Crawl, limited to the 5000 news domains indexed by Google News. The authors used the Newspaper Python library to extract the body and metadata from each article. News from Common Crawl dumps from December 2016 through March 2019\r\nwere used as training data; articles published in April 2019 from the April 2019 dump were used for evaluation. After deduplication, RealNews is 120 gigabytes without compression.\r\n\r\nImage Source: [https://arxiv.org/pdf/1905.12616v3.pdf](https://arxiv.org/pdf/1905.12616v3.pdf)", "variants": ["RealNews"], "title": "Defending Against Neural Fake News"}
{"id": "PerKey", "contents": "A corpus of 553k news articles from six Persian news websites and agencies with relatively high quality author extracted keyphrases, which is then filtered and cleaned to achieve higher quality keyphrases. \r\n\r\nSource: [PerKey: A Persian News Corpus for Keyphrase Extraction and Generation](/paper/perkey-a-persian-news-corpus-for-keyphrase)", "variants": ["PerKey"], "title": "PerKey: A Persian News Corpus for Keyphrase Extraction and Generation"}
{"id": "GeoWebNews", "contents": "GeoWebNews provides test/train examples and enable fine-grained Geotagging and Toponym Resolution (Geocoding). This dataset is also suitable for prototyping and evaluating machine learning NLP models.\r\n\r\nSource: [A Pragmatic Guide to Geoparsing Evaluation](/paper/a-pragmatic-guide-to-geoparsing-evaluation)\r\nImage Source: [https://arxiv.org/pdf/1810.12368.pdf](https://arxiv.org/pdf/1810.12368.pdf)", "variants": ["GeoWebNews"], "title": "A Pragmatic Guide to Geoparsing Evaluation"}
{"id": "R2R", "contents": "R2R is a dataset for visually-grounded natural language navigation in real buildings. The dataset requires autonomous agents to follow human-generated navigation instructions in previously unseen buildings, as illustrated in the demo above. For training, each instruction is associated with a Matterport3D Simulator trajectory. 22k instructions are available, with an average length of 29 words. There is a test evaluation server for this dataset available at EvalAI.\r\n\r\nSource: [Natural language interaction with robots](https://bringmeaspoon.org/)", "variants": ["Room2Room", "R2R"], "title": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments"}
{"id": "TrMor2018", "contents": "A new high accuracy Turkish morphology dataset. \r\n\r\nSource: [Morphological analysis using a sequence decoder](/paper/morphnet-a-sequence-to-sequence-model-that)", "variants": ["TrMor2018"], "title": "Morphological Analysis Using a Sequence Decoder"}
{"id": "AW-OIE", "contents": "**All Words Open IE** (**AW-OIE**) is an open information extraction dataset derived from [Question-Answer Meaning Representation (QAMR)](/dataset/qamr) dataset.", "variants": ["AW-OIE"], "title": "Supervised Open Information Extraction"}
{"id": "YouTube-VOS", "contents": "Youtube-VOS is a Video Object Segmentation dataset that contains 4,453 videos - 3,471 for training, 474 for validation, and 508 for testing. The training and validation videos have pixel-level ground truth annotations for every 5th frame (6 fps). It also contains Instance Segmentation annotations. It has more than 7,800 unique objects, 190k high-quality manual annotations and more than 340 minutes in duration.\r\n\r\nSource: [CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing](https://arxiv.org/abs/1910.00132)\r\nImage Source: [https://youtube-vos.org/](https://youtube-vos.org/)", "variants": ["YouTube-VOS"], "title": "YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark"}
{"id": "DDAD", "contents": "**DDAD** is a new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. It contains monocular videos and accurate ground-truth depth (across a full 360 degree field of view) generated from high-density LiDARs mounted on a fleet of self-driving cars operating in a cross-continental setting. DDAD contains scenes from urban settings in the United States (San Francisco, Bay Area, Cambridge, Detroit, Ann Arbor) and Japan (Tokyo, Odaiba).\n\nSource: [https://github.com/TRI-ML/DDAD](https://github.com/TRI-ML/DDAD)\nImage Source: [https://github.com/TRI-ML/DDAD](https://github.com/TRI-ML/DDAD)", "variants": ["DDAD"], "title": "3D Packing for Self-Supervised Monocular Depth Estimation"}
{"id": "PAWS", "contents": "Paraphrase Adversaries from Word Scrambling (PAWS) is a dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. The dataset has two subsets, one based on Wikipedia and the other one based on the Quora Question Pairs (QQP) dataset.\r\n\r\nSource: [PAWS](https://github.com/google-research-datasets/paws)", "variants": ["PAWS"], "title": "PAWS: Paraphrase Adversaries from Word Scrambling"}
{"id": "IPN Hand", "contents": "The **IPN Hand** dataset is a benchmark video dataset with sufficient size, variation, and real-world elements able to train and evaluate deep neural networks for continuous Hand Gesture Recognition (HGR).\n\nSource: [https://github.com/GibranBenitez/IPN-hand](https://github.com/GibranBenitez/IPN-hand)", "variants": ["IPN Hand"], "title": "Real-time Hand Gesture Detection and Classification Using Convolutional Neural Networks"}
{"id": "100DOH", "contents": "The 100 Days Of Hands Dataset (100DOH) is a large-scale video dataset containing hands and hand-object interactions. It consists of 27.3K Youtube videos from 11 categories with nearly 131 days of footage of everyday interaction. The focus of the dataset is hand contact, and it includes both first-person and third-person perspectives. The videos in 100DOH are unconstrained and content-rich, ranging from records of daily life to specific instructional videos. To enforce diversity, the dataset contains no more than 20 videos from each uploader.\r\n\r\nSource: [](http://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/100DOH.html)", "variants": ["100DOH"], "title": "Analysis and Evaluation of Handwriting in Patients with Parkinson's Disease Using kinematic, Geometrical, and Non-linear Features"}
{"id": "LUNA16", "contents": "The **LUNA16** (LUng Nodule Analysis) dataset is a dataset for lung segmentation. It consists of 1,186 lung nodules annotated in 888 CT scans.\r\n\r\nSource: [Universal Lesion Detection by Learning from Multiple Heterogeneously Labeled Datasets](https://arxiv.org/abs/2005.13753)\r\nImage Source: [https://luna16.grand-ch](https://luna16.grand-ch)", "variants": ["LUNA2016 FPRED", "LUNA16"], "title": "Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The LUNA16 challenge"}
{"id": "CUHK03", "contents": "The **CUHK03** consists of 14,097 images of 1,467 different identities, where 6 campus cameras were deployed for image collection and each identity is captured by 2 campus cameras. This dataset provides two types of annotations, one by manually labelled bounding boxes and the other by bounding boxes produced by an automatic detector. The dataset also provides 20 random train/test splits in which 100 identities are selected for testing and the rest for training\r\n\r\nSource: [Attention Driven Person Re-identification](https://arxiv.org/abs/1810.05866)\r\n\r\nImage Source: [Person Re-Identification Techniques for Intelligent Video Surveillance Systems\r\n](https://www.researchgate.net/publication/324031366_Person_Re-Identification_Techniques_for_Intelligent_Video_Surveillance_Systems)", "variants": ["CUHK03", "CUHK03 (detected)", "CUHK03 detected", "CUHK03 labeled", "CUHK", "CUHK - Blur Detection Dataset"], "title": "DeepReID: Deep Filter Pairing Neural Network for Person Re-identification"}
{"id": "GICoref", "contents": "GICoref is a fully annotated coreference resolution dataset written by and about trans people.\r\n\r\nSource: [Toward Gender-Inclusive Coreference Resolution](https://www.aclweb.org/anthology/2020.acl-main.418.pdf)", "variants": ["GICoref"], "title": "Toward Gender-Inclusive Coreference Resolution"}
{"id": "COVID-19 Twitter Chatter Dataset", "contents": "A large-scale curated dataset of over 152 million tweets, growing daily, related to COVID-19 chatter generated from January 1st to April 4th at the time of writing.\r\n\r\nSource: [A large-scale COVID-19 Twitter chatter dataset for open scientific research -- an international collaboration](/paper/a-large-scale-covid-19-twitter-chatter)", "variants": ["COVID-19 Twitter Chatter Dataset"], "title": "A large-scale COVID-19 Twitter chatter dataset for open scientific research -- an international collaboration"}
{"id": "C3", "contents": "C3 is a free-form multiple-Choice Chinese machine reading Comprehension dataset.", "variants": ["C3"], "title": "Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"}
{"id": "TrecQA", "contents": "**Text Retrieval Conference Question Answering** (**TrecQA**) is a dataset created from the TREC-8 (1999) to TREC-13 (2004) Question Answering tracks. There are two versions of TrecQA: raw and clean. Both versions have the same training set but their development and test sets differ. The commonly used clean version of the dataset excludes questions in development and test sets with no answers or only positive/negative answers. The clean version has 1,229/65/68 questions and 53,417/1,117/1,442 question-answer pairs for the train/dev/test split.\r\n\r\nSource: [A Gated Self-attention Memory Network for Answer Selection](https://arxiv.org/abs/1909.09696)", "variants": ["TrecQA"], "title": "What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA"}
{"id": "UMDFaces", "contents": "UMDFaces is a face dataset divided into two parts:\r\n\r\n* Still Images - 367,888 face annotations for 8,277 subjects.\r\n* Video Frames - Over 3.7 million annotated video frames from over 22,000 videos of 3100 subjects.\r\n\r\n*Part 1 - Still Images*\r\n\r\nThe dataset contains 367,888 face annotations for 8,277 subjects divided into 3 batches. The annotations contain human curated bounding boxes for faces and estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.\r\n\r\n*Part 2 - Video Frames*\r\n\r\nThe second part contains 3,735,476 annotated video frames extracted from a total of 22,075 for 3,107 subjects. The annotations contain the estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.", "variants": ["UMDFaces"], "title": "UMDFaces: An annotated face dataset for training deep networks"}
{"id": "MSR ActionPairs", "contents": "This is a 3D action recognition dataset, also known as 3D Action Pairs dataset. The actions in this dataset are selected in pairs such that the two actions of each pair are similar in motion (have similar trajectories) and shape (have similar objects); however, the motion-shape relation is different. \r\n\r\nSource: [HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences](/paper/hon4d-histogram-of-oriented-4d-normals-for)", "variants": ["MSR ActionPairs"], "title": "HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences"}
{"id": "MetaLWOz", "contents": "Collected by leveraging background knowledge from a larger, more highly represented dialogue source.\r\n\r\nSource: [Few-Shot Dialogue Generation Without Annotated Data: A Transfer Learning Approach](/paper/few-shot-dialogue-generation-without)", "variants": ["MetaLWOz"], "title": "Few-Shot Dialogue Generation Without Annotated Data: A Transfer Learning Approach"}
{"id": "Cornell Movie-Quotes Corpus", "contents": "A corpus of movie quotes, annotated with memorability information, in which one is able to control for both the speaker and the setting of the quotes.\r\n\r\nSource: [You Had Me at Hello: How Phrasing Affects Memorability](/paper/you-had-me-at-hello-how-phrasing-affects)", "variants": ["Cornell Movie-Quotes Corpus"], "title": "You Had Me at Hello: How Phrasing Affects Memorability"}
{"id": "AADB", "contents": "Contains aesthetic scores and meaningful attributes assigned to each image by multiple human raters. \r\n\r\nSource: [Photo Aesthetics Ranking Network with Attributes and Content Adaptation](/paper/photo-aesthetics-ranking-network-with)", "variants": ["AADB"], "title": "Photo Aesthetics Ranking Network with Attributes and Content Adaptation"}
{"id": "ARID", "contents": "ARID is a large-scale, multi-view object dataset collected with an RGB-D camera mounted on a mobile robot.\r\n\r\nSource: [ARID](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/autonomous-robot-indoor-dataset/)", "variants": ["ARID"], "title": "Recognizing Objects in-the-Wild: Where do we Stand?"}
{"id": "NarrativeQA", "contents": "The NarrativeQA dataset includes a list of documents with Wikipedia summaries, links to full stories, and questions and answers.\r\n\r\nSource: [DeepMind](https://deepmind.com/research/open-source/narrativeqa)\r\nImage Source: [Kočiský et al ](https://arxiv.org/pdf/1712.07040v1.pdf)", "variants": ["NarrativeQA"], "title": "The NarrativeQA Reading Comprehension Challenge"}
{"id": "x-stance", "contents": "A large-scale stance detection dataset from comments written by candidates of elections in Switzerland. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection. It contains 67 000 comments on more than 150 political issues (targets).\r\n\r\nSource: [X-Stance: A Multilingual Multi-Target Dataset for Stance Detection](/paper/x-stance-a-multilingual-multi-target-dataset)", "variants": ["x-stance"], "title": "X-Stance: A Multilingual Multi-Target Dataset for Stance Detection"}
{"id": "BLUE", "contents": "The BLUE benchmark consists of five different biomedicine text-mining tasks with ten corpora. These tasks cover a diverse range of text genres (biomedical literature and clinical notes), dataset sizes, and degrees of difficulty and, more importantly, highlight common biomedicine text-mining challenges.\r\n\r\nSource: [Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets](/paper/transfer-learning-in-biomedical-natural)", "variants": ["BLUE"], "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"}
{"id": "First-Person Hand Action Benchmark", "contents": "**First-Person Hand Action Benchmark** is a collection of RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. \r\n\r\nSource: [First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations](https://arxiv.org/pdf/1704.02463v2.pdf)", "variants": ["First-Person Hand Action Benchmark"], "title": "First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations"}
{"id": "Scan-CAD Object Similarity Dataset", "contents": "A dataset of ranked scan-CAD similarity annotations, enabling new, fine-grained evaluation of CAD model retrieval to cluttered, noisy, partial scans. \r\n\r\nSource: [Joint Embedding of 3D Scan and CAD Objects](/paper/joint-embedding-of-3d-scan-and-cad-objects)", "variants": ["Scan-CAD Object Similarity Dataset"], "title": "Joint Embedding of 3D Scan and CAD Objects"}
{"id": "PA-100K", "contents": "**PA-100K** is a recent-proposed large pedestrian attribute dataset, with 100,000 images in total collected from outdoor surveillance cameras. It is split into 80,000 images for the training set, and 10,000 for the validation set and 10,000 for the test set. This dataset is labeled by 26 binary attributes. The common features existing in both selected dataset is that the images are blurry due to the relatively low resolution and the positive ratio of each binary attribute is low.\r\n\r\nSource: [Localization Guided Learning for Pedestrian Attribute Recognition](https://arxiv.org/abs/1808.09102)\r\nImage Source: [https://github.com/xh-liu/HydraPlus-Net](https://github.com/xh-liu/HydraPlus-Net)", "variants": ["PA-100K"], "title": "HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis"}
{"id": "VLUC", "contents": "VLUC (Video-Like Urban Computing) is a benchmark for video-like computing on citywide traffic density and crowd prediction. It consists of two new datasets BousaiTYO and BousaiOSA and existing datasets TaxiBJ, BikeNYC I-II, and TaxiNYC.\r\n\r\n\r\nSource: [VLUC: An Empirical Benchmark for Video-Like Urban Computing on Citywide Crowd and Traffic Prediction](https://arxiv.org/abs/1911.06982)", "variants": ["VLUC"], "title": "VLUC: An Empirical Benchmark for Video-Like Urban Computing on Citywide Crowd and Traffic Prediction"}
{"id": "MilkQA", "contents": "A question answering dataset from the dairy domain dedicated to the study of consumer questions. The dataset contains 2,657 pairs of questions and answers, written in the Portuguese language and originally collected by the Brazilian Agricultural Research Corporation (Embrapa). All questions were motivated by real situations and written by thousands of authors with very different backgrounds and levels of literacy, while answers were elaborated by specialists from Embrapa's customer service. \r\n\r\nSource: [MilkQA: a Dataset of Consumer Questions for the Task of Answer Selection](/paper/milkqa-a-dataset-of-consumer-questions-for)", "variants": ["MilkQA"], "title": "MilkQA: a Dataset of Consumer Questions for the Task of Answer Selection"}
{"id": "Gibson Environment", "contents": "Gibson is an opensource perceptual and physics simulator to explore active and real-world perception. The Gibson Environment is used for Real-World Perception Learning.", "variants": ["Gibson Environment"], "title": "Gibson Env: Real-World Perception for Embodied Agents"}
{"id": "4DFAB", "contents": "4DFAB is a large scale database of dynamic high-resolution 3D faces which consists of recordings of 180 subjects captured in four different sessions spanning over a five-year period (2012 - 2017), resulting in a total of over 1,800,000 3D meshes. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour.\r\n\r\nSource: [ibug](https://ibug.doc.ic.ac.uk/resources/4dfab/)", "variants": ["4DFAB"], "title": "4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications"}
{"id": "VegFru", "contents": "**VegFru** is a domain-specific dataset for fine-grained visual categorization. VegFru categorizes vegetables and fruits according to their eating characteristics, and each image contains at least one edible part of vegetables or fruits with the same cooking usage. Particularly, all the images are labelled hierarchically. The current version covers vegetables and fruits of 25 upper-level categories and 292 subordinate classes. And it contains more than 160,000 images in total and at least 200 images for each subordinate class.\n\nSource: [https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf](https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf)\nImage Source: [https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf](https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf)", "variants": ["VegFru"], "title": "VegFru: A Domain-Specific Dataset for Fine-Grained Visual Categorization"}
{"id": "VideoMem", "contents": "Composed of 10,000 videos annotated with memorability scores. In contrast to previous work on image memorability -- where memorability was measured a few minutes after memorization -- memory performance is measured twice: a few minutes after memorization and again 24-72 hours later. \r\n\r\nSource: [VideoMem: Constructing, Analyzing, Predicting Short-term and Long-term Video Memorability](/paper/videomem-constructing-analyzing-predicting)", "variants": ["VideoMem"], "title": "VideoMem: Constructing, Analyzing, Predicting Short-Term and Long-Term Video Memorability"}
{"id": "EmpatheticDialogues", "contents": "The **EmpatheticDialogues** dataset is a large-scale multi-turn empathetic dialogue dataset collected on the Amazon Mechanical Turk, containing 24,850 one-to-one open-domain conversations. Each conversation was obtained by pairing two crowd-workers: a speaker and a listener. The speaker is asked to talk about the personal emotional feelings. The listener infers the underlying emotion through what the speaker says and responds empathetically. The dataset provides 32 evenly distributed emotion labels.\r\n\r\nSource: [Empathetic Dialogue Generation viaKnowledge Enhancing and Emotion Dependency Modeling](https://arxiv.org/abs/2009.09708)\r\nImage Source: [Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset](https://arxiv.org/pdf/1811.00207.pdf)", "variants": ["EmpatheticDialogues"], "title": "Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset"}
{"id": "DublinCity", "contents": "A novel benchmark dataset that includes a manually annotated point cloud for over 260 million laser scanning points into 100'000 (approx.) assets from Dublin LiDAR point cloud [12] in 2015. Objects are labelled into 13 classes using hierarchical levels of detail from large (i.e., building, vegetation and ground) to refined (i.e., window, door and tree) elements. \r\n\r\nSource: [DublinCity: Annotated LiDAR Point Cloud and its Applications](/paper/dublincity-annotated-lidar-point-cloud-and)", "variants": ["DublinCity"], "title": "DublinCity: Annotated LiDAR Point Cloud and its Applications"}
{"id": "COWC", "contents": "The Cars Overhead With Context (COWC) data set is a large set of annotated cars from overhead. It is useful for training a device such as a deep neural network to learn to detect and/or count cars.\r\n\r\nSource: [A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning](/paper/a-large-contextual-dataset-for-classification)\r\nImage Source: [https://gdo152.llnl.gov/cowc/](https://gdo152.llnl.gov/cowc/)", "variants": ["COWC"], "title": "A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning"}
{"id": "LCSTS", "contents": "LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. The authors also manually tagged the relevance of 10,666 short summaries with their corresponding short texts 10,666 short summaries with their corresponding short texts.\r\n\r\nSource: [LCSTS: A Large Scale Chinese Short Text Summarization Dataset](/paper/lcsts-a-large-scale-chinese-short-text)", "variants": ["LCSTS"], "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset"}
{"id": "VisDial", "contents": "**Visual Dialog** (**VisDial**) dataset contains human annotated questions based on images of MS COCO dataset. This dataset was developed by pairing two subjects on Amazon Mechanical Turk to chat about an image. One person was assigned the job of a ‘questioner’ and the other person acted as an ‘answerer’. The questioner sees only the text description of an image (i.e., an image caption from MS COCO dataset) and the original image remains hidden to the questioner. Their task is to ask questions about this hidden image to “imagine the scene better”. The answerer sees the image, caption and answers the questions asked by the questioner. The two of them can continue the conversation by asking and answering questions for 10 rounds at max.\r\n\r\n**VisDial v1.0** contains 123K dialogues on MS COCO (2017 training set) for training split, 2K dialogues with validation images for validation split and 8K dialogues on test set for test-standard set. The previously released v0.5 and v0.9 versions of VisDial dataset (corresponding to older splits of MS COCO) are considered deprecated.\r\n\r\nSource: [Granular Multimodal Attention Networks for Visual Dialog](https://arxiv.org/abs/1910.05728)\r\nImage Source: [https://arxiv.org/pdf/1611.08669.pdf](https://arxiv.org/pdf/1611.08669.pdf)", "variants": ["VisDial", "Visual Dialog  v0.9", "Visual Dialog v1.0 test-std", "VisDial v0.9 val", "VisDial v1.0 test-std", "Visual Dialog v0.9", "Visual Dialog v1.0"], "title": "Visual Dialog"}
{"id": "N-CARS", "contents": "A large real-world event-based dataset for object classification.\r\n\r\nSource: [HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification](/paper/hats-histograms-of-averaged-time-surfaces-for)", "variants": ["N-CARS"], "title": "HATS: Histograms of Averaged Time Surfaces for Robust Event-Based Object Classification"}
{"id": "RT-GENE", "contents": "Presents a diverse eye-gaze dataset.\r\n\r\nSource: [RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments](/paper/rt-gene-real-time-eye-gaze-estimation-in)", "variants": ["RT-GENE"], "title": "RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments"}
{"id": "FQuAD", "contents": "A French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version.\r\n\r\nSource: [FQuAD: French Question Answering Dataset](/paper/fquad-french-question-answering-dataset)", "variants": ["FQuAD"], "title": "FQuAD: French Question Answering Dataset"}
{"id": "Adience", "contents": "The **Adience** dataset, published in 2014, contains 26,580 photos across 2,284 subjects with a binary gender label and one label from eight different age groups, partitioned into five splits. The key principle of the data set is to capture the images as close to real world conditions as possible, including all variations in appearance, pose, lighting condition and image quality, to name a few.\r\n\r\nSource: [Understanding and Comparing Deep Neural Networksfor Age and Gender Classification](https://arxiv.org/abs/1708.07689)\r\nImage Source: [https://talhassner.github.io/home/projects/Adience/Adience-data.html](https://talhassner.github.io/home/projects/Adience/Adience-data.html)", "variants": ["Adience", "Adience Age", "Adience Gender", "Adience (Online Open Set)"], "title": "Age and Gender Estimation of Unfiltered Faces"}
{"id": "PASCAL Face", "contents": "The PASCAL FACE dataset is a dataset for face detection and face recognition. It has a total of 851 images which are a subset of the PASCAL VOC and has a total of 1,341 annotations. These datasets contain only a few hundreds of images and have limited variations in face appearance.\r\n\r\nSource: [Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results](https://arxiv.org/abs/1804.10275)\r\nImage Source: [https://www.researchgate.net/figure/Precision-recall-curves-on-PASCAL-face-dataset_fig7_332998926](https://www.researchgate.net/figure/Precision-recall-curves-on-PASCAL-face-dataset_fig7_332998926)", "variants": ["PASCAL Face"], "title": "Face detection by structural models"}
{"id": "Image Editing Request Dataset", "contents": "A new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. \r\n\r\nSource: [Expressing Visual Relationships via Language](/paper/expressing-visual-relationships-via-language)", "variants": ["Image Editing Request Dataset"], "title": "Expressing Visual Relationships via Language"}
{"id": "Machine Number Sense", "contents": "Consists of visual arithmetic problems automatically generated using a grammar model--And-Or Graph (AOG). These visual arithmetic problems are in the form of geometric figures: each problem has a set of geometric shapes as its context and embedded number symbols. \r\n\r\nSource: [Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning](/paper/machine-number-sense-a-dataset-of-visual)", "variants": ["Machine Number Sense"], "title": "Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning"}
{"id": "VQA-OV", "contents": "Collects 60 reference sequences and 540 impaired sequences. \r\n\r\nSource: [Bridge the Gap Between VQA and Human Behavior on Omnidirectional Video: A Large-Scale Dataset and a Deep Learning Model](/paper/bridge-the-gap-between-vqa-and-human-behavior)", "variants": ["VQA-OV"], "title": "Bridge the Gap Between VQA and Human Behavior on Omnidirectional Video: A Large-Scale Dataset and a Deep Learning Model"}
{"id": "EgoCap", "contents": "EgoCap is a dataest of 100,000 egocentric images of eight people in different clothing, with 75,000 images from six people used for training. The images have been captured with two fisheye cameras.\r\n\r\nSource: [EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras](/paper/egocap-egocentric-marker-less-motion-capture-1)", "variants": ["EgoCap"], "title": "EgoCap: egocentric marker-less motion capture with two fisheye cameras"}
{"id": "CDTB", "contents": "\n\nSource: [https://www.vicos.si/Projects/CDTB 4.2 State-of-the-art Comparison A TH CTB (color-and-depth visual object tracking) dataset is recorded by several passive and active RGB-D setups and contains indoor as well as outdoor sequences acquired in direct sunlight. The sequences were recorded to contain significant object pose change, clutter, occlusion, and periods of long-term target absence to enable tracker evaluation under realistic conditions. Sequences are per-frame annotated with 13 visual attributes for detailed analysis. It contains around 100,000 samples.](https://www.vicos.si/Projects/CDTB 4.2 State-of-the-art Comparison A TH CTB (color-and-depth visual object tracking) dataset is recorded by several passive and active RGB-D setups and contains indoor as well as outdoor sequences acquired in direct sunlight. The sequences were recorded to contain significant object pose change, clutter, occlusion, and periods of long-term target absence to enable tracker evaluation under realistic conditions. Sequences are per-frame annotated with 13 visual attributes for detailed analysis. It contains around 100,000 samples.)\nImage Source: [https://www.vicos.si/Projects/CDTB](https://www.vicos.si/Projects/CDTB)", "variants": ["CDTB"], "title": "CDTB: A Color and Depth Visual Object Tracking Dataset and Benchmark"}
{"id": "Chinese Classifier", "contents": "Classifiers are function words that are used to express quantities in Chinese and are especially difficult for language learners. This dataset of **Chinese Classifiers** can be used to predict Chinese classifiers from context.\nThe dataset contains a large collection of example sentences for Chinese classifier usage derived from three language corpora (Lancaster Corpus of Mandarin Chinese, UCLA Corpus of Written Chinese and Leiden Weibo Corpus). The data was cleaned and processed for a context-based classifier prediction task.\n\nSource: [https://github.com/wuningxi/ChineseClassifierDataset](https://github.com/wuningxi/ChineseClassifierDataset)", "variants": ["Chinese Classifier"], "title": "ClassifierGuesser: A Context-based Classifier Prediction System for Chinese Language Learners"}
{"id": "MMAct", "contents": "MMAct is a large-scale dataset for multi/cross modal action understanding. This dataset has been recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios. \r\n\r\nSource: [MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding](/paper/mmact-a-large-scale-dataset-for-cross-modal)", "variants": ["MMAct"], "title": "MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding"}
{"id": "CamVid", "contents": "**CamVid** (**Cambridge-driving Labeled Video Database**) is a road/driving scene understanding database which was originally captured as five video sequences with a 960×720 resolution camera mounted on the dashboard of a car. Those sequences were sampled (four of them at 1 fps and one at 15 fps) adding up to 701 frames. Those stills were manually annotated with 32 classes: void, building, wall, tree, vegetation, fence, sidewalk, parking block, column/pole, traffic cone, bridge, sign, miscellaneous text, traffic light, sky, tunnel, archway, road, road shoulder, lane markings (driving), lane markings (non-driving), animal, pedestrian, child, cart luggage, bicyclist, motorcycle, car, SUV/pickup/truck, truck/bus, train, and other moving object\r\n\r\nSource: [A Review on Deep Learning TechniquesApplied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\r\nImage Source: [http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)", "variants": ["CamVid"], "title": "Semantic object classes in video: A high-definition . . ."}
{"id": "CodeSwitch-Reddit", "contents": "A diverse dataset of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of spoken language thus far.\r\n\r\nSource: [CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online Discussion Forums](/paper/codeswitch-reddit-exploration-of-written)", "variants": ["CodeSwitch-Reddit"], "title": "CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online Discussion Forums"}
{"id": "Airport", "contents": "The **Airport** dataset is a dataset for person re-identification which consists of 39,902 images and 9,651 identities across six cameras.\n\nSource: [An Evaluation of Deep CNN Baselines for Scene-Independent Person Re-Identification](https://arxiv.org/abs/1805.06086)\nImage Source: [http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-airport-re-identification-dataset/](http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-airport-re-identification-dataset/)", "variants": ["Airport"], "title": "A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets"}
{"id": "Make3D", "contents": "The **Make3D** dataset is a monocular Depth Estimation dataset that contains 400 single training RGB and depth map pairs, and 134 test samples. The RGB images have high resolution, while the depth maps are provided at low resolution.\r\n\r\nSource: [Structured Coupled Generative Adversarial Networks for Unsupervised Monocular Depth Estimation](https://arxiv.org/abs/1908.05794)\r\nImage Source: [http://make3d.cs.cornell.edu/data.html#make3d](http://make3d.cs.cornell.edu/data.html#make3d)", "variants": ["Make3D"], "title": "Learning 3-D Scene Structure from a Single Still Image"}
{"id": "Scan2CAD", "contents": "**Scan2CAD** is an alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoints pairs between 14225 (3049 unique) CAD models from ShapeNet and their counterpart objects in the scans. The top 3 annotated model classes are chairs, tables and cabinets which arises due to the nature of indoor scenes in ScanNet. The number of objects aligned per scene ranges from 1 to 40 with an average of 9.3.\r\n\r\nAdditionally, all ShapeNet CAD models used in the Scan2CAD dataset are annotated with their rotational symmetries: either none, 2-fold, 4-fold or infinite rotational symmetries around a canonical axis of the object.\r\n\r\nSource: [Scan2CAD: Learning CAD Model Alignment in RGB-D Scans](https://paperswithcode.com/paper/scan2cad-learning-cad-model-alignment-in-rgb/)\r\nImage Source: [Scan2CAD: Learning CAD Model Alignment in RGB-D Scans](https://paperswithcode.com/paper/scan2cad-learning-cad-model-alignment-in-rgb/)", "variants": ["Scan2CAD"], "title": "Scan2CAD: Learning CAD Model Alignment in RGB-D Scans"}
{"id": "SKU110K", "contents": "The Sku110k dataset provides 11,762 images with more than 1.7 million annotated bounding boxes captured in densely packed scenarios, including 8,233 images for training, 588 images for validation, and 2,941 images for testing. There are around 1,733,678 instances in total. The images are collected from thousands of supermarket stores and are of various scales, viewing angles, lighting conditions, and noise levels. All the images are resized into a resolution of one megapixel. Most of the instances in the dataset are tightly packed and typically of a certain orientation in the rage of [−15∘, 15∘].\n\nSource: [Rethinking Object Detection in Retail Stores](https://arxiv.org/abs/2003.08230)\nImage Source: [https://github.com/eg4000/SKU110K_CVPR19](https://github.com/eg4000/SKU110K_CVPR19)", "variants": ["SKU-110K", "SKU110K"], "title": "Precise Detection in Densely Packed Scenes"}
{"id": "MAESTRO", "contents": "The **MAESTRO** dataset contains over 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. The MIDI data includes key strike velocities and sustain/sostenuto/una corda pedal positions. Audio and MIDI files are aligned with ∼3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. Uncompressed audio is of CD quality or higher (44.1–48 kHz 16-bit PCM stereo).\r\n\r\nSource: [https://magenta.tensorflow.org/datasets/maestro](https://magenta.tensorflow.org/datasets/maestro)\r\nImage Source: [https://www.researchgate.net/figure/Results-generated-with-the-MAESTRO-dataset_fig3_333392458](https://www.researchgate.net/figure/Results-generated-with-the-MAESTRO-dataset_fig3_333392458)", "variants": ["MAESTRO"], "title": "Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset"}
{"id": "OFEQ-10k", "contents": "The **OFEQ-10k** dataset contains 12,548 detailed questions with corresponding math headlines from MathOverflow.\n\nSource: [https://arxiv.org/pdf/1912.00839.pdf](https://arxiv.org/pdf/1912.00839.pdf)", "variants": ["OFEQ-10k"], "title": "Automatic Generation of Headlines for Online Math Questions"}
{"id": "Agriculture-Vision", "contents": "A large-scale aerial farmland image dataset for semantic segmentation of agricultural patterns. Collects 94,986 high-quality aerial images from 3,432 farmlands across the US, where each image consists of RGB and Near-infrared (NIR) channels with resolution as high as 10 cm per pixel. \r\n\r\nSource: [Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis](/paper/agriculture-vision-a-large-aerial-image)", "variants": ["Agriculture-Vision"], "title": "Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis"}
{"id": "DeepFashion2", "contents": "DeepFashion2 is a versatile benchmark of four tasks including clothes detection, pose estimation, segmentation, and retrieval. It has 801K clothing items where each item has rich annotations such as style, scale, viewpoint, occlusion, bounding box, dense landmarks and masks. There are also 873K Commercial-Consumer clothes pairs\r\n\r\nSource: [DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images](https://arxiv.org/pdf/1901.07973v1.pdf)\r\nImage Source: [https://github.com/switchablenorms/DeepFashion2](https://github.com/switchablenorms/DeepFashion2)", "variants": ["Deepfashion2 validation", "Deepfashion2 test", "DeepFashion2"], "title": "DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images"}
{"id": "Europarl-ST", "contents": "Europarl-ST is a multilingual SLT corpus containing paired audio-text samples for SLT from and into 6 European languages, for a total of 30 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012. \r\n\r\nSource: [Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates](/paper/europarl-st-a-multilingual-corpus-for-speech)", "variants": ["Europarl-ST"], "title": "Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates"}
{"id": "DIRHA", "contents": "**DIRHA**-English is a multi-microphone database composed of real and simulated sequences of 1-minute. The overall corpus is composed of different types of sequences including: 1) Phonetically-rich sentences; 2) WSJ 5-k utterances; 3) WSJ 20-k utterances; 4) Conversational speech (also including keywords and commands).\r\nThe sequences are available for both UK and US English at 48 kHz. The DIRHA-English dataset offers the possibility to work with a very large number of microphone channels, to use of microphone arrays having different characteristics and to work considering different speech recognition tasks (e.g., phone-loop, keyword spotting, ASR with small and very large language models).\r\n\r\nSource: [The DIRHA-English Corpus](http://dirha.fbk.eu/DIRHA_English)\r\nImage Source: [https://arxiv.org/pdf/1710.02560v1.pdf](https://arxiv.org/pdf/1710.02560v1.pdf)", "variants": ["DIRHA English WSJ", "DIRHA"], "title": "The DIRHA-ENGLISH corpus and related tasks for distant-speech recognition in domestic environments"}
{"id": "HappyDB", "contents": "**HyppyDB** is a corpus of 100,000 crowdsourced happy moments.\r\n\r\nSource: [HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments](/paper/happydb-a-corpus-of-100000-crowdsourced-happy)", "variants": ["HappyDB"], "title": "HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments"}
{"id": "Flickr30K Entities", "contents": "The **Flickr30K Entities** dataset is an extension to the Flickr30K dataset. It augments the original 158k captions with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. This is used to define a new benchmark for localization of textual entity mentions in an image.\r\n\r\nSource: [http://bryanplummer.com/Flickr30kEntities/](http://bryanplummer.com/Flickr30kEntities/)\r\nImage Source: [http://bryanplummer.com/Flickr30kEntities/](http://bryanplummer.com/Flickr30kEntities/)", "variants": ["Flickr30k Entities Dev", "Flickr30k Entities Test", "Flickr30K Entities"], "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}
{"id": "BVI-DVC", "contents": "Contains 800 sequences at various spatial resolutions from 270p to 2160p and has been evaluated on ten existing network architectures for four different coding tools.\r\n\r\nSource: [BVI-DVC: A Training Database for Deep Video Compression](/paper/bvi-dvc-a-training-database-for-deep-video)", "variants": ["BVI-DVC"], "title": "BVI-DVC: A Training Database for Deep Video Compression"}
{"id": "ErhuPT", "contents": "This dataset is an audio dataset containing about 1500 audio clips recorded by multiple professional players.\r\n\r\nSource: [Zenodo](https://zenodo.org/record/4320991)", "variants": ["ErhuPT"], "title": "Musical Instrument Playing Technique Detection Based on FCN: Using Chinese Bowed-Stringed Instrument as an Example"}
{"id": "WIQA", "contents": "The WIQA dataset V1 has 39705 questions containing a perturbation and a possible effect in the context of a paragraph. The dataset is split into 29808 train questions, 6894 dev questions and 3003 test questions.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/wiqa)", "variants": ["WIQA"], "title": "WIQA: A dataset for\"What if...\"reasoning over procedural text"}
{"id": "The RobotriX", "contents": "Photorealistic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems. The RobotriX consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. \r\n\r\nSource: [The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions](/paper/the-robotrix-an-extremely-photorealistic-and)", "variants": ["The RobotriX"], "title": "The RobotriX: An Extremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions"}
{"id": "RCV1", "contents": "The **RCV1** dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.\r\n\r\nSource: [Random Projections for Linear Support Vector Machines](https://arxiv.org/abs/1211.6085)\r\nImage Source: [https://www.nasdaq.com/publishers/reuters](https://www.nasdaq.com/publishers/reuters)", "variants": ["RCV1-v2", "Reuters RCV1/RCV2 English-to-German", "Reuters RCV1/RCV2 German-to-English", "RCV1"], "title": "RCV1: A New Benchmark Collection for Text Categorization Research"}
{"id": "VizWiz-Priv", "contents": "VizWiz-Priv includes 8,862 regions showing private content across 5,537 images taken by blind people. Of these, 1,403 are paired with questions and 62% of those directly ask about the private content.\r\n\r\nSource: [VizWiz-Priv: A Dataset for Recognizing the Presence and Purpose of Private Visual Information in Images Taken by Blind People](/paper/vizwiz-priv-a-dataset-for-recognizing-the)", "variants": ["VizWiz-Priv"], "title": "VizWiz-Priv: A Dataset for Recognizing the Presence and Purpose of Private Visual Information in Images Taken by Blind People"}
{"id": "ContactDB", "contents": "**ContactDB** is a dataset of contact maps for household objects that captures the rich hand-object contact that occurs during grasping, enabled by use of a thermal camera. ContactDB includes 3,750 3D meshes of 50 household objects textured with contact maps and 375K frames of synchronized RGB-D+thermal images.\r\n\r\nSource: [https://arxiv.org/abs/1904.06830](https://arxiv.org/abs/1904.06830)\r\nImage Source: [https://github.com/samarth-robo/contactdb_utils](https://github.com/samarth-robo/contactdb_utils)", "variants": ["ContactDB"], "title": "ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging"}
{"id": "WildDash", "contents": "WildDash is a benchmark evaluation method is presented that uses the meta-information to calculate the robustness of a given algorithm with respect to the individual hazards.\r\n\r\nSource: [WildDash - Creating Hazard-Aware Benchmarks](/paper/wilddash-creating-hazard-aware-benchmarks)\r\nImage Source: [https://wilddash.cc/](https://wilddash.cc/)", "variants": ["WildDash"], "title": "WildDash - Creating Hazard-Aware Benchmarks"}
{"id": "iMaterialist", "contents": "Constructed from over one million fashion images with a label space that includes 8 groups of 228 fine-grained attributes in total. Each image is annotated by experts with multiple, high-quality fashion attributes.\r\n\r\nSource: [The iMaterialist Fashion Attribute Dataset](/paper/the-imaterialist-fashion-attribute-dataset)", "variants": ["iMaterialist"], "title": "The iMaterialist Fashion Attribute Dataset"}
{"id": "Moments in Time", "contents": "Moments in Time is a large-scale dataset for recognizing and understanding action in videos. The dataset includes a collection of one million labeled 3 second videos, involving people, animals, objects or natural phenomena, that capture the gist of a dynamic scene.", "variants": ["Moments in Time", "Moments in Time Dataset"], "title": "Moments in Time Dataset: One Million Videos for Event Understanding"}
{"id": "IntPhys 2019", "contents": "A benchmark for visual intuitive physics reasoning.\r\n\r\nSource: [IntPhys 2019](http://www.intphys.com/)", "variants": ["IntPhys 2019"], "title": "IntPhys: A Framework and Benchmark for Visual Intuitive Physics Reasoning"}
{"id": "PDBBind", "contents": "The **PDBBind** database provides a comprehensive collection of structures of protein-ligand complexes and their binding affinity data. The original experimental data in Protein Data Bank (PDB) are selected to PDBBind database based on certain quality requirements and curated for applications.\n\nSource: [Representability of algebraic topology for biomolecules in machine learning based scoring and virtual screening](https://arxiv.org/abs/1708.08135)\nImage Source: [http://www.pdbbind.org.cn/browse.php](http://www.pdbbind.org.cn/browse.php)", "variants": ["PDBbind", "PDBBind"], "title": "PDB-wide collection of binding data: current status of the PDBbind database"}
{"id": "NSynth", "contents": "**NSynth** is a dataset of one shot instrumental notes, containing 305,979 musical notes with unique pitch, timbre and envelope. The sounds were collected from 1006 instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. Four second monophonic 16kHz audio snippets were generated (notes) for the instruments.\r\n\r\nSource: [Data Augmentation for Instrument Classification Robust to Audio Effects](https://arxiv.org/abs/1907.08520)\r\nImage Source: [https://magenta.tensorflow.org/nsynth](https://magenta.tensorflow.org/nsynth)", "variants": ["NSynth"], "title": "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders"}
{"id": "MetaQA", "contents": "The **MetaQA** dataset consists of a movie ontology derived from the WikiMovies Dataset and three sets of question-answer pairs written in natural language: 1-hop, 2-hop, and 3-hop queries.\r\n\r\nSource: [https://arxiv.org/abs/1907.08176](https://arxiv.org/abs/1907.08176)\r\nImage Source: [https://github.com/yuyuz/MetaQA](https://github.com/yuyuz/MetaQA)", "variants": ["MetaQA"], "title": "Variational Reasoning for Question Answering with Knowledge Graph"}
{"id": "Omniglot", "contents": "**Omniglot** is a large dataset of hand-written characters with 1623 characters and 20 examples for each character. These characters are collected based upon 50 alphabets from different countries. It contains both images and strokes data. Stroke data are coordinates with time in miliseconds.\r\n\r\nSource: [A Comprehensive Overview and Survey of Recent Advances in Meta-Learning](https://arxiv.org/abs/2004.11149)", "variants": ["Cluttered Omniglot", "OMNIGLOT", "Omniglot", "OMNIGLOT - 1-Shot, 20-way", "OMNIGLOT - 1-Shot, 5-way", "OMNIGLOT - 5-Shot, 20-way", "OMNIGLOT - 5-Shot, 5-way", "OMNIGLOT - 1-Shot, 1000 way", "OMNIGLOT - 1-Shot, 423 way", "OMNIGLOT - 5-Shot, 1000 way", "OMNIGLOT - 5-Shot, 423 way", "OMNIGLOT-EMNIST 5-way (1-shot)", "OMNIGLOT-EMNIST 5-way (5-shot)", "OMNIGLOT - 1-Shot Learning", "OMNIGLOT - 5-Shot Learning"], "title": "Human-level concept learning through probabilistic program induction"}
{"id": "Visual Beliefs", "contents": "Visual Beliefs is a dataset of abstract scenes to study visual beliefs. The dataset consists of 8-frame scenes, and in each scene a person has a mistaken belief. The dataset can be used for two tasks: predicting who is mistaken and predicting when are they mistaken. \r\n\r\nSource: [Who is Mistaken?](https://arxiv.org/pdf/1612.01175.pdf)", "variants": ["Visual Beliefs"], "title": "Who is Mistaken?"}
{"id": "KVRET", "contents": "The **KVRET** corpus introduced to evaluate Key-Value Retrieval Networks for Task-Oriented Dialogue is a multi-turn, multi-domain dialogue dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation.\r\n\r\nSource: [Key-Value Retrieval Networks for Task-Oriented Dialogue](https://paperswithcode.com/paper/key-value-retrieval-networks-for-task/)\r\nImage Source: [Key-Value Retrieval Networks for Task-Oriented Dialogue](https://paperswithcode.com/paper/key-value-retrieval-networks-for-task/)", "variants": ["Kvret", "KVRET"], "title": "Key-Value Retrieval Networks for Task-Oriented Dialogue"}
{"id": "LAG", "contents": "Includes 5,824 fundus images labeled with either positive glaucoma (2,392) or negative glaucoma (3,432).\r\n\r\nSource: [Attention Based Glaucoma Detection: A Large-scale Database and CNN Model](/paper/attention-based-glaucoma-detection-a-large)", "variants": ["LAG"], "title": "Attention Based Glaucoma Detection: A Large-Scale Database and CNN Model"}
{"id": "COCO", "contents": "The MS **COCO** (**Microsoft Common Objects in Context**) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images.\r\n\r\n**Splits:**\r\nThe first version of MS COCO dataset was released in 2014. It contains 164K images split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images was released, including all the previous test images and 40K new images.\r\n\r\nBased on community feedback, in 2017 the training/validation split was changed from 83K/41K to 118K/5K. The new split uses the same images and annotations. The 2017 test set is a subset of 41K images of the 2015 test set. Additionally, the 2017 release contains a new unannotated dataset of 123K images.\r\n\r\n**Annotations:**\r\nThe dataset has annotations for\r\n\r\n* object detection: bounding boxes and per-instance segmentation masks with 80 object categories,\r\n* captioning: natural language descriptions of the images (see MS COCO Captions),\r\n* keypoints detection: containing more than 200,000 images and 250,000 person instances labeled with keypoints (17 possible keypoints, such as left eye, nose, right hip, right ankle),\r\n* stuff image segmentation – per-pixel segmentation masks with 91 stuff categories, such as grass, wall, sky (see MS COCO Stuff),\r\n* panoptic: full scene segmentation, with 80 thing categories (such as person, bicycle, elephant) and a subset of 91 stuff categories (grass, sky, road),\r\n* dense pose: more than 39,000 images and 56,000 person instances labeled with DensePose annotations – each labeled person is annotated with an instance id and a mapping between image pixels that belong to that person body and a template 3D model.\r\nThe annotations are publicly available only for training and validation images.\r\n\r\nSource: [https://cocodataset.org/](https://cocodataset.org/)\r\nImage Source: [https://cocodataset.org/](https://cocodataset.org/)", "variants": ["COCO", "COCO (image as query)", "COCO Visual Question Answering (VQA) real images 1.0 multiple choice", "COCO count-test", "COCO minival", "COCO panoptic", "COCO test-challenge", "COCO test-dev", "COCO+", "COCO-Animals", "COCO_20k", "DensePose-COCO", "MSCOCO", "Microsoft COCO dataset", "COCO Visual Question Answering (VQA) real images 1.0 open ended", "COCO Visual Question Answering (VQA) real images 2.0 open ended", "MS COCO", "COCO_Visual_Question_Answering__VQA__abstract_1_0_multiple_choice", "COCO_Visual_Question_Answering__VQA__abstract_images_1_0_open_ended", "MS-COCO", "MS-COCO (10-shot)", "COCO 2015", "COCO 256 x 256", "COCO-Stuff 256x256", "COCO-Stuff-3", "coco minval", "COCO 2014", "COCO 2017", "COCO Visual Question Answering (VQA) abstract 1.0 multiple choice", "COCO Visual Question Answering (VQA) abstract images 1.0 open ended", "COCO 2017 (Electronic, Indoor, Kitchen, Furniture)", "COCO 2017 (Outdoor, Accessories, Appliance, Truck)", "COCO 2017 (Sports, Food)"], "title": "Microsoft COCO: Common Objects in Context"}
{"id": "nocaps", "contents": "The nocaps benchmark consists of 166,100 human-generated captions describing 15,100 images from the OpenImages validation and test sets.\r\n\r\nSource: [nocaps: novel object captioning at scale](/paper/nocaps-novel-object-captioning-at-scale)\r\nImage Source: [https://nocaps.org/](https://nocaps.org/)", "variants": ["nocaps", "nocaps entire", "nocaps in-domain", "nocaps near-domain", "nocaps out-of-domain", "nocaps-XD entire", "nocaps-XD in-domain", "nocaps-XD near-domain", "nocaps-XD out-of-domain"], "title": "Nocaps: novel object captioning at scale"}
{"id": "CrossWOZ", "contents": "**CrossWOZ** is the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides.\r\n\r\nSource: [CrossWOZ](https://github.com/thu-coai/CrossWOZ)", "variants": ["CrossWOZ"], "title": "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset"}
{"id": "AKCES-GEC", "contents": "AKCES-GEC is a new dataset on grammatical error correction for Czech.\r\n\r\nSource: [Grammatical Error Correction in Low-Resource Scenarios](https://arxiv.org/pdf/1910.00353)", "variants": ["AKCES-GEC"], "title": "Grammatical Error Correction in Low-Resource Scenarios"}
{"id": "BlendedMVS", "contents": "**BlendedMVS** is a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. The dataset was created by applying a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, these mesh models were rendered to color images and depth maps. \r\n\r\nSource: [BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks](/paper/blendedmvs-a-large-scale-dataset-for)", "variants": ["BlendedMVS"], "title": "BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks"}
{"id": "MDD", "contents": "Movie Dialog dataset (MDD) is designed to measure how well models can perform at goal and non-goal orientated dialog centered around the topic of movies (question answering, recommendation and discussion).\r\n\r\nSource: [Movie Dialog dataset](https://research.fb.com/downloads/babi/)", "variants": ["MDD"], "title": "Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems"}
{"id": "WikiQAar", "contents": "A publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering.\r\n\r\nSource: [WikiQA: A Challenge Dataset for Open-Domain Question Answering](/paper/wikiqa-a-challenge-dataset-for-open-domain)", "variants": ["WikiQAar"], "title": "WikiQA: A Challenge Dataset for Open-Domain Question Answering"}
{"id": "FixaTons", "contents": "FixaTons is a large collection of datasets human scanpaths (temporally ordered sequences of fixations) and saliency maps. \r\n\r\nSource: [FixaTons: A collection of Human Fixations Datasets and Metrics for Scanpath Similarity](/paper/fixatons-a-collection-of-human-fixations)", "variants": ["FixaTons"], "title": "FixaTons: A collection of Human Fixations Datasets and Metrics for Scanpath Similarity"}
{"id": "MLS", "contents": "The **Multiple Light Source** dataset (**MLS**) is a collection of 24 multiple object scenes each recorded under 18 multiple light source illumination scenarios. The illuminants are varying in dominant spectral colours, intensity and distance from the scene. The dataset can be used for the evaluation of computational colour constancy algorithms. Along with the images of the scenes the spectral characteristics of the camera, light sources and the objects are also provided, and each image includes pixel-by-pixel ground truth annotation of uniformly coloured object surfaces thus making this useful for benchmarking colour-based image segmentation algorithms.\n\nSource: [https://arxiv.org/abs/1908.06126](https://arxiv.org/abs/1908.06126)\nImage Source: [https://github.com/Visillect/mls-dataset](https://github.com/Visillect/mls-dataset)", "variants": ["MLS"], "title": "Multiple Light Source Dataset for Colour Research"}
{"id": "LAD", "contents": "LAD (Large-scale Attribute Dataset) has 78,017 images of 5 super-classes and 230 classes. The image number of LAD is larger than the sum of the four most popular attribute datasets (AwA, CUB, aP/aY and SUN). 359 attributes of visual, semantic and subjective properties are defined and annotated in instance-level.\r\n\r\nSource: [A Large-scale Attribute Dataset for Zero-shot Learning](/paper/a-large-scale-attribute-dataset-for-zero-shot)", "variants": ["LAD"], "title": "A Large-scale Attribute Dataset for Zero-shot Learning"}
{"id": "CURE-TSD", "contents": "Based on simulated challenging conditions that correspond to adversaries that can occur in real-world environments and systems. \r\n\r\nSource: [Traffic Sign Detection under Challenging Conditions: A Deeper Look Into Performance Variations and Spectral Characteristics](/paper/traffic-sign-detection-under-challenging)", "variants": ["CURE-TSD"], "title": "Traffic Sign Detection under Challenging Conditions: A Deeper Look Into Performance Variations and Spectral Characteristics"}
{"id": "VDQG", "contents": "The **Visual Discriminative Question Generation (VDQG)** dataset contains 11202 ambiguous image pairs collected from Visual Genome. Each image pair is annotated with 4.6 discriminative questions and 5.9 non-discriminative questions on average.", "variants": ["VDQG"], "title": "Learning to Disambiguate by Asking Discriminative Questions"}
{"id": "OpeReid", "contents": "The **OpeReid** dataset is a person re-identification dataset that consists of 7,413 images of 200 persons.\n\nSource: [Scalable Metric Learning via Weighted Approximate Rank Component Analysis](https://arxiv.org/abs/1603.00370)", "variants": ["OpeReid"], "title": "Open-set Person Re-identification"}
{"id": "PTC", "contents": "**PTC** is a collection of 344 chemical compounds represented as graphs which report the carcinogenicity for rats. There are 19 node labels for each node.\r\n\r\nSource: [Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity](https://arxiv.org/abs/1904.01098)", "variants": ["PTC"], "title": "Large Scale Holistic Video Understanding"}
{"id": "PASCAL3D+", "contents": "The Pascal3D+ multi-view dataset consists of images in the wild, i.e., images of object categories exhibiting high variability, captured under uncontrolled settings, in cluttered scenes and under many different poses. Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset. These objects are annotated with pose information (azimuth, elevation and distance to camera). Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset.\r\n\r\nSource: [Convolutional Models for Joint Object Categorization and Pose Estimation](https://arxiv.org/abs/1511.05175)\r\nImage Source: [Beyond PASCAL: A benchmark for 3D object detection in the wild](https://doi.org/10.1109/WACV.2014.6836101)", "variants": [" Pascal3D+", "PASCAL3D+"], "title": "Beyond PASCAL: A benchmark for 3D object detection in the wild"}
{"id": "FaceForensics++", "contents": "FaceForensics++ is a forensics dataset consisting of 1000 original video sequences that have been manipulated with four automated face manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The data has been sourced from 977 youtube videos and all videos contain a trackable mostly frontal face without occlusions which enables automated tampering methods to generate realistic forgeries.\r\n\r\nSource: [https://github.com/ondyari/FaceForensics](https://github.com/ondyari/FaceForensics)\r\nImage Source: [https://github.com/ondyari/FaceForensics](https://github.com/ondyari/FaceForensics)", "variants": ["FaceForensics++"], "title": "FaceForensics++: Learning to Detect Manipulated Facial Images"}
{"id": "Reddit TIFU", "contents": "**Reddit TIFU** dataset is a newly collected Reddit dataset, where TIFU denotes the name of /r/tifu subbreddit.\r\nThere are 122,933 text-summary pairs in total.\r\n\r\nSource: [https://github.com/ctr4si/MMN](https://github.com/ctr4si/MMN)", "variants": ["Reddit TIFU"], "title": "Abstractive Summarization of Reddit Posts with Multi-level Memory Networks"}
{"id": "RWWD", "contents": "Real World Worry Dataset (RWWD) captures the emotional responses of UK residents to COVID-19 at a point in time where the impact of the COVID19 situation affected the lives of all individuals in the UK. The data were collected on the 6th and 7th of April 2020, a time at which the UK was under lockdown (news, 2020), and death tolls were increasing. On April 6, 5,373 people in the UK had died of the virus, and 51,608 tested positive. On the day before data collection, the Queen addressed the nation via a television broadcast. Furthermore, it was also announced that Prime Minister Boris Johnson was admitted to intensive care in a hospital for COVID-19 symptoms.\r\n\r\nThe RWWD is a ground truth dataset that used a direct survey method and obtained written accounts of people alongside data of their felt emotions while writing. As such, the dataset does not rely on third-person annotation but can resort to direct self-reported emotions. Two versions of RWWD are presented, each consisting of 2,500 English\r\ntexts representing the participants’ genuine emotional responses to Corona situation in the UK: the Long RWWD consists of texts that were openended in length and asked the participants to express their feelings as they wish. The Short RWWD asked the same people also to express their feelings in Tweet-sized texts. The latter was chosen\r\nto facilitate the use of this dataset for Twitter data research.\r\n\r\nSource: [Measuring Emotions in the COVID-19 Real World Worry Dataset](https://arxiv.org/pdf/2004.04225v2.pdf)", "variants": ["RWWD"], "title": "Measuring Emotions in the COVID-19 Real World Worry Dataset"}
{"id": "Fongbe  audio", "contents": "Fongbe Data collected by Fréjus A. A LALEYE\r\n\r\nThis dataset contains Fongbe speech corpus with audio data and transcriptions. \r\n\r\nSource: [Fongbe dataset](https://github.com/laleye/ALFFA_PUBLIC/tree/master/ASR/FONGBE)", "variants": ["Fongbe  audio"], "title": "First automatic fongbe continuous speech recognition system: Development of acoustic models and language models"}
{"id": "PACS", "contents": "**PACS** is an image dataset for domain generalization. It consists of four domains, namely Photo (1,670 images), Art Painting (2,048 images), Cartoon (2,344 images) and Sketch (3,929 images). Each domain contains seven categories.\r\n\r\nSource: [Deep Domain-Adversarial Image Generation for Domain Generalisation](https://arxiv.org/abs/2003.06054)\r\nImage Source: [https://www.researchgate.net/figure/Sample-images-from-PACS-dataset-Each-row-represents-a-domain-and-each-column-represents_fig1_334695033](https://www.researchgate.net/figure/Sample-images-from-PACS-dataset-Each-row-represents-a-domain-and-each-column-represents_fig1_334695033)", "variants": ["PACS", "PACS-StaQC-py", "PACS-SO-DS", "PACS-CoNaLa"], "title": "Deeper, Broader and Artier Domain Generalization"}
{"id": "HotpotQA", "contents": "**HotpotQA** is a question answering dataset collected on the English Wikipedia, containing about 113K crowd-sourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. \r\n\r\nA diverse range of reasoning strategies are featured in HotpotQA, including questions involving missing entities in the question, intersection questions (What satisfies property A and property B?), and comparison questions, where two entities are compared by a common attribute, among others. In the few-document distractor setting, the QA models are given ten paragraphs in which the gold paragraphs are guaranteed to be found; in the open-domain fullwiki setting, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F1, and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F1). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously.\r\n\r\nSource: [Answering Complex Open-domain Questions Through Iterative Query Generation](https://arxiv.org/abs/1910.07000)\r\nImage Source: [Yang et al](https://arxiv.org/pdf/1809.09600v1.pdf)", "variants": ["HotpotQA"], "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"}
{"id": "MS-ASL", "contents": "**MS-ASL** is a real-life large-scale sign language data set comprising over 25,000 annotated videos.\r\n\r\nSource: [MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American Sign Language](/paper/ms-asl-a-large-scale-data-set-and-benchmark)", "variants": ["MS-ASL"], "title": "MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American Sign Language"}
{"id": "VIST", "contents": "The **Visual Storytelling** Dataset (**VIST**) consists of 210,819 unique photos and 50,000 stories. The images were collected from albums on Flickr. The albums included 10 to 50 images and all the images in an album are taken in a 48-hour span. The stories were created by workers on Amazon Mechanical Turk, where the workers were instructed to choose five images from the album and write a story about them. Every story has five sentences, and every sentence is paired with its appropriate image. The dataset is split into 3 subsets, a training set (80%), a validation set (10%) and a test set (10%). All the words and interpunction signs in the stories are separated by a space character and all the location names are replaced with the word location. All the names of people are replaced with the words male or female depending on the gender of the person.\r\n\r\nSource: [Stories for Images-in-Sequence by using Visual and Narrative Components This research was partially funded by Pendulibrium and the Faculty of computer science and engineering, Ss. Cyril and Methodius University in Skopje.](https://arxiv.org/abs/1805.05622)\r\nImage Source: [https://arxiv.org/pdf/1604.03968.pdf](https://arxiv.org/pdf/1604.03968.pdf)", "variants": ["VIST"], "title": "Visual Storytelling"}
{"id": "ReClor", "contents": "Logical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary language as the definition from Law School Admission Council. **ReClor** is a dataset extracted from logical reasoning questions of standardized graduate admission examinations.\r\n\r\nSource: [ReClor](https://whyu.me/reclor/)", "variants": ["ReClor"], "title": "ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning"}
{"id": "CLINC150", "contents": "This dataset is for evaluating the performance of intent classification systems in the presence of \"out-of-scope\" queries, i.e., queries that do not fall into any of the system-supported intent classes. The dataset includes both in-scope and out-of-scope data.\r\n\r\nSource: [CLINC150](https://github.com/clinc/oos-eval)", "variants": ["CLINC150"], "title": "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction"}
{"id": "Wikidata-Disamb", "contents": "The Wikidata-Disamb dataset is intended to allow a clean and scalable evaluation of NED with Wikidata entries, and to be used as a reference in future research.\r\n\r\nSource: [Named Entity Disambiguation using Deep Learning on Graphs](https://arxiv.org/pdf/1810.09164.pdf)", "variants": ["Wikidata-Disamb"], "title": "Named Entity Disambiguation using Deep Learning on Graphs"}
{"id": "MaSS", "contents": "MaSS (Multilingual corpus of Sentence-aligned Spoken utterances) is an extension of the CMU Wilderness Multilingual Speech Dataset, a speech dataset based on recorded readings of the New Testament.\r\n\r\nMaSS extends it by providing a large and clean dataset of 8,130 parallel spoken utterances across 8 languages (56 language pairs). The covered languages are: Basque, English, Finnish, French, Hungarian, Romanian, Russian and Spanish.\r\n\r\nSource: [MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken Utterances Extracted from the Bible](/paper/mass-a-large-and-clean-multilingual-corpus-of)\r\nImage Source: [https://arxiv.org/pdf/1907.12895v3.pdf](https://arxiv.org/pdf/1907.12895v3.pdf)", "variants": ["MASS SS2", "MaSS"], "title": "MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken Utterances Extracted from the Bible"}
{"id": "ShARC", "contents": "**ShARC** is a Conversational Question Answering dataset focussing on question answering from texts containing rules.\r\n\r\nSource: [ShARC](https://sharc-data.github.io/data.html)\r\nImage Source: [https://arxiv.org/abs/1809.01494](https://arxiv.org/abs/1809.01494)", "variants": ["ShARC"], "title": "Interpretation of Natural Language Rules in Conversational Machine Reading"}
{"id": "Hollywood 3D dataset", "contents": "A dataset for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use of the 3D data, and five new interest point detection strategies are also proposed, that extend to the 3D data. \r\n\r\nSource: [Hollywood 3D: Recognizing Actions in 3D Natural Scenes](/paper/hollywood-3d-recognizing-actions-in-3d)", "variants": ["Hollywood 3D dataset"], "title": "Hollywood 3D: Recognizing Actions in 3D Natural Scenes"}
{"id": "TVQA", "contents": "The **TVQA** dataset is a large-scale vido dataset for video question answering. It is based on 6 popular TV shows (Friends, The Big Bang Theory, How I Met Your Mother, House M.D., Grey's Anatomy, Castle). It includes 152,545 QA pairs from 21,793 TV show clips. The QA pairs are split into the ratio of 8:1:1 for training, validation, and test sets. The TVQA dataset provides the sequence of video frames extracted at 3 FPS, the corresponding subtitles with the video clips, and the query consisting of a question and four answer candidates. Among the four answer candidates, there is only one correct answer.\r\n\r\nSource: [Two-stream Spatiotemporal Feature for Video QA Task](https://arxiv.org/abs/1907.05006)\r\nImage Source: [https://arxiv.org/abs/1809.01696](https://arxiv.org/abs/1809.01696)", "variants": ["TVQA"], "title": "TVQA: Localized, Compositional Video Question Answering"}
{"id": "TRIPOD", "contents": "TRIPOD contains screenplays and plot synopses with turning point (TP) annotations for 99 movies. Each movie contains:\r\n\r\n1. The Wikipedia plot synopsis (extended summary of 35 sentences on average) with sentence-level TP annotations.\r\n2. The screenplay (all dialogue and description parts of the movie) segmented into scenes (selected from the Scriptbase dataset).\r\n3. Gold scene-level TP labels for the screenplays of the test set.\r\n3. The cast information (according to IMDb).\r\n\r\nTRIPOD is extended in [Movie Summarization via Sparse Graph Construction](https://arxiv.org/pdf/2012.07536.pdf) with more movies in the test set (122 now in total) and multimodal features extracted from the full-length movie videos. The multimodal version can be found here: https://datashare.ed.ac.uk/handle/10283/3819", "variants": ["TRIPOD"], "title": "Movie Plot Analysis via Turning Point Identification"}
{"id": "MS-Celeb-1M", "contents": "The **MS-Celeb-1M** dataset is a large-scale face recognition dataset consists of 100K identities, and each identity has about 100 facial images. The original identity labels are obtained automatically from webpages.\r\n\r\n**NOTE**: This dataset [is currently inactive](https://exposing.ai/msceleb/). \r\n\r\nSource: [Learning to Cluster Faces on an Affinity Graph](https://arxiv.org/abs/1904.02749)\r\nImage Source: [MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition](https://arxiv.org/abs/1607.08221)", "variants": ["MS-Celeb-1M"], "title": "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition"}
{"id": "ListOps", "contents": "The ListOps examples are comprised of summary operations on lists of single digit integers, written in prefix notation. The full sequence has a corresponding solution which is\r\nalso a single-digit integer, thus making it a ten-way balanced classification problem. For example, [MAX 2 9 [MIN 4 7 ] 0 ] has the solution 9. Each operation has a corresponding closing square bracket that defines the list of numbers for the operation. In this example, MIN operates on {4, 7}, while MAX operates on {2, 9, 4, 0}. \r\n\r\nSource: [ListOps: A Diagnostic Dataset for Latent Tree Learning](https://arxiv.org/pdf/1804.06028v1.pdf)", "variants": ["ListOps"], "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}
{"id": "DOTA", "contents": "Dota is a large-scale dataset for object detection in aerial images. It can be used to develop and evaluate object detectors in aerial images. It contains 2806 aerial images from different sensors and platforms. Each image is of the size in the range from about 800 × 800 to 4000 × 4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These **DOTA** images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188, 282 instances, each of which is labelled by an arbitrary (8 d.o.f.) quadrilateral.\r\n\r\nSource: [https://captain-whu.github.io/DOTA/index.html](https://captain-whu.github.io/DOTA/index.html)\r\nImage Source: [https://captain-whu.github.io/DOTA/](https://captain-whu.github.io/DOTA/)", "variants": ["DOTA"], "title": "DOTA: A Large-scale Dataset for Object Detection in Aerial Images"}
{"id": "iFakeFaceDB", "contents": "**iFakeFaceDB** is a face image dataset for the study of synthetic face manipulation detection, comprising about 87,000 synthetic face images generated by the Style-GAN model and transformed with the GANprintR approach. All images were aligned and resized to the size of 224 x 224.\n\nSource: [https://github.com/socialabubi/iFakeFaceDB](https://github.com/socialabubi/iFakeFaceDB)\nImage Source: [https://github.com/socialabubi/iFakeFaceDB](https://github.com/socialabubi/iFakeFaceDB)", "variants": ["iFakeFaceDB"], "title": "Real or Fake? Spoofing State-Of-The-Art Face Synthesis Detection Systems."}
{"id": "CORe50", "contents": "CORe50 is a dataset designed for assessing Continual Learning techniques in an Object Recognition context.\r\n\r\nSource: [CORe50: a New Dataset and Benchmark for Continuous Object Recognition](/paper/core50-a-new-dataset-and-benchmark-for)", "variants": ["CORe50"], "title": "CORe50: a New Dataset and Benchmark for Continuous Object Recognition"}
{"id": "SweetRS", "contents": "Uses a  platform with 77 candies and sweets to rank. Over 2000 users submitted over 44000 grades resulting in a matrix with 28% coverage.\r\n\r\nSource: [SweetRS: Dataset for a recommender systems of sweets](/paper/sweetrs-dataset-for-a-recommender-systems-of)", "variants": ["SweetRS"], "title": "SweetRS: Dataset for a recommender systems of sweets"}
{"id": "OpenMIC-2018", "contents": "**OpenMIC-2018** is an instrument recognition dataset containing 20,000 examples of Creative Commons-licensed music available on the [Free Music Archive](http://freemusicarchive.org/). Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform.\n\nSource: [OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition](http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf)\nImage Source: [OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition](http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf)\nAudio Source: [https://zenodo.org/record/1432913](https://zenodo.org/record/1432913)", "variants": ["OpenMIC-2018"], "title": "Cutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity"}
{"id": "QASC", "contents": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/qasc)", "variants": ["QASC"], "title": "QASC: A Dataset for Question Answering via Sentence Composition"}
{"id": "HARRISON", "contents": "HARRISON dataset is a benchmark on hashtag recommendation for real world images in social networks. The HARRISON dataset is a realistic dataset, composed of 57,383 photos from Instagram and an average of 4.5 associated hashtags for each photo.\r\n\r\nSource: [HARRISON: A Benchmark on HAshtag Recommendation for Real-world Images in Social Networks](https://arxiv.org/pdf/1605.05054)", "variants": ["HARRISON"], "title": "HARRISON: A Benchmark on HAshtag Recommendation for Real-world Images in Social Networks"}
{"id": "MOTChallenge", "contents": "The **MOTChallenge** datasets are designed for the task of multiple object tracking. There are several variants of the dataset released each year, such as MOT15, MOT17, MOT20.\r\n\r\nSource: [MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking](https://arxiv.org/pdf/1504.01942v1.pdf)", "variants": ["MOTChallenge", "MOT15", "MOT16", "MOT17", "MOT20"], "title": "MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking"}
{"id": "EDUB-Seg", "contents": "Egocentric Dataset of the University of Barcelona – Segmentation (EDUB-Seg) is a dataset for egocentric event segmentation acquired by the Narrative Clip, which takes a picture every 30 seconds. The dataset contains a total of 18,735 images captured by 7 different users during overall 20 days. To ensure diversity, all users were wearing the camera in different contexts: while attending a conference, on holiday, during the weekend, and during the week.", "variants": ["EDUB-Seg"], "title": "SR-Clustering: Semantic Regularized Clustering for Egocentric Photo Streams Segmentation"}
{"id": "PointDenoisingBenchmark", "contents": "The **PointDenoisingBenchmark** dataset features 28 different shapes, split into 18 training shapes and 10 test shapes.\r\n\r\n* PointDenoisingBenchmark for outliers removal: contains noisy point clouds with different levels of gaussian noise and the corresponding clean ground truths.\r\n* PointDenoisingBenchmark for denoising: contains noisy point clouds with different levels of noise and density of outliers and the corresponding clean ground truths.\r\n\r\nSource: [PointCleanNet: Learning to Denoise and Remove Outliers from Dense Point Clouds](/paper/pointcleannet-learning-to-denoise-and-remove)", "variants": ["PointDenoisingBenchmark"], "title": "PointCleanNet : Learning to Denoise and Remove Outliers from Dense Point Clouds"}
{"id": "RFW", "contents": "To validate the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. \r\n\r\nSource: [Racial Faces in-the-Wild: Reducing Racial Bias by Information Maximization Adaptation Network](/paper/racial-faces-in-the-wild-reducing-racial-bias)", "variants": ["RFW"], "title": "Racial Faces in-the-Wild: Reducing Racial Bias by Deep Unsupervised Domain Adaptation"}
{"id": "TimeTravel", "contents": "TimeTravel contains 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event. \r\n\r\nSource: [Counterfactual Story Reasoning and Generation](/paper/counterfactual-story-reasoning-and-generation)", "variants": ["TimeTravel"], "title": "Counterfactual Story Reasoning and Generation"}
{"id": "BCWS", "contents": "Dataset for evaluating English-Chinese Bilingual Contextual Word Similarity. The dataset consists of 2,091 English-Chinese word pairs with the corresponding sentential contexts and their similarity scores annotated by the human. \r\n\r\nSource: [BCWS: Bilingual Contextual Word Similarity](/paper/bcws-bilingual-contextual-word-similarity)", "variants": ["BCWS"], "title": "BCWS: Bilingual Contextual Word Similarity"}
{"id": "Argoverse", "contents": "**Argoverse** is a tracking benchmark with over 30K scenarios collected in Pittsburgh and Miami. Each scenario is a sequence of frames sampled at 10 HZ. Each sequence has an interesting object called “agent”, and the task is to predict the future locations of agents in a 3 seconds future horizon. The sequences are split into training, validation and test sets, which have 205,942, 39,472 and 78,143 sequences respectively. These splits have no geographical overlap.\r\n\r\nSource: [Learning Lane Graph Representations for Motion Forecasting](https://arxiv.org/abs/2007.13732)\r\nImage Source: [https://arxiv.org/pdf/1911.02620.pdf](https://arxiv.org/pdf/1911.02620.pdf)", "variants": ["Argoverse", "Argoverse CVPR 2020"], "title": "Argoverse: 3D Tracking and Forecasting With Rich Maps"}
{"id": "ASD", "contents": "The Annotated Semantic Dataset is composed of $11$ videos, divided in $3$ activity categories: Biking; Driving and Walking, according to their amount of semantic information. The classes are: $0p$, which represents the videos with approximately no semantic information; $25p$, for the videos containing relevant semantic information in ∼$25%$ of its frames ; the same ideia for the classes $50p$ and $75p$,\r\nThe videos were record using a GoPro Hero 3 camera mounted in a helmet for the Biking and Walking videos and attached to a head strap for the Driving videos.", "variants": ["ASD"], "title": "Towards Semantic Fast-Forward and Stabilized Egocentric Videos"}
{"id": "Real Rain Dataset", "contents": "A large-scale dataset of ~29.5K rain/rain-free image pairs that covers a wide range of natural rain scenes.\r\n\r\nSource: [Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset](https://arxiv.org/pdf/1904.01538v2.pdf)", "variants": ["Real Rain Dataset"], "title": "Spatial Attentive Single-Image Deraining With a High Quality Real Rain Dataset"}
{"id": "Middlebury MVS", "contents": "**Middlebury MVS** is the earliest MVS dataset for multi-view stereo network evaluation. It contains two indoor objects with low-resolution (640 × 480) images and calibrated cameras.\n\nSource: [BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks](https://arxiv.org/abs/1911.10127)\nImage Source: [https://vision.middlebury.edu/mview/data/](https://vision.middlebury.edu/mview/data/)", "variants": ["Middlebury MVS"], "title": "A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms"}
{"id": "ShapeNet", "contents": "**ShapeNet** is a large scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA. The repository contains over 300M models with 220,000 classified into 3,135 classes arranged using WordNet hypernym-hyponym relationships. ShapeNet Parts subset contains 31,693 meshes categorised into 16 common object classes (i.e. table, chair, plane etc.). Each shapes ground truth contains 2-5 parts (with a total of 50 part classes).\r\n\r\nSource: [A review on deep learning techniques for 3D sensed data classification](https://arxiv.org/abs/1907.04444)\r\nImage Source: [ShapeNet: An Information-Rich 3D Model Repository](https://arxiv.org/abs/1512.03012)", "variants": ["ShapeNet", "ShapeNet dataset", "ShapeNet Airplane", "ShapeNet Car", "ShapeNet Chair", "ShapeNet-Part"], "title": "ShapeNet: An Information-Rich 3D Model Repository"}
{"id": "MobilityAids", "contents": "**MobilityAids** is a dataset for perception of people and their mobility aids. The annotated dataset contains five classes: pedestrian, person in wheelchair, pedestrian pushing a person in a wheelchair, person using crutches and person using a walking frame. In total the hospital dataset has over 17, 000 annotated RGB-D images, containing people categorized according to the mobility aids they use. The images were collected in the facilities of the Faculty of Engineering of the University of Freiburg and in a hospital in Frankfurt.\n\nSource: [http://mobility-aids.informatik.uni-freiburg.de/](http://mobility-aids.informatik.uni-freiburg.de/)\nImage Source: [http://mobility-aids.informatik.uni-freiburg.de/](http://mobility-aids.informatik.uni-freiburg.de/)", "variants": ["MobilityAids"], "title": "Deep Detection of People and their Mobility Aids for a Hospital Robot"}
{"id": "AI2D-RST", "contents": "AI2D-RST is a multimodal corpus of 1000 English-language diagrams that represent topics in primary school natural sciences, such as food webs, life cycles, moon phases and human physiology. The corpus is based on the Allen Institute for Artificial Intelligence Diagrams (AI2D) dataset, a collection of diagrams with crowd-sourced descriptions, which was originally developed to support research on automatic diagram understanding and visual question answering. \r\n\r\nSource: [AI2D-RST: A multimodal corpus of 1000 primary school science diagrams](/paper/ai2d-rst-a-multimodal-corpus-of-1000-primary)", "variants": ["AI2D-RST"], "title": "AI2D-RST: A multimodal corpus of 1000 primary school science diagrams"}
{"id": "European Flood 2013 Dataset", "contents": "This dataset consists of 3,710 flood images, annotated by domain experts regarding their relevance with respect to three tasks (determining the flooded area, inundation depth, water pollution).\n\nSource: [https://github.com/cvjena/eu-flood-dataset](https://github.com/cvjena/eu-flood-dataset)\nImage Source: [https://github.com/cvjena/eu-flood-dataset](https://github.com/cvjena/eu-flood-dataset)", "variants": ["European Flood 2013 Dataset"], "title": "Enhancing Flood Impact Analysis using Interactive Retrieval of Social Media Images"}
{"id": "MDID", "contents": "The Multimodal Document Intent Dataset (MDID) is a dataset for computing author intent from multimodal data from Instagram. It contains 1,299 Instagram posts covering a variety of topics, annotated with labels from three taxonomies. The samples are labelled with 7 labels of intent: Provocative, Informative, Advocative, Entertainment, Expositive, Expressive, Promotive\r\n\r\nSource: [Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts](https://arxiv.org/abs/1904.09073)", "variants": ["MDID"], "title": "Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts"}
{"id": "SciTSR", "contents": "**SciTSR** is a large-scale table structure recognition dataset, which contains 15,000 tables in PDF format and their corresponding structure labels obtained from LaTeX source files.\n\nSource: [https://github.com/Academic-Hammer/SciTSR](https://github.com/Academic-Hammer/SciTSR)", "variants": ["SciTSR"], "title": "Complicated Table Structure Recognition"}
{"id": "Colorectal Adenoma", "contents": "Colorectal Adenoma contains 177 whole slide images (156 contain adenoma) gathered and labelled by pathologists from the Department of Pathology, The Chinese PLA General Hospital.\r\n\r\nSource: [https://github.com/ThoroughImages/CAMEL](https://github.com/ThoroughImages/CAMEL)", "variants": ["Colorectal Adenoma"], "title": "CAMEL: A Weakly Supervised Learning Framework for Histopathology Image Segmentation"}
{"id": "decaNLP", "contents": "Natural Language Decathlon Benchmark (decaNLP) is a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. The tasks as cast as question answering over a context.\r\n\r\nSource: [The Natural Language Decathlon: Multitask Learning as Question Answering](https://arxiv.org/pdf/1806.08730v1.pdf)\r\nImage Source: [http://decanlp.com/](http://decanlp.com/)", "variants": ["decaNLP"], "title": "The Natural Language Decathlon: Multitask Learning as Question Answering"}
{"id": "QuickDraw-Extended", "contents": "Consists of 330,000 sketches and 204,000 photos spanning across 110 categories.\r\n\r\nSource: [Doodle to Search: Practical Zero-Shot Sketch-based Image Retrieval](/paper/doodle-to-search-practical-zero-shot-sketch)", "variants": ["QuickDraw-Extended"], "title": "Doodle to Search: Practical Zero-Shot Sketch-Based Image Retrieval"}
{"id": "FaceForensics", "contents": "FaceForensics is a video dataset consisting of more than 500,000 frames containing faces from 1004 videos that can be used to study image or video forgeries. All videos are downloaded from Youtube and are cut down to short continuous clips that contain mostly frontal faces. This dataset has two versions:\r\n\r\n* Source-to-Target: where the authors reenact over 1000 videos with new facial expressions extracted from other videos, which e.g. can be used to train a classifier to detect fake images or videos.\r\n\r\n* Selfreenactment: where the authors use Face2Face to reenact the facial expressions of videos with their own facial expressions as input to get pairs of videos, which e.g. can be used to train supervised generative refinement models.", "variants": ["FaceForensics"], "title": "FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces"}
{"id": "UIT-ViIC", "contents": "UIT-ViIC contains manually written captions for images from Microsoft COCO dataset relating to sports played with ball. UIT-ViIC consists of 19,250 Vietnamese captions for 3,850 images.\r\n\r\nSource: [UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning](https://arxiv.org/pdf/2002.00175)", "variants": ["UIT-ViIC"], "title": "UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning"}
{"id": "Places365", "contents": "The **Places365** dataset is a scene recognition dataset. It is composed of 10 million images comprising 434 scene classes. There are two versions of the dataset: Places365-Standard with 1.8 million train and 36000 validation images from K=365 scene classes, and Places365-Challenge-2016, in which the size of the training set is increased up to 6.2 million extra images, including 69 new scene classes (leading to a total of 8 million train images from 434 scene classes).\r\n\r\nSource: [Semantic-Aware Scene Recognition](https://arxiv.org/abs/1909.02410)\r\nImage Source: [Places](http://places2.csail.mit.edu/index.html)", "variants": ["Places365-Standard", "Places365"], "title": "Semantic-Aware Scene Recognition"}
{"id": "WLASL", "contents": "**WLASL** is a larege video dataset for **Word-Level American Sign Language** (ASL) recognition, which features 2,000 common different words in ASL.\n\nSource: [https://github.com/dxli94/WLASL](https://github.com/dxli94/WLASL)", "variants": ["WLASL"], "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison"}
{"id": "Processed Twitter", "contents": "Processed Twitter is a dataset that is used for Twitter topic recognition.  It contains tweets from 6 different topics.\r\n\r\nSource: [https://arxiv.org/pdf/1908.09931.pdf](https://arxiv.org/pdf/1908.09931.pdf)", "variants": ["Processed Twitter"], "title": "Multi-stage Deep Classifier Cascades for Open World Recognition"}
{"id": "ART Dataset", "contents": "ART consists of over 20k commonsense narrative contexts and 200k explanations.", "variants": ["ART Dataset"], "title": "Abductive Commonsense Reasoning"}
{"id": "PathVQA", "contents": "PathVQA consists of 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness.\r\n\r\nSource: [PathVQA: 30000+ Questions for Medical Visual Question Answering](/paper/pathvqa-30000-questions-for-medical-visual)", "variants": ["PathVQA"], "title": "PathVQA: 30000+ Questions for Medical Visual Question Answering"}
{"id": "TechQA", "contents": "TECHQA is a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size – 600 training, 310 dev, and 490 evaluation question/answer pairs – thus reflecting the cost of creating large labeled datasets with actual data. Consequently, TECHQA is meant to stimulate research in domain adaptation rather than being a resource to build QA systems from scratch. The dataset was obtained by crawling the IBM Developer and IBM DeveloperWorks forums for questions with accepted answers that appear in a published IBM Technote—a technical document that addresses a specific technical issue.\r\n\r\nSource: [The TechQA Dataset](https://arxiv.org/pdf/1911.02984v1.pdf)", "variants": ["TechQA"], "title": "The TechQA Dataset"}
{"id": "NAS-Bench-201", "contents": "**NAS-Bench-201** is a benchmark (and search space) for neural architecture search. Each architecture consists of a predefined skeleton with a stack of the searched cell. In this way, architecture search is transformed into the problem of searching a good cell.\r\n\r\nSource: [NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search](/paper/nas-bench-102-extending-the-scope-of)", "variants": ["NAS-Bench-201, ImageNet-16-120", "NAS-Bench-201", "NAS-Bench-201, CIFAR-10", "NAS-Bench-201, CIFAR-100"], "title": "NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search"}
{"id": "TrecQA", "contents": "**Text Retrieval Conference Question Answering** (**TrecQA**) is a dataset created from the TREC-8 (1999) to TREC-13 (2004) Question Answering tracks. There are two versions of TrecQA: raw and clean. Both versions have the same training set but their development and test sets differ. The commonly used clean version of the dataset excludes questions in development and test sets with no answers or only positive/negative answers. The clean version has 1,229/65/68 questions and 53,417/1,117/1,442 question-answer pairs for the train/dev/test split.\r\n\r\nSource: [A Gated Self-attention Memory Network for Answer Selection](https://arxiv.org/abs/1909.09696)", "variants": ["TrecQA"], "title": "Large-scale Simple Question Answering with Memory Networks"}
{"id": "CASIA V2", "contents": "**CASIA V2** is a dataset for forgery classification. It contains 4795 images, 1701 authentic and 3274 forged.\n\nSource: [Copy-Move Forgery Classification via Unsupervised Domain Adaptation](https://arxiv.org/abs/1911.07932)\nImage Source: [https://www.mdpi.com/1099-4300/21/4/371](https://www.mdpi.com/1099-4300/21/4/371)", "variants": ["CASIA V2"], "title": "A multi-device dataset for urban acoustic scene classification"}
{"id": "Standardized Project Gutenberg Corpus", "contents": "The **Standardized Project Gutenberg Corpus** (SPGC) is an open science approach to a curated version of the complete PG data containing more than 50,000 books and more than 3×109 word-tokens.\n\nSource: [https://arxiv.org/abs/1812.08092](https://arxiv.org/abs/1812.08092)", "variants": ["Standardized Project Gutenberg Corpus"], "title": "A standardized Project Gutenberg corpus for statistical analysis of natural language and quantitative linguistics"}
{"id": "PolarRR", "contents": "PolarRR is a new dataset with more than 100 types of glass in which obtained transmission images are perfectly aligned with input mixed images.\r\n\r\nSource: [Polarized Reflection Removal with Perfect Alignment in the Wild](https://openaccess.thecvf.com/content_CVPR_2020/papers/Lei_Polarized_Reflection_Removal_With_Perfect_Alignment_in_the_Wild_CVPR_2020_paper.pdf)", "variants": ["PolarRR"], "title": "Polarized Reflection Removal with Perfect Alignment in the Wild"}
{"id": "DeepScores", "contents": "DeepScores contains high quality images of musical scores, partitioned into 300,000 sheets of written music that contain symbols of different shapes and sizes. For advancing the state-of-the-art in small objects recognition, and by placing the question of object recognition in the context of scene understanding.\r\n\r\nSource: [DeepScores -- A Dataset for Segmentation, Detection and Classification of Tiny Objects](/paper/deepscores-a-dataset-for-segmentation)\r\nImage Source: [https://tuggeluk.github.io/deepscores/](https://tuggeluk.github.io/deepscores/)", "variants": ["DeepScores"], "title": "DeepScores-A Dataset for Segmentation, Detection and Classification of Tiny Objects"}
{"id": "UT-Kinect", "contents": "The **UT-Kinect** dataset is a dataset for action recognition from depth sequences. The videos were captured using a single stationary Kinect. There are 10 action types: walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, clap hands. There are 10 subjects, Each subject performs each actions twice. Three channels were recorded: RGB, depth and skeleton joint locations. The three channel are synchronized. The framerate is 30f/s.\r\n\r\nSource: [https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html)\r\nImage Source: [https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html)", "variants": ["UT-Kinect"], "title": "View invariant human action recognition using histograms of 3D joints"}
{"id": "Workplace Sexual Harassment", "contents": "The goal of this dataset is to understand how people experience sexism and sexual harassment in the workplace by discovering themes in 2,362 experiences posted on the Everyday Sexism Project's website\n\nSource: [https://arxiv.org/abs/1907.00510](https://arxiv.org/abs/1907.00510)", "variants": ["Workplace Sexual Harassment"], "title": "Hidden in Plain Sight For Too Long: Using Text Mining Techniques to Shine a Light on Workplace Sexism and Sexual Harassment"}
{"id": "MVS1K", "contents": "Contains about 1, 000 videos from 10 queries and their video tags, manual annotations, and associated web images.\r\n\r\nSource: [Query-Aware Sparse Coding for Multi-Video Summarization](/paper/query-aware-sparse-coding-for-multi-video)", "variants": ["MVS1K"], "title": "Query-Aware Sparse Coding for Multi-Video Summarization"}
{"id": "SEWA DB", "contents": "A database of more than 2000 minutes of audio-visual data of 398 people coming from six cultures, 50% female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies.\r\n\r\nSource: [SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild](/paper/sewa-db-a-rich-database-for-audio-visual)", "variants": ["SEWA DB"], "title": "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild"}
{"id": "YouTubeVIS", "contents": "YouTubeVIS is a new dataset tailored for tasks like simultaneous detection, segmentation and tracking of object instances in videos and is collected based on the current largest video object segmentation dataset YouTubeVOS.\r\n\r\nSource: [YouTubeVIS](https://github.com/youtubevos/MaskTrackRCNN)", "variants": ["YouTube-VIS", "YouTube-VIS validation", "YouTubeVIS"], "title": "Video Instance Segmentation"}
{"id": "TUT-SED Synthetic 2016", "contents": "**TUT-SED Synthetic 2016** contains of mixture signals artificially generated from isolated sound events samples. This approach is used to get more accurate onset and offset annotations than in dataset using recordings from real acoustic environments where the annotations are always subjective.\nMixture signals in the dataset are created by randomly selecting and mixing isolated sound events from 16 sound event classes together. The resulting mixtures contains sound events with varying polyphony. All together 994 sound event samples were purchased from Sound Ideas. From the 100 mixtures created, 60% were assigned for training, 20% for testing and 20% for validation. The total amount of audio material in the dataset is 566 minutes.\nDifferent instances of the sound events are used to synthesize the training, validation and test partitions. Mixtures were created by randomly selecting event instance and from it, randomly, a segment of length 3-15 seconds. Between events, random length silent region was introduced. Such tracks were created for four to nine event classes, and were then mixed together to form the mixture signal. As sound events are not consistently active during the samples (e.g. footsteps), automatic signal energy based annotation was applied to obtain accurate event activity within the sample. Annotation of the mixture signal was created by pooling together event activity annotation of used samples.\n\nSource: [https://webpages.tuni.fi/arg/paper/taslp2017-crnn-sed/tut-sed-synthetic-2016](https://webpages.tuni.fi/arg/paper/taslp2017-crnn-sed/tut-sed-synthetic-2016)\nImage Source: [https://arxiv.org/abs/1702.06286](https://arxiv.org/abs/1702.06286)", "variants": ["TUT-SED Synthetic 2016"], "title": "Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection"}
{"id": "Marmara Turkish Coreference Resolution Corpus", "contents": "Describe the Marmara Turkish Coreference Corpus, which is an annotation of the whole METU-Sabanci Turkish Treebank with mentions and coreference chains.\r\n\r\nSource: [Marmara Turkish Coreference Corpus and Coreference Resolution Baseline](/paper/marmara-turkish-coreference-corpus-and)", "variants": ["Marmara Turkish Coreference Resolution Corpus"], "title": "Marmara Turkish Coreference Corpus and Coreference Resolution Baseline"}
{"id": "NCLS", "contents": "Presents two high-quality large-scale CLS datasets based on existing monolingual summarization datasets.\r\n\r\nSource: [NCLS: Neural Cross-Lingual Summarization](/paper/ncls-neural-cross-lingual-summarization)", "variants": ["NCLS"], "title": "NCLS: Neural Cross-Lingual Summarization"}
{"id": "EMNIST", "contents": "**EMNIST** (extended MNIST) has 4 times more data than MNIST. It is a set of handwritten digits with a 28 x 28 format.\r\n\r\nSource: [Domain Discrepancy Measure for Complex Models in Unsupervised Domain Adaptation](https://arxiv.org/abs/1901.10654)\r\nImage Source: [https://arxiv.org/pdf/1803.01900.pdf](https://arxiv.org/pdf/1803.01900.pdf)", "variants": ["EMNIST-Letters", "EMNIST-Digits", "EMNIST-Balanced", "EMNIST"], "title": "EMNIST: an extension of MNIST to handwritten letters"}
{"id": "OpenEDS", "contents": "OpenEDS (Open Eye Dataset) is a large scale data set of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eyefacing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from randomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. \r\n\r\nSource: [OpenEDS: Open Eye Dataset](/paper/190503702)\r\nImage Source: [https://research.fb.com/programs/openeds-challenge](https://research.fb.com/programs/openeds-challenge)", "variants": ["OpenEDS"], "title": "OpenEDS: Open Eye Dataset"}
{"id": "YUD+", "contents": "Additional Vanishing Point Labels for the York Urban Database\r\n\r\nSource: [YUD+](https://github.com/fkluger/yud_plus)", "variants": ["YUD+"], "title": "CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus"}
{"id": "MTG-Jamendo", "contents": "The **MTG-Jamendo** dataset is an open dataset for music auto-tagging. The dataset contains over 55,000 full audio tracks with 195 tags categories (87 genre tags, 40 instrument tags, and 56 mood/theme tags). It is built using music available at Jamendo under Creative Commons licenses and tags provided by content uploaders. All audio is distributed in 320kbps MP3 format.\n\nA subset of the dataset is used in the Emotion and Theme Recognition in Music Task within MediaEval 2019.\n\nSource: [https://mtg.github.io/mtg-jamendo-dataset/](https://mtg.github.io/mtg-jamendo-dataset/)\nAudio Source: [https://essentia.upf.edu/datasets/mtg-jamendo/raw_30s/audio/](https://essentia.upf.edu/datasets/mtg-jamendo/raw_30s/audio/)", "variants": ["MTG-Jamendo"], "title": "Multitask Learning for Frame-level Instrument Recognition"}
{"id": "XTREME Benchmark", "contents": "A multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. \r\n\r\nSource: [XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization](/paper/xtreme-a-massively-multilingual-multi-task)", "variants": ["XTREME Benchmark"], "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"}
{"id": "BioASQ", "contents": "**BioASQ** is a question answering dataset. Instances in the BioASQ dataset are composed of a question (Q), human-annotated answers (A), and the relevant contexts (C) (also called snippets).\r\n\r\nSource: [Transferability of Natural Language Inference to Biomedical Question Answering](https://arxiv.org/abs/2007.00217)\r\nImage Source: [http://participants-area.bioasq.org/datasets/](http://participants-area.bioasq.org/datasets/)", "variants": ["BioASQ"], "title": "An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition"}
{"id": "MVOR", "contents": "Multi-View Operating Room (MVOR) is a dataset recorded during real clinical interventions. It consists of 732 synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR. It also includes the visual challenges present in such environments, such as occlusions and clutter. \r\n\r\nSource: [MVOR: A Multi-view RGB-D Operating Room Dataset for 2D and 3D Human Pose Estimation](/paper/mvor-a-multi-view-rgb-d-operating-room)", "variants": ["MVOR"], "title": "MVOR: A Multi-view RGB-D Operating Room Dataset for 2D and 3D Human Pose Estimation"}
{"id": "FRSign", "contents": "A large-scale and accurate dataset for vision-based railway traffic light detection and recognition.The recordings were made on selected running trains in France and benefited from carefully hand-labeled annotations.\r\n\r\nSource: [FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains](/paper/frsign-a-large-scale-traffic-light-dataset)", "variants": ["FRSign"], "title": "FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains"}
{"id": "UrbanLoco", "contents": "UrbanLoco is a mapping/localization dataset collected in highly-urbanized environments with a full sensor-suite. The dataset includes 13 trajectories collected in San Francisco and Hong Kong, covering a total length of over 40 kilometers.\r\n\r\nSource: [UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes](/paper/urbanloco-a-full-sensor-suite-dataset-for)\r\nImage Source: [https://advdataset2019.wixsite.com/urbanloco](https://advdataset2019.wixsite.com/urbanloco)", "variants": ["UrbanLoco"], "title": "UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes"}
{"id": "ETH3D", "contents": "ETHD is a multi-view stereo benchmark / 3D reconstruction benchmark that covers a variety of indoor and outdoor scenes.\r\nGround truth geometry has been obtained using a high-precision laser scanner.\r\nA DSLR camera as well as a synchronized multi-camera rig with varying field-of-view was used to capture images.\r\n\r\nSource: [A Multi-View Stereo Benchmark With High-Resolution Images and Multi-Camera Videos](/paper/a-multi-view-stereo-benchmark-with-high)", "variants": ["ETH3D"], "title": "A Multi-view Stereo Benchmark with High-Resolution Images and Multi-camera Videos"}
{"id": "AIRS", "contents": "This dataset provides a wide coverage of aerial imagery with 7.5 cm resolution and contains over 220,000 buildings. The task posed for AIRS is defined as roof segmentation. \r\n\r\nSource: [Aerial Imagery for Roof Segmentation: A Large-Scale Dataset towards Automatic Mapping of Buildings](/paper/aerial-imagery-for-roof-segmentation-a-large)", "variants": ["AIRS"], "title": "Aerial Imagery for Roof Segmentation: A Large-Scale Dataset towards Automatic Mapping of Buildings"}
{"id": "NABirds", "contents": "**NABirds** V1 is a collection of 48,000 annotated photographs of the 400 species of birds that are commonly observed in North America. More than 100 photographs are available for each species, including separate annotations for males, females and juveniles that comprise 700 visual categories. This dataset is to be used for fine-grained visual categorization experiments.\r\n\r\nSource: [https://dl.allaboutbirds.org/nabirds](https://dl.allaboutbirds.org/nabirds)\r\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf)", "variants": ["NABirds"], "title": "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection"}
{"id": "CECW", "contents": "The **CECW** dataset is a color-extended version of the Cleanup World (CW) borrowed from the mobile-manipulation robot domain. CW refers to a world equipped with a movable object as well as four rooms in four colors, including \"blue,\" \"green,\" \"red,\" and \"yellow,\" which is designed as a simulation environment where the agent can act based on the instructions received. CW obeys a particular Geometric Linear Temporal Logic (GLTL) to parse commands by grammatical syntax, resulting in a total of 3,382 commands reflecting 39 GLTL expressions.\n\nSource: [https://github.com/MrShininnnnn/CECW](https://github.com/MrShininnnnn/CECW)\nImage Source: [https://github.com/MrShininnnnn/CECW](https://github.com/MrShininnnnn/CECW)", "variants": ["CECW"], "title": "Synonymous Generalization in Sequence-to-Sequence Recurrent Networks"}
{"id": "SemanticPOSS", "contents": "The SemanticPOSS dataset for 3D semantic segmentation contains 2988 various and complicated LiDAR scans with large quantity of dynamic instances. The data is collected in Peking University and uses the same data format as SemanticKITTI.\r\n\r\nSource: [SemanticPOSS: A Point Cloud Dataset with Large Quantity of Dynamic Instances](/paper/semanticposs-a-point-cloud-dataset-with-large)", "variants": ["SemanticPOSS"], "title": "SemanticPOSS: A Point Cloud Dataset with Large Quantity of Dynamic Instances"}
{"id": "EURLEX57K", "contents": "EURLEX57K is a new publicly available legal LMTC dataset, dubbed EURLEX57K, containing 57k English EU legislative documents from the EUR-LEX portal, tagged with ∼4.3k labels (concepts) from the European Vocabulary (EUROVOC).\r\n\r\nSource: [Large-Scale Multi-Label Text Classification on EU Legislation](https://www.aclweb.org/anthology/P19-1636.pdf)", "variants": ["EURLEX57K"], "title": "Large-Scale Multi-Label Text Classification on EU Legislation"}
{"id": "Kinetics-600", "contents": "The **Kinetics-600** is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset.\r\n\r\nSource: [Learning to Localize Actions from Moments](https://arxiv.org/abs/2008.13705)\r\nImage Source: [https://towardsdatascience.com/downloading-the-kinetics-dataset-for-human-action-recognition-in-deep-learning-500c3d50f776](https://towardsdatascience.com/downloading-the-kinetics-dataset-for-human-action-recognition-in-deep-learning-500c3d50f776)", "variants": ["Kinetics-600 12 frames, 64x64", "Kinetics-600 48 frames, 64x64", "Kinetics-600 12 frames, 128x128", "Kinetics-600"], "title": "A Short Note about Kinetics-600"}
{"id": "CLOTH", "contents": "The Cloze Test by Teachers (**CLOTH**) benchmark is a collection of nearly 100,000 4-way multiple-choice cloze-style questions from middle- and high school-level English language exams, where the answer fills a blank in a given text. Each question is labeled with a type of deep reasoning it involves, where the four possible types are grammar, short-term reasoning, matching/paraphrasing, and long-term reasoning, i.e., reasoning over multiple sentences\n\nSource: [Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)\nImage Source: [https://arxiv.org/pdf/1711.03225.pdf](https://arxiv.org/pdf/1711.03225.pdf)", "variants": ["CLOTH"], "title": "Large-scale Cloze Test Dataset Created by Teachers"}
{"id": "Oxford-Affine", "contents": "The **Oxford-Affine** dataset is a small dataset containing 8 scenes with sequence of 6 images per scene. The images in a sequence are related by homographies.\n\nSource: [A Large Dataset for Improving Patch Matching](https://arxiv.org/abs/1801.01466)\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/affine/](https://www.robots.ox.ac.uk/~vgg/data/affine/)", "variants": ["Oxford-Affine"], "title": "A Comparison of Affine Region Detectors"}
{"id": "CliCR", "contents": "CliCR is a new dataset for domain specific reading comprehension used to construct around 100,000 cloze queries from clinical case reports.\r\n\r\nSource: [CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension](https://arxiv.org/pdf/1803.09720v1.pdf)", "variants": ["CliCR"], "title": "CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension"}
{"id": "BotNet", "contents": "The **BotNet** dataset is a set of topological botnet detection datasets forgraph neural networks.\n\nSource: [https://github.com/harvardnlp/botnet-detection](https://github.com/harvardnlp/botnet-detection)\nImage Source: [https://github.com/harvardnlp/botnet-detection](https://github.com/harvardnlp/botnet-detection)", "variants": ["BotNet"], "title": "Automating Botnet Detection with Graph Neural Networks"}
{"id": "DVQA", "contents": "DVQA is a synthetic question-answering dataset on images of bar-charts.", "variants": ["DVQA"], "title": "DVQA: Understanding Data Visualizations via Question Answering"}
{"id": "ShanghaiTech", "contents": "The Shanghaitech dataset is a large-scale crowd counting dataset. It consists of 1198 annotated crowd images. The dataset is divided into two parts, Part-A containing 482 images and Part-B containing 716 images. Part-A is split into train and test subsets consisting of 300 and 182 images, respectively. Part-B is split into train and test subsets consisting of 400 and 316 images. Each person in a crowd image is annotated with one point close to the center of the head. In total, the dataset consists of 330,165 annotated people. Images from Part-A were collected from the Internet, while images from Part-B were collected on the busy streets of Shanghai.\r\n\r\nSource: [Iterative Crowd Counting](https://arxiv.org/abs/1807.09959)\r\n\r\nImage Source: [Li et al](https://www.researchgate.net/figure/Test-images-from-the-ShanghaiTech-A-28-dataset-The-goal-of-this-paper-is-to-calculate_fig1_322652466)", "variants": ["ShanghaiTech", "ShanghaiTech A", "ShanghaiTech B"], "title": "Single-Image Crowd Counting via Multi-Column Convolutional Neural Network"}
{"id": "FewRel 2.0", "contents": "A more challenging task to investigate two aspects of few-shot relation classification models: (1) Can they adapt to a new domain with only a handful of instances? (2) Can they detect none-of-the-above (NOTA) relations?\r\n\r\nSource: [FewRel 2.0: Towards More Challenging Few-Shot Relation Classification](/paper/fewrel-20-towards-more-challenging-few-shot)", "variants": ["FewRel 2.0"], "title": "FewRel 2.0: Towards More Challenging Few-Shot Relation Classification"}
{"id": "ORGaze", "contents": "A new video dataset for OR, with 30, 000 objects over 5, 000 stereo video sequences annotated for their descriptions and gaze.\r\n\r\nSource: [Object Referring in Videos with Language and Human Gaze](/paper/object-referring-in-videos-with-language-and)", "variants": ["ORGaze"], "title": "Object Referring in Videos with Language and Human Gaze"}
{"id": "PMC-SA", "contents": "**PMC-SA** (**PMC Structured Abstracts**) is a dataset of academic publications, used for the task of structured summarization.\n\nSource: [https://arxiv.org/abs/1905.07695](https://arxiv.org/abs/1905.07695)", "variants": ["PMC-SA"], "title": "Structured Summarization of Academic Publications"}
{"id": "FollowUp", "contents": "1000 query triples on 120 tables.\r\n\r\nSource: [FANDA: A Novel Approach to Perform Follow-up Query Analysis](/paper/fanda-a-novel-approach-to-perform-follow-up)", "variants": ["FollowUp"], "title": "FANDA: A Novel Approach to Perform Follow-up Query Analysis"}
{"id": "A*3D", "contents": "The **A*3D** dataset is a step forward to make autonomous driving safer for pedestrians and the public in the real world.\nCharacteristics:\n* 230K human-labeled 3D object annotations in 39,179 LiDAR point cloud frames and corresponding frontal-facing RGB images.\n* Captured at different times (day, night) and weathers (sun, cloud, rain).\n\nSource: [https://github.com/I2RDL2/ASTAR-3D](https://github.com/I2RDL2/ASTAR-3D)\nImage Source: [https://github.com/I2RDL2/ASTAR-3D](https://github.com/I2RDL2/ASTAR-3D)", "variants": ["A*3D"], "title": "A*3D Dataset: Towards Autonomous Driving in Challenging Environments"}
{"id": "MusicNet", "contents": "MusicNet is a collection of 330 freely-licensed classical music recordings, together with over 1 million annotated labels indicating the precise time of each note in every recording, the instrument that plays each note, and the note's position in the metrical structure of the composition. The labels are acquired from musical scores aligned to recordings by dynamic time warping. The labels are verified by trained musicians; we estimate a labeling error rate of 4%. We offer the MusicNet labels to the machine learning and music communities as a resource for training models and a common benchmark for comparing results.\r\n\r\nSource: [MusicNet](https://homes.cs.washington.edu/~thickstn/musicnet.html)", "variants": ["MusicNet"], "title": "Learning Features of Music from Scratch"}
{"id": "LaMem", "contents": "An annotated image memorability dataset to date (with 60,000 labeled images from a diverse array of sources).\r\n\r\nSource: [Understanding and Predicting Image Memorability at a Large Scale](/paper/understanding-and-predicting-image)", "variants": ["LaMem"], "title": "Understanding and Predicting Image Memorability at a Large Scale"}
{"id": "TalkDown", "contents": "**TalkDown** is a labelled dataset for condescension detection in context. The dataset is derived from Reddit, a set of online communities that is diverse in content and tone. The dataset is built from COMMENT and REPLY pairs in which the REPLY targets a specific quoted span (QUOTED) in the COMMENT as being condescending. The dataset contains 3,255 positive (condescend) samples and 3,255 negative ones.\n\nSource: [https://arxiv.org/pdf/1909.11272.pdf](https://arxiv.org/pdf/1909.11272.pdf)", "variants": ["TalkDown"], "title": "TalkDown: A Corpus for Condescension Detection in Context"}
{"id": "ETH BIWI Walking Pedestrians", "contents": "The BIWI Walking Pedestrians dataset consists of walking pedestrians in busy scenarios from a birds eye view.\n\nSource: [https://icu.ee.ethz.ch/research/datsets.html](https://icu.ee.ethz.ch/research/datsets.html)\nImage Source: [https://icu.ee.ethz.ch/research/datsets.html](https://icu.ee.ethz.ch/research/datsets.html)", "variants": ["ETH BIWI Walking Pedestrians"], "title": "You'll never walk alone: Modeling social behavior for multi-target tracking"}
{"id": "ITOP", "contents": "The **ITOP** dataset consists of 40K training and 10K testing depth images for each of the front-view and top-view tracks. This dataset contains depth images with 20 actors who perform 15 sequences each and is recorded by two Asus Xtion Pro cameras. The ground-truth of this dataset is the 3D coordinates of 15 body joints.\n\nSource: [V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map](https://arxiv.org/abs/1711.07399)\nImage Source: [https://www.youtube.com/watch?v=4gPI-GOf9wg](https://www.youtube.com/watch?v=4gPI-GOf9wg)", "variants": [" ITOP front-view", "ITOP top-view", "ITOP front-view", "ITOP"], "title": "Towards Viewpoint Invariant 3D Human Pose Estimation"}
{"id": "SNLI", "contents": "The **SNLI** dataset (**Stanford Natural Language Inference**) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as “entailment”, “neutral”, “contradiction” or “-”, where “-” indicates that an agreement could not be reached.\r\n\r\nSource: [Breaking NLI Systemswith Sentences that Require Simple Lexical Inferences](https://arxiv.org/abs/1805.02266)", "variants": ["SNLI"], "title": "A large annotated corpus for learning natural language inference"}
{"id": "SemClinBr", "contents": "A semantically annotated corpus using clinical texts from multiple medical specialties, document types, and institutions.\r\n\r\nSource: [SemClinBr -- a multi institutional and multi specialty semantically annotated corpus for Portuguese clinical NLP tasks](/paper/semclinbr-a-multi-institutional-and-multi)", "variants": ["SemClinBr"], "title": "SemClinBr -- a multi institutional and multi specialty semantically annotated corpus for Portuguese clinical NLP tasks"}
{"id": "MovieQA", "contents": "The **MovieQA** dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\r\n\r\nSource: [Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents](https://arxiv.org/abs/1804.09412)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-multiple-choice-QA-from-the-MovieQA-dataset-Each-question-has-5_fig2_321379716](https://www.researchgate.net/figure/Examples-of-multiple-choice-QA-from-the-MovieQA-dataset-Each-question-has-5_fig2_321379716)", "variants": ["MovieQA"], "title": "MovieQA: Understanding Stories in Movies through Question-Answering"}
{"id": "Clothing1M", "contents": "**Clothing1M** contains 1M clothing images in 14 classes. It is a dataset with noisy labels, since the data is collected from several online shopping websites and include many mislabelled samples. This dataset also contains 50k, 14k, and 10k images with clean labels for training, validation, and testing, respectively.\r\n\r\nSource: [Label-Noise Robust Generative Adversarial Networks](https://arxiv.org/abs/1811.11165)\r\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf)", "variants": ["Clothing1M"], "title": "Learning from massive noisy labeled data for image classification"}
{"id": "CelebA-HQ", "contents": "The **CelebA-HQ** dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.\r\n\r\nSource: [IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis](https://arxiv.org/abs/1807.06358)\r\nImage Source: [Karras et al](https://arxiv.org/pdf/1710.10196v3.pdf)", "variants": ["CelebA-HQ", "CelebA-HQ 128x128", "CelebA-HQ 64x64", "Celeb-HQ 4x upscaling", "CelebA-HQ 1024x1024", "CelebA-HQ 256x256"], "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation"}
{"id": "DailyDialog", "contents": "**DailyDialog** is a high-quality multi-turn open-domain English dialog dataset. It contains 13,118 dialogues split into a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. On average there are around 8 speaker turns per dialogue with around 15 tokens per turn.\r\n\r\nSource: [http://yanran.li/dailydialog](http://yanran.li/dailydialog)\r\nImage Source: [https://paperswithcode.com/paper/dailydialog-a-manually-labelled-multi-turn/](https://paperswithcode.com/paper/dailydialog-a-manually-labelled-multi-turn/)", "variants": ["DailyDialog"], "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset"}
{"id": "BLVD", "contents": "BLVD is a large scale 5D semantics dataset collected by the Visual Cognitive Computing and Intelligent Vehicles Lab.\r\nThis dataset contains 654 high-resolution video clips owing 120k frames extracted from Changshu, Jiangsu Province, China, where the Intelligent Vehicle Proving Center of China (IVPCC) is located. The frame rate is 10fps/sec for RGB data and 3D point cloud. The dataset contains fully annotated frames which yield 249,129 3D annotations, 4,902 independent individuals for tracking with the length of overall 214,922 points, 6,004 valid fragments for 5D interactive event recognition, and 4,900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime).\r\n\r\nSource: [BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving](/paper/blvd-building-a-large-scale-5d-semantics)", "variants": ["BLVD"], "title": "BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving"}
{"id": "AV Digits Database", "contents": "AV Digits Database is an audiovisual database which contains normal, whispered and silent speech. 53 participants were recorded from 3 different views (frontal, 45 and profile) pronouncing digits and phrases in three speech modes.\r\n\r\nThe database consists of two parts: digits and short phrases. In the first part, participants were asked to read 10 digits, from 0 to 9, in English in random order five times. In case of non-native English speakers this part was also repeated in the participant’s native language. In total, 53 participants (41 males and 12 females) from 16 nationalities, were recorded with a mean age and standard deviation of 26.7 and 4.3 years, respectively.\r\n\r\nIn the second part, participants were asked to read 10 short phrases. The phrases are the same as the ones used in the OuluVS2 database: “Excuse me”, “Goodbye”, “Hello”, “How are you”, “Nice to meet you”, “See you”, “I am sorry”,   “Thank you”, “Have a good time”, “You are welcome”. Again, each phrase was repeated five times in 3 different modes, neutral, whisper and silent speech. Thirty nine participants (32 males and 7 females) were recorded for this part with a mean age and standard deviation of 26.3 and 3.8 years, respectively.\r\n\r\nSource: [AV Digits Database](https://ibug-avs.eu/)", "variants": ["AV Digits Database"], "title": "Visual-Only Recognition of Normal, Whispered and Silent Speech"}
{"id": "FSVQA", "contents": "Full-Sentence Visual Question Answering (FSVQA) dataset, consisting of nearly 1 million pairs of questions and full-sentence answers for images, built by applying a number of rule-based natural language processing techniques to original VQA dataset and captions in the MS COCO dataset.\r\n\r\nSource: [The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)](https://arxiv.org/abs/1609.06657)", "variants": ["FSVQA"], "title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)"}
{"id": "COVID-CT", "contents": "Contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19 CTs. The utility of this dataset is confirmed by a senior radiologist who has been diagnosing and treating COVID-19 patients since the outbreak of this pandemic. \r\n\r\nSource: [COVID-CT-Dataset: A CT Scan Dataset about COVID-19](/paper/covid-ct-dataset-a-ct-scan-dataset-about)", "variants": ["COVID-CT"], "title": "COVID-CT-Dataset: A CT Scan Dataset about COVID-19"}
{"id": "DRCD", "contents": "Delta Reading Comprehension Dataset (DRCD) is an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators.\r\n\r\nSource: [https://github.com/DRCKnowledgeTeam/DRCD](https://github.com/DRCKnowledgeTeam/DRCD)\r\nImage Source: [https://arxiv.org/pdf/1806.00920.pdf](https://arxiv.org/pdf/1806.00920.pdf)", "variants": ["DRCD (Traditional Chinese) Dev", "DRCD (Traditional Chinese)", "DRCD"], "title": "DRCD: a Chinese Machine Reading Comprehension Dataset"}
{"id": "SUN3D", "contents": "**SUN3D** contains a large-scale RGB-D video database, with 8 annotated sequences. Each frame has a semantic segmentation of the objects in the scene and information about the camera pose. It is composed by 415 sequences captured in 254 different spaces, in 41 different buildings. Moreover, some places have been captured multiple times at different moments of the day.\r\n\r\nSource: [A Review on Deep Learning TechniquesApplied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\r\nImage Source: [http://sun3d.cs.princeton.edu/](http://sun3d.cs.princeton.edu/)", "variants": ["SUN3D"], "title": "SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels"}
{"id": "CLUENER2020", "contents": "CLUENER2020 is a well-defined fine-grained dataset for named entity recognition in Chinese. CLUENER2020 contains 10 categories. \r\n\r\nSource: [CLUENER2020: Fine-grained Named Entity Recognition Dataset and Benchmark for Chinese](/paper/cluener2020-fine-grained-name-entity)", "variants": ["CLUENER2020"], "title": "CLUENER2020: Fine-grained Named Entity Recognition Dataset and Benchmark for Chinese"}
{"id": "TACoS Multi-Level Corpus", "contents": "Augments the video-description dataset TACoS with short and single sentence descriptions.\r\n\r\nSource: [Coherent Multi-Sentence Video Description with Variable Level of Detail](/paper/coherent-multi-sentence-video-description)", "variants": ["TACoS Multi-Level Corpus"], "title": "Coherent Multi-Sentence Video Description with Variable Level of Detail"}
{"id": "Oxford Town Center", "contents": "The **Oxford Town Center** dataset is a 5-minute video with 7500 frames annotated, which is divided into 6500 for training and 1000 for testing data for pedestrian detection. The data was recorded from a CCTV camera in Oxford for research and development into activity and face recognition.\n\nSource: [LCrowdV: Generating Labeled Videos for Simulation-based Crowd Behavior Learning](https://arxiv.org/abs/1606.08998)\nImage Source: [https://megapixels.cc/oxford_town_centre/](https://megapixels.cc/oxford_town_centre/)", "variants": ["Oxford Town Center"], "title": "Stable multi-target tracking in real-time surveillance video"}
{"id": "ReviewQA", "contents": "ReviewQA is a question-answering dataset based on hotel reviews. The questions of this dataset are linked to a set of relational understanding competencies that a model is expected to master. Indeed, each question comes with an associated type that characterizes the required competency.\r\n\r\nSource: [ReviewQA: a relational aspect-based opinion reading dataset](/paper/reviewqa-a-relational-aspect-based-opinion)", "variants": ["ReviewQA"], "title": "ReviewQA: a relational aspect-based opinion reading dataset"}
{"id": "LEAF-QA", "contents": "LEAF-QA, a comprehensive dataset of 250,000 densely annotated figures/charts, constructed from real-world open data sources, along with ~2 million question-answer (QA) pairs querying the structure and semantics of these charts. LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community. Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz. FigureQA and DVQA, which present only limited variations in chart data. LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering.\r\n\r\nSource: [LEAF-QA: Locate, Encode & Attend for Figure Question Answering](https://arxiv.org/pdf/1907.12861)", "variants": ["LEAF-QA"], "title": "LEAF-QA: Locate, Encode&Attend for Figure Question Answering"}
{"id": "COCO-QA", "contents": "**COCO-QA** is a dataset for visual question answering. It consists of:\r\n\r\n- 123287 images\r\n- 78736 train questions\r\n- 38948 test questions\r\n- 4 types of questions: object, number, color, location\r\n- Answers are all one-word.\r\n\r\nSource: [Exploring Models and Data for Image Question Answering](https://arxiv.org/pdf/1505.02074v4.pdf)", "variants": ["COCO-QA"], "title": "Exploring Models and Data for Image Question Answering"}
{"id": "M-VAD Names", "contents": "The dataset contains the annotations of characters' visual appearances, in the form of tracks of face bounding boxes, and the associations with characters' textual mentions, when available. The detection and annotation of the visual appearances of characters in each video clip of each movie was achieved through a semi-automatic approach. The released dataset contains more than 24k annotated video clips, including 63k visual tracks and 34k textual mentions, all associated with their character identities.\r\n\r\nSource: [M-VAD Names Dataset](https://github.com/aimagelab/mvad-names-dataset)", "variants": ["M-VAD Names"], "title": "M-VAD Names: a Dataset for Video Captioning with Naming"}
{"id": "DiaBLa", "contents": "A new English-French test set for the evaluation of Machine Translation (MT) for informal, written bilingual dialogue. The test set contains 144 spontaneous dialogues (5,700+ sentences) between native English and French speakers, mediated by one of two neural MT systems in a range of role-play settings. The dialogues are accompanied by fine-grained sentence-level judgments of MT quality, produced by the dialogue participants themselves, as well as by manually normalised versions and reference translations produced a posteriori. \r\n\r\nSource: [DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation](/paper/diabla-a-corpus-of-bilingual-spontaneous)", "variants": ["DiaBLa"], "title": "DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation"}
{"id": "XSum", "contents": "The Extreme Summarization (**XSum**) dataset is a dataset for evaluation of abstractive single-document summarization systems. The goal is to create a short, one-sentence new summary answering the question “What is the article about?”. The dataset consists of 226,711 news articles accompanied with a one-sentence summary. The articles are collected from BBC articles (2010 to 2017) and cover a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts). The official random split contains 204,045 (90%), 11,332 (5%) and 11,334 (5) documents in training, validation and test sets, respectively.\n\nSource: [https://arxiv.org/pdf/1808.08745.pdf](https://arxiv.org/pdf/1808.08745.pdf)\nImage Source: [https://arxiv.org/pdf/1808.08745.pdf](https://arxiv.org/pdf/1808.08745.pdf)", "variants": ["BBC XSum", "XSum", "X-Sum"], "title": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}
{"id": "CCNet", "contents": "CCNet is a dataset extracted from Common Crawl with a different filtering process than for OSCAR. It was built using a language model trained on Wikipedia, in order to filter out bad quality texts such as code or tables. CCNet contains longer documents on average compared to OSCAR with smaller—and often noisier—documents weeded out.\r\n\r\nSource: [Martin et al](https://arxiv.org/pdf/1911.03894.pdf)", "variants": ["CCNet"], "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}
{"id": "Sarcasm Corpus V2", "contents": "The Sarcasm Corpus contains sarcastic and non-sarcastic utterances of three different types, which are balanced with half of the samples being sarcastic and half non-sarcastic.\r\nThe three types are:\r\n\r\n* Generic: 6,520 samples\r\n* Rhetorical Questions: 1,702 samples\r\n* Hyperbole: 1,164 samples\r\n\r\nSource: [Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue](https://arxiv.org/pdf/1709.05404.pdf)", "variants": ["Sarcasm Corpus V2"], "title": "Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue"}
{"id": "FAT", "contents": "Falling Things (FAT) is a dataset for advancing the state-of-the-art in object detection and 3D pose estimation in the context of robotics. It consists of generated photorealistic images with accurate 3D pose annotations for all objects in 60k images.\r\n\r\nThe 60k annotated photos of 21 household objects are taken from the YCB objects set. For each image, the dataset contains the 3D poses, per-pixel class segmentation, and 2D/3D bounding box coordinates for all objects.\r\n\r\nSource: [Falling Things: A Synthetic Dataset for 3D Object Detection and Pose Estimation](/paper/falling-things-a-synthetic-dataset-for-3d)", "variants": ["FAT"], "title": "Falling Things: A Synthetic Dataset for 3D Object Detection and Pose Estimation"}
{"id": "OLID", "contents": "The **OLID** is a hierarchical dataset to identify the type and the target of offensive texts in social media. The dataset is collected on Twitter and publicly available. There are 14,100 tweets in total, in which 13,240 are in the training set, and 860 are in the test set. For each tweet, there are three levels of labels: (A) Offensive/Not-Offensive, (B) Targeted-Insult/Untargeted, (C) Individual/Group/Other. The relationship between them is hierarchical. If a tweet is offensive, it can have a target or no target. If it is offensive to a specific target, the target can be an individual, a group, or some other objects. This dataset is used in the OffensEval-2019 competition in SemEval-2019.\r\n\r\nSource: [Kungfupanda at SemEval-2020 Task 12: BERT-Based Multi-Task Learning for Offensive Language Detection](https://arxiv.org/abs/2004.13432)\r\nImage Source: [https://arxiv.org/pdf/1902.09666.pdf](https://arxiv.org/pdf/1902.09666.pdf)", "variants": ["OLID"], "title": "Predicting the Type and Target of Offensive Posts in Social Media"}
{"id": "DBpedia NIF", "contents": "The dataset provides the content of all articles for 128 Wikipedia languages. The dataset has been further enriched with about 25% more links and selected partitions published as Linked Data. \r\n\r\nSource: [DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus](/paper/dbpedia-nif-open-large-scale-and-multilingual)", "variants": ["DBpedia NIF"], "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus"}
{"id": "Automating Dynamic Consent", "contents": "This dataset is used to evaluate a predictive consent model for users’ information shared in social media. In this task, the goal is to predict whether the users will give their consent to share that data with different hypothetical audiences within a medical context. The dataset is built from information the users posted on Facebook and their consent answers about each piece of information.\n\nSource: [https://github.com/cnorval/automating-dynamic-consent-dataset](https://github.com/cnorval/automating-dynamic-consent-dataset)", "variants": ["Automating Dynamic Consent"], "title": "Automating dynamic consent decisions for the processing of social media data in health research"}
{"id": "CED", "contents": "Contains 50 minutes of footage with both color frames and events. CED features a wide variety of indoor and outdoor scenes.\r\n\r\nSource: [CED: Color Event Camera Dataset](/paper/ced-color-event-camera-dataset)", "variants": ["CED"], "title": "CED: Color Event Camera Dataset"}
{"id": "WikiMovies", "contents": "WikiMovies is a dataset for question answering for movies content. It contains ~100k questions in the movie domain, and was designed to be answerable by using either a perfect KB (based on OMDb),\r\n\r\nSource: [ WikiMovies](https://research.fb.com/downloads/babi/)", "variants": ["WikiMovies"], "title": "Key-Value Memory Networks for Directly Reading Documents"}
{"id": "miniImageNet", "contents": "The **miniImageNet** dataset contains 100 classes randomly chosen from ImageNet ILSVRC-2012 challenge with 600 images of size 84×84 pixels per class. It is split into 64 base classes, 16 validation classes and 20 novel classes\r\n\r\nSource: [Leveraging the Feature Distribution in Transfer-based Few-Shot Learning](https://arxiv.org/abs/2006.03806)", "variants": ["mini-ImageNet - 100-Way", "Mini-ImageNet - 1-Shot Learning", "Mini-ImageNet to CUB - 5 shot learning", "Mini-ImageNet-CUB 5-way (1-shot)", "Mini-ImageNet-CUB 5-way (5-shot)", "Mini-Imagenet 5-way (1-shot)", "Mini-Imagenet 5-way (10-shot)", "Mini-Imagenet 5-way (5-shot)", "miniImagenet", "miniImageNet", "Mini-Imagenet 20-way (1-shot)", "Mini-Imagenet 20-way (5-shot)", "miniImagenet → CUB (5-way 1-shot)", "miniImagenet → CUB (5-way 5-shot)", "Mini-ImageNet - 5-Shot Learning", "Mini-Imagenet 5-way (1-shot) "], "title": "Matching Networks for One Shot Learning"}
{"id": "simply-CLEVR", "contents": "The **simply-CLEVR** dataset aims to provide a benchmark dataset that can be used for transparent quantitative evaluation of explanation methods (aka heatmaps/XAI methods).\nIt is made of simple Visual Question Answering (VQA) questions, which are derived from the original CLEVR task, and where each question is accompanied by two Ground Truth Masks that serve as a basis for evaluating explanations on the input image.\n\nSource: [https://github.com/ahmedmagdiosman/simply-clevr-dataset](https://github.com/ahmedmagdiosman/simply-clevr-dataset)\nImage Source: [https://github.com/ahmedmagdiosman/simply-clevr-dataset](https://github.com/ahmedmagdiosman/simply-clevr-dataset)", "variants": ["simply-CLEVR"], "title": "Towards Ground Truth Evaluation of Visual Explanations"}
{"id": "Cross-Dataset Testbed", "contents": "The Cross-dataset Testbed is a Decaf7 based cross-dataset image classification dataset, which contains 40 categories of images from 3 domains: 3,847 images in Caltech256, 4,000 images in ImageNet, and 2,626 images for SUN. In total there are 10,473 images of 40 categories from these three domains.\n\nSource: [Probability Weighted Compact Feature for Domain Adaptive Retrieval](https://arxiv.org/abs/2003.03293)\nImage Source: [https://sites.google.com/site/crossdataset/](https://sites.google.com/site/crossdataset/)", "variants": ["Cross-Dataset Testbed"], "title": "A Testbed for Cross-Dataset Analysis"}
{"id": "DeepLoc", "contents": "**DeepLoc** is a large-scale urban outdoor localization dataset. The dataset is currently comprised of one scene spanning an area of 110 x 130 m, that a robot traverses multiple times with different driving patterns. The dataset creators use a LiDAR-based SLAM system with sub-centimeter and sub-degree accuracy to compute the pose labels that provided as groundtruth. Poses in the dataset are approximately spaced by 0.5 m which is twice as dense as other relocalization datasets.\r\n\r\nFurthermore, for each image the dataset creators provide pixel-wise semantic segmentation annotations for ten categories: Background, Sky, Road, Sidewalk, Grass, Vegetation, Building, Poles & Fences, Dynamic and Void. The dataset is divided into a train and test splits such that the train set comprises seven loops with alternating driving styles amounting to 2737 images, while the test set comprises three loops with a total of 1173 images. The dataset also contains global GPS/INS data and LiDAR measurements.\r\n\r\nThis dataset can be very challenging for vision based applications such as global localization, camera relocalization, semantic segmentation, visual odometry and loop closure detection, as it contains substantial lighting, weather changes, repeating structures, reflective and transparent glass buildings.\r\n\r\nSource: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)\nImage Source: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)", "variants": ["DeepLoc"], "title": "VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry"}
{"id": "WFLW", "contents": "The **Wider Facial Landmarks in the Wild** or **WFLW** database contains 10000 faces (7500 for training and 2500 for testing) with 98 annotated landmarks. This database also features rich attribute annotations in terms of occlusion, head pose, make-up, illumination, blur and expressions.\r\n\r\nSource: [Deep Entwined Learning Head Pose and Face Alignment Inside an Attentional Cascade with Doubly-Conditional fusion](https://arxiv.org/abs/2004.06558)\r\nImage Source: [https://wywu.github.io/projects/LAB/WFLW.html](https://wywu.github.io/projects/LAB/WFLW.html)", "variants": ["WFLW", "WIDER Face  Hard ", "WIDER Face  Medium ", "WIDER Face  Easy "], "title": "Look at Boundary: A Boundary-Aware Face Alignment Algorithm"}
{"id": "RoboCupSimData", "contents": "A large dataset from games of some of the top teams (from 2016 and 2017) in RoboCup Soccer Simulation League (2D), where teams of 11 robots (agents) compete against each other. \r\n\r\nSource: [RoboCupSimData: A RoboCup soccer research dataset](/paper/robocupsimdata-a-robocup-soccer-research)", "variants": ["RoboCupSimData"], "title": "RoboCupSimData: A RoboCup soccer research dataset"}
{"id": "PoseTrack", "contents": "The **PoseTrack** dataset is a large-scale benchmark for multi-person pose estimation and tracking in videos. It requires not only pose estimation in single frames, but also temporal tracking across frames. It contains 514 videos including 66,374 frames in total, split into 300, 50 and 208 videos for training, validation and test set respectively. For training videos, 30 frames from the center are annotated. For validation and test videos, besides 30 frames from the center, every fourth frame is also annotated for evaluating long range articulated tracking. The annotations include 15 body keypoints location, a unique person id and a head bounding box for each person instance.\r\n\r\nSource: [Simple Baselines for Human Pose Estimation and Tracking](https://arxiv.org/abs/1804.06208)\r\nImage Source: [https://posetrack.net/](https://posetrack.net/)", "variants": ["PoseTrack2017", "PoseTrack2018", "PoseTrack"], "title": "PoseTrack: A Benchmark for Human Pose Estimation and Tracking"}
{"id": "RAVEN", "contents": "RAVEN consists of 1,120,000 images and 70,000 RPM (Raven's Progressive Matrices)\r\nproblems, equally distributed in 7 distinct figure configurations.", "variants": ["RAVEN"], "title": "RAVEN: A Dataset for Relational and Analogical Visual REasoNing"}
{"id": "News Interactions on Globo.com", "contents": "### Context\r\n\r\nThis large dataset with users interactions logs (page views) from a news portal was kindly provided by [Globo.com][1], the most popular news portal in Brazil, for reproducibility of the experiments with CHAMELEON - a meta-architecture for contextual hybrid session-based news recommender systems. The source code was made available at [GitHub][2].\r\n\r\nThe **first version (v1)** ([download][13]) of this dataset was released for reproducibility of the experiments presented in the following paper:\r\n\r\n&gt; Gabriel de Souza Pereira Moreira, Felipe Ferreira, and Adilson Marques da Cunha. 2018.  [News Session-Based Recommendations using Deep Neural Networks][3]. In [3rd Workshop on Deep Learning for Recommender Systems (DLRS 2018)][4], October 6, 2018, Vancouver, BC, Canada. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3270323.3270328\r\n\r\nA **second version (v2)** ([download][14]) of this dataset was made available for reproducibility of the experiments presented in the following paper. Compared to the v1, the only differences are:\r\n\r\n* Included four additional user contextual attributes (click\\_os, click\\_country, click_region, click_referrer_type)\r\n* Removed repeated clicks (clicks in the same articles) within sessions. Those sessions with less than two clicks (minimum for the next-click prediction task) were removed\r\n\r\n&gt; Gabriel de Souza Pereira Moreira, Dietmar Jannach, and Adilson Marques da Cunha. 2019.  [Contextual Hybrid Session-based News Recommendation with Recurrent Neural Networks][15]. arXiv preprint arXiv:1904.10367, 49 pages\r\n\r\nYou are not allowed to use this dataset for commercial purposes, only with academic objectives (like education or research). \r\n**If used for research, please cite the above papers.**\r\n\r\n### Content\r\nThe dataset contains a sample of user interactions (page views) in [G1 news portal][5] from Oct. 1 to 16, 2017, including about 3 million clicks, distributed in more than 1 million sessions from 314,000 users who read more than 46,000 different news articles during that period.\r\n\r\nIt is composed by three files/folders:\r\n\r\n - **clicks.zip** - Folder with CSV files (one per hour), containing user sessions interactions in the news portal.\r\n - **articles_metadata.csv** - CSV file with metadata information about all (364047) published articles \r\n - **articles_embeddings.pickle** Pickle (Python 3) of a NumPy matrix containing the Article Content Embeddings (250-dimensional vectors), trained upon articles' text and metadata by the CHAMELEON's ACR module (see [paper][6] for details) for 364047 published articles.  \r\n P.s. The full text of news articles could not be provided due to license restrictions, but those embeddings can be used by Neural Networks to represent their content. See this [paper][7] for a t-SNE visualization of these embeddings, colored by category.\r\n\r\n### Acknowledgements\r\n\r\nI would like to acknowledge [Globo.com][8] for providing this dataset for this research and for the academic community, in special to [Felipe Ferreira][9] for preparing the original dataset by Globo.com.\r\n\r\n*Dataset banner photo by [rawpixel][10] on [Unsplash][11]*\r\n\r\n### Inspiration\r\n\r\nThis dataset might be very useful if you want to implement and evaluate hybrid and contextual news recommender systems, using both user interactions and articles content and metadata to provide recommendations. You might also use it for analytics, trying to understand how users interactions in a news portal are distributed by user, by article, or by category, for example.\r\n\r\nIf you are interested in a dataset of user interactions on articles with the full text provided, to experiment with some different text representations using NLP, you might want to take a look in this smaller [dataset][12].\r\n\r\n\r\n  [1]: https://www.globo.com/\r\n  [2]: https://github.com/gabrielspmoreira/chameleon_recsys\r\n  [3]: https://arxiv.org/abs/1808.00076\r\n  [4]: https://recsys.acm.org/recsys18/dlrs/\r\n  [5]: http://g1.com.br/\r\n  [6]: https://arxiv.org/abs/1808.00076\r\n  [7]: https://arxiv.org/abs/1808.00076\r\n  [8]: https://www.globo.com/\r\n  [9]: https://www.linkedin.com/in/feliferr/\r\n  [10]: https://unsplash.com/photos/O7lbegmDGEw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\r\n  [11]: https://unsplash.com\r\n  [12]: https://www.kaggle.com/gspmoreira/articles-sharing-reading-from-cit-deskdrop\r\n  [13]: https://www.kaggle.com/gspmoreira/news-portal-user-interactions-by-globocom/downloads/news-portal-user-interactions-by-globocom.zip/1\r\n  [14]: https://www.kaggle.com/gspmoreira/news-portal-user-interactions-by-globocom/downloads/news-portal-user-interactions-by-globocom.zip/2\r\n  [15]: https://arxiv.org/abs/1904.10367", "variants": ["News Interactions on Globo.com"], "title": "Contextual Hybrid Session-based News Recommendation with Recurrent Neural Networks"}
{"id": "GeBioCorpus", "contents": "A high-quality dataset for machine translation evaluation that aims at being one of the first non-synthetic gender-balanced test datasets.\r\n\r\nSource: [GeBioToolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of Wikipedia Biographies](/paper/gebiotoolkit-automatic-extraction-of-gender)", "variants": ["GeBioCorpus"], "title": "GeBioToolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of Wikipedia Biographies"}
{"id": "fastMRI", "contents": "The **fastMRI** dataset includes two types of MRI scans: knee MRIs and the brain (neuro) MRIs, and containing training, validation, and masked test sets.\r\nThe deidentified imaging dataset provided by NYU Langone comprises raw k-space data in several sub-dataset groups. Curation of these data are part of an IRB approved study. Raw and DICOM data have been deidentified via conversion to the vendor-neutral ISMRMD format and the RSNA clinical trial processor, respectively. Also, each DICOM image is manually inspected for the presence of any unexpected protected health information (PHI), with spot checking of both metadata and image content.\r\n**Knee MRI**: Data from more than 1,500 fully sampled knee MRIs obtained on 3 and 1.5 Tesla magnets and DICOM images from 10,000 clinical knee MRIs also obtained at 3 or 1.5 Tesla. The raw dataset includes coronal proton density-weighted images with and without fat suppression. The DICOM dataset contains coronal proton density-weighted with and without fat suppression, axial proton density-weighted with fat suppression, sagittal proton density, and sagittal T2-weighted with fat suppression.\r\n**Brain MRI**: Data from 6,970 fully sampled brain MRIs obtained on 3 and 1.5 Tesla magnets. The raw dataset includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent.\r\n\r\nSource: [https://fastmri.med.nyu.edu/](https://fastmri.med.nyu.edu/)\r\nImage Source: [https://fastmri.med.nyu.edu/](https://fastmri.med.nyu.edu/)", "variants": ["fastMRI"], "title": "fastMRI: An Open Dataset and Benchmarks for Accelerated MRI"}
{"id": "Part Whole Relations", "contents": "The Part-Whole Relations dataset is a dataset of semantic relations between entities. It contains the following subtypes:\n- Component-Of\n- Member-Of\n- Portion-Of\n- Stuff-Of\n- Located-In\n- Contained-In\n- Phase-Of\n- Participates-In\n\nSource: [https://github.com/pvthuy/part-whole-relations](https://github.com/pvthuy/part-whole-relations)", "variants": ["Part Whole Relations"], "title": "Ranking-Based Automatic Seed Selection and Noise Reduction for Weakly Supervised Relation Extraction"}
{"id": "QMUL-SurvFace", "contents": "**QMUL-SurvFace** is a surveillance face recognition benchmark that contains 463,507 face images of 15,573 distinct identities captured in real-world uncooperative surveillance scenes over wide space and time.\r\n\r\nSource: [Surveillance Face Recognition Challenge](/paper/surveillance-face-recognition-challenge)", "variants": ["QMUL-SurvFace"], "title": "Surveillance Face Recognition Challenge"}
{"id": "Street Dataset", "contents": "A real-world image dataset that contains more than 900 images generated from 26 street cameras and 7 object categories annotated with detailed bounding box. The data distribution is non-IID and unbalanced, reflecting the characteristic real-world federated learning scenarios.\r\n\r\nSource: [Real-World Image Datasets for Federated Learning](/paper/real-world-image-datasets-for-federated)", "variants": ["Street Dataset"], "title": "Real-World Image Datasets for Federated Learning"}
{"id": "Contract Discovery", "contents": "A new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed, where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts.\r\n\r\nSource: [Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines](/paper/searching-for-legal-clauses-by-analogy-few)", "variants": ["Contract Discovery"], "title": "Searching for Legal Clauses by Analogy. Few-shot Semantic Retrieval Shared Task"}
{"id": "PhotoBook", "contents": "A large-scale collection of visually-grounded, task-oriented dialogues in English designed to investigate shared dialogue history accumulating during conversation.\r\n\r\nSource: [The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue](/paper/the-photobook-dataset-building-common-ground)", "variants": ["PhotoBook"], "title": "The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue"}
{"id": "Aachen Day-Night", "contents": "**Aachen Day-Night** is a dataset designed for benchmarking 6DOF outdoor visual localization in changing conditions. It focuses on localizing high-quality night-time images against a day-time 3D model. There are 14,607 images with changing conditions of weather, season and day-night cycles.\r\n\r\nSource: [Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions](/paper/benchmarking-6dof-outdoor-visual-localization)", "variants": ["Aachen Day-Night benchmark", "Aachen Day-Night"], "title": "Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions"}
{"id": "CITE", "contents": "CITE is a crowd-sourced resource for multimodal discourse: this resource characterises inferences in image-text contexts in the domain of cooking recipes in the form of coherence relations.\r\n\r\nSource: [CITE: A Corpus of Image-Text Discourse Relations](/paper/cite-a-corpus-of-image-text-discourse)", "variants": ["CiteULike", "CITE"], "title": "CITE: A Corpus of Image-Text Discourse Relations"}
{"id": "Occluded REID", "contents": "**Occluded REID** is an occluded person dataset captured by mobile cameras, consisting of 2,000 images of 200 occluded persons (see Fig. (c)). Each identity has 5 full-body person images and 5 occluded person images with different types of occlusion.\n\nSource: [Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification](https://arxiv.org/abs/1904.04975)\nImage Source: [https://github.com/tinajia2012/ICME2018_Occluded-Person-Reidentification_datasets](https://github.com/tinajia2012/ICME2018_Occluded-Person-Reidentification_datasets)", "variants": ["Occluded REID"], "title": "Occluded Person Re-identification"}
{"id": "SpatialVOC2K", "contents": "A multilingual image dataset with spatial relation annotations and object features for image-to-text generation, built using 2,026 images from the PASCAL VOC2008 dataset. \r\n\r\nSource: [SpatialVOC2K: A Multilingual Dataset of Images with Annotations and Features for Spatial Relations between Objects](/paper/spatialvoc2k-a-multilingual-dataset-of-images)", "variants": ["SpatialVOC2K"], "title": "SpatialVOC2K: A Multilingual Dataset of Images with Annotations and Features for Spatial Relations between Objects"}
{"id": "NEWSROOM", "contents": "CORNELL NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining extraction and abstraction.\r\n\r\nSource: [CORNELL NEWSROOM](http://lil.nlp.cornell.edu/newsroom/)", "variants": ["NEWSROOM"], "title": "Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies"}
{"id": "THEODORE", "contents": "Recent work about synthetic indoor datasets from perspective views has shown significant improvements of object detection results with Convolutional Neural Networks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale indoor dataset containing 100,000 high- resolution diversified fisheye images with 14 classes. To this end, we create 3D virtual environments of living rooms, different human characters and interior textures. Beside capturing fisheye images from virtual environments we create annotations for semantic segmentation, instance masks and bounding boxes for object detection tasks. We compare our synthetic dataset to state of the art real-world datasets for omnidirectional images. Based on MS COCO weights, we show that our dataset is well suited for fine-tuning CNNs for object detection. Through a high generalization of our models by means of image synthesis and domain randomization we reach an AP up to 0.84 for class person on High-Definition Analytics dataset.", "variants": ["THEODORE"], "title": "Learning from THEODORE: A Synthetic Omnidirectional Top-View Indoor Dataset for Deep Transfer Learning"}
{"id": "VizDoom", "contents": "ViZDoom is an AI research platform based on the classical First Person Shooter game Doom. The most popular game mode is probably the so-called Death Match, where several players join in a maze and fight against each other. After a fixed time, the match ends and all the players are ranked by the FRAG scores defined as kills minus suicides. During the game, each player can access various observations, including the first-person view screen pixels, the corresponding depth-map and segmentation-map (pixel-wise object labels), the bird-view maze map, etc. The valid actions include almost all the keyboard-stroke and mouse-control a human player can take, accounting for moving, turning, jumping, shooting, changing weapon, etc. ViZDoom can run a game either synchronously or asynchronously, indicating whether the game core waits until all players’ actions are collected or runs in a constant frame rate without waiting.\r\n\r\nSource: [Arena: a toolkit for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1907.09467)\r\nImage Source: [https://github.com/mwydmuch/ViZDoom](https://github.com/mwydmuch/ViZDoom)", "variants": ["ViZDoom Basic Scenario", "VizDoom"], "title": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning"}
{"id": "DMQA", "contents": "The DeepMind Q&A Dataset consists of two datasets for Question Answering, CNN and DailyMail. Each dataset contains many documents (90k and 197k each), and each document companies on average 4 questions approximately. Each question is a sentence with one missing word/phrase which can be found from the accompanying document/context.", "variants": ["DMQA"], "title": "Teaching Machines to Read and Comprehend"}
{"id": "PASCAL-Person-Part", "contents": "**PASCAL-Person-Part** is a dataset for human semantic part segmentation. It consists of a set of additional annotations for PASCAL VOC 2010. It contains multiple humans per image in unconstrained poses and occlusions (1,716 for training and 1,817 for testing). It provides careful pixel-wise annotations for six body parts (i.e., head, torso, upper/lower-arms, and upper-/lower-legs).\r\n\r\nSource: [The Ultimate Theory of Human Parsing](https://arxiv.org/abs/2001.06804)\r\nImage Source: [https://www.researchgate.net/profile/Zhedong_Zheng/publication/328123707/figure/fig4/AS:704683136016384@1545020960225/Qualitative-parsing-results-on-the-Pascal-Person-Part-dataset.png](https://www.researchgate.net/profile/Zhedong_Zheng/publication/328123707/figure/fig4/AS:704683136016384@1545020960225/Qualitative-parsing-results-on-the-Pascal-Person-Part-dataset.png)", "variants": ["PASCAL-Person-Part"], "title": "Detect What You Can: Detecting and Representing Objects Using Holistic Models and Body Parts"}
{"id": "Multi30k", "contents": "Multi30K is a dataset to stimulate multilingual multimodal research for English-German. It is based on the Flickr30k dataset, which contains 31,014 images sourced from online photo-sharing websites. Each image is paired with\r\nfive English descriptions, which were collected from Amazon Mechanical Turk. The dataset contains 145,000 training, 5,070 development, and 5,000 test descriptions. The Multi30K dataset extends the Flickr30K dataset with translated and independent German sentences.\r\n\r\nSource: [Multi30K: Multilingual English-German Image Descriptions](https://arxiv.org/pdf/1605.00459v1.pdf)", "variants": ["Multi30K", "Multi30k"], "title": "Multi30K: Multilingual English-German Image Descriptions"}
{"id": "Natural Stories", "contents": "The Natural Stories dataset consists of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected parse trees and includes self-paced reading time data.\r\n\r\nSource: [The Natural Stories Corpus](/paper/the-natural-stories-corpus)", "variants": ["Natural Stories"], "title": "The Natural Stories Corpus"}
{"id": "NIND", "contents": "An open dataset of real photographs with real noise, from identical scenes captured with varying ISO values.\nMost images are taken with a Fujifilm X-T1 and XF18-55mm, other photographers are encouraged to contribute images for a more diverse crowdsourced effort.\n\nSource: [https://commons.wikimedia.org/wiki/Natural_Image_Noise_Dataset](https://commons.wikimedia.org/wiki/Natural_Image_Noise_Dataset)\nImage Source: [https://commons.wikimedia.org/wiki/Natural_Image_Noise_Dataset](https://commons.wikimedia.org/wiki/Natural_Image_Noise_Dataset)", "variants": ["NIND"], "title": "Natural Image Noise Dataset"}
{"id": "ActivityNet Entities", "contents": "ActivityNet-Entities, augments the challenging ActivityNet Captions dataset with 158k bounding box annotations, each grounding a noun phrase. This allows training video description models with this data, and importantly, evaluate how grounded or \"true\" such model are to the video they describe.\n\nSource: [https://github.com/facebookresearch/ActivityNet-Entities](https://github.com/facebookresearch/ActivityNet-Entities)\nImage Source: [https://github.com/facebookresearch/ActivityNet-Entities](https://github.com/facebookresearch/ActivityNet-Entities)", "variants": ["ActivityNet Entities"], "title": "Grounded Video Description"}
{"id": "CATER", "contents": "Rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning.\r\n\r\nSource: [CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning](/paper/cater-a-diagnostic-dataset-for-compositional)\r\n\r\nImage Source: [CATER](https://rohitgirdhar.github.io/CATER/)", "variants": ["CATER"], "title": "CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning"}
{"id": "MLFP", "contents": "The **MLFP** dataset consists of face presentation attacks captured with seven 3D latex masks and three 2D print attacks. The dataset contains videos captured from color, thermal and infrared channels.\n\nSource: [Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks](https://arxiv.org/abs/2007.11457)\nImage Source: [http://iab-rubric.org/papers/2017_cvprw_18.pdf](http://iab-rubric.org/papers/2017_cvprw_18.pdf)", "variants": ["MLFP"], "title": "Face Presentation Attack with Latex Masks in Multispectral Videos"}
{"id": "Fluent Speech Commands", "contents": "Fluent Speech Commands is an open source audio dataset for spoken language understanding (SLU) experiments. Each utterance is labeled with \"action\", \"object\", and \"location\" values; for example, \"turn the lights on in the kitchen\" has the label {\"action\": \"activate\", \"object\": \"lights\", \"location\": \"kitchen\"}. A model must predict each of these values, and a prediction for an utterance is deemed to be correct only if all values are correct. \r\n\r\nThe task is very simple, but the dataset is large and flexible to allow for many types of experiments: for instance, one can vary the number of speakers, or remove all instances of a particular sentence and test whether a model trained on the remaining sentences can generalize.", "variants": ["Fluent Speech Commands"], "title": "Speech Model Pre-training for End-to-End Spoken Language Understanding"}
{"id": "TinyPerson", "contents": "**TinyPerson** is a benchmark for tiny object detection in a long distance and with massive backgrounds. The images in TinyPerson are collected from the Internet. First, videos with a high resolution are collected from different websites. Second, images from the video are sampled every 50 frames. Then images with a certain repetition (homogeneity) are deleted, and the resulting images are annotated with 72,651 objects with bounding boxes by hand.\n\nSource: [https://arxiv.org/abs/1912.10664](https://arxiv.org/abs/1912.10664)\nImage Source: [https://arxiv.org/pdf/1912.10664.pdf](https://arxiv.org/pdf/1912.10664.pdf)", "variants": ["TinyPerson"], "title": "Scale Match for Tiny Person Detection"}
{"id": "SBU Captions Dataset", "contents": "A collection that allows researchers to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results.\r\n\r\nSource: [Im2Text: Describing Images Using 1 Million Captioned Photographs](/paper/im2text-describing-images-using-1-million)", "variants": ["SBU Captions Dataset"], "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}
{"id": "iQIYI-VID", "contents": "iQIYI-VID dataset, which comprises video clips from iQIYI variety shows, films, and television dramas. The whole dataset contains 500,000 videos clips of 5,000 celebrities. The length of each video is 1~30 seconds. \r\n\r\nSource: [iQIYI-VID](http://challenge.ai.iqiyi.com/detail?raceId=5afc36639689443e8f815f9e)", "variants": ["iQIYI-VID"], "title": "iQIYI-VID: A Large Dataset for Multi-modal Person Identification"}
{"id": "VizWiz", "contents": "The **VizWiz**-VQA dataset originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. The proposed challenge addresses the following two tasks for this dataset: predict the answer to a visual question and (2) predict whether a visual question cannot be answered.\r\n\r\nSource: [https://vizwiz.org/tasks-and-datasets/vqa/](https://vizwiz.org/tasks-and-datasets/vqa/)\r\nImage Source: [https://vizwiz.org/tasks-and-datasets/vqa/](https://vizwiz.org/tasks-and-datasets/vqa/)", "variants": ["VizWiz", "VizWiz 2018", "VizWiz 2018 Answerability", "VizWiz 2020 Answerability", "VizWiz 2020 VQA", "VizWiz 2020 test", "VizWiz 2020 test-dev"], "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"}
{"id": "IJB-C", "contents": "The **IJB-C** dataset is a video-based face recognition dataset. It is an extension of the IJB-A dataset with about 138,000 face images, 11,000 face videos, and 10,000 non-face images.\r\n\r\nSource: [Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results](https://arxiv.org/abs/1804.10275)\r\nImage Source: [https://noblis.org/wp-content/uploads/2018/03/icb2018.pdf](https://noblis.org/wp-content/uploads/2018/03/icb2018.pdf)", "variants": ["IJB-C"], "title": "IARPA Janus Benchmark - C: Face Dataset and Protocol"}
{"id": "AI2D", "contents": "AI2 Diagrams (AI2D) is a dataset of over 5000 grade school science diagrams with over 150000 rich annotations, their ground truth syntactic parses, and more than 15000 corresponding multiple choice questions.\r\n\r\nSource: [A Diagram Is Worth A Dozen Images](https://arxiv.org/pdf/1603.07396.pdf)\r\nImage Source: [Kembhavi et al](https://arxiv.org/pdf/1603.07396.pdf)", "variants": ["AI2D"], "title": "A Diagram Is Worth A Dozen Images"}
{"id": "Industrial Benchmark", "contents": "A benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. \r\n\r\nSource: [A Benchmark Environment Motivated by Industrial Control Problems](/paper/a-benchmark-environment-motivated-by)", "variants": ["Industrial Benchmark"], "title": "A Benchmark Environment Motivated by Industrial Control Problems"}
{"id": "ICDAR 2013", "contents": "The **ICDAR 2013** dataset consists of 229 training images and 233 testing images, with word-level annotations provided. It is the standard benchmark dataset for evaluating near-horizontal text detection.\r\n\r\nSource: [Single Shot Text Detector with Regional Attention](https://arxiv.org/abs/1709.00138)\r\nImage Source: [https://plos.figshare.com/articles/Detection_examples_of_the_proposed_method_on_the_ICDAR_2013_dataset_17_/5325856](https://plos.figshare.com/articles/Detection_examples_of_the_proposed_method_on_the_ICDAR_2013_dataset_17_/5325856)", "variants": ["ICDAR 2013", "ICDAR2013"], "title": "ICDAR 2013 Robust Reading Competition"}
{"id": "MEx", "contents": "A multi-sensor, multi-modal dataset, implemented to benchmark Human Activity Recognition(HAR) and Multi-modal Fusion algorithms. Collection of this dataset was inspired by the need for recognising and evaluating quality of exercise performance to support patients with Musculoskeletal Disorders(MSD).\r\n\r\nSource: [MEx: Multi-modal Exercises Dataset for Human Activity Recognition](/paper/mex-multi-modal-exercises-dataset-for-human)", "variants": ["MEx"], "title": "MEx: Multi-modal Exercises Dataset for Human Activity Recognition"}
{"id": "MLMA Hate Speech", "contents": "A new multilingual multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art multilingual multitask learning approaches.\r\n\r\nSource: [Multilingual and Multi-Aspect Hate Speech Analysis](/paper/multilingual-and-multi-aspect-hate-speech)", "variants": ["MLMA Hate Speech"], "title": "Multilingual and Multi-Aspect Hate Speech Analysis"}
{"id": "WikiDataSets", "contents": "Topical subsets of WikiData, assembled using the WikiDataSets python library. Extracted from Wikidata in April 2020.\r\n\r\nSource: [WikiDataSets](https://graphs.telecom-paris.fr/Home_page.html#wikidatasets-section)", "variants": ["WikiDataSets"], "title": "WikiDataSets : Standardized sub-graphs from WikiData"}
{"id": "NoReC", "contents": "The Norwegian Review Corpus (NoReC) was created for the purpose of training and evaluating models for document-level sentiment analysis. More than 43,000 full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review is labeled with a manually assigned score of 1–6, as provided by the rating of the original author.\r\n\r\nSource: [NoReC: The Norwegian Review Corpus](/paper/norec-the-norwegian-review-corpus)", "variants": ["NoReC"], "title": "NoReC: The Norwegian Review Corpus"}
{"id": "UCO-LAEO", "contents": "A dataset for building models that detect people Looking At Each Other (LAEO) in video sequences.\r\n\r\nSource: [LAEO-Net: revisiting people Looking At Each Other in videos](/paper/laeo-net-revisiting-people-looking-at-each-1)", "variants": ["UCO-LAEO"], "title": "LAEO-Net: Revisiting People Looking at Each Other in Videos"}
{"id": "VehicleID", "contents": "The “**VehicleID**” dataset contains CARS captured during the daytime by multiple real-world surveillance cameras distributed in a small city in China. There are 26,267 vehicles (221,763 images in total) in the entire dataset. Each image is attached with an id label corresponding to its identity in real world. In addition, the dataset contains manually labelled 10319 vehicles (90196 images in total) of their vehicle model information(i.e.“MINI-cooper”, “Audi A6L” and “BWM 1 Series”).\r\n\r\nSource: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)\r\nImage Source: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)", "variants": ["VehicleID", "VehicleID Small", "VehicleID Medium", "VehicleID Large"], "title": "Deep Relative Distance Learning: Tell the Difference between Similar Vehicles"}
{"id": "JHU-CROWD++", "contents": "JHU-CROWD++ is A large-scale unconstrained crowd counting dataset with 4,372 images and 1.51 million annotations. This dataset is collected under a variety of diverse scenarios and environmental conditions. In addition, the dataset provides comparatively richer set of annotations like dots, approximate bounding boxes, blur levels, etc.", "variants": ["JHU-CROWD++"], "title": "JHU-CROWD++: Large-Scale Crowd Counting Dataset and A Benchmark Method"}
{"id": "VeRi-776", "contents": "**VeRi-776** is a vehicle re-identification dataset which contains 49,357 images of 776 vehicles from 20 cameras. The dataset is collected in the real traffic scenario, which is close to the setting of CityFlow. The dataset contains bounding boxes, types, colors and brands.\r\n\r\nSource: [VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification](https://arxiv.org/abs/2004.06305)\r\nImage Source: [https://vehiclereid.github.io/VeRi/](https://vehiclereid.github.io/VeRi/)", "variants": ["VeRi-776"], "title": "A Deep Learning-Based Approach to Progressive Vehicle Re-identification for Urban Surveillance"}
{"id": "Famulus", "contents": "This is a dataset for segmentation and classification of epistemic activities in diagnostic reasoning texts.\r\n\r\nSource: [Analysis of Automatic Annotation Suggestions for Hard Discourse-Level Tasks in Expert Domains](/paper/analysis-of-automatic-annotation-suggestions)", "variants": ["Famulus"], "title": "Analysis of Automatic Annotation Suggestions for Hard Discourse-Level Tasks in Expert Domains"}
{"id": "SUIM", "contents": "The Segmentation of Underwater IMagery (SUIM) dataset contains over 1500 images with pixel annotations for eight object categories: fish (vertebrates), reefs (invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and sea-floor. The images have been rigorously collected during oceanic explorations and human-robot collaborative experiments, and annotated by human participants.\r\n\r\nSource: [Semantic Segmentation of Underwater Imagery: Dataset and Benchmark](https://arxiv.org/abs/2004.01241)\r\nImage Source: [http://irvlab.cs.umn.edu/resources/suim-dataset](http://irvlab.cs.umn.edu/resources/suim-dataset)", "variants": ["SUIM"], "title": "Semantic Segmentation of Underwater Imagery: Dataset and Benchmark"}
{"id": "DomainNet", "contents": "**DomainNet** is a dataset of common objects in six different domain. All domains include 345 categories (classes) of objects such as Bracelet, plane, bird and cello. The domains include clipart: collection of clipart images; real: photos and real world images; sketch: sketches of specific objects; infograph: infographic images with specific object; painting artistic depictions of objects in the form of paintings and quickdraw: drawings of the worldwide players of game “Quick Draw!”.\r\n\r\nSource: [What is being transferred in transfer learning?](https://arxiv.org/abs/2008.11687)\r\nImage Source: [http://ai.bu.edu/M3SDA/](http://ai.bu.edu/M3SDA/)", "variants": ["DomainNet"], "title": "Moment Matching for Multi-Source Domain Adaptation"}
{"id": "LoDoPaB-CT", "contents": "LoDoPaB-CT is a dataset of computed tomography images and simulated low-dose measurements. It contains over 40,000 scan slices from around 800 patients selected from the LIDC/IDRI Database. \r\n\r\nSource: [The LoDoPaB-CT Dataset: A Benchmark Dataset for Low-Dose CT Reconstruction Methods](/paper/the-lodopab-ct-dataset-a-benchmark-dataset)", "variants": ["LoDoPaB-CT"], "title": "The LoDoPaB-CT Dataset: A Benchmark Dataset for Low-Dose CT Reconstruction Methods"}
{"id": "RCTW-17", "contents": "Features a large-scale dataset with 12,263 annotated images. Two tasks, namely text localization and end-to-end recognition, are set up. The competition took place from January 20 to May 31, 2017. 23 valid submissions were received from 19 teams.\r\n\r\nSource: [ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17)](/paper/icdar2017-competition-on-reading-chinese-text)", "variants": ["RCTW-17"], "title": "ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17)"}
{"id": "CONCODE", "contents": "A new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment.\r\n\r\nSource: [Mapping Language to Code in Programmatic Context](/paper/mapping-language-to-code-in-programmatic)", "variants": ["CONCODE"], "title": "Mapping Language to Code in Programmatic Context"}
{"id": "TUM-GAID", "contents": "**TUM-GAID** (TUM Gait from Audio, Image and Depth) collects 305 subjects performing two walking trajectories in an indoor environment. The first trajectory is traversed from left to right and the second one from right to left. Two recording sessions were performed, one in January, where subjects wore heavy jackets and mostly winter boots, and another one in April, where subjects wore lighter clothes. The action is captured by a Microsoft Kinect sensor which provides a video stream with a resolution of 640×480 pixels and a frame rate around 30 FPS.\n\nSource: [Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs](https://arxiv.org/abs/1808.00286)\nImage Source: [https://www.ei.tum.de/mmk/verschiedenes/tum-gaid-database/](https://www.ei.tum.de/mmk/verschiedenes/tum-gaid-database/)", "variants": ["TUM-GAID"], "title": "The TUM Gait from Audio, Image and Depth (GAID) Database: Multimodal Recognition of Subjects and Traits"}
{"id": "Photi-LakeIce", "contents": "A new benchmark dataset of webcam images, Photi-LakeIce, from multiple cameras and two different winters, along with pixel-wise ground truth annotations. \r\n\r\nSource: [Lake Ice Monitoring with Webcams and Crowd-Sourced Images](/paper/lake-ice-monitoring-with-webcams-and-crowd)", "variants": ["Photi-LakeIce"], "title": "Lake Ice Monitoring with Webcams and Crowd-Sourced Images"}
{"id": "PAWS-X", "contents": "PAWS-X contains 23,659 human translated PAWS evaluation pairs and 296,406 machine translated training pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. All translated pairs are sourced from examples in PAWS-Wiki.\r\n\r\nSource: [Google Research](https://github.com/google-research-datasets/paws/tree/master/pawsx)\r\nImage Source: [https://arxiv.org/pdf/1908.11828v1.pdf](https://arxiv.org/pdf/1908.11828v1.pdf)", "variants": ["PAWS-X"], "title": "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"}
{"id": "MORPH", "contents": "**MORPH** is a facial age estimation dataset, which contains 55,134 facial images of 13,617 subjects ranging from 16 to 77 years old.\r\n\r\nSource: [Deep Ordinal Regression Forests](https://arxiv.org/abs/2008.03077)\r\nImage Source: [https://uncw.edu/oic/tech/morph.html](https://uncw.edu/oic/tech/morph.html)", "variants": ["MORPH", "MORPH Album2"], "title": "MORPH: a longitudinal image database of normal adult age-progression"}
{"id": "FreiHAND", "contents": "**FreiHAND** is a 3D hand pose dataset which records different hand actions performed by 32 people. For each hand image, MANO-based 3D hand pose annotations are provided. It currently contains 32,560 unique training samples and 3960 unique samples for evaluation. The training samples are recorded with a green screen background allowing for background removal. In addition, it applies three different post processing strategies to training samples for data augmentation. However, these post processing strategies are not applied to evaluation samples.\r\n\r\nSource: [Knowledge as Priors: Cross-Modal Knowledge Generalizationfor Datasets without Superior Knowledge](https://arxiv.org/abs/2004.00176)\r\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html)", "variants": ["FreiHAND"], "title": "FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape From Single RGB Images"}
{"id": "CARER", "contents": "CARER is an emotion dataset collected through noisy labels, annotated via distant supervision as in (Go et al., 2009). It uses the eight basic emotions: anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The hashtags (339 total) serve as noisy labels, which allow annotation via distant supervision as in (Go et al., 2009).\r\n\r\nSource: [CARER: Contextualized Affect Representations for Emotion Recognition](/paper/carer-contextualized-affect-representations)", "variants": ["CARER"], "title": "CARER: Contextualized Affect Representations for Emotion Recognition"}
{"id": "300W", "contents": "The 300-W is a face dataset that consists of 300 Indoor and 300 Outdoor in-the-wild images. It covers a large variation of identity, expression, illumination conditions, pose, occlusion and face size. The images were downloaded from google.com by making queries such as “party”, “conference”, “protests”, “football” and “celebrities”. Compared to the rest of in-the-wild datasets, the 300-W database contains a larger percentage of partially-occluded images and covers more expressions than the common “neutral” or “smile”, such as “surprise” or “scream”.\r\nImages were annotated with the 68-point mark-up using a semi-automatic methodology. The images of the database were carefully selected so that they represent a characteristic sample of challenging but natural face instances under totally unconstrained conditions. Thus, methods that achieve accurate performance on the 300-W database can demonstrate the same accuracy in most realistic cases.\r\nMany images of the database contain more than one annotated faces (293 images with 1 face, 53 images with 2 faces and 53 images with [3, 7] faces). Consequently, the database consists of 600 annotated face instances, but 399 unique images. Finally, there is a large variety of face sizes. Specifically, 49.3% of the faces have size in the range [48.6k, 2.0M] and the overall mean size is 85k (about 292 × 292) pixels.\r\n\r\nSource: [https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf](https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf)\r\nImage Source: [https://www.researchgate.net/profile/Xuanyi_Dong/publication/323722412/figure/fig1/AS:679426136227845@1538999222829/Face-samples-from-300-W-dataset-Different-faces-have-different-styles-whereas-the-style_Q640.jpg](https://www.researchgate.net/profile/Xuanyi_Dong/publication/323722412/figure/fig1/AS:679426136227845@1538999222829/Face-samples-from-300-W-dataset-Different-faces-have-different-styles-whereas-the-style_Q640.jpg)", "variants": ["300W", "300W (Full)"], "title": "300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge"}
{"id": "Drone Tracking", "contents": "This dataset contains videos where a flying drone (hexacopter) is captured with multiple consumer-grade cameras (smartphones, compact cameras, gopro,...) with highly accurate 3D drone trajectory ground truth recorderd by a precise real-time RTK system from Fixposition. In some videos, the ground truth temporal synchronization and ground truth camera locations are also provided.\n\nSource: [https://github.com/CenekAlbl/drone-tracking-datasets](https://github.com/CenekAlbl/drone-tracking-datasets)\nImage Source: [https://github.com/CenekAlbl/drone-tracking-datasets](https://github.com/CenekAlbl/drone-tracking-datasets)", "variants": ["Drone Tracking"], "title": "Reconstruction of 3D flight trajectories from ad-hoc camera networks"}
{"id": "iLIDS-VID", "contents": "The **iLIDS-VID** dataset is a person re-identification dataset which involves 300 different pedestrians observed across two disjoint camera views in public open space. It comprises 600 image sequences of 300 distinct individuals, with one pair of image sequences from two camera views for each person. Each image sequence has variable length ranging from 23 to 192 image frames, with an average number of 73. The iLIDS-VID dataset is very challenging due to clothing similarities among people, lighting and viewpoint variations across camera views, cluttered background and random occlusions.\n\nSource: [http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html](http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html)\nImage Source: [http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html](http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html)", "variants": ["iLIDS-VID"], "title": "Unsupervised Person Re-identification by Deep Learning Tracklet Association"}
{"id": "TDIUC", "contents": "**Task Directed Image Understanding Challenge** (**TDIUC**) dataset is a Visual Question Answering dataset which consists of 1.6M questions and 170K images sourced from MS COCO and the Visual Genome Dataset. The image-question pairs are split into 12 categories and 4 additional evaluation matrices which help evaluate models’ robustness against answer imbalance and its ability to answer questions that require higher reasoning capability. The TDIUC dataset divides the VQA paradigm into 12 different task directed question types. These include questions that require a simpler task (e.g., object presence, color attribute) and more complex tasks (e.g., counting, positional reasoning). The dataset includes also an “Absurd” question category in which questions are irrelevant to the image contents to help balance the dataset.\r\n\r\nSource: [Question-Agnostic Attention for Visual Question Answering](https://arxiv.org/abs/1908.03289)\r\nImage Source: [https://kushalkafle.com/projects/tdiuc.html](https://kushalkafle.com/projects/tdiuc.html)", "variants": ["TDIUC"], "title": "An Analysis of Visual Question Answering Algorithms"}
{"id": "CARPK", "contents": "The Car Parking Lot Dataset (**CARPK**) contains nearly 90,000 cars from 4 different parking lots collected by means of drone (PHANTOM 3 PROFESSIONAL). The images are collected with the drone-view at approximate 40 meters height. The image set is annotated by bounding box per car. All labeled bounding boxes have been well recorded with the top-left points and the bottom-right points. It is supporting object counting, object localizing, and further investigations with the annotation format in bounding boxes.\r\n\r\nSource: [https://lafi.github.io/LPN/](https://lafi.github.io/LPN/)\r\nImage Source: [https://www.researchgate.net/figure/Sample-results-on-the-CARPK-dataset-Top-row-original-images-Bottom-row-predicted_fig4_328685610](https://www.researchgate.net/figure/Sample-results-on-the-CARPK-dataset-Top-row-original-images-Bottom-row-predicted_fig4_328685610)", "variants": ["CARPK"], "title": "Drone-Based Object Counting by Spatially Regularized Regional Proposal Network"}
{"id": "Pix3D", "contents": "The **Pix3D** dataset is a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc.\r\n\r\nSource: [http://pix3d.csail.mit.edu/](http://pix3d.csail.mit.edu/)\r\nImage Source: [http://pix3d.csail.mit.edu/](http://pix3d.csail.mit.edu/)", "variants": ["Pix3D S2", "Pix3D S1", "Pix3D"], "title": "Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling"}
{"id": "MAMS", "contents": "MAMS is a challenge dataset for aspect-based sentiment analysis (ABSA), in which each sentences contain at least two aspects with different sentiment polarities. MAMS dataset contains two versions: one for aspect-term sentiment analysis (ATSA) and one for aspect-category sentiment analysis (ACSA).\r\n\r\nSource: [MAMS](https://github.com/siat-nlp/MAMS-for-ABSA)", "variants": ["MAMS"], "title": "A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis"}
{"id": "SPoC", "contents": "Pseudocode-to-Code (SPoC) is a program synthesis dataset, containing 18,356 programs with human-authored pseudocode and test cases.\r\n\r\nImage source: [https://sumith1896.github.io/spoc/](https://sumith1896.github.io/spoc/)", "variants": ["SPoC TestP", "SPoC TestW", "SPoC"], "title": "SPoC: Search-based Pseudocode to Code"}
{"id": "JESC", "contents": "Japanese-English Subtitle Corpus is a large Japanese-English parallel corpus covering the underrepresented domain of conversational dialogue. It consists of more than 3.2 million examples, making it the largest freely available dataset of its kind. The corpus was assembled by crawling and aligning subtitles found on the web. \r\n\r\nSource: [JESC: Japanese-English Subtitle Corpus](https://arxiv.org/pdf/1710.10639v4.pdf)", "variants": ["JESC"], "title": "JESC: Japanese-English Subtitle Corpus"}
{"id": "CUFSF", "contents": "The CUHK Face Sketch FERET (**CUFSF**) is a dataset for research on face sketch synthesis and face sketch recognition. It contains two types of face images: photo and sketch. Total 1,194 images (one image per subject) were collected with lighting variations from the FERET dataset. For each subject, a sketch is drawn with shape exaggeration.\r\n\r\nSource: [Deeply Coupled Auto-encoder Networks forCross-view Classification](https://arxiv.org/abs/1402.2031)\r\nImage Source: [http://mmlab.ie.cuhk.edu.hk/archive/cufsf/](http://mmlab.ie.cuhk.edu.hk/archive/cufsf/)", "variants": ["CUFSF"], "title": "Coupled information-theoretic encoding for face photo-sketch recognition"}
{"id": "TGIF", "contents": "The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques.\r\n\r\nSource: [GitHub](https://github.com/raingo/TGIF-Release)", "variants": ["TGIF"], "title": "TGIF: A New Dataset and Benchmark on Animated GIF Description"}
{"id": "CHiME-Home", "contents": "**CHiME-Home** is a dataset for sound source recognition in a domestic environment. It uses around 6.8 hours of domestic environment audio recordings. The recordings were obtained from the CHiME projects – computational hearing in multisource environments – where recording equipment was positioned inside an English Victorian semi-detached house. The recordings were selected from 22 sessions totalling 19.5 hours, with each session made between 7:30 in the morning and 20:00 in the evening. In the considered recordings, the equipment was placed in the lounge (sitting room) near the door opening onto a hallway, with the hallway opening onto a kitchen with no door. With the lounge door typically open, prominent sounds thus may originate from sources both in the lounge and kitchen.\r\n\r\nThe choice of permitted labels was motivated by the sources present in the considered acoustic environment: Human speakers (c,m,f); human activity (p); television (v); household appliances (b). Further labels o,S,U respectively relate to any other identifiable sounds, silence, unidentifiable sounds. Labels S,U may respectively only be assigned in isolation. Annotators were acquired to assign at least one label to a chunk, thus annotators may either assign one or more labels from the set {c,m,f,v,p,b,o}, or may alternatively ‘flag’ the chunk using a single label from the set {S,U}.\r\n\r\nSource: [https://www.researchgate.net/publication/308732345_CHiME-Home_A_dataset_for_sound_source_recognition_in_a_domestic_environment](https://www.researchgate.net/publication/308732345_CHiME-Home_A_dataset_for_sound_source_recognition_in_a_domestic_environment)\nImage Source: [https://www.researchgate.net/publication/308732345_CHiME-Home_A_dataset_for_sound_source_recognition_in_a_domestic_environment](https://www.researchgate.net/publication/308732345_CHiME-Home_A_dataset_for_sound_source_recognition_in_a_domestic_environment)", "variants": ["CHiME-Home"], "title": "Chime-home: A dataset for sound source recognition in a domestic environment"}
{"id": "CONVERSE", "contents": "A novel dataset that represents complex conversational interactions between two individuals via 3D pose. 8 pairwise interactions describing 7 separate conversation based scenarios were collected using two Kinect depth sensors.\r\n\r\nSource: [From Pose to Activity: Surveying Datasets and Introducing CONVERSE](/paper/from-pose-to-activity-surveying-datasets-and)", "variants": ["CONVERSE"], "title": "From Pose to Activity: Surveying Datasets and Introducing CONVERSE"}
{"id": "FaceScape", "contents": "FaceScape dataset provides 3D face models, parametric models and multi-view images in large-scale and high-quality. The camera parameters, the age and gender of the subjects are also included. The data have been released to public for non-commercial research purpose.\r\n\r\nSource: [FaceScape](https://facescape.nju.edu.cn/)", "variants": ["FaceScape"], "title": "FaceScape: a Large-scale High Quality 3D Face Dataset and Detailed Riggable 3D Face Prediction"}
{"id": "FPL", "contents": "Supports new task that predicts future locations of people observed in first-person videos.\r\n\r\nSource: [Future Person Localization in First-Person Videos](/paper/future-person-localization-in-first-person)", "variants": ["FPL"], "title": "Future Person Localization in First-Person Videos"}
{"id": "SpectroVision", "contents": "**SpectroVision** is a dataset of 14,400 high resolution texture images and spectral measurements collected from a PR2 mobile manipulator that interacted with 144 household objects from eight material categories.\n\nSource: [https://github.com/Healthcare-Robotics/spectrovision](https://github.com/Healthcare-Robotics/spectrovision)\nImage Source: [https://github.com/Healthcare-Robotics/spectrovision](https://github.com/Healthcare-Robotics/spectrovision)", "variants": ["SpectroVision"], "title": "Multimodal Material Classification for Robots using Spectroscopy and High Resolution Texture Imaging"}
{"id": "Street View Image, Pose, and 3D Cities Dataset", "contents": "A large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information.\r\n\r\nSource: [Generic 3D Representation via Pose Estimation and Matching](/paper/generic-3d-representation-via-pose-estimation)", "variants": ["Street View Image, Pose, and 3D Cities Dataset"], "title": "Generic 3D Representation via Pose Estimation and Matching"}
{"id": "CUB-200-2011", "contents": "The **Caltech-UCSD Birds-200-2011** (**CUB-200-2011**) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from [Reed et al.]( https://paperswithcode.com/paper/learning-deep-representations-of-fine-grained). They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.\r\n\r\nSource: [Fine-grained Visual-textual Representation Learning](https://arxiv.org/abs/1709.00340)\r\nImage Source: [http://www.vision.caltech.edu/visipedia/CUB-200-2011.html](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)", "variants": ["CUB-200-2011", "CUB", "CUB 128 x 128", "CUB 200 5-way 1-shot", "CUB 200 5-way 5-shot", "CUB 200 50-way (0-shot)", "CUB Birds", "CUB-200 - 0-Shot Learning", "CUB-200-2011 - 0-Shot", "CUB-LT", "CUB-200-2011, 30 samples per class", "CUB-200-2011, 5 samples per class", "Imbalanced CUB-200-2011", "CUB-200-2011 5-way (1-shot)", "CUB-200-2011 5-way (5-shot)", "CUB-200-2011, 10 samples per class"], "title": "The Caltech-UCSD Birds-200-2011 Dataset"}
{"id": "Multi-News", "contents": "**Multi-News**, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited.\r\n\r\nSource: [Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model](https://arxiv.org/pdf/1906.01749.pdf)\r\nImage Source: [https://arxiv.org/pdf/1906.01749.pdf](https://arxiv.org/pdf/1906.01749.pdf)", "variants": ["MultiNews val", "MultiNews test", "Multi-News"], "title": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model"}
{"id": "CITR & DUT", "contents": "Consists of two pedestrian trajectory datasets, CITR dataset and DUT dataset, so that the pedestrian motion models can be further calibrated and verified, especially when vehicle influence on pedestrians plays an important role. \r\n\r\nCITR dataset consists of experimentally designed fundamental VCI scenarios (front, back, and lateral VCIs) and provides unique ID for each pedestrian, which is suitable for exploring a specific aspect of VCI.\r\n\r\nDUT dataset gives two ordinary and natural VCI scenarios in crowded university campus, which can be used for more general purpose VCI exploration.\r\n\r\nSource: [Top-view Trajectories: A Pedestrian Dataset of Vehicle-Crowd Interaction from Controlled Experiments and Crowded Campus](/paper/top-view-trajectories-a-pedestrian-dataset-of)", "variants": ["CITR & DUT"], "title": "Top-view Trajectories: A Pedestrian Dataset of Vehicle-Crowd Interaction from Controlled Experiments and Crowded Campus"}
{"id": "HAM10000", "contents": "**HAM10000** is a dataset of 10000 training images for detecting pigmented skin lesions. The authors collected dermatoscopic images from different populations, acquired and stored by different modalities.\r\n\r\nSource: [https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000](https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000)\r\nImage Source: [https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000](https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000)", "variants": ["Ham10000", "HAM10000"], "title": "The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions"}
{"id": "StreetLearn", "contents": "An interactive, first-person, partially-observed visual environment that uses Google Street View for its photographic content and broad coverage, and give performance baselines for a challenging goal-driven navigation task. \r\n\r\nSource: [The StreetLearn Environment and Dataset](/paper/the-streetlearn-environment-and-dataset)", "variants": ["StreetLearn"], "title": "The StreetLearn Environment and Dataset"}
{"id": "LaSOT", "contents": "LaSOT is a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT one of the largest densely annotated\r\ntracking benchmark. The average video length of LaSOT\r\nis more than 2,500 frames, and each sequence comprises\r\nvarious challenges deriving from the wild where target objects may disappear and re-appear again in the view.", "variants": ["LaSOT"], "title": "LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking"}
{"id": "OPIEC", "contents": "OPIEC is an Open Information Extraction (OIE) corpus, constructed from the entire English Wikipedia. It containing more than 341M triples. Each triple from the corpus is composed of rich meta-data: each token from the subj / obj / rel along with NLP annotations (POS tag, NER tag, ...), provenance sentence (along with its dependency parse, sentence order relative to the article), original (golden) links contained in the Wikipedia articles, space / time.\r\n\r\nSource: [OPIEC](https://www.uni-mannheim.de/dws/research/resources/opiec/)", "variants": ["OPIEC"], "title": "OPIEC: An Open Information Extraction Corpus"}
{"id": "WikiHowQA", "contents": "WikiHowQA is a Community-based Question Answering dataset, which can be used for both answer selection and abstractive summarization tasks. It contains 76,687 questions in the train set, 8,000 in the development set and 22,354 in the test set.\r\n\r\nSource: [WikiHowQA](https://github.com/dengyang17/wikihowQA)", "variants": ["WikiHowQA"], "title": "Joint Learning of Answer Selection and Answer Summary Generation in Community Question Answering"}
{"id": "WikiTableQuestions", "contents": "**WikiTableQuestions** is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. WikiTableQuestions contains 22,033 questions. The questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets and datasets for querying knowledge bases. Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions.\r\n\r\nSource: [Explaining Queries over Web Tables to Non-Experts](https://arxiv.org/abs/1808.04614)\r\nImage Source: [https://ppasupat.github.io/WikiTableQuestions/](https://ppasupat.github.io/WikiTableQuestions/)", "variants": ["WikiTableQuestions"], "title": "Compositional Semantic Parsing on Semi-Structured Tables"}
{"id": "SemEval 2014 Task 4 Sub Task 2", "contents": "Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided.\r\n\r\n***Subtask 2: Aspect term polarity***\r\n\r\nFor a given set of aspect terms within a sentence, determine whether the polarity of each aspect term is positive, negative, neutral or conflict (i.e., both positive and negative).\r\n\r\nFor example:\r\n\r\n“I loved their fajitas” → {fajitas: positive}\r\n“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive}\r\n“The fajitas are their first plate” → {fajitas: neutral}\r\n“The fajitas were great to taste, but not to see” → {fajitas: conflict}", "variants": ["SemEval 2014 Task 4 Sub Task 2"], "title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis"}
{"id": "RAP", "contents": "The **Richly Annotated Pedestrian** (**RAP**) dataset is a dataset for pedestrian attribute recognition. It contains 41,585 images collected from indoor surveillance cameras. Each image is annotated with 72 attributes, while only 51 binary attributes with the positive ratio above 1% are selected for evaluation. There are 33,268 images for the training set and 8,317 for testing.\r\n\r\nSource: [Localization Guided Learning for Pedestrian Attribute Recognition](https://arxiv.org/abs/1808.09102)\r\nImage Source: [http://www.rapdataset.com/rapv1.html](http://www.rapdataset.com/rapv1.html)", "variants": ["RAP"], "title": "A Richly Annotated Dataset for Pedestrian Attribute Recognition"}
{"id": "WikiSQL", "contents": "**WikiSQL** consists of a corpus of 87,726 hand-annotated SQL query and natural language question pairs. These SQL queries are further split into training (61,297 examples), development (9,145 examples) and test sets (17,284 examples). It can be used for natural language inference tasks related to relational databases.\r\n\r\nSource: [SQL-to-Text Generation with Graph-to-Sequence Model](https://arxiv.org/abs/1809.05255)\r\nImage Source: [https://blog.einstein.ai/how-to-talk-to-your-database/](https://blog.einstein.ai/how-to-talk-to-your-database/)", "variants": ["WikiSQL"], "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"}
{"id": "Lytro Illum", "contents": "Lytro Illum is a new light field dataset using a Lytro Illum camera. 640 light fields are collected with significant variations in terms of size, textureness, background clutter and illumination, etc. Micro-lens image arrays and central viewing images are generated, and corresponding ground-truth maps are produced.\r\n\r\nSource: [Lytro Illum](https://github.com/pencilzhang/MAC-light-field-saliency-net)", "variants": ["Lytro Illum"], "title": "Light Field Saliency Detection With Deep Convolutional Networks"}
{"id": "BCN_20000", "contents": "**BCN_20000** is a dataset composed of 19,424 dermoscopic images of skin lesions captured from 2010 to 2016 in the facilities of the Hospital Clínic in Barcelona. The dataset can be used for lesion recognition tasks such as lesion segmentation, lesion detection and lesion classification.\n\nSource: [https://arxiv.org/abs/1908.02288](https://arxiv.org/abs/1908.02288)\nImage Source: [https://arxiv.org/abs/1908.02288](https://arxiv.org/abs/1908.02288)", "variants": ["BCN_20000"], "title": "BCN20000: Dermoscopic Lesions in the Wild"}
{"id": "fMoW", "contents": "Functional Map of the World (fMoW) is a dataset that aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. \r\n\r\nSource: [Functional Map of the World](/paper/functional-map-of-the-world)\r\nImage Source: [Christie et al](https://arxiv.org/pdf/1711.07846.pdf)", "variants": ["fMoW"], "title": "Functional Map of the World"}
{"id": "AND Dataset", "contents": "The **AND Dataset** contains 13700 handwritten samples and 15 corresponding expert examined features for each sample. The dataset is released for public use and the methods can be extended to provide explanations on other verification tasks like face verification and bio-medical comparison. This dataset can serve as the basis and benchmark for future research in explanation based handwriting verification.\r\n\r\nSource: [Explanation based Handwriting Verification](https://paperswithcode.com/paper/explanation-based-handwriting-verification)", "variants": ["AND Dataset"], "title": "Explanation based Handwriting Verification"}
{"id": "MSRA-TD500", "contents": "The **MSRA-TD500** dataset is a text detection dataset that contains 300 training images and 200 test images. Text regions are arbitrarily orientated and annotated at sentence level. Different from the other datasets, it contains both English and Chinese text.\r\n\r\nSource: [Detecting Text in the Wild with Deep Character Embedding Network](https://arxiv.org/abs/1901.00363)\r\nImage Source: [http://www.iapr-tc11.org/mediawiki/images/MSRA-TD500_Example.jpg](http://www.iapr-tc11.org/mediawiki/images/MSRA-TD500_Example.jpg)", "variants": ["MSRA-TD500"], "title": "Detecting texts of arbitrary orientations in natural images"}
{"id": "RadioTalk", "contents": "**RadioTalk** is a corpus of speech recognition transcripts sampled from talk radio broadcasts in the United States between October of 2018 and March of 2019. The corpus is intended for use by researchers in the fields of natural language processing, conversational analysis, and the social sciences. The corpus encompasses approximately 2.8 billion words of automatically transcribed speech from 284,000 hours of radio, together with metadata about the speech, such as geographical location, speaker turn boundaries, gender, and radio program information.\n\nSource: [https://github.com/social-machines/RadioTalk](https://github.com/social-machines/RadioTalk)", "variants": ["RadioTalk"], "title": "RadioTalk: a large-scale corpus of talk radio transcripts"}
{"id": "ApolloCar3D", "contents": "**ApolloCar3DT** is a dataset that contains 5,277 driving images and over 60K car instances, where each car is fitted with an industry-grade 3D CAD model with absolute model size and semantically labelled keypoints. This dataset is above 20 times larger than PASCAL3D+ and KITTI, the current state-of-the-art. \r\n\r\nSource: [ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving](https://arxiv.org/pdf/1811.12222v2.pdf)\r\nImage Source: [http://apolloscape.auto/car_instance.html](http://apolloscape.auto/car_instance.html)", "variants": ["ApolloCar3D"], "title": "ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving"}
{"id": "BirdSong", "contents": "The **BirdSong** dataset consists of audio recordings of bird songs at the H. J. Andrews (HJA) Experimental Forest, using unattended microphones. The goal of the dataset is to provide data to automatically identify the species of bird responsible for each utterance in these recordings. The dataset contains 548 10-seconds audio recordings.", "variants": ["BirdSong"], "title": "Rank-loss support instance machines for MIML instance annotation"}
{"id": "FEVER", "contents": "FEVER is a publicly available dataset for fact extraction and verification against textual sources.\r\n\r\nIt consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGHINFO. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim.\r\n\r\nThe claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering. The verification of each claim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was\r\nextracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.\r\n\r\nSource: [FEVER: a large-scale dataset for Fact Extraction and VERification](https://arxiv.org/pdf/1803.05355v3.pdf)", "variants": ["FEVER"], "title": "FEVER: a large-scale dataset for Fact Extraction and VERification"}
{"id": "PHM2017", "contents": "PHM2017 is a new dataset consisting of 7,192 English tweets across six diseases and conditions: Alzheimer’s Disease, heart attack (any severity), Parkinson’s disease, cancer (any type), Depression (any severity), and Stroke. The Twitter search API was used to retrieve the data using the colloquial disease names as search keywords, with the expectation of retrieving a high-recall, low precision dataset. After removing the re-tweets and replies, the tweets were manually annotated. The labels are:\r\n\r\n- self-mention. The tweet contains a health mention with a health self-report of the Twitter account owner, e.g., \"However, I worked hard and ran for Tokyo Mayer Election Campaign in January through February, 2014, without publicizing the cancer.\"\r\n- other-mention. The tweet contains a health mention of a health report about someone other than the account owner, e.g., \"Designer with Parkinson’s couldn’t work then engineer invents bracelet + changes her world\"\r\n- awareness. The tweet contains the disease name, but does not mention a specific person, e.g., \"A Month Before a Heart Attack, Your Body Will Warn You With These 8 Signals\"\r\n- non-health. The tweet contains the disease name, but the tweet topic is not about health. \"Now I can have cancer on my wall for all to see <3\"\r\n\r\nSource: [Did You Really Just Have a Heart Attack? Towards Robust Detection of Personal Health Mentions in Social Media](https://arxiv.org/pdf/1802.09130v2.pdf)", "variants": ["PHM2017"], "title": "Did You Really Just Have a Heart Attack? Towards Robust Detection of Personal Health Mentions in Social Media"}
{"id": "PeopleArt", "contents": "People-Art is an object detection dataset which consists of people in 43 different styles. People contained in this dataset are quite different from those in common photographs. There are 42 categories of art styles and movements including Naturalism, Cubism, Socialist Realism, Impressionism, and Suprematism\n\nSource: [Point Linking Network for Object Detection](https://arxiv.org/abs/1706.03646)\nImage Source: [https://www.researchgate.net/figure/Generalization-results-on-Picasso-and-People-Art-datasets-Joseph-Redmon-2016_fig12_328175597](https://www.researchgate.net/figure/Generalization-results-on-Picasso-and-People-Art-datasets-Joseph-Redmon-2016_fig12_328175597)", "variants": ["PeopleArt"], "title": "Detecting People in Artwork with CNNs"}
{"id": "NREC Agricultural Person-Detection", "contents": "A dataset to encourage research in these environments. It consists of labeled stereo video of people in orange and apple orchards taken from two perception platforms (a tractor and a pickup truck), along with vehicle position data from RTK GPS. \r\n\r\nSource: [Comparing Apples and Oranges: Off-Road Pedestrian Detection on the NREC Agricultural Person-Detection Dataset](/paper/comparing-apples-and-oranges-off-road)", "variants": ["NREC Agricultural Person-Detection"], "title": "Comparing Apples and Oranges: Off-Road Pedestrian Detection on the NREC Agricultural Person-Detection Dataset"}
{"id": "Freesound One-Shot Percussive Sounds", "contents": "The **Freesound One-Shot Percussive Sounds** dataset contains 10254 one-shot (single event) percussive sounds from Freesound.org and the corresponding timbral analysis. These were used to train the generative model for \"Neural Percussive Synthesis Parameterised by High-Level Timbral Features\".\n\nSource: [https://zenodo.org/record/3665275](https://zenodo.org/record/3665275)\nImage Source: [https://freesound.org/people/Robinhood76/sounds/63616/](https://freesound.org/people/Robinhood76/sounds/63616/)", "variants": ["Freesound One-Shot Percussive Sounds"], "title": "Neural Percussive Synthesis Parameterised by High-Level Timbral Features"}
{"id": "Wiki-zh", "contents": "**Wiki-zh** is an annotated Chinese dataset for domain detection extracted from Wikipedia. It includes texts from 7 different domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW), “Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). It contains 26,280 documents split into training, validation and test.\n\nSource: [https://arxiv.org/pdf/1907.11499.pdf](https://arxiv.org/pdf/1907.11499.pdf)", "variants": ["Wiki-zh"], "title": "Weakly Supervised Domain Detection"}
{"id": "Procedural Human Action Videos", "contents": "Procedural Human Action Videos contains a total of 39,982 videos, with more than 1,000 examples for each action of 35 categories. \r\n\r\nSource: [Procedural Generation of Videos to Train Deep Action Recognition Networks](/paper/procedural-generation-of-videos-to-train-deep)", "variants": ["Procedural Human Action Videos"], "title": "Procedural Generation of Videos to Train Deep Action Recognition Networks"}
{"id": "CK+", "contents": "The Extended Cohn-Kanade (**CK+**) dataset contains 593 video sequences from a total of 123 different subjects, ranging from 18 to 50 years of age with a variety of genders and heritage. Each video shows a facial shift from the neutral expression to a targeted peak expression, recorded at 30 frames per second (FPS) with a resolution of either 640x490 or 640x480 pixels. Out of these videos, 327 are labelled with one of seven expression classes: anger, contempt, disgust, fear, happiness, sadness, and surprise. The CK+ database is widely regarded as the most extensively used laboratory-controlled facial expression classification database available, and is used in the majority of facial expression classification methods.\r\n\r\nSource: [EmotionNet Nano: An Efficient Deep Convolutional Neural Network Design for Real-time Facial Expression Recognition](https://arxiv.org/abs/2006.15759)", "variants": ["CK+"], "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression"}
{"id": "SPIDER", "contents": "Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. In Spider 1.0, different complex SQL queries and databases appear in train and test sets. To do well on it, systems must generalize well to not only new SQL queries but also new database schemas.\r\n\r\nSource: [Spider](https://yale-lily.github.io/spider)\r\nImage Source: [https://yale-lily.github.io/spider](https://yale-lily.github.io/spider)", "variants": ["spider", "SPIDER"], "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"}
{"id": "ReCoRD", "contents": "**Reading Comprehension with Commonsense Reasoning Dataset** (ReCoRD) is a large-scale reading comprehension dataset which requires commonsense reasoning. ReCoRD consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of ReCoRD is to evaluate a machine's ability of commonsense reasoning in reading comprehension. ReCoRD is pronounced as [ˈrɛkərd].\r\n\r\nImage Source: [Zhang et al](https://arxiv.org/pdf/1810.12885v1.pdf)", "variants": ["ReCoRD"], "title": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}
{"id": "Event-Camera Dataset", "contents": "The **Event-Camera Dataset** is a collection of datasets with an event-based camera for high-speed robotics. The data also include intensity images, inertial measurements, and ground truth from a motion-capture system. An event-based camera is a revolutionary vision sensor with three key advantages: a measurement rate that is almost 1 million times faster than standard cameras, a latency of 1 microsecond, and a high dynamic range of 130 decibels (standard cameras only have 60 dB). These properties enable the design of a new class of algorithms for high-speed robotics, where standard cameras suffer from motion blur and high latency. All the data are released both as text files and binary (i.e., rosbag) files.\r\n\r\nSource: [The Event-Camera Dataset and Simulator: Event-based Data for Pose Estimation, Visual Odometry, and SLAM](/paper/the-event-camera-dataset-and-simulator-event)", "variants": ["Event-Camera Dataset"], "title": "The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and SLAM"}
{"id": "SYNTHIA-PANO", "contents": "SYNTHIA-PANO is the panoramic version of SYNTHIA dataset. Five sequences are included: Seqs02-summer, Seqs02-fall, Seqs04-summer, Seqs04-fall and Seqs05-summer. Panomaramic images with fine annotation for semantic segmentation.\r\n\r\nSource: [SYNTHIA-PANO](https://github.com/Francis515/SYNTHIA-PANO)\r\nImage Source: [https://github.com/Francis515/SYNTHIA-PANO](https://github.com/Francis515/SYNTHIA-PANO)", "variants": ["SYNTHIA-PANO"], "title": "Semantic segmentation of panoramic images using a synthetic dataset"}
{"id": "IPRE", "contents": "A dataset for inter-personal relationship extraction which aims to facilitate information extraction and knowledge graph construction research. In total, IPRE has over 41,000 labeled sentences for 34 types of relations, including about 9,000 sentences annotated by workers.\r\n\r\nSource: [IPRE: a Dataset for Inter-Personal Relationship Extraction](/paper/ipre-a-dataset-for-inter-personal)", "variants": ["IPRE"], "title": "IPRE: a Dataset for Inter-Personal Relationship Extraction"}
{"id": "CLEVR-Humans", "contents": "We collect  a  new  dataset  of  human-posed  free-form  natural  language  questions  about  CLEVR  images.    Many  of  these questions have out-of-vocabulary words and require reasoning skills that are absent from our model’s repertoire", "variants": ["CLEVR-Humans"], "title": "Inferring and Executing Programs for Visual Reasoning"}
{"id": "PadChest", "contents": "PadChest is a labeled large-scale, high resolution chest x-ray dataset for the automated exploration\r\nof medical images along with their associated reports. This dataset includes more than 160,000\r\nimages obtained from 67,000 patients that were interpreted and reported by radiologists at Hospital\r\nSan Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional\r\ninformation on image acquisition and patient demography. The reports were labeled with 174 different\r\nradiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical\r\ntaxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of\r\nthese reports, 27% were manually annotated by trained physicians and the remaining set was labeled\r\nusing a supervised method based on a recurrent neural network with attention mechanisms. The labels\r\ngenerated were then validated in an independent test set achieving a 0.93 Micro-F1 score.", "variants": ["PadChest"], "title": "PadChest: A large chest x-ray image dataset with multi-label annotated reports"}
{"id": "TotalCapture", "contents": "The **TotalCapture** dataset consists of 5 subjects performing several activities such as walking, acting, a range of motion sequence (ROM) and freestyle motions, which are recorded using 8 calibrated, static HD RGB cameras and 13 IMUs attached to head, sternum, waist, upper arms, lower arms, upper legs, lower legs and feet, however the IMU data is not required for our experiments. The dataset has publicly released foreground mattes and RGB images. Ground-truth poses are obtained using a marker-based motion capture system, with the markers are <5mm in size. All data is synchronised and operates at a framerate of 60Hz, providing ground truth poses as joint positions.\r\n\r\nSource: [Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras](https://arxiv.org/abs/1908.03030)\r\nImage Source: [https://cvssp.org/data/totalcapture/](https://cvssp.org/data/totalcapture/)", "variants": ["Total Capture", "TotalCapture"], "title": "Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors"}
{"id": "Video Storytelling", "contents": "A new dataset describing textual stories for events. \r\n\r\nSource: [Video Storytelling: Textual Summaries for Events](/paper/video-storytelling)", "variants": ["Video Storytelling"], "title": "Video Storytelling"}
{"id": "VGGFace2", "contents": "The **VGGFace2** dataset is made of around 3.31 million images divided into 9131 classes, each representing a different person identity. The dataset is divided into two splits, one for the training and one for test. The latter contains around 170000 images divided into 500 identities while all the other images belong to the remaining 8631 classes available for training. While constructing the datasets, the authors focused their efforts on reaching a very low label noise and a high pose and age diversity thus, making the VGGFace2 dataset a suitable choice to train state-of-the-art deep learning models on face-related tasks. The images of the training set have an average resolution of 137x180 pixels, with less than 1% at a resolution below 32 pixels (considering the shortest side).\r\n\r\n**CAUTION**: Authors note that the distribution of identities in the VGG-Face dataset may not be representative of the global human population. Please be careful of unintended societal, gender, racial and other biases when training or deploying models trained on this data.\r\n\r\nSource: [Cross-Resolution Learning for Face Recognition](https://arxiv.org/abs/1912.02851)", "variants": ["VggFace2", "VGGFace2 (2.3M)", "VggFace2 - 8x upscaling", "VGGFace2"], "title": "VGGFace2: A Dataset for Recognising Faces across Pose and Age"}
{"id": "Sketch", "contents": "The **Sketch** dataset contains over 20,000 sketches evenly distributed over 250 object categories.\r\n\r\nSource: [http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/)\r\nImage Source: [http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/)", "variants": ["Sketch (Fine-grained 6 Tasks)", "Sketch"], "title": "How do humans sketch objects?"}
{"id": "CodeSearchNet", "contents": "The **CodeSearchNet** Corpus is a large dataset of functions with associated documentation written in Go, Java, JavaScript, PHP, Python, and Ruby from open source projects on GitHub. The CodeSearchNet Corpus includes:\r\n* Six million methods overall\r\n* Two million of which have associated documentation (docstrings, JavaDoc, and more)\r\n* Metadata that indicates the original location (repository or line number, for example) where the data was found\r\n\r\nSource: [https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/)", "variants": ["CodeSearchNet"], "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"}
{"id": "Okutama-Action", "contents": "A new video dataset for aerial view concurrent human action detection. It consists of 43 minute-long fully-annotated sequences with 12 action classes. Okutama-Action features many challenges missing in current datasets, including dynamic transition of actions, significant changes in scale and aspect ratio, abrupt camera movement, as well as multi-labeled actors.\r\n\r\nSource: [Okutama-Action: An Aerial View Video Dataset for Concurrent Human Action Detection](/paper/okutama-action-an-aerial-view-video-dataset)", "variants": ["Okutama-Action"], "title": "Okutama-Action: An Aerial View Video Dataset for Concurrent Human Action Detection"}
{"id": "MCTest", "contents": "MCTest is a freely available set of stories and associated questions intended for research on the machine comprehension of text. \r\n\r\nMCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.\r\n\r\nSource: [MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text](https://www.aclweb.org/anthology/D13-1020.pdf)\r\nImage Source: [Richardson et al](https://www.aclweb.org/anthology/D13-1020)", "variants": ["MCTest-160", "MCTest-500", "MCTest"], "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text"}
{"id": "xView", "contents": "xView is one of the largest publicly available datasets of overhead imagery. It contains images from complex scenes around the world, annotated using bounding boxes. It contains over 1M object instances from 60 different classes.\r\n\r\nSource: [xView dataset](http://xviewdataset.org/)", "variants": ["xView"], "title": "xView: Objects in Context in Overhead Imagery"}
{"id": "WIKIOG", "contents": "WIKIOG is a public collection which consists of over 1.75 million document-outline pairs for research on the OG task. \r\n\r\nSource: [Outline Generation: Understanding the Inherent Content Structure of Documents](https://arxiv.org/pdf/1905.10039)", "variants": ["WIKIOG"], "title": "Outline Generation: Understanding the Inherent Content Structure of Documents"}
{"id": "PTB-TIR", "contents": "PTB-TIR is a Thermal InfraRed (TIR) pedestrian tracking benchmark, which provides 60  TIR sequences with mannuly annoations.  The benchmark is used to fair evaluate TIR trackers.\r\n\r\nSource: [PTB-TIR: A Thermal Infrared Pedestrian Tracking Benchmark](/paper/ptb-tir-a-thermal-infrared-pedestrian)", "variants": ["PTB-TIR"], "title": "PTB-TIR: A Thermal Infrared Pedestrian Tracking Benchmark"}
{"id": "Freiburg Street Crossing", "contents": "The **Freiburg Street Crossing** dataset consists of data collected from three different street crossings in Freiburg, Germany; ; two of which were traffic light regulated intersections and one a zebra crossing without traffic lights. The data can be used to train agents to cross roads autonomously.\n\nSource: [http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/](http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/)\nImage Source: [http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/](http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/)", "variants": ["Freiburg Street Crossing"], "title": "Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing"}
{"id": "I-HAZE", "contents": "The I-Haze dataset contains 25 indoor hazy images (size 2833×4657 pixels) training. It has 5 hazy images for validation along with their corresponding ground truth images.\n\nSource: [Single image dehazing for a variety of haze scenarios using back projected pyramid network](https://arxiv.org/abs/2008.06713)\nImage Source: [https://data.vision.ee.ethz.ch/cvl/ntire18//i-haze/](https://data.vision.ee.ethz.ch/cvl/ntire18//i-haze/)", "variants": ["I-Haze", "I-HAZE"], "title": "I-HAZE: a dehazing benchmark with real hazy and haze-free indoor images"}
{"id": "WikiText-2", "contents": "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\r\n\r\nCompared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\r\n\r\nSource: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\r\nImage Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)", "variants": ["WikiText-2"], "title": "Pointer Sentinel Mixture Models"}
{"id": "CLEVR-Dialog", "contents": "CLEVR-Dialog is a large diagnostic dataset for studying multi-round reasoning in visual dialog. Specifically, that authors construct a dialog grammar that is grounded in the scene graphs of the images from the CLEVR dataset. This combination results in a dataset where all aspects of the visual dialog are fully annotated. In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs.\r\n\r\nThe CLEVR-Dialog is used to benchmark performance of standard visual dialog models; in particular, on visual coreference resolution (as a function of the coreference distance). This is the first analysis of its kind for visual dialog models that was not possible without this dataset. \r\n\r\nCLEVR-Dialog is aims to help inform the development of future models for visual dialog.\r\n\r\nSource: [CLEVR-Dialog](https://github.com/satwikkottur/clevr-dialog)", "variants": ["CLEVR-Dialog"], "title": "CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog"}
{"id": "PMLB", "contents": "The **Penn Machine Learning Benchmarks** (**PMLB**) is a large, curated set of benchmark datasets used to evaluate and compare supervised machine learning algorithms. These datasets cover a broad range of applications, and include binary/multi-class classification problems and regression problems, as well as combinations of categorical, ordinal, and continuous features.\r\n\r\nSource: [https://arxiv.org/abs/1703.00512](https://arxiv.org/abs/1703.00512)", "variants": ["PMLB"], "title": "PMLB: a large benchmark suite for machine learning evaluation and comparison"}
{"id": "ConvAI2", "contents": "The **ConvAI2** NeurIPS competition aimed at finding approaches to creating high-quality dialogue agents capable of meaningful open domain conversation. The ConvAI2 dataset for training models is based on the PERSONA-CHAT dataset. The speaker pairs each have assigned profiles coming from a set of 1155 possible personas (at training time), each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation. As the original PERSONA-CHAT test set was released, a new hidden test set consisted of 100 new personas and over 1,015 dialogs was created by crowdsourced workers.\r\n\r\nTo avoid modeling that takes advantage of trivial word overlap, additional rewritten sets of the same train and test personas were crowdsourced, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging. For example “I just got my nails done” is revised as “I love to pamper myself on a regular basis” and “I am on a diet now” is revised as “I need to lose weight.”\r\n\r\nThe training, validation and hidden test sets consists of 17,878, 1,000 and 1,015 dialogues, respectively.\r\n\r\nSource: [The Second Conversational Intelligence Challenge (ConvAI2)](https://paperswithcode.com/paper/the-second-conversational-intelligence/)\r\nImage Source: [The Second Conversational Intelligence Challenge (ConvAI2)](https://paperswithcode.com/paper/the-second-conversational-intelligence/)", "variants": ["ConvAI2"], "title": "The Second Conversational Intelligence Challenge (ConvAI2)"}
{"id": "UFPR-AMR Dataset", "contents": "2,000 fully and manually annotated images for Automatic Meter Reading (AMR).\r\n\r\nSource: [Convolutional Neural Networks for Automatic Meter Reading](/paper/convolutional-neural-networks-for-automatic)", "variants": ["UFPR-AMR Dataset"], "title": "Convolutional Neural Networks for Automatic Meter Reading"}
{"id": "Story Commonsense", "contents": "Story Commonsense is a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.\r\n\r\nSource: [Modeling Naive Psychology of Characters in Simple Commonsense Stories](/paper/modeling-naive-psychology-of-characters-in)", "variants": ["Story Commonsense"], "title": "Modeling Naive Psychology of Characters in Simple Commonsense Stories"}
{"id": "CoSQL", "contents": "CoSQL is a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. \r\n\r\nSource: [CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases](https://arxiv.org/abs/1909.05378)", "variants": ["CoSQL"], "title": "CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases"}
{"id": "FERG", "contents": "**FERG** is a database of cartoon characters with annotated facial expressions containing 55,769 annotated face images of six characters. The images for each character are grouped into 7 types of cardinal expressions, viz. anger, disgust, fear, joy, neutral, sadness and surprise.\n\nSource: [VGAN-Based Image Representation Learningfor Privacy-Preserving Facial Expression Recognition](https://arxiv.org/abs/1803.07100)\nImage Source: [http://grail.cs.washington.edu/projects/deepexpr/ferg-2d-db.html](http://grail.cs.washington.edu/projects/deepexpr/ferg-2d-db.html)", "variants": ["FERG"], "title": "Modeling Stylized Character Expressions via Deep Learning"}
{"id": "Arxiv Academic Paper Dataset", "contents": "A dataset to enable automatic academic paper rating.\r\n\r\nSource: [Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network](/paper/automatic-academic-paper-rating-based-on)", "variants": ["Arxiv Academic Paper Dataset"], "title": "Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network"}
{"id": "MSK", "contents": "The **MSK** dataset is a dataset for lesion recognition from the Memorial Sloan-Kettering Cancer Center. It is used as part of the ISIC lesion recognition challenges.\n\nSource: [https://arxiv.org/pdf/1710.05006.pdf](https://arxiv.org/pdf/1710.05006.pdf)\nImage Source: [https://arxiv.org/pdf/1902.03368.pdf](https://arxiv.org/pdf/1902.03368.pdf)", "variants": ["MSK"], "title": "Skin lesion analysis toward melanoma detection: A challenge at the 2017 International symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC)"}
{"id": "FCE", "contents": "The Cambridge Learner Corpus **First Certificate in English** (CLC **FCE**) dataset consists of short texts, written by learners of English as an additional language in response to exam prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. The full dataset consists of 323,192 sentences. The publicly released subset of the dataset, named FCE-public, consists of 33,673 sentences split into test and training sets of 2,720 and 30,953 sentences, respectively.\r\n\r\nSource: [Compositional Sequence Labeling Models for Error Detection in Learner Writing](https://arxiv.org/abs/1607.06153)", "variants": ["FCE"], "title": "A New Dataset and Method for Automatically Grading ESOL Texts"}
{"id": "GVGAI", "contents": "The **General Video Game AI** (**GVGAI**) framework is widely used in research which features a corpus of over 100 single-player games and 60 two-player games. These are fairly small games, each focusing on specific mechanics or skills the players should be able to demonstrate, including clones of classic arcade games such as Space Invaders, puzzle games like Sokoban, adventure games like Zelda or game-theory problems such as the Iterative Prisoners Dilemma. All games are real-time and require players to make decisions in only 40ms at every game tick, although not all games explicitly reward or require fast reactions; in fact, some of the best game-playing approaches add up the time in the beginning of the game to run Breadth-First Search in puzzle games in order to find an accurate solution. However, given the large variety of games (many of which are stochastic and difficult to predict accurately), scoring systems and termination conditions, all unknown to the players, highly-adaptive general methods are needed to tackle the diverse challenges proposed.\r\n\r\nSource: [Rolling Horizon Evolutionary Algorithms for General Video Game Playing](https://arxiv.org/abs/2003.12331)\r\nImage Source: [http://www.gvgai.net/](http://www.gvgai.net/)", "variants": ["GVGAI"], "title": "DeepMind Control Suite"}
{"id": "AtariARI", "contents": "The **AtariARI** (**Atari Annotated RAM Interface**) is an environment for representation learning. The Atari Arcade Learning Environment (ALE) does not explicitly expose any ground truth state information. However, ALE does expose the RAM state (128 bytes per timestep) which are used by the game programmer to store important state information such as the location of sprites, the state of the clock, or the current room the agent is in. To extract these variables, the dataset creators consulted commented disassemblies (or source code) of Atari 2600 games which were made available by Engelhardt and Jentzsch and CPUWIZ. The dataset creators were able to find and verify important state variables for a total of 22 games. Once this information was acquired, combining it with the ALE interface produced a wrapper that can automatically output a state label for every example frame generated from the game. The dataset creators make this available with an easy-to-use gym wrapper, which returns this information with no change to existing code using gym interfaces.\n\nSource: [https://arxiv.org/pdf/1906.08226.pdf](https://arxiv.org/pdf/1906.08226.pdf)\nImage Source: [https://github.com/mila-iqia/atari-representation-learning](https://github.com/mila-iqia/atari-representation-learning)", "variants": ["AtariARI"], "title": "Unsupervised State Representation Learning in Atari"}
{"id": "Tiny Images", "contents": "The image dataset TinyImages contains 80 million images of size 32×32 collected from the Internet, crawling the words in WordNet. \r\n\r\n**The authors have decided to withdraw it because it contains offensive content, and have asked the community to stop using it.**\r\n\r\nSource: [Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments](https://arxiv.org/abs/1502.03032)", "variants": ["Tiny Images"], "title": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition"}
{"id": "ALFRED", "contents": "ALFRED (Action Learning From Realistic Environments and Directives), is a new benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks.\r\n\r\nSource: [ALFRED](https://github.com/askforalfred/alfred)", "variants": ["ALFRED"], "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks"}
{"id": "Fashion-Gen", "contents": "Fashion-Gen consists of 293,008 high definition (1360 x 1360 pixels) fashion images paired with item descriptions provided by professional stylists. Each item is photographed from a variety of angles.\r\n\r\nSource: [Fashion-Gen: The Generative Fashion Dataset and Challenge](https://arxiv.org/pdf/1806.08317v2.pdf)\r\nImage Source: [Rostamzadeh et al](https://arxiv.org/pdf/1806.08317v2.pdf)", "variants": ["Fashion-Gen"], "title": "Fashion-Gen: The Generative Fashion Dataset and Challenge"}
{"id": "DiDeMo", "contents": "The **Distinct Describable Moments** (**DiDeMo**) dataset is one of the largest and most diverse datasets for the temporal localization of events in videos given natural language descriptions. The videos are collected from Flickr and each video is trimmed to a maximum of 30 seconds. The videos in the dataset are divided into 5-second segments to reduce the complexity of annotation. The dataset is split into training, validation and test sets containing 8,395, 1,065 and 1,004 videos respectively. The dataset contains a total of 26,892 moments and one moment could be associated with descriptions from multiple annotators. The descriptions in DiDeMo dataset are detailed and contain camera movement, temporal transition indicators, and activities. Moreover, the descriptions in DiDeMo are verified so that each description refers to a single moment.\r\n\r\nSource: [Weakly Supervised Video Moment Retrieval From Text Queries](https://arxiv.org/abs/1904.03282)\r\nImage Source: [https://www.di.ens.fr/~miech/datasetviz/](https://www.di.ens.fr/~miech/datasetviz/)", "variants": ["DiDeMo"], "title": "Localizing Moments in Video with Natural Language"}
{"id": "YouTube Movie Summaries", "contents": "This dataset contains 94 movie summary videos from various YouTube channels.\r\n\r\nSource: [YouTube Movie Summaries](https://github.com/pelindogan/NeuMATCH)", "variants": ["YouTube Movie Summaries"], "title": "A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)"}
{"id": "AGRR-2019", "contents": "Consists of 7.5k sentences with gapping (as well as 15k relevant negative sentences) and comprises data from various genres: news, fiction, social media and technical texts. The dataset was prepared for the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019) - a competition aimed at stimulating the development of NLP tools and methods for processing of ellipsis. \r\n\r\nSource: [AGRR-2019: A Corpus for Gapping Resolution in Russian](/paper/agrr-2019-a-corpus-for-gapping-resolution-in)", "variants": ["AGRR-2019"], "title": "AGRR-2019: A Corpus for Gapping Resolution in Russian"}
{"id": "30MQA", "contents": "An enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions.\r\n\r\nSource: [Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus](/paper/generating-factoid-questions-with-recurrent)", "variants": ["30MQA"], "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus"}
{"id": "Definite Pronoun Resolution Dataset", "contents": "Composes sentence pairs (i.e., twin sentences).\r\n\r\nSource: [Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge](/paper/resolving-complex-cases-of-definite-pronouns)", "variants": ["Definite Pronoun Resolution Dataset"], "title": "Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge"}
{"id": "FFHQ-Aging", "contents": "**FFHQ-Aging** is a Dataset of human faces designed for benchmarking age transformation algorithms as well as many other possible vision tasks.\nThis dataset is an extention of the NVIDIA FFHQ dataset, on top of the 70,000 original FFHQ images, it also contains the following information for each image:\n* Gender information (male/female with confidence score)\n* Age group information (10 classes with confidence score)\n* Head pose (pitch, roll & yaw)\n* Glasses type (none, normal or dark)\n* Eye occlusion score (0-100, different score for each eye)\n* Full semantic map (19 classes, based on CelebAMask-HQ labels)\n\nSource: [https://github.com/royorel/FFHQ-Aging-Dataset](https://github.com/royorel/FFHQ-Aging-Dataset)\nImage Source: [https://github.com/royorel/FFHQ-Aging-Dataset](https://github.com/royorel/FFHQ-Aging-Dataset)", "variants": ["FFHQ-Aging"], "title": "Lifespan Age Transformation Synthesis"}
{"id": "MNIST", "title": "Gradient-based learning applied to document recognition", "contents": "The **MNIST** database (**Modified National Institute of Standards and Technology** database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.\r\n\r\nSource: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)\r\nImage Source: [https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png](https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png)", "variants": ["75 Superpixel MNIST", "MNIST", "MNIST-full", "MNIST-test", "SVNH-to-MNIST", "Sequential MNIST", "Moving MNIST", "Rotated MNIST", "Indexed Rotating MNIST", "MNIST (Conditional)", "Noisy MNIST", "Noisy MNIST (AWGN)", "Noisy MNIST (Contrast)", "Noisy MNIST (Motion)", "Rotating MNIST", "MNIST-to-USPS", "USPS-to-MNIST"]}
{"id": "GLUE", "title": "", "contents": "General Language Understanding Evaluation (**GLUE**) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.\r\n\r\nSource: [Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models](https://arxiv.org/abs/1908.06725)\r\nImage Source: [https://gluebenchmark.com/](https://gluebenchmark.com/)", "variants": ["CoLA", "SST-2 Binary classification", "MRPC", "STS Benchmark", "Quora Question Pairs", "MultiNLI", "QNLI", "WNLI", "RTE", "GLUE"]}
{"id": "ImageNet", "title": "ImageNet: A large-scale hierarchical image database", "contents": "The **ImageNet** dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection.\r\nThe publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld.\r\nILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”.\r\nThe ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.\r\n\r\n* Total number of non-empty WordNet synsets: 21841\r\n* Total number of images: 14197122\r\n* Number of images with bounding box annotations: 1,034,908\r\n* Number of synsets with SIFT features: 1000\r\n* Number of images with SIFT features: 1.2 million\r\n\r\nSource: [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575)\r\nImage Source: [https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/)", "variants": ["ILSVRC 2015", "ILSVRC 2016", "ImageNet", "ImageNet (Fine-grained 6 Tasks)", "ImageNet (non-targeted PGD, max perturbation=4)", "ImageNet (targeted PGD, max perturbation=16)", "ImageNet - 0-Shot", "ImageNet - 1% labeled data", "ImageNet - 10% labeled data", "ImageNet 128x128", "ImageNet 32x32", "ImageNet ReaL", "ImageNet-10", "ImageNet-A", "ImageNet-LT", "ImageNet-R", "Imagenet-dog-15", "NAS-Bench-201, ImageNet-16-120", "ImageNet-100 - 50 classes + 10 steps of 5 classes", "ImageNet-100 - 50 classes + 25 steps of 2 classes", "ImageNet-100 - 50 classes + 5 steps of 10 classes", "ImageNet-100 - 50 classes + 50 steps of 1 class", "ImageNet - 500 classes + 5 steps of 100 classes", "ImageNet - 500 classes + 50 steps of 10 classes", "ImageNet 64x64", "ImageNet64x64", "ImageNet - ResNet 50 - 90% sparsity", "ImageNet ResNet-50 - 50 Epochs", "ImageNet ResNet-50 - 60 Epochs", "ImageNet ResNet-50 - 90 Epochs", "ImageNet-100", "ImageNet-50 (5 tasks) ", "ImageNet-Caltech", "ImageNet Detection"]}
{"id": "WikiText-103", "title": "", "contents": "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\r\n\r\nCompared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\r\n\r\nSource: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\r\nImage Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)", "variants": ["WikiText-103"]}
{"id": "LFW", "title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "contents": "The **LFW** dataset contains 13,233 images of faces collected from the web. This dataset consists of the 5749 identities with 1680 people with two or more images. In the standard LFW evaluation protocol the verification accuracies are reported on 6000 face pairs.\r\n\r\nSource: [A Performance Evaluation of Loss Functions for Deep Face Recognition](https://arxiv.org/abs/1901.05903)\r\nImage Source: [http://vis-www.cs.umass.edu/lfw](http://vis-www.cs.umass.edu/lfw)", "variants": ["LFW", "Labeled Faces in the Wild", "LFW (Online Open Set)"]}
{"id": "Food-101", "title": "Food-101 - Mining Discriminative Components with Random Forests", "contents": "The  **Food-101** dataset consists of 101 food categories with 750 training and 250 test images per category, making a total of 101k images. The labels for the test images have been manually cleaned, while the training set contains some noise.\r\n\r\nSource: [Combining Weakly and Webly Supervised Learning for Classifying Food Images](https://arxiv.org/abs/1712.08730)\r\nImage Source: [https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)", "variants": ["Food-101", "Food-101N"]}
{"id": "MHP", "title": "Multiple-Human Parsing in the Wild", "contents": "The MHP dataset contains multiple persons captured in real-world scenes with pixel-level fine-grained semantic annotations in an instance-aware setting.\r\n\r\nSource: [Multiple-Human Parsing in the Wild](https://arxiv.org/pdf/1705.07206)\r\nImage Source: [Li et al](https://arxiv.org/pdf/1705.07206.pdf)", "variants": ["MHP", "MHP v2.0", "MHP v1.0", "MHP v1 0"]}
{"id": "LRW", "title": "Lip Reading in the Wild", "contents": "The **Lip Reading in the Wild** (**LRW**) dataset  a large-scale audio-visual database that contains 500 different words from over 1,000 speakers. Each utterance has 29 frames, whose boundary is centered around the target word. The database is divided into training, validation and test sets. The training set contains at least 800 utterances for each class while the validation and test sets contain 50 utterances.\r\n\r\nSource: [Towards Pose-invariant Lip-Reading](https://arxiv.org/abs/1911.06095)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)", "variants": ["LRW", "Lip Reading in the Wild", "Lipreading in the Wild"]}
{"id": "HMDB51", "title": "HMDB: A large video database for human motion recognition", "contents": "The **HMDB51** dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,849 video clips from 51 action categories (such as “jump”, “kiss” and “laugh”), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\r\n\r\nSource: [Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors](https://arxiv.org/abs/1505.04868)\r\nImage Source: [https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database)", "variants": ["HMDB-51", "HMDB51", "HMDB --> UCF (full)", "HMDBfull-to-UCF", "HMDBsmall-to-UCF", "Olympic-to-HMDBsmall", "UCF --> HMDB (full)", "UCF-to-HMDBfull", "UCF-to-HMDBsmall"]}
{"id": "TID2013", "title": "", "contents": "**TID2013** is a dataset for image quality assessment that contains 25 reference images and 3000 distorted images (25 reference images x 24 types of distortions x 5 levels of distortions). \r\n\r\nSource: [Lukin et al](https://www.researchgate.net/figure/Reference-images-in-databases-TID2008-and-TID2013_fig3_268038378)", "variants": ["TID2013"]}
{"id": "Market-1501", "title": "Scalable Person Re-Identification: A Benchmark", "contents": "**Market-1501** is a large-scale public benchmark dataset for person re-identification. It contains 1501 identities which are captured by six different cameras, and 32,668 pedestrian image bounding-boxes obtained using the Deformable Part Models pedestrian detector. Each person has 3.6 images on average at each viewpoint. The dataset is split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images are selected as probe set to find the correct match across 19,732 reference gallery images.\r\n\r\nSource: [A Survey of Pruning Methods for Efficient Person Re-identification Across Domains](https://arxiv.org/abs/1907.02547)\r\nImage Source: [https://www.researchgate.net/publication/306358716_A_Discriminative_Null_Space_based_Deep_Learning_Approach_for_Person_Re-Identification](https://www.researchgate.net/publication/306358716_A_Discriminative_Null_Space_based_Deep_Learning_Approach_for_Person_Re-Identification)", "variants": ["Duke to Market", "Market to Duke", "Market to MSMT", "DukeMTMC-reID->Market-1501", "Market-1501"]}
{"id": "CUHK01", "title": "", "contents": "This dataset contains 971 identities from two disjoint camera views. Each identity has two samples per camera view. It is used for Person Re-identification.\r\n\r\nPaper: [Li W., Zhao R., Wang X. (2013) Human Reidentification with Transferred Metric Learning. In: Lee K.M., Matsushita Y., Rehg J.M., Hu Z. (eds) Computer Vision – ACCV 2012. ACCV 2012. Lecture Notes in Computer Science, vol 7724. Springer, Berlin, Heidelberg](https://doi.org/10.1007/978-3-642-37331-2_3)", "variants": ["CUHK01"]}
{"id": "BP4D", "title": "BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database", "contents": "The **BP4D**-Spontaneous dataset is a 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains using both person-specific and generic approaches.\r\nThe database includes forty-one participants (23 women, 18 men). They were 18 – 29 years of age; 11 were Asian, 6 were African-American, 4 were Hispanic, and 20 were Euro-American.  An emotion elicitation protocol was designed to elicit emotions of participants effectively. Eight tasks were covered with an interview process and a series of activities to elicit eight emotions.\r\nThe database is structured by participants. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos. As well, the Metadata include manually annotated action units (FACS AU), automatically tracked head pose, and 2D/3D facial landmarks.  The database is in the size of about 2.6TB (without compression).\r\n\r\nSource: [http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)\r\nImage Source: [http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)", "variants": ["BP4D"]}
{"id": "DISFA", "title": "DISFA: A Spontaneous Facial Action Intensity Database", "contents": "The **Denver Intensity of Spontaneous Facial Action** (**DISFA**) dataset consists of 27 videos of 4844 frames each, with 130,788 images in total. Action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all.\r\n\r\nSource: [Deep Learning For Smile Recognition](https://arxiv.org/abs/1602.00172)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-images-extracted-from-the-DISFA-dataset_fig5_301830237](https://www.researchgate.net/figure/Examples-of-images-extracted-from-the-DISFA-dataset_fig5_301830237)", "variants": ["DISFA"]}
{"id": "SUN397", "title": "", "contents": "The Scene UNderstanding (SUN) database contains 899 categories and 130,519 images. There are 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition.\r\n\r\nImage Source: [The Selection of Useful Visual Words in Class-Imbalanced Image Classification](https://www.semanticscholar.org/paper/The-Selection-of-Useful-Visual-Words-in-Image-Chimlek-Pramokchon/2b23cff4d2072dfc85cf8b09f54475791690a68d)", "variants": ["SUN397"]}
{"id": "Hutter Prize", "title": "", "contents": "The Hutter Prize Wikipedia dataset, also known as enwiki8, is a byte-level dataset consisting of the first 100 million bytes of a Wikipedia XML dump. For simplicity we shall refer to it as a character-level dataset. Within these 100 million bytes are 205 unique tokens.\r\n\r\nSource: [NLP Progress](http://nlpprogress.com/english/language_modeling.html)", "variants": ["Hutter Prize"]}
{"id": "QuAC", "title": "QuAC: Question Answering in Context", "contents": "Question Answering in Context is a large-scale dataset that consists of around 14K crowdsourced Question Answering dialogs with 98K question-answer pairs in total. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text.\r\n\r\nSource: [https://paperswithcode.com/paper/quac-question-answering-in-context-1/](https://paperswithcode.com/paper/quac-question-answering-in-context-1/)\r\nImage Source: [https://paperswithcode.com/paper/quac-question-answering-in-context-1/](https://paperswithcode.com/paper/quac-question-answering-in-context-1/)", "variants": ["QuAC"]}
{"id": "SciTail", "title": "", "contents": "The **SciTail** dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis. We use information retrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We crowdsource the annotation of such premise-hypothesis pair as supports (entails) or not (neutral), in order to create the SciTail dataset. The dataset contains 27,026 examples with 10,101 examples with entails label and 16,925 examples with neutral label.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/scitail)\r\nImage source: [Allen Institute for AI](https://allenai.org/data/scitail)", "variants": ["SciTail"]}
{"id": "CHB-MIT", "title": "", "contents": "The **CHB-MIT** dataset is a dataset of EEG recordings from pediatric subjects with intractable seizures. Subjects were monitored for up to several days following withdrawal of anti-seizure mediation in order to characterize their seizures and assess their candidacy for surgical intervention. The dataset contains 23 patients divided among 24 cases (a patient has 2 recordings, 1.5 years apart). The dataset consists of 969 Hours of scalp EEG recordings with 173 seizures. There exist various types of seizures in the dataset (clonic, atonic, tonic). The diversity of patients (Male, Female, 10-22 years old) and different types of seizures contained in the datasets are ideal for assessing the performance of automatic seizure detection methods in realistic settings.\r\n\r\nSource: [Learning Robust Features using Deep Learning for Automatic Seizure Detection](https://arxiv.org/abs/1608.00220)\nImage Source: [https://archive.physionet.org/pn6/chbmit/](https://archive.physionet.org/pn6/chbmit/)", "variants": ["CHB-MIT"]}
{"id": "TimeBank", "title": "Enriching TimeBank: Towards a more precise annotation of temporal relations in a text", "contents": "Enriches the TimeML annotations of TimeBank by adding information about the Topic Time in terms of Klein (1994). The annotations are partly automatic, partly inferential and partly manual. The corpus was converted into the native format of the annotation software GraphAnno and POS-tagged using the Stanford bidirectional dependency network tagger. \r\n\r\nSource: [Enriching TimeBank: Towards a more precise annotation of temporal relations in a text](/paper/enriching-timebank-towards-a-more-precise)", "variants": ["TimeBank"]}
{"id": "IEMOCAP", "title": "IEMOCAP: interactive emotional dyadic motion capture database", "contents": "Multimodal Emotion Recognition **IEMOCAP** The IEMOCAP dataset consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. The dataset is recorded across 5 sessions with 5 pairs of speakers.\r\n\r\nSource: [Multi-attention Recurrent Network for Human Communication Comprehension](https://arxiv.org/abs/1802.00923)\r\nImage Source: [https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf](https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf)", "variants": ["IEMOCAP"]}
{"id": "ESC-50", "title": "ESC: Dataset for Environmental Sound Classification", "contents": "The **ESC-50** dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org.\r\n\r\nSource: [The NIGENS General Sound Events Database](https://arxiv.org/abs/1902.08314)\r\nImage Source: [https://github.com/karolpiczak/ESC-50](https://github.com/karolpiczak/ESC-50)", "variants": ["ESC-50"]}
{"id": "MMI", "title": "Web-based database for facial expression analysis", "contents": "The **MMI** Facial Expression Database consists of over 2900 videos and high-resolution still images of 75 subjects. It is fully annotated for the presence of AUs in videos (event coding), and partially coded on frame-level, indicating for each frame whether an AU is in either the neutral, onset, apex or offset phase. A small part was annotated for audio-visual laughters.\r\n\r\nSource: [https://mmifacedb.eu/](https://mmifacedb.eu/)\r\nImage Source: [https://mmifacedb.eu/](https://mmifacedb.eu/)", "variants": ["MMI"]}
{"id": "Oulu-CASIA", "title": "Facial expression recognition from near-infrared videos", "contents": "The **Oulu-CASIA** NIR&VIS facial expression database consists of six expressions (surprise, happiness, sadness, anger, fear and disgust) from 80 people between 23 and 58 years old. 73.8% of the subjects are males. The subjects were asked to sit on a chair in the observation room in a way that he/ she is in front of camera. Camera-face distance is about 60 cm. Subjects were asked to make a facial expression according to an expression example shown in picture sequences. The imaging hardware works at the rate of 25 frames per second and the image resolution is 320 × 240 pixels.\r\n\r\nSource: [Facial expression recognition from near-infrared videos](https://ieeexplore.ieee.org/abstract/document/4761697)\r\nImage Source: [https://arxiv.org/abs/1712.03474](https://arxiv.org/abs/1712.03474)", "variants": ["Oulu-CASIA", "CASIA NIR-VIS 2.0", "Oulu-CASIA NIR-VIS"]}
{"id": "SFEW", "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark", "contents": "The Static Facial Expressions in the Wild (**SFEW**) dataset is a dataset for facial expression recognition. It was created by selecting static frames from the AFEW database by computing key frames based on facial point clustering. The most commonly used version, SFEW 2.0, was the benchmarking data for the SReco sub-challenge in EmotiW 2015. SFEW 2.0 has been divided into three sets: Train (958 samples), Val (436 samples) and Test (372 samples). Each of the images is assigned to one of seven expression categories, i.e., anger, disgust, fear, neutral, happiness, sadness, and surprise. The expression labels of the training and validation sets are publicly available, whereas those of the testing set are held back by the challenge organizer.\r\n\r\nSource: [Deep Facial Expression Recognition: A Survey](https://arxiv.org/abs/1804.08348)\r\nImage Source: [https://computervisiononline.com/dataset/1105138659](https://computervisiononline.com/dataset/1105138659)", "variants": ["SFEW"]}
{"id": "UCSD", "title": "Anomaly detection in crowded scenes", "contents": "The **UCSD** Anomaly Detection Dataset was acquired with a stationary camera mounted at an elevation, overlooking pedestrian walkways. The crowd density in the walkways was variable, ranging from sparse to very crowded. In the normal setting, the video contains only pedestrians. Abnormal events are due to either:\r\nthe circulation of non pedestrian entities in the walkways\r\nanomalous pedestrian motion patterns\r\nCommonly occurring anomalies include bikers, skaters, small carts, and people walking across a walkway or in the grass that surrounds it. A few instances of people in wheelchair were also recorded. All abnormalities are naturally occurring, i.e. they were not staged for the purposes of assembling the dataset. The data was split into 2 subsets, each corresponding to a different scene. The video footage recorded from each scene was split into various clips of around 200 frames.\r\n\r\nSource: [The UCSD Anomaly Detection Dataset](http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm)\r\nImage Source: [http://www.svcl.ucsd.edu/publications/conference/2010/cvpr2010/cvpr_anomaly_2010.pdf](http://www.svcl.ucsd.edu/publications/conference/2010/cvpr2010/cvpr_anomaly_2010.pdf)", "variants": ["UCSD", "UCSD-MIT Human Motion"]}
{"id": "DCASE 2017", "title": "", "contents": "The **DCASE 2017** rare sound events dataset contains isolated sound events for three classes: 148 crying babies (mean duration 2.25s), 139 glasses breaking (mean duration 1.16s), and 187 gun shots (mean duration 1.32s). As with the DCASE 2016 data, silences are not excluded from active event markings in the annotations. While this data set contains many samples per class, there are only three classes\n\nSource: [The NIGENS General Sound Events Database](https://arxiv.org/abs/1902.08314)\nImage Source: [https://arxiv.org/pdf/1911.06878.pdf](https://arxiv.org/pdf/1911.06878.pdf)", "variants": ["DCASE 2017"]}
{"id": "OTB-2015", "title": "", "contents": "**OTB-2015**, also referred as Visual Tracker Benchmark, is a visual tracking dataset. It contains 100 commonly used video sequences for evaluating visual tracking.\r\nImage Source: [http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html](http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html)", "variants": ["OTB-2015"]}
{"id": "VOT2018", "title": "The Sixth Visual Object Tracking VOT2018 Challenge Results", "contents": "**VOT2018** is a dataset for visual object tracking. It consists of 60 challenging videos collected from real-life datasets.\r\n\r\nSource: [Remove Cosine Window from Correlation Filter-based Visual Trackers: When and How](https://arxiv.org/abs/1905.06648)\r\nImage Source: [https://www.researchgate.net/figure/Screenshots-of-the-tracking-result-from-the-proposed-algorithm-from-VOT2018-dataset-bag_fig2_336038770](https://www.researchgate.net/figure/Screenshots-of-the-tracking-result-from-the-proposed-algorithm-from-VOT2018-dataset-bag_fig2_336038770)", "variants": ["VOT2018", "VOT-2018", "VOT2017 18"]}
{"id": "SUBJ", "title": "", "contents": "Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., \"two and a half stars\") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity.\r\n\r\nSource: [SUBJ](http://www.cs.cornell.edu/people/pabo/movie-review-data/)", "variants": ["SUBJ"]}
{"id": "Middlebury", "title": "", "contents": "The **Middlebury** Stereo dataset consists of high-resolution stereo sequences with complex geometry and pixel-accurate ground-truth disparity data. The ground-truth disparities are acquired using a novel technique that employs structured lighting and does not require the calibration of the light projectors.\r\n\r\nSource: [https://vision.middlebury.edu/stereo/data/](https://vision.middlebury.edu/stereo/data/)\r\nImage Source: [https://www.researchgate.net/figure/The-stereo-matching-results-on-the-Middlebury-dataset-From-left-to-right-each-set-of_fig3_273399625](https://www.researchgate.net/figure/The-stereo-matching-results-on-the-Middlebury-dataset-From-left-to-right-each-set-of_fig3_273399625)", "variants": ["Middlebury", "Middlebury - 2x upscaling", "Middlebury - 4x upscaling"]}
{"id": "MPII", "title": "", "contents": "The **MPII** Human Pose Dataset for single person pose estimation is composed of about 25K images of which 15K are training samples, 3K are validation samples and 7K are testing samples (which labels are withheld by the authors). The images are taken from YouTube videos covering 410 different human activities and the poses are manually annotated with up to 16 body joints.\r\n\r\nSource: [2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning](https://arxiv.org/abs/1802.09232)\r\nImage Source: [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)", "variants": ["MPII", "MPII Multi-Person", "MPII Single Person"]}
{"id": "Kinetics", "title": "", "contents": "The **Kinetics** dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube.\r\n\r\nSource: [Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey](https://arxiv.org/abs/1902.06162)", "variants": ["Kinetics", "Kinetics-600", "Kinetics-700", "Kinetics-600 12 frames, 128x128", "Kinetics-600 12 frames, 64x64", "Kinetics-600 48 frames, 64x64", "Kinetics-400"]}
{"id": "TIMIT", "title": "", "contents": "The **TIMIT** Acoustic-Phonetic Continuous Speech Corpus is a standard dataset used for evaluation of automatic speech recognition systems. It consists of recordings of 630 speakers of 8 dialects of American English each reading 10 phonetically-rich sentences. It also comes with the word and phone-level transcriptions of the speech.\r\n\r\nSource: [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580)\r\nImage Source: [https://roboticrun.wordpress.com/2016/06/21/timit-introduction-the-official-doc/](https://roboticrun.wordpress.com/2016/06/21/timit-introduction-the-official-doc/)", "variants": ["TIMIT"]}
{"id": "Collective Activity", "title": "", "contents": "The **Collective Activity** Dataset contains 5 different collective activities: crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point.\n\nSource: [http://vhosts.eecs.umich.edu/vision//activity-dataset.html](http://vhosts.eecs.umich.edu/vision//activity-dataset.html)\nImage Source: [http://vhosts.eecs.umich.edu/vision//activity-dataset.html](http://vhosts.eecs.umich.edu/vision//activity-dataset.html)", "variants": ["Collective Activity"]}
{"id": "MOT16", "title": "", "contents": "The **MOT16** dataset is a dataset for multiple object tracking. It a collection of existing and new data (part of the sources are from and ), containing 14 challenging real-world videos of both static scenes and moving scenes, 7 for training and 7 for testing. It is a large-scale dataset, composed of totally 110407 bounding boxes in training set and 182326 bounding boxes in test set. All video sequences are annotated under strict standards, their ground-truths are highly accurate, making the evaluation meaningful.\r\n\r\nSource: [SOT for MOT](https://arxiv.org/abs/1712.01059)\r\nImage Source: [https://www.researchgate.net/figure/Sample-results-on-the-sequence-MOT16-07-encoded-as-in-the-previous-figure-Table-1_fig3_309641746](https://www.researchgate.net/figure/Sample-results-on-the-sequence-MOT16-07-encoded-as-in-the-previous-figure-Table-1_fig3_309641746)", "variants": ["MOT16"]}
{"id": "PASCAL VOC 2007", "title": "", "contents": "**PASCAL VOC 2007** is a dataset for image recognition. The twenty object classes that have been selected are:\r\n\r\nPerson: person\r\nAnimal: bird, cat, cow, dog, horse, sheep\r\nVehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\r\nIndoor: bottle, chair, dining table, potted plant, sofa, tv/monitor\r\n\r\nThe dataset can be used for image classification and object detection tasks.\r\n\r\nImage Source: [Object Detection and Recognition in Images](https://arxiv.org/abs/1708.01241)", "variants": ["PASCAL VOC 2007", "Pascal VOC 2007 count-test"]}
{"id": "GTEA", "title": "Learning to recognize objects in egocentric activities", "contents": "The Georgia Tech Egocentric Activities (**GTEA**) dataset contains seven types of daily activities such as making sandwich, tea, or coffee. Each activity is performed by four different people, thus totally 28 videos. For each video, there are about 20 fine-grained action instances such as take bread, pour ketchup, in approximately one minute.\r\n\r\nSource: [TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation](https://arxiv.org/abs/1705.07818)\r\nImage Source: [http://cbs.ic.gatech.edu/fpv/](http://cbs.ic.gatech.edu/fpv/)", "variants": ["GTEA"]}
{"id": "CASIA-B", "title": "", "contents": "CASIA-B is a large multiview gait database, which is created in January 2005. There are 124 subjects, and the gait data was captured from 11 views. Three variations, namely view angle, clothing and carrying condition changes, are separately considered. Besides the video files, we still provide human silhouettes extracted from video files. The detailed information about Dataset B and an evaluation framework can be found in this paper .\r\n\r\nThe format of the video filename in Dataset B is 'xxx-mm-nn-ttt.avi', where\r\n\r\n    xxx: subject id, from 001 to 124.\r\n    mm: walking status, can be 'nm' (normal), 'cl' (in a coat) or 'bg' (with a bag).\r\n    nn: sequence number.\r\n    ttt: view angle, can be '000', '018', ..., '180'.", "variants": ["CASIA-B"]}
{"id": "BIWI", "title": "Real Time Head Pose Estimation from Consumer Depth Cameras", "contents": "The dataset contains over 15K images of 20 people (6 females and 14 males - 4 people were recorded twice). For each frame, a depth image, the corresponding rgb image (both 640x480 pixels), and the annotation is provided. The head pose range covers about +-75 degrees yaw and +-60 degrees pitch. Ground truth is provided in the form of the 3D location of the head and its rotation.\r\n\r\nSource: [https://www.kaggle.com/kmader/biwi-kinect-head-pose-database](https://www.kaggle.com/kmader/biwi-kinect-head-pose-database)\r\nImage Source: [https://icu.ee.ethz.ch/research/datsets.html](https://icu.ee.ethz.ch/research/datsets.html)", "variants": ["BIWI"]}
{"id": "DAVIS 2016", "title": "", "contents": "DAVIS16 is a dataset for video object segmentation which consists of 50 videos in total (30 videos for training and 20 for testing). Per-frame pixel-wise annotations are offered.\r\n\r\nSource: [Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2008.01270)\r\nImage Source: [https://davischallenge.org/](https://davischallenge.org/)", "variants": ["DAVIS 2016", "DAVIS-2016"]}
{"id": "FBMS-59", "title": "", "contents": "The **Freiburg-Berkeley Motion Segmentation** Dataset (**FBMS-59**) is a dataset for motion segmentation, which extends the BMS-26 dataset with 33 additional video sequences. A total of 720 frames is annotated. FBMS-59 comes with a split into a training set and a test set. Typical challenges appear in both sets.\n\nSource: [https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)", "variants": ["FBMS-59"]}
{"id": "DAVIS 2017", "title": "The 2017 DAVIS Challenge on Video Object Segmentation", "contents": "DAVIS17 is a dataset for video object segmentation.  It contains a total of 150 videos - 60 for training, 30 for validation, 60 for testing\r\n\r\nSource: [Siam R-CNN: Visual Tracking by Re-Detection](https://arxiv.org/abs/1911.12836)\r\nImage Source: [https://www.researchgate.net/figure/LucidTracker-qualitative-results-on-DAVIS-17-test-dev-set-Frames-sampled-along-the_fig5_331792902](https://www.researchgate.net/figure/LucidTracker-qualitative-results-on-DAVIS-17-test-dev-set-Frames-sampled-along-the_fig5_331792902)", "variants": ["DAVIS 2017", "DAVIS 2017 (test-dev)", "DAVIS 2017 (val)", "DAVIS-2017"]}
{"id": "SVHN", "title": "Reading digits in natural images with unsupervised feature learning", "contents": "The Street View House Number (**SVHN**) is a digit classification benchmark dataset that contains 600000 32×32 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images are centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and an extra set with 530000 images that are less difficult and can be used for helping with the training process\r\n\r\nSource: [Competitive Multi-scale Convolution](https://arxiv.org/abs/1511.05635)\r\nImage Source: [http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)", "variants": ["SVHN", "SVHN, 1000 labels", "SVHN, 250 Labels", "SVHN, 40 Labels", "SVHN, 500 Labels", "SVHN-to-MNIST", "SVHN, 2000 Labels", "SVHN, 4000 Labels", "svhn,1000"]}
{"id": "CIFAR-10", "title": "", "contents": "The **CIFAR-10** dataset (Canadian Institute for Advanced Research, 10 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The images are labelled with one of 10 mutually exclusive classes: airplane, automobile (but not truck or pickup truck), bird, cat, deer, dog, frog, horse, ship, and truck (but not pickup truck). There are 6000 images per class with 5000 training and 1000 testing images per class.\r\n\r\nThe criteria for deciding whether an image belongs to a class were as follows:\r\n\r\n* The class name should be high on the list of likely answers to the question “What is in this picture?”\r\n* The image should be photo-realistic. Labelers were instructed to reject line drawings.\r\n* The image should contain only one prominent instance of the object to which the class refers.\r\nThe object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.\r\n\r\nSource: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\r\nImage Source: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)", "variants": ["CIFAR-10", "CIFAR - 10", "CIFAR-10 Image Classification", "CIFAR-10, 1000 Labels", "CIFAR-10, 2000 Labels", "CIFAR-10, 250 Labels", "CIFAR-10, 40 Labels", "CIFAR-10, 4000 Labels", "One-class CIFAR-10", "cifar10, 250 Labels", "CIFAR-10 (Conditional)", "CIFAR-10 ResNet-18 - 200 Epochs", "CIFAR-10 vs CIFAR-100", "CIFAR-10, 20 Labels", "CIFAR-10, 500 Labels", "CIFAR-10, 80 Labels", "CIFAR-10-LT (ρ=10)", "CIFAR-10-LT (ρ=100)", "CIFAR-10 WRN-28-10 - 200 Epochs", "CIFAR-10 image generation", "CIFAR-10 model detecting CIFAR-10", "CIFAR-100 WRN-28-10 - 200 Epochs", "CIFAR-100 vs CIFAR-10", "CIFAR10 100k", "cifar-10,4000", "cifar10, 10 labels"]}
{"id": "CIFAR-100", "title": "Learning multiple layers of features from tiny images", "contents": "The **CIFAR-100** dataset (Canadian Institute for Advanced Research, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 images per class. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs). There are 500 training images and 100 testing images per class.\r\n\r\nThe criteria for deciding whether an image belongs to a class were as follows:\r\n\r\n* The class name should be high on the list of likely answers to the question “What is in this picture?”\r\n* The image should be photo-realistic. Labelers were instructed to reject line drawings.\r\n* The image should contain only one prominent instance of the object to which the class refers.\r\n* The object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.\r\n\r\nSource: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\r\nImage Source: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)", "variants": ["CIFAR-100", "CIFAR-100 - 50 classes + 10 steps of 5 classes", "CIFAR-100 - 50 classes + 25 steps of 2 classes", "CIFAR-100 - 50 classes + 5 steps of 10 classes", "CIFAR-100 - 50 classes + 50 steps of 1 class", "CIFAR-100, 1000 Labels", "CIFAR-100, 2500 Labels", "CIFAR-100, 400 Labels", "CIFAR-100, 5000Labels", "CIFAR-100-LT (ρ=10)", "CIFAR-100-LT (ρ=100)", "CIFAR100 5-way (1-shot)", "Cifar100 (20 tasks)", "One-class CIFAR-100", "cifar-100, 10000 Labels", "CIFAR-10, 2000 Labeled Samples", "CIFAR-100 ResNet-18 - 200 Epochs"]}
{"id": "SK-LARGE", "title": "DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images", "contents": "**SK-LARGE** is a benchmark dataset for object skeleton detection, built on the MS COCO dataset. It contains 1491 images, 746 for training and 745 for testing.\n\nSource: [DeepFlux for Skeletons in the Wild](https://arxiv.org/abs/1811.12608)\nImage Source: [http://kaizhao.net/sk-large](http://kaizhao.net/sk-large)", "variants": ["SK-LARGE"]}
{"id": "Indian Pines", "title": "220 band AVIRIS hyperspectral image data set", "contents": "**Indian Pines** is a Hyperspectral image segmentation dataset. The input data consists of hyperspectral bands over a single landscape in Indiana, US, (Indian Pines data set) with 145×145 pixels. For each pixel, the data set contains 220 spectral reflectance bands which represent different portions of the electromagnetic spectrum in the wavelength range 0.4−2.5⋅10−6.\r\n\r\nSource: [Layer-Parallel Training of Deep Residual Neural Networks](https://arxiv.org/abs/1812.04352)\r\nImage Source: [http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Indian_Pines](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Indian_Pines)", "variants": ["Indian Pines"]}
{"id": "WebNLG", "title": "Creating Training Corpora for NLG Micro-Planners", "contents": "The **WebNLG** corpus comprises of sets of triplets describing facts (entities and relations between them) and the corresponding facts in form of natural language text. The corpus contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories.\r\n\r\nInitially, the dataset was used for the WebNLG natural language generation challenge which consists of mapping the sets of triplets to text, including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation.\r\nThe corpus is also used for a reverse task of triplets extraction.\r\n\r\nSource: [Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation](https://arxiv.org/abs/1904.03396)\r\nImage Source: [https://paperswithcode.com/paper/creating-training-corpora-for-nlg-micro/](https://paperswithcode.com/paper/creating-training-corpora-for-nlg-micro/)", "variants": ["WebNLG", "WebNLG Full", "WebNLG v2.1", "WebNLG en"]}
{"id": "IJB-A", "title": "Pushing the Frontiers of Unconstrained Face Detection and Recognition: IARPA Janus Benchmark A", "contents": "The **IARPA Janus Benchmark A** (**IJB-A**) database is developed with the aim to augment more challenges to the face recognition task by collecting facial images with a wide variations in pose, illumination, expression, resolution and occlusion. IJB-A is constructed by collecting 5,712 images and 2,085 videos from 500 identities, with an average of 11.4 images and 4.2 videos per identity.\r\n\r\nSource: [von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification](https://arxiv.org/abs/1706.04264)\r\n\r\nImage Source: [Ruan et al](https://www.researchgate.net/figure/The-IARPA-Janus-Benchmark-A-IJB-A-dataset-face-verification-11-test-protocol-a_fig12_342756996)", "variants": ["IJB-A"]}
{"id": "FG-NET", "title": "Toward Automatic Simulation of Aging Effects on Face Images", "contents": "FGNet is a dataset for age estimation and face recognition across ages. It is composed of a total of 1,002 images of 82 people with age range from 0 to 69 and an age gap up to 45 years\r\n\r\nSource: [Large age-gap face verification by feature injection in deep networks](https://arxiv.org/abs/1602.06149)\r\nImage Source: [https://www.researchgate.net/figure/Sample-images-from-the-FG-NET-Aging-database_fig1_220057621](https://www.researchgate.net/figure/Sample-images-from-the-FG-NET-Aging-database_fig1_220057621)", "variants": ["FG-NET", "FGNET"]}
{"id": "Florence", "title": "The florence 2D/3D hybrid face dataset", "contents": "The **Florence** 3D faces dataset consists of:\r\n\r\n* High-resolution 3D scans of human faces from many subjects.\r\n* Several video sequences of varying resolution, conditions and zoom level for each subject.\r\nEach subject is recorded in the following situations:\r\n* In a controlled setting in HD video.\r\n* In a less-constrained (but still indoor) setting using a standard, PTZ surveillance camera.\r\n* In an unconstrained, outdoor environment under challenging recording conditions.\r\n\r\nSource: [https://www.micc.unifi.it/resources/datasets/florence-3d-faces/](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)\r\nImage Source: [https://www.micc.unifi.it/resources/datasets/florence-3d-faces/](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)", "variants": ["Florence"]}
{"id": "CUFS", "title": "", "contents": "CUHK Face Sketch database (CUFS) is for research on face sketch synthesis and face sketch recognition. It includes 188 faces from the Chinese University of Hong Kong (CUHK) student database, 123 faces from the AR database [1], and 295 faces from the XM2VTS database [2]. There are 606 faces in total. For each face, there is a sketch drawn by an artist based on a photo taken in a frontal pose, under normal lighting condition, and with a neutral expression.\r\n\r\n[1] A. M. Martinez, and R. Benavente, “The AR Face Database,” CVC Technical Report #24, June 1998.\r\n\r\n[2] K. Messer, J. Matas, J. Kittler, J. Luettin, and G. Maitre, “XM2VTSDB: the Extended of M2VTS Database,” in Proceedings of International Conference on Audio- and Video-Based Person Authentication, pp. 72-77, 1999.\r\n\r\nSource Paper: [X. Wang and X. Tang, “Face Photo-Sketch Synthesis and Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), Vol. 31, 2009](https://ieeexplore.ieee.org/document/4624272)\r\n\r\nImage Source: [CUHK Face Sketch Database (CUFS)](http://mmlab.ie.cuhk.edu.hk/archive/facesketch.html)\r\n\r\nSource: [CUHK Face Sketch Database (CUFS)](http://mmlab.ie.cuhk.edu.hk/archive/facesketch.html)", "variants": ["CUFS"]}
{"id": "Caltech-101", "title": "Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories", "contents": "The Caltech101 dataset contains images from 101 object categories (e.g., “helicopter”, “elephant” and “chair” etc.) and a background category that contains the images not from the 101 object categories. For each object category, there are about 40 to 800 images, while most classes have about 50 images. The resolution of the image is roughly about 300×200 pixels.\r\n\r\nSource: [Simple and Efficient Learning using Privileged Information](https://arxiv.org/abs/1604.01518)", "variants": ["Caltech-101", "Caltech-101, 202 Labels"]}
{"id": "Stanford Dogs", "title": "", "contents": "The **Stanford Dogs** dataset contains 20,580 images of 120 classes of dogs from around the world, which are divided into 12,000 images for training and 8,580 images for testing.\n\nSource: [Universal-to-Specific Framework for Complex Action Recognition](https://arxiv.org/abs/2007.06149)\nImage Source: [https://www.tensorflow.org/datasets/catalog/stanford_dogs](https://www.tensorflow.org/datasets/catalog/stanford_dogs)", "variants": ["Stanford Dogs", "Stanford Dogs 5-way (1-shot)", "Stanford Dogs 5-way (5-shot)"]}
{"id": "FFHQ", "title": "A Style-Based Generator Architecture for Generative Adversarial Networks", "contents": "**Flickr-Faces-HQ (FFHQ)** consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped using dlib. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Amazon Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.\r\n\r\nSource: [Flickr-Faces-HQ Dataset (FFHQ)](https://github.com/NVlabs/ffhq-dataset)\r\nImage Source: [https://github.com/NVlabs/ffhq-dataset](https://github.com/NVlabs/ffhq-dataset)", "variants": ["FFHQ", "FFHQ 1024 x 1024", "FFHQ 1024 x 1024 - 4x upscaling", "FFHQ 256 x 256", "FFHQ 256 x 256 - 4x upscaling", "FFHQ 512 x 512 - 16x upscaling", "FFHQ 512 x 512 - 4x upscaling"]}
{"id": "RaFD", "title": "Presentation and validation of the radboud faces database", "contents": "The **Radboud Faces Database** (**RaFD**) is a set of pictures of 67 models (both adult and children, males and females) displaying 8 emotional expressions.\r\n\r\nSource: [http://www.socsci.ru.nl:8180/RaFD2/RaFD](http://www.socsci.ru.nl:8180/RaFD2/RaFD)\r\nImage Source: [http://www.socsci.ru.nl:8180/RaFD2/RaFD](http://www.socsci.ru.nl:8180/RaFD2/RaFD)", "variants": ["RaFD"]}
{"id": "WikiQA", "title": "", "contents": "The **WikiQA** corpus is a publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. In order to reflect the true information need of general users, Bing query logs were used as the question source. Each question is linked to a Wikipedia page that potentially has the answer. Because the summary section of a Wikipedia page provides the basic and usually most important information about the topic, sentences in this section were used as the candidate answers. The corpus includes 3,047 questions and 29,258 sentences, where 1,473 sentences were labeled as answer sentences to their corresponding questions.\r\n\r\nSource: [http://aka.ms/WikiQA](http://aka.ms/WikiQA)\r\nImage Source: [Yang et al](https://www.aclweb.org/anthology/D15-1237)", "variants": ["WikiQA"]}
{"id": "SimpleQuestions", "title": "Large-scale Simple Question Answering with Memory Networks", "contents": "**SimpleQuestions** is a large-scale factoid question answering dataset. It consists of 108,442 natural language questions, each paired with a corresponding fact from Freebase knowledge base. Each fact is a triple (subject, relation, object) and the answer to the question is always the object. The dataset is divided into training, validation, and test  sets with 75,910, 10,845 and 21,687 questions respectively.\r\n\r\nSource: [Hierarchical Memory Networks](https://arxiv.org/abs/1605.07427)\r\nImage Source: [https://paperswithcode.com/paper/large-scale-simple-question-answering-with/](https://paperswithcode.com/paper/large-scale-simple-question-answering-with/)", "variants": ["SimpleQuestions"]}
{"id": "WikiHop", "title": "", "contents": "**WikiHop** is a multi-hop question-answering dataset. The query of WikiHop is constructed with entities and relations from WikiData, while supporting documents are from WikiReading. A bipartite graph connecting entities and documents is first built and the answer for each query is located by traversal on this graph. Candidates that are type-consistent with the answer and share the same relation in query with the answer are included, resulting in a set of candidates. Thus, WikiHop is a multi-choice style reading comprehension data set. There are totally about 43K samples in training set, 5K samples in development set and 2.5K samples in test set. The test set is not provided. The task is to predict the correct answer given a query and multiple supporting documents.\r\n\r\nThe dataset includes a masked variant, where all candidates and their mentions in the supporting documents are replaced by random but consistent placeholder tokens.\r\n\r\nSource: [Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs](https://arxiv.org/abs/1905.07374)\r\nImage Source: [http://qangaroo.cs.ucl.ac.uk/](http://qangaroo.cs.ucl.ac.uk/)", "variants": ["WikiHop"]}
{"id": "TuSimple", "title": "TuSimple benchmark", "contents": "The **TuSimple** dataset consists of 6,408 road images on US highways. The resolution of image is 1280×720. The dataset is composed of 3,626 for training, 358 for validation, and 2,782 for testing called the TuSimple test set of which the images are under different weather conditions.\r\n\r\nSource: [End-to-End Lane Marker Detection via Row-wise Classification](https://arxiv.org/abs/2005.08630)\r\nImage Source: [https://www.researchgate.net/figure/a-Example-from-TuSimple-dataset-b-Derived-dataset-for-training-coordinate-network_fig4_330589970](https://www.researchgate.net/figure/a-Example-from-TuSimple-dataset-b-Derived-dataset-for-training-coordinate-network_fig4_330589970)", "variants": ["TuSimple"]}
{"id": "GTSRB", "title": "computer: Benchmarking machine learning algorithms for traffic sign recognition", "contents": "The **German Traffic Sign Recognition Benchmark** (**GTSRB**) contains 43 classes of traffic signs, split into 39,209 training images and 12,630 test images. The images have varying light conditions and rich backgrounds.\r\n\r\nSource: [Invisible Backdoor Attacks Against Deep Neural Networks](https://arxiv.org/abs/1909.02742)\r\nImage Source: [https://www.researchgate.net/figure/An-example-of-the-43-traffic-sign-classes-of-GTSRB-dataset_fig9_311896388](https://www.researchgate.net/figure/An-example-of-the-43-traffic-sign-classes-of-GTSRB-dataset_fig9_311896388)", "variants": ["GTSRB", "Synth Signs-to-GTSRB", "SYNSIG-to-GTSRB"]}
{"id": "PETA", "title": "Pedestrian Attribute Recognition At Far Distance", "contents": "The PEdesTrian Attribute dataset (**PETA**) is a dataset fore recognizing pedestrian attributes, such as gender and clothing style, at a far distance. It is of interest in video surveillance scenarios where face and body close-shots and hardly available. It consists of 19,000 pedestrian images with 65 attributes (61 binary and 4 multi-class). Those images contain 8705 persons.\r\n\r\nSource: [Attribute Aware Pooling for Pedestrian Attribute Recognition](https://arxiv.org/abs/1907.11837)\r\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/PETA.html](http://mmlab.ie.cuhk.edu.hk/projects/PETA.html)", "variants": ["PETA"]}
{"id": "DRIVE", "title": "Ridge-based vessel segmentation in color images of the retina", "contents": "The **Digital Retinal Images for Vessel Extraction** (**DRIVE**) dataset is a dataset for retinal vessel segmentation. It consists of a total of JPEG 40 color fundus images; including 7 abnormal pathology cases. The images were obtained from a diabetic retinopathy screening program in the Netherlands. The images were acquired using Canon CR5 non-mydriatic 3CCD camera with FOV equals to 45 degrees. Each image resolution is 584*565 pixels with eight bits per color channel (3 channels). \r\n\r\nThe set of 40 images was equally divided into 20 images for the training set and 20 images for the testing set. Inside both sets, for each image, there is circular field of view (FOV) mask of diameter that is approximately 540 pixels. Inside training set, for each image, one manual segmentation by an ophthalmological expert has been applied. Inside testing set, for each image, two manual segmentations have been applied by two different observers, where the first observer segmentation is accepted as the ground-truth for performance evaluation.\r\n\r\nSource: [Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation](https://arxiv.org/abs/1403.1735)\r\nImage Source: [https://drive.grand-challenge.org/](https://drive.grand-challenge.org/)", "variants": ["DRIVE"]}
{"id": "LUNA", "title": "", "contents": "The **LUNA** challenges provide datasets for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In [LUNA16](https://paperswithcode.com/dataset/luna16), participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified.\r\n\r\nSource: [Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge](/paper/validation-comparison-and-combination-of)", "variants": ["LUNA", "LUNA2016 FPRED", "LUNA16"]}
{"id": "Tox21", "title": "", "contents": "The **Tox21** data set comprises 12,060 training samples and 647 test samples that represent chemical compounds. There are 801 \"dense features\" that represent chemical descriptors, such as molecular weight, solubility or surface area, and 272,776 \"sparse features\" that represent chemical substructures (ECFP10, DFS6, DFS8; stored in Matrix Market Format ). Machine learning methods can either use sparse or dense data or combine them. For each sample there are 12 binary labels that represent the outcome (active/inactive) of 12 different toxicological experiments. Note that the label matrix contains many missing values (NAs). The original data source and Tox21 challenge site is https://tripod.nih.gov/tox21/challenge/.\n\nSource: [Tox21 Machine Learning Data Set](http://bioinf.jku.at/research/DeepTox/tox21.html)\nImage Source: [https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full)", "variants": ["Tox21", "Tox21 "]}
{"id": "QM9", "title": "", "contents": "**QM9** provides quantum chemical properties for a relevant, consistent, and comprehensive chemical space of small organic molecules. This database may serve the benchmarking of existing methods, development of new methods, such as hybrid quantum mechanics/machine learning, and systematic identification of structure-property relationships.\n\nSource: [QM9 Dataset](http://quantum-machine.org/datasets/)\nImage Source: [https://pubs.acs.org/doi/pdf/10.1021/ci300415d](https://pubs.acs.org/doi/pdf/10.1021/ci300415d)", "variants": ["QM9"]}
{"id": "Criteo", "title": "", "contents": "**Criteo** contains 7 days of click-through data, which is widely used for CTR prediction benchmarking. There are 26 anonymous categorical fields and 13 continuous fields in Criteo dataset.\n\nSource: [AMER: Automatic Behavior Modeling and Interaction Exploration in Recommender System](https://arxiv.org/abs/2006.05933)\nImage Source: [https://www.kaggle.com/c/criteo-display-ad-challenge](https://www.kaggle.com/c/criteo-display-ad-challenge)", "variants": ["Criteo"]}
{"id": "iPinYou", "title": "iPinYou Global RTB Bidding Algorithm Competition Dataset", "contents": "The **iPinYou** Global RTB(Real-Time Bidding) Bidding Algorithm Competition is organized by iPinYou from April 1st, 2013 to December 31st, 2013.The competition has been divided into three seasons. For each season, a training dataset is released to the competition participants, the testing dataset is reserved by iPinYou. The complete testing dataset is randomly divided into two parts: one part is the leaderboard testing dataset to score and rank the participating teams on the leaderboard, and the other part is reserved for the final offline evaluation. The participant's last offline submission is evaluated by the reserved testing dataset to get a team's offline final score. This dataset contains all three seasons training datasets and leaderboard testing datasets.The reserved testing datasets are withheld by iPinYou. The training dataset includes a set of processed iPinYou DSP bidding, impression, click, and conversion logs.\n\nSource: [iPinYou Global RTB Bidding Algorithm Competition Dataset](https://contest.ipinyou.com/)\nImage Source: [http://contest.ipinyou.com/ipinyou-dataset.pdf](http://contest.ipinyou.com/ipinyou-dataset.pdf)", "variants": ["iPinYou"]}
{"id": "Citeseer", "title": "CiteSeer: An Automatic Citation Indexing System", "contents": "The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words.\r\n\r\nSource: [https://linqs.soe.ucsc.edu/data](https://linqs.soe.ucsc.edu/data)", "variants": ["CiteSeer (0.5%)", "CiteSeer (1%)", "CiteSeer with Public Split: fixed 20 nodes per class", "CiteSeer with Public Split: fixed 5 nodes per class", "Citeseer", "Citeseer (biased evaluation)", "Citeseer (nonstandard variant)", "Citeseer Full-supervised", "Citeseer random partition", "Citeseer (weighted evaluation)"]}
{"id": "Cora", "title": "Automating the Construction of Internet Portals with Machine Learning", "contents": "The **Cora** dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\r\n\r\nSource: [https://relational.fit.cvut.cz/dataset/CORA](https://relational.fit.cvut.cz/dataset/CORA)\r\nImage Source: [https://arxiv.org/abs/1611.08402](https://arxiv.org/abs/1611.08402)", "variants": ["Cora", "Cora (0.5%)", "Cora (1%)", "Cora (3%)", "Cora with Public Split: fixed 20 nodes per class", "Cora (biased evaluation)", "Cora (nonstandard variant)", "Cora Full-supervised", "Cora random partition", "Cora: fixed 10 node per class", "Cora: fixed 20 node per class", "Cora: fixed 5 node per class", "Cora (weighted evaluation)"]}
{"id": "Pubmed", "title": "Collective Classification in Network Data", "contents": "The **Pubmed** dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words\r\n\r\nSource: [https://linqs.soe.ucsc.edu/data](https://linqs.soe.ucsc.edu/data)", "variants": ["PubMed (0.03%)", "PubMed (0.05%)", "PubMed (0.1%)", "PubMed with Public Split: fixed 20 nodes per class", "Pubmed", "Pubmed Full-supervised", "PubMed 20k RCT", "Pubmed (biased evaluation)", "Pubmed (nonstandard variant)", "Pubmed random partition", "Pubmed (weighted evaluation)"]}
{"id": "NELL", "title": "Toward an Architecture for Never-Ending Language Learning", "contents": "**NELL** is a dataset built from the Web via an intelligent agent called Never-Ending Language Learner. This agent attempts to learn over time to read the web. NELL has accumulated over 50 million candidate beliefs by reading the web, and it is considering these at different levels of confidence. NELL has high confidence in 2,810,379 of these beliefs.\r\n\r\nSource: [A Survey on Knowledge Graphs: Representation, Acquisition and Applications](https://arxiv.org/abs/2002.00388)\r\nImage Source: [http://rtw.ml.cmu.edu/rtw/](http://rtw.ml.cmu.edu/rtw/)", "variants": ["NELL"]}
{"id": "BlogCatalog", "title": "", "contents": "**BlogCatalog** is a graph dataset for a network of social relationships of bloggers listed in the BlogCatalog website. The network has 88,800 nodes and 2.1M edges.\n\nSource: [Graph Representation Learning: A Survey](https://arxiv.org/abs/1909.00958)\nImage Source: [https://www.blogcatalog.com/](https://www.blogcatalog.com/)", "variants": ["BlogCatalog"]}
{"id": "WN18", "title": "", "contents": "The **WN18** dataset has 18 relations scraped from WordNet for roughly 41,000 synsets, resulting in 141,442 triplets. It was found out that a large number of the test triplets can be found in the training set with another relation or the inverse relation. Therefore, a new version of the dataset WN18RR has been proposed to address this issue.\r\n\r\nSource: [http://nlpprogress.com/english/relation_prediction.html](http://nlpprogress.com/english/relation_prediction.html)", "variants": ["WN18", "WN18RR", "WN18 (filtered)"]}
{"id": "CACD", "title": "Cross-Age Reference Coding for Age-Invariant Face Recognition and Retrieval", "contents": "The **Cross-Age Celebrity Dataset** (**CACD**) contains 163,446 images from 2,000 celebrities collected from the Internet. The images are collected from search engines using celebrity name and year (2004-2013) as keywords. Therefore, it is possible to estimate the ages of the celebrities on the images by simply subtract the birth year from the year of which the photo was taken.\r\n\r\nSource: [https://bcsiriuschen.github.io/CARC/](https://bcsiriuschen.github.io/CARC/)\r\nImage Source: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)", "variants": ["CACD"]}
{"id": "T-LESS", "title": "T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects", "contents": "**T-LESS** is a dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects.\r\n\r\nSource: [http://cmp.felk.cvut.cz/t-less/](http://cmp.felk.cvut.cz/t-less/)\r\nImage Source: [http://cmp.felk.cvut.cz/t-less/](http://cmp.felk.cvut.cz/t-less/)", "variants": ["T-LESS"]}
{"id": "ACE 2004", "title": "Ace 2004 multilingual training corpus", "contents": "**ACE 2004** Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2004 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities and relations and was created by Linguistic Data Consortium with support from the ACE Program, with additional assistance from the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) Program.\r\nThe objective of the ACE program is to develop automatic content extraction technology to support automatic processing of human language in text form. In September 2004, sites were evaluated on system performance in six areas: Entity Detection and Recognition (EDR), Entity Mention Detection (EMD), EDR Co-reference, Relation Detection and Recognition (RDR), Relation Mention Detection (RMD), and RDR given reference entities. All tasks were evaluated in three languages: English, Chinese and Arabic.\r\n\r\nSource: [https://catalog.ldc.upenn.edu/LDC2005T09](https://catalog.ldc.upenn.edu/LDC2005T09)", "variants": ["ACE 2004", "ACE2004"]}
{"id": "ACE 2005", "title": "", "contents": "**ACE 2005** Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2005 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities, relations and events by the Linguistic Data Consortium (LDC) with support from the ACE Program and additional assistance from LDC.\n\nSource: [https://catalog.ldc.upenn.edu/LDC2006T06](https://catalog.ldc.upenn.edu/LDC2006T06)\nImage Source: [https://arxiv.org/pdf/1811.06031.pdf](https://arxiv.org/pdf/1811.06031.pdf)", "variants": ["ACE 2005"]}
{"id": "MR", "title": "", "contents": "**MR** Movie Reviews is a dataset for use in sentiment-analysis experiments. Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., \"two and a half stars\") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity.\n\nSource: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)\nImage Source: [https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png](https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png)", "variants": ["MR"]}
{"id": "STS Benchmark", "title": "", "contents": "STS Benchmark comprises a selection of the English datasets used in the STS tasks organized in the context of SemEval between 2012 and 2017. The selection of datasets include text from image captions, news headlines and user forums.\r\n\r\nSource: [STS Benchmark](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark)", "variants": ["STS Benchmark"]}
{"id": "Resume NER", "title": "Chinese NER Using Lattice LSTM", "contents": "Resume contains eight fine-grained entity categories -score from 74.5% to 86.88%.\n\nSource: [Query-Based Named Entity Recognition](https://arxiv.org/abs/1908.09138)\nImage Source: [https://arxiv.org/pdf/1805.02023.pdf](https://arxiv.org/pdf/1805.02023.pdf)", "variants": ["Resume NER"]}
{"id": "Reuters-21578", "title": "Reuters-21578", "contents": "The **Reuters-21578** dataset is a collection of documents with news articles. The original corpus has 10,369 documents and a vocabulary of 29,930 words.\r\n\r\nSource: [Topic Model Based Multi-Label Classification from the Crowd](https://arxiv.org/abs/1604.00783)", "variants": ["Reuters-21578"]}
{"id": "TACRED", "title": "TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task", "contents": "TACRED is one of the largest and most widely used RE datasets. It contains more than 106k examples annotated by crowd workers.\r\n\r\nSource: [TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task](https://arxiv.org/pdf/2004.14855v1.pdf)", "variants": ["TACRED"]}
{"id": "Natural Questions", "title": "Natural Questions: a Benchmark for Question Answering Research", "contents": "The **Natural Questions** corpus is a question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples. Each example is comprised of a google.com query and a corresponding Wikipedia page. Each Wikipedia page has a passage (or long answer) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer. The long and the short answer annotations can however be empty. If they are both empty, then there is no answer on the page at all. If the long answer annotation is non-empty, but the short answer annotation is empty, then the annotated passage answers the question but no explicit short answer could be found. Finally 1% of the documents have a passage annotated with a short answer that is “yes” or “no”, instead of a list of short spans.\r\n\r\nSource: [A BERT Baseline for the Natural Questions](https://arxiv.org/abs/1901.08634)\r\nImage Source: [https://paperswithcode.com/paper/natural-questions-a-benchmark-for-question/](https://paperswithcode.com/paper/natural-questions-a-benchmark-for-question/)", "variants": ["Natural Questions", "Natural Questions (long)", "Natural Questions (short)"]}
{"id": "PROTEINS", "title": "", "contents": "**PROTEINS** is a dataset of proteins that are classified as enzymes or non-enzymes. Nodes represent the amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart.\r\n\r\nSource: [Fast and Deep Graph Neural Networks](https://arxiv.org/abs/1911.08941)", "variants": ["PROTEINS"]}
{"id": "ENZYMES", "title": "Protein function prediction via graph kernels", "contents": "**ENZYMES** is a dataset of 600 protein tertiary structures obtained from the BRENDA enzyme database. The ENZYMES dataset contains 6 enzymes.\r\n\r\nSource: [When Work Matters: Transforming Classical Network Structures to Graph CNN](https://arxiv.org/abs/1807.02653)", "variants": ["ENZYMES"]}
{"id": "COLLAB", "title": "", "contents": "**COLLAB** is a scientific collaboration dataset. A graph corresponds to a researcher’s ego network, i.e., the researcher and its collaborators are nodes and an edge indicates collaboration between two researchers. A researcher’s ego network has three possible labels, i.e., High Energy Physics, Condensed Matter Physics, and Astro Physics, which are the fields that the researcher belongs to. The dataset has 5,000 graphs and each graph has label 0, 1, or 2.\r\n\r\nSource: [1 Introduction](https://arxiv.org/abs/2006.11165)", "variants": ["COLLAB"]}
{"id": "ChemProt", "title": "", "contents": "**ChemProt** consists of 1,820 PubMed abstracts with chemical-protein interactions annotated by domain experts and was used in the BioCreative VI text mining chemical-protein interactions shared task.\r\n\r\nSource: [Peng et al.](https://arxiv.org/pdf/1906.05474v2.pdf)", "variants": ["ChemProt"]}
{"id": "Kuzushiji-MNIST", "title": "", "contents": "Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images). Since MNIST restricts us to 10 classes, the authors chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST. Kuzushiji is a Japanese cursive writing style.\r\n\r\nSource: [Deep Learning for Classical Japanese Literature](/paper/deep-learning-for-classical-japanese)\r\nImage Source: [https://github.com/rois-codh/kmnist](https://github.com/rois-codh/kmnist)", "variants": ["Kuzushiji-MNIST"]}
{"id": "Slashdot", "title": "Signed Networks in Social Media", "contents": "The **Slashdot** dataset is a relational dataset obtained from Slashdot. Slashdot is a technology-related news website know for its specific user community. The website features user-submitted and editor-evaluated current primarily technology oriented news. In 2002 Slashdot introduced the Slashdot Zoo feature which allows users to tag each other as friends or foes. The network cotains friend/foe links between the users of Slashdot. The network was obtained in February 2009.\r\n\r\nSource: [http://snap.stanford.edu/data/soc-sign-Slashdot090221.html](http://snap.stanford.edu/data/soc-sign-Slashdot090221.html)", "variants": ["Slashdot"]}
{"id": "Udacity", "title": "", "contents": "The **Udacity** dataset is mainly composed of video frames taken from urban roads. It provides a total number of 404,916 video frames for training and 5,614 video frames for testing. This dataset is challenging due to severe lighting changes, sharp road curves and busy traffic.\n\nSource: [Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks](https://arxiv.org/abs/1811.02759)\nImage Source: [https://www.researchgate.net/figure/Sample-from-the-Udacity-dataset-with-the-original-ground-truth-bounding-boxes-Note-that_fig3_345652980](https://www.researchgate.net/figure/Sample-from-the-Udacity-dataset-with-the-original-ground-truth-bounding-boxes-Note-that_fig3_345652980)", "variants": ["Udacity"]}
{"id": "KT3DMoSeg", "title": "", "contents": "Please find more details of this dataset at https://alex-xun-xu.github.io/ProjectPage/CVPR_18/index.html\r\n\r\n3D motion segmentation has been the key problem in computer vision research due to the application in structure from motion and robotics. Traditional motion segmentation approaches are often evaluated on artificial dataset like Hopkins 155 [1] and its variants. Because the vanishing camera translation effect is often overlooked, these approaches would fail in real world scenes where camera is carrying out significant translation and scene has complex structure. We proposed the KT3DMoSeg to address the 3D motion segmentation problem in real world scenes. The KT3DMoSeg dataset was created upon the KITTI benchmark [2] by manually selecting 22 sequences and labelling each individual foreground object. We select sequence with more significant camera translation so camera mounted on moving cars are preferred. We are interested in the interplay of multiple motions, so clips with more than 3 motions are also chosen, as long as these moving objects contain enough features for forming motion hypotheses. 22 short clips, each with 10-20 frames, are chosen for evaluation. We extract dense trajectories from each sequence using [3] and prune out trajectories shorter than 5 frames.\r\n\r\nReference\r\n[1] R. Tron and R. Vidal. A Benchmark for the Comparison of 3-D Motion Segmentation Algorithms. CVPR, 2007.\r\n[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research, 2013.\r\n[3] N. Sundaram, T. Brox, and K. Keutzer. Dense point trajectories by GPU-accelerated large displacement optical flow. In ECCV, 2010.", "variants": ["KT3DMoSeg"]}
{"id": "S3DIS", "title": "", "contents": "The Stanford 3D Indoor Scene Dataset (**S3DIS**) dataset contains 6 large-scale indoor areas with 271 rooms. Each point in the scene point cloud is annotated with one of the 13 semantic categories.\n\nSource: [Grid-GCN for Fast and Scalable Point Cloud Learning](https://arxiv.org/abs/1912.02984)\nImage Source: [https://www.researchgate.net/figure/Examples-of-classified-scenes-in-S3DIS-dataset-left-with-groundtruth-right_fig2_328307943](https://www.researchgate.net/figure/Examples-of-classified-scenes-in-S3DIS-dataset-left-with-groundtruth-right_fig2_328307943)", "variants": ["S3DIS", "S3DIS Area5"]}
{"id": "VoxCeleb1", "title": "", "contents": "**VoxCeleb1** is an audio dataset containing over 100,000 utterances for 1,251 celebrities, extracted from videos uploaded to YouTube.", "variants": ["VoxCeleb1", "VoxCeleb1 - 1-shot learning", "VoxCeleb1 - 8-shot learning", "VoxCeleb1 - 32-shot learning"]}
{"id": "VQA-CP", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering", "contents": "The **VQA-CP** dataset was constructed by reorganizing VQA v2 such that the correlation between the question type and correct answer differs in the training and test splits. For example, the most common answer to questions starting with What sport… is tennis in the training set, but skiing in the test set. A model that guesses an answer primarily from the question will perform poorly.\n\nSource: [Unshuffling Data for Improved Generalization](https://arxiv.org/abs/2002.11894)\nImage Source: [https://arxiv.org/pdf/1712.00377.pdf](https://arxiv.org/pdf/1712.00377.pdf)", "variants": ["VQA-CP"]}
{"id": "QNLI", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "contents": "The **QNLI** (**Question-answering NLI**) dataset is a Natural Language Inference dataset automatically derived from the Stanford Question Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The dataset was converted into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. The QNLI dataset is part of GLEU benchmark.\r\n\r\nSource: [https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)", "variants": ["QNLI"]}
{"id": "RTE", "title": "", "contents": "The **Recognizing Textual Entailment (RTE)** datasets come from a series of textual entailment challenges. Data from RTE1, RTE2, RTE3 and RTE5 is combined. Examples are constructed based on news and Wikipedia text.", "variants": ["RTE"]}
{"id": "CODAH", "title": "CODAH: An Adversarially-Authored Question Answering Dataset for Common Sense", "contents": "The COmmonsense Dataset Adversarially-authored by Humans (**CODAH**) is an evaluation set for commonsense question-answering in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions. It contains 2801 questions in total, and uses 5-fold cross validation for evaluation.\r\n\r\nSource: [CODAH Dataset](https://github.com/Websail-NU/CODAH)\r\nImage Source: [https://www.aclweb.org/anthology/W19-2008.pdf](https://www.aclweb.org/anthology/W19-2008.pdf)", "variants": ["CODAH"]}
{"id": "DUT-OMRON", "title": "Saliency Detection via Graph-Based Manifold Ranking", "contents": "The **DUT-OMRON** dataset is used for evaluation of Salient Object Detection task and it contains 5,168 high quality images. The images have one or more salient objects and relatively cluttered background.\r\n\r\nSource: [Global Context-Aware Progressive Aggregation Network for Salient Object Detection](https://arxiv.org/abs/2003.00651)", "variants": ["DUT-OMRON"]}
{"id": "USPS", "title": "A Database for Handwritten Text Recognition Research", "contents": "**USPS** is a digit dataset automatically scanned from envelopes by the U.S. Postal Service containing a total of 9,298 16×16 pixel grayscale samples; the images are centered, normalized and show a broad range of font styles.\r\n\r\nSource: [Hallucinating Agnostic Images to Generalize Across Domains](https://arxiv.org/abs/1808.01102)\r\nImage Source: [https://ieeexplore.ieee.org/document/291440](https://ieeexplore.ieee.org/document/291440)", "variants": ["USPS", "MNIST-to-USPS", "USPS-to-MNIST"]}
{"id": "Freiburg Forest", "title": "", "contents": "The **Freiburg Forest** dataset was collected using a Viona autonomous mobile robot platform equipped with cameras for capturing multi-spectral and multi-modal images. The dataset may be used for evaluation of different perception algorithms for segmentation, detection, classification, etc. All scenes were recorded at 20 Hz with a camera resolution of 1024x768 pixels. The data was collected on three different days to have enough variability in lighting conditions as shadows and sun angles play a crucial role in the quality of acquired images. The robot traversed about 4.7 km each day. The dataset creators provide manually annotated pixel-wise ground truth segmentation masks for 6 classes: Obstacle, Trail, Sky, Grass, Vegetation, and Void.\n\nSource: [http://deepscene.cs.uni-freiburg.de/](http://deepscene.cs.uni-freiburg.de/)\nImage Source: [http://deepscene.cs.uni-freiburg.de/](http://deepscene.cs.uni-freiburg.de/)", "variants": ["Freiburg Forest", "Freiburg Forest Dataset"]}
{"id": "Nottingham", "title": "", "contents": "The **Nottingham** Dataset is a collection of 1200 American and British folk songs.\n\nSource: [Rethinking Recurrent Latent Variable Model for Music Composition](https://arxiv.org/abs/1810.03226)\nImage Source: [https://highnoongmt.wordpress.com/2018/10/02/going-to-use-the-nottingham-music-database/](https://highnoongmt.wordpress.com/2018/10/02/going-to-use-the-nottingham-music-database/)", "variants": ["Nottingham"]}
{"id": "MSR-VTT", "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language", "contents": "**MSR-VTT** (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.\r\n\r\nSource: [Learning to Discretely Compose Reasoning Module Networksfor Video Captioning](https://arxiv.org/abs/2007.09049)", "variants": ["MSR-VTT", "MSRVTT-QA", "MSR-VTT-1kA"]}
{"id": "MuPoTS-3D", "title": "", "contents": "MuPoTs-3D (Multi-person Pose estimation Test Set in 3D) is a dataset for pose estimation composed of more than 8,000 frames from 20 real-world scenes with up to three subjects. The poses are annotated with a 14-point skeleton model.\r\n\r\nSource: [DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild](https://arxiv.org/abs/2008.09457)\r\nImage Source: [http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/](http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/)", "variants": ["MuPoTS-3D"]}
{"id": "AQUAINT", "title": "", "contents": "The **AQUAINT** Corpus consists of newswire text data in English, drawn from three sources: the Xinhua News Service (People's Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. It was prepared by the LDC for the AQUAINT Project, and will be used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST).\n\nSource: [Linguistic Data Consortium](https://catalog.ldc.upenn.edu/LDC2002T31)\nImage Source: [https://catalog.ldc.upenn.edu/LDC2002T31](https://catalog.ldc.upenn.edu/LDC2002T31)", "variants": ["AQUAINT"]}
{"id": "LINNAEUS", "title": "", "contents": "LINNAEUS is a general-purpose dictionary matching software, capable of processing multiple types of document formats in the biomedical domain (MEDLINE, PMC, BMC, OTMI, text, etc.). It can produce multiple types of output (XML, HTML, tab-separated-value file, or save to a database). It also contains methods for acting as a server (including load balancing across several servers), allowing clients to request matching over a network. A package with files for recognizing and identifying species names is available for LINNAEUS, showing 94% recall and 97% precision compared to LINNAEUS-species-corpus.\r\n\r\nSource: [LINNAEUS](http://linnaeus.sourceforge.net/)", "variants": ["LINNAEUS"]}
{"id": "NLVR", "title": "A Corpus of Natural Language for Visual Reasoning", "contents": "**NLVR** contains 92,244 pairs of human-written English sentences grounded in synthetic images. Because the images are synthetically generated, this dataset can be used for semantic parsing.\r\n\r\nSource: [http://lil.nlp.cornell.edu/nlvr/](http://lil.nlp.cornell.edu/nlvr/)\r\nImage Source: [http://lil.nlp.cornell.edu/nlvr/](http://lil.nlp.cornell.edu/nlvr/)", "variants": ["NLVR", "NLVR2 Dev", "NLVR2 Test"]}
{"id": "ChestX-ray14", "title": "", "contents": "**ChestX-ray14** is a medical imaging dataset which comprises 108,948 frontal-view X-ray images of 32,717 (collected from the year of 1992 to 2015) unique patients with the text-mined fourteen common disease labels, mined from the text radiological reports via NLP techniques. It expands on ChestX-ray8 by adding six additional thorax diseases: Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia.\r\n\r\nSource: [https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610](https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610)\r\nImage Source: [https://nihcc.app.box.com/v/ChestXray-NIHCC](https://nihcc.app.box.com/v/ChestXray-NIHCC)", "variants": ["ChestX-ray14", "ChestXray14 1024x1024"]}
{"id": "Something-Something V2", "title": "", "contents": "The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels.\r\n\r\nSource: [https://20bn.com/datasets/something-something](https://20bn.com/datasets/something-something)\r\nImage Source: [https://20bn.com/datasets/something-something](https://20bn.com/datasets/something-something)", "variants": ["Something-Something V2"]}
{"id": "Jester", "title": "Eigentaste: A Constant Time Collaborative Filtering Algorithm", "contents": "6.5 million anonymous ratings of jokes by users of the **Jester** Joke Recommender System.\r\n\r\nSource: [Jester Datasets for Recommender Systems and Collaborative Filtering Research](http://eigentaste.berkeley.edu/dataset/)\r\nImage Source: [http://eigentaste.berkeley.edu/](http://eigentaste.berkeley.edu/)", "variants": ["Jester", "Jester val", "Jester test"]}
{"id": "HVU", "title": "Large Scale Holistic Video Understanding", "contents": "HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios.\r\n\r\nSource: [Large Scale Holistic Video Understanding](/paper/holistic-large-scale-video-understanding)", "variants": ["HVU"]}
{"id": "IPC-grounded", "title": "", "contents": "", "variants": ["IPC-grounded"]}
{"id": "PhysioNet Challenge 2012", "title": "", "contents": "The **PhysioNet Challenge 2012** dataset is publicly available and contains the de-identified records of 8000 patients in Intensive Care Units (ICU). Each record consists of roughly 48 hours of multivariate time series data with up to 37 features recorded at various times from the patients during their stay such as respiratory rate, glucose etc.\n\nSource: [Multi-resolution Networks For Flexible Irregular Time Series Modeling (Multi-FIT)](https://arxiv.org/abs/1905.00125)\nImage Source: [https://physionet.org/content/challenge-2016/1.0.0/](https://physionet.org/content/challenge-2016/1.0.0/)", "variants": ["PhysioNet Challenge 2012"]}
{"id": "MuJoCo", "title": "MuJoCo: A physics engine for model-based control", "contents": "**MuJoCo** (multi-joint dynamics with contact) is a physics engine used to implement environments to benchmark Reinforcement Learning methods.", "variants": ["MuJoCo"]}
{"id": "REDS", "title": "", "contents": "The realistic and dynamic scenes (**REDS**) dataset was proposed in the NTIRE19 Challenge. The dataset is composed of 300 video sequences with resolution of 720×1,280, and each video has 100 frames, where the training set, the validation set and the testing set have 240, 30 and 30 videos, respectively\n\nSource: [Video Super Resolution Based on Deep Learning: A comprehensive survey](https://arxiv.org/abs/2007.12928)\nImage Source: [https://seungjunnah.github.io/Datasets/reds.html](https://seungjunnah.github.io/Datasets/reds.html)", "variants": ["REDS"]}
{"id": "nuScenes", "title": "nuScenes: A multimodal dataset for autonomous driving", "contents": "The **nuScenes** dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360° coverage. The 3D object detection challenge evaluates the performance on 10 classes: cars, trucks, buses, trailers, construction vehicles, pedestrians, motorcycles, bicycles, traffic cones and barriers.\r\n\r\nSource: [PointPainting: Sequential Fusion for 3D Object Detection](https://arxiv.org/abs/1911.10150)", "variants": ["nuScenes", "nuScenes-F", "nuScenes-FB"]}
{"id": "VOT2019", "title": "", "contents": "**VOT2019** is a Visual Object Tracking benchmark for short-term tracking in RGB.\n\nSource: [https://www.votchallenge.net/vot2019/dataset.html](https://www.votchallenge.net/vot2019/dataset.html)\nImage Source: [https://www.votchallenge.net/vot2019/dataset.html](https://www.votchallenge.net/vot2019/dataset.html)", "variants": ["VOT2019"]}
{"id": "MUSDB18", "title": "The MUSDB18 corpus for music separation", "contents": "The **MUSDB18** is a dataset of 150 full lengths music tracks (~10h duration) of different genres along with their isolated drums, bass, vocals and others stems.\r\n\r\nThe dataset is split into training and test sets with 100 and 50 songs, respectively. All signals are stereophonic and encoded at 44.1kHz.\r\n\r\nSource: [https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems)\r\nImage Source: [https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems)", "variants": ["MUSDB18"]}
{"id": "BoolQ", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "contents": "**BoolQ** is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring – they are generated in unprompted and unconstrained settings.\r\nEach example is a triplet of (question, passage, answer), with the title of the page as optional additional context.\r\n\r\nQuestions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified and questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable” if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question’s answer is “yes” or “no”. Only questions that were marked as having a yes/no answer are used, and each question is paired with the selected passage instead of the entire document.\r\n\r\nSource: [https://github.com/google-research-datasets/boolean-questions](https://github.com/google-research-datasets/boolean-questions)\r\nImage Source: [BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions](https://paperswithcode.com/paper/boolq-exploring-the-surprising-difficulty-of/)", "variants": ["BoolQ"]}
{"id": "ORL", "title": "Parameterisation of a stochastic model for human face identification", "contents": "The **ORL** Database of Faces contains 400 images from 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). The size of each image is 92x112 pixels, with 256 grey levels per pixel.\r\n\r\nSource: [https://cam-orl.co.uk/facedatabase.html](https://cam-orl.co.uk/facedatabase.html)\r\nImage Source: [https://www.researchgate.net/publication/221786184_PCA_and_LDA_Based_Neural_Networks_for_Human_Face_Recognition](https://www.researchgate.net/publication/221786184_PCA_and_LDA_Based_Neural_Networks_for_Human_Face_Recognition)", "variants": ["ORL"]}
{"id": "Street Scene", "title": "Street Scene: A new dataset and evaluation protocol for video anomaly detection", "contents": "**Street Scene** is a dataset for video anomaly detection. Street Scene consists of 46 training and 35 testing high resolution 1280×720 video sequences taken from a USB camera overlooking a scene of a two-lane street with bike lanes and pedestrian sidewalks during daytime. The dataset is challenging because of the variety of activity taking place such as cars driving, turning, stopping and parking; pedestrians walking, jogging and pushing strollers; and bikers riding in bike lanes. In addition the videos contain changing shadows, moving background such as a flag and trees blowing in the wind, and occlusions caused by trees and large vehicles. There are a total of 56,847 frames for training and 146,410 frames for testing, extracted from the original videos at 15 frames per second. The dataset contains a total of 205 naturally occurring anomalous events ranging from illegal activities such as jaywalking and illegal U-turns to simply those that do not occur in the training set such as pets being walked and a metermaid ticketing a car.\n\nSource: [A Survey of Single-SceneVideo Anomaly Detection](https://arxiv.org/abs/2004.05993)\nImage Source: [https://www.merl.com/demos/video-anomaly-detection](https://www.merl.com/demos/video-anomaly-detection)", "variants": ["Street Scene"]}
{"id": "Wine", "title": "", "contents": "These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n\nSource: [UCI Machine Learning Repository Wine Dataset](https://archive.ics.uci.edu/ml/datasets/Wine)\nImage Source: [https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)", "variants": ["Wine"]}
{"id": "Tiny ImageNet", "title": "Tiny imagenet visual recognition challenge", "contents": "**Tiny ImageNet** contains 100000 images of 200 classes (500 for each class) downsized to 64×64 colored images. Each class has 500 training images, 50 validation images and 50 test images.\r\n\r\nSource: [Embedded Encoder-Decoder in Convolutional Networks Towards Explainable AI](https://arxiv.org/abs/2007.06712)\r\nImage Source: [https://arxiv.org/pdf/1707.08819.pdf](https://arxiv.org/pdf/1707.08819.pdf)", "variants": ["Tiny ImageNet", "Tiny ImageNet Classification", "Tiny-ImageNet"]}
{"id": "DBRD", "title": "", "contents": "The DBRD (pronounced dee-bird) dataset contains over 110k book reviews along with associated binary sentiment polarity labels. It is greatly influenced by the Large Movie Review Dataset and intended as a benchmark for sentiment classification in Dutch. \r\n\r\nSource: [DBRD](https://github.com/benjaminvdb/DBRD)", "variants": ["DBRD"]}
{"id": "Thyroid", "title": "", "contents": "**Thyroid** is a dataset for detection of thyroid diseases, in which patients diagnosed with hypothyroid or subnormal are anomalies against normal patients. It contains 2800 training data instance and 972 test instances, with 29 or so attributes.\n\nSource: [Deep Reinforcement Learning for Unknown Anomaly Detection](https://arxiv.org/abs/2009.06847)\nImage Source: [https://www.researchgate.net/figure/Features-of-Thyroid-dataset_tbl1_285711967](https://www.researchgate.net/figure/Features-of-Thyroid-dataset_tbl1_285711967)", "variants": ["Thyroid"]}
{"id": "HARD", "title": "", "contents": "The Hotel Arabic-Reviews Dataset (HARD) contains 93700 hotel reviews in Arabic language. The hotel reviews were collected from Booking.com website during June/July 2016. The reviews are expressed in Modern Standard Arabic as well as dialectal Arabic.\r\n\r\nSource: [HARD](https://github.com/elnagara/HARD-Arabic-Dataset)", "variants": ["HARD"]}
{"id": "3DFAW", "title": "The First 3D Face Alignment in the Wild (3DFAW) Challenge", "contents": "**3DFAW** contains 23k images with 66 3D face keypoint annotations.\n\nSource: [Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild](https://arxiv.org/abs/1911.11130)\nImage Source: [http://mhug.disi.unitn.it/workshop/3dfaw/](http://mhug.disi.unitn.it/workshop/3dfaw/)", "variants": ["3DFAW"]}
{"id": "ASLG-PC12", "title": "", "contents": "An artificial corpus built using grammatical dependencies rules due to the lack of resources for Sign Language.\r\n\r\nSource: [ASLG-PC12](https://achrafothman.net/site/asl-smt/)", "variants": ["ASLG-PC12"]}
{"id": "CIFAR10-DVS", "title": "", "contents": "**CIFAR10-DVS** is an event-stream dataset for object classification. 10,000 frame-based images that come from CIFAR-10 dataset are converted into 10,000 event streams with an event-based sensor, whose resolution is 128×128 pixels. The dataset has an intermediate difficulty with 10 different classes. The repeated closed-loop smooth (RCLS) movement of frame-based images is adopted to implement the conversion. Due to the transformation, they produce rich local intensity changes in continuous time which are quantized by each pixel of the event-based camera.\n\nSource: [Structure-Aware Network for Lane Marker Extraction with Dynamic Vision Sensor](https://arxiv.org/abs/2008.06204)\nImage Source: [https://www.frontiersin.org/articles/10.3389/fnins.2017.00309/full](https://www.frontiersin.org/articles/10.3389/fnins.2017.00309/full)", "variants": ["CIFAR10-DVS"]}
{"id": "Stanford Online Products", "title": "Deep Metric Learning via Lifted Structured Feature Embedding", "contents": "**Stanford Online Products** (SOP) dataset has 22,634 classes with 120,053 product images. The first 11,318 classes (59,551 images) are split for training and the other 11,316 (60,502 images) classes are used for testing\r\n\r\nSource: [Deep Metric Learning with Alternating Projections onto Feasible Sets](https://arxiv.org/abs/1907.07585)\r\nImage Source: [https://cvgl.stanford.edu/projects/lifted_struct/](https://cvgl.stanford.edu/projects/lifted_struct/)", "variants": ["Stanford Online Products"]}
{"id": "Ecoli", "title": "A Probabilistic Classification System for Predicting the Cellular Localization Sites of Proteins", "contents": "The **Ecoli** dataset is a dataset for protein localization. It contains 336 E.coli proteins split into 8 different classes.", "variants": ["Ecoli"]}
{"id": "MOT20", "title": "", "contents": "**MOT20** is a dataset for multiple object tracking. The dataset contains 8 challenging video sequences (4 train, 4 test) in unconstrained environments, from crowded places such as train stations, town squares and a sports stadium.\nImage Source: [https://motchallenge.net/vis/MOT20-04](https://motchallenge.net/vis/MOT20-04)", "variants": ["MOT20"]}
{"id": "StereoSet", "title": "StereoSet: Measuring stereotypical bias in pretrained language models", "contents": "A large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion.\r\n\r\nSource: [StereoSet: Measuring stereotypical bias in pretrained language models](/paper/stereoset-measuring-stereotypical-bias-in)", "variants": ["StereoSet"]}
{"id": "MIT-States", "title": "Discovering States and Transformations in Image Collections", "contents": "The **MIT-States** dataset has 245 object classes, 115 attribute classes and ∼53K images. There is a wide range of objects (e.g., fish, persimmon, room) and attributes (e.g., mossy, deflated, dirty). On average, each object instance is modified by one of the 9 attributes it affords.\r\n\r\nSource: [Attributes as Operators: Factorizing Unseen Attribute-Object Compositions](https://arxiv.org/abs/1803.09851)\r\nImage Source: [http://web.mit.edu/phillipi/Public/states_and_transformations/index.html](http://web.mit.edu/phillipi/Public/states_and_transformations/index.html)", "variants": ["MIT-States", "MIT-States, generalized split"]}
{"id": "SCDE", "title": "SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations", "contents": "**SCDE** is a human-created sentence cloze dataset, collected from public school English examinations in China. The task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers.\r\n\r\nSource: [SCDE](https://vgtomahawk.github.io/sced.html)", "variants": ["SCDE"]}
{"id": "Ciao", "title": "mTrust: discerning multi-faceted trust in a connected world", "contents": "The **Ciao** dataset contains rating information of users given to items, and also contain item category information. The data comes from the Epinions dataset.\r\n\r\nSource: [Collaborative Translational Metric Learning](https://arxiv.org/abs/1906.01637)", "variants": ["Ciao"]}
{"id": "SICK", "title": "A SICK cure for the evaluation of compositional distributional semantic models", "contents": "The **Sentences Involving Compositional Knowledge** (**SICK**) dataset is a dataset for compositional distributional semantics. It includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena. Each pair of sentences is annotated in two dimensions: relatedness and entailment. The relatedness score ranges from 1 to 5, and Pearson’s r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral. There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split. The sentence pairs are generated from image and video caption datasets before being paired up using some algorithm.\r\n\r\nSource: [Multi-Label Transfer Learning for Multi-Relational Semantic Similarity](https://arxiv.org/abs/1805.12501)\r\nImage Source: [https://www.researchgate.net/figure/Example-of-SICK-dataset-sentence-expansion-process-14_fig1_344863619](https://www.researchgate.net/figure/Example-of-SICK-dataset-sentence-expansion-process-14_fig1_344863619)", "variants": ["SICK"]}
{"id": "FB15k", "title": "Translating Embeddings for Modeling Multi-relational Data", "contents": "The **FB15k** dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. It has a total of  592,213 triplets with 14,951 entities and 1,345 relationships. FB15K-237 is a variant of the original dataset where inverse relations are removed, since it was found that a large number of test triplets could be obtained by inverting triplets in the training set.\r\n\r\nSource: [https://www.microsoft.com/en-us/download/details.aspx?id=52312](https://www.microsoft.com/en-us/download/details.aspx?id=52312)\r\nImage Source: [http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf](http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf)", "variants": [" FB15k", "FB15k-237", "FB15k", "FB15k (filtered)"]}
{"id": "HyperLex", "title": "HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment", "contents": "A dataset and evaluation resource that quantifies the extent of of the semantic category membership, that is, type-of relation also known as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616 concept pairs. \r\n\r\nSource: [HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment](/paper/hyperlex-a-large-scale-evaluation-of-graded)", "variants": ["HyperLex"]}
{"id": "DBLP", "title": "", "contents": "The **DBLP** is a citation network dataset. The citation data is extracted from DBLP, ACM, MAG (Microsoft Academic Graph), and other sources. The first version contains 629,814 papers and 632,752 citations. Each paper is associated with abstract, authors, year, venue, and title.\r\nThe data set can be used for clustering with network and side information, studying influence in the citation network, finding the most influential papers, topic modeling analysis, etc.\r\n\r\nSource: [https://www.aminer.org/citation](https://www.aminer.org/citation)", "variants": ["DBLP (PACT) 14k", "DBLP"]}
{"id": "ACM", "title": "", "contents": "The **ACM** dataset contains papers published in KDD, SIGMOD, SIGCOMM, MobiCOMM, and VLDB and are divided into three classes (Database, Wireless Communication, Data Mining). An heterogeneous graph is constructed, which comprises 3025 papers, 5835 authors, and 56 subjects. Paper features correspond to elements of a bag-of-words represented of keywords.\n\nSource: [https://arxiv.org/pdf/1903.07293.pdf](https://arxiv.org/pdf/1903.07293.pdf)", "variants": ["ACM"]}
{"id": "FNC-1", "title": "The fake news challenge: Exploring how artificial intelligence technologies could be leveraged to combat fake news", "contents": "**FNC-1** was designed as a stance detection dataset and it contains 75,385 labeled headline and article pairs. The pairs are labelled as either agree, disagree, discuss, and unrelated. Each headline in the dataset is phrased as a statement\r\n\r\nSource: [Investigating Rumor News Using Agreement-Aware Search](https://arxiv.org/abs/1802.07398)\nImage Source: [http://www.fakenewschallenge.org/](http://www.fakenewschallenge.org/)", "variants": ["FNC-1"]}
{"id": "AIDS", "title": "", "contents": "**AIDS** is a graph dataset. It consists of 2000 graphs representing molecular compounds which are constructed from the AIDS Antiviral Screen Database of Active Compounds. It contains 4395 chemical compounds, of which 423 belong to class CA, 1081 to CM, and the remaining compounds to CI.\r\n\r\nSource: [DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model](https://arxiv.org/abs/1712.03563)\r\nImage Source: [https://www.researchgate.net/figure/Sample-component-in-AIDS-kernel-dataset-with-Graphwave-based-structural-role-colors-Here_fig1_338282222](https://www.researchgate.net/figure/Sample-component-in-AIDS-kernel-dataset-with-Graphwave-based-structural-role-colors-Here_fig1_338282222)", "variants": ["AIDS"]}
{"id": "Sydney Urban Objects", "title": "", "contents": "This dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR, collected in the CBD of Sydney, Australia. There are 631 individual scans of objects across classes of vehicles, pedestrians, signs and trees.\n\nIt was collected in order to test matching and classification algorithms. It aims to provide non-ideal sensing conditions that are representative of practical urban sensing systems, with a large variability in viewpoint and occlusion.\n\nSource: [http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml](http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml)\nImage Source: [http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml](http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml)", "variants": ["Sydney Urban Objects"]}
{"id": "Digits", "title": "Optical recognition of handwritten digits data set", "contents": "The DIGITS dataset consists of 1797 8×8 grayscale images (1439 for training and 360 for testing) of handwritten digits.\r\n\r\nSource: [Differentially Private Variational Dropout](https://arxiv.org/abs/1712.02629)\r\nImage Source: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)", "variants": ["Optical Recognition of Handwritten Digits", "Digits"]}
{"id": "Mutagenicity", "title": "", "contents": "**Mutagenicity** is a chemical compound dataset of drugs, which can be categorized into two classes: mutagen and non-mutagen.\n\nSource: [Hierarchical Graph Pooling with Structure Learning](https://arxiv.org/abs/1911.05954)", "variants": ["Mutagenicity"]}
{"id": "SIDER", "title": "", "contents": "**SIDER** contains information on marketed medicines and their recorded adverse drug reactions. The information is extracted from public documents and package inserts. The available information include side effect frequency, drug and side effect classifications as well as links to further information, for example drug–target relations.\n\nSource: [Side Effect Resource](http://sideeffects.embl.de/)\nImage Source: [http://sideeffects.embl.de/drugs/2756/](http://sideeffects.embl.de/drugs/2756/)", "variants": ["SIDER"]}
{"id": "NVGesture", "title": "Online Detection and Classification of Dynamic Hand Gestures With Recurrent 3D Convolutional Neural Network", "contents": "The **NVGesture** dataset focuses on touchless driver controlling. It contains 1532 dynamic gestures fallen into 25 classes. It includes 1050 samples for training and 482 for testing. The videos are recorded with three modalities (RGB, depth, and infrared).\n\nSource: [Searching Multi-Rate and Multi-Modal Temporal Enhanced Networks for Gesture Recognition](https://arxiv.org/abs/2008.09412)\nImage Source: [Online Detection and Classification of Dynamic Hand Gestures With Recurrent 3D Convolutional Neural Network](https://paperswithcode.com/paper/online-detection-and-classification-of/)", "variants": [" NVGesture", "NVGesture"]}
{"id": "PenDigits", "title": "Combining multiple representations and classifiers for pen-based handwritten digit recognition", "contents": "**PenDigits** is a handwritten digits database created by collecting 250 samples from 44 writers. The samples written by 30 writers are used for training, cross-validation and writer dependent testing, and the digits written by the other 14 are used for writer independent testing.\r\n\r\nThe samples were collected using a WACOM PL-100V pressure sensitive tablet with an integrated LCD display and a cordless stylus. The tablet sends $x$ and $y$ tablet coordinates and pressure level values of the pen at fixed time intervals (sampling rate) of 100 milliseconds.\r\n\r\nThe writers were asked to write 250 digits in random order inside boxes of 500 by 500 tablet pixel resolution. Subject were monitored only during the first entry screens. Each screen contained five boxes with the digits to be written displayed above. Subjects were told to write only inside these boxes. If they made a mistake or were unhappy with their writing, they were instructed to clear the content of a box by using an on-screen button. The first ten digits were ignored because most writers were not familiar with this type of input devices, but subjects were not aware of this.\r\n\r\nSource: [UCI Machine Learning Repository PenDigits](https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits)\nImage Source: [https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.641.9269&rep=rep1&type=pdf](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.641.9269&rep=rep1&type=pdf)", "variants": ["PenDigits", "pendigits"]}
{"id": "FRGC", "title": "Overview of the Face Recognition Grand Challenge", "contents": "The data for **FRGC** consists of 50,000 recordings divided into training and validation partitions. The training partition is designed for training algorithms and the validation partition is for assessing performance of an approach in a laboratory setting. The validation partition consists of data from 4,003 subject sessions. A subject session is the set of all images of a person taken each time a person's biometric data is collected and consists of four controlled still images, two uncontrolled still images, and one three-dimensional image. The controlled images were taken in a studio setting, are full frontal facial images taken under two lighting conditions and with two facial expressions (smiling and neutral). The uncontrolled images were taken in varying illumination conditions; e.g., hallways, atriums, or outside. Each set of uncontrolled images contains two expressions, smiling and neutral. The 3D image was taken under controlled illumination conditions. The 3D images consist of both a range and a texture image. The 3D images were acquired by a Minolta Vivid 900/910 series sensor.\r\n\r\nSource: [https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc](https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc)\r\nImage Source: [https://www.researchgate.net/figure/Example-of-images-in-FRGC-20-dataset-The-dataset-consist-of-controlled-images-a-c-as_fig10_285759105](https://www.researchgate.net/figure/Example-of-images-in-FRGC-20-dataset-The-dataset-consist-of-controlled-images-a-c-as_fig10_285759105)", "variants": ["FRGC"]}
{"id": "HAR", "title": "A Public Domain Dataset for Human Activity Recognition using Smartphones", "contents": "The Human Activity Recognition Dataset has been collected from 30 subjects performing six different activities (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying). It consists of inertial sensor data that was collected using a smartphone carried by the subjects.\r\n\r\nSource: [http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones](http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)\r\nImage Source: [https://www.youtube.com/watch?v=XOEN9W05_4A](https://www.youtube.com/watch?v=XOEN9W05_4A)", "variants": ["HAR"]}
{"id": "MOT15", "title": "", "contents": "MOT2015 is a dataset for multiple object tracking. It contains 11 different indoor and outdoor scenes of public places with pedestrians as the objects of interest, where camera motion, camera angle and imaging condition vary greatly. The dataset provides detections generated by the ACF-based detector.\r\n\r\nSource: [FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional Assignment for Online Multiple Object Tracking](https://arxiv.org/abs/1904.04989)\r\nImage Source: [https://www.researchgate.net/figure/Exemplary-qualitative-tracking-results-for-the-MOT15-benchmark-dataset-a-d-are-from-a_fig1_340328377](https://www.researchgate.net/figure/Exemplary-qualitative-tracking-results-for-the-MOT15-benchmark-dataset-a-d-are-from-a_fig1_340328377)", "variants": ["2DMOT15", "2D MOT 2015", "MOT15"]}
{"id": "CASIA-MFSD", "title": "", "contents": "**CASIA-MFSD** is a dataset for face anti-spoofing. It contains 50 subjects, and 12 videos for each subject under different resolutions and light conditions. Three different spoof attacks are designed: replay, warp print and cut print attacks. The database contains 600 video recordings, in which 240 videos of 20 subjects are used for training and 360 videos of 30 subjects for testing.\r\n\r\nSource: [Improving Face Anti-Spoofing by 3D Virtual Synthesis](https://arxiv.org/abs/1901.00488)\r\nImage Source: [https://link.springer.com/referenceworkentry/10.1007%2F978-1-4899-7488-4_9067](https://link.springer.com/referenceworkentry/10.1007%2F978-1-4899-7488-4_9067)", "variants": ["CASIA-MFSD"]}
{"id": "Delicious", "title": "", "contents": "**Delicious** : This data set contains tagged web pages retrieved from the website delicious.com.\n\nSource: [Text segmentation on multilabel documents: A distant-supervised approach](https://arxiv.org/abs/1904.06730)\nImage Source: [http://mlkd.csd.auth.gr/multilabel.html](http://mlkd.csd.auth.gr/multilabel.html)", "variants": ["Delicious"]}
{"id": "DiscoFuse", "title": "DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion", "contents": "DiscoFuse was created by applying a rule-based splitting method on two corpora -\r\nsports articles crawled from the Web, and Wikipedia. See the paper for a detailed\r\ndescription of the dataset generation process and evaluation.\r\n\r\nDiscoFuse has two parts with 44,177,443 and 16,642,323 examples sourced from Sports articles and Wikipedia, respectively.\r\n\r\nFor each part, a random split is provided to train (98% of the examples), development (1%) and test (1%) sets. In addition, as the original data distribution is highly skewed (see details in the paper), a balanced version for each part is also provided.\r\n\r\nSource: [Google Research](https://github.com/google-research-datasets/discofuse)", "variants": ["DiscoFuse"]}
{"id": "QED", "title": "QED: A Framework and Dataset for Explanations in Question Answering", "contents": "**QED** is a linguistically principled framework for explanations in question answering. Given a question and a passage, QED represents an explanation of the answer as a combination of discrete, human-interpretable steps:\nsentence selection := identification of a sentence implying an answer to the question\nreferential equality := identification of noun phrases in the question and the answer sentence that refer to the same thing\npredicate entailment := confirmation that the predicate in the sentence entails the predicate in the question once referential equalities are abstracted away.\nThe QED dataset is an expert-annotated dataset of QED explanations build upon a subset of the Google Natural Questions dataset.\n\nSource: [https://github.com/google-research-datasets/QED](https://github.com/google-research-datasets/QED)\nImage Source: [https://github.com/google-research-datasets/QED](https://github.com/google-research-datasets/QED)", "variants": ["QED"]}
{"id": "DICM", "title": "Contrast enhancement based on layered difference representation", "contents": "**DICM** is a dataset for low-light enhancement which consists of 69 images collected with commercial digital cameras.\n\nSource: [Deep Retinex Decomposition for Low-Light Enhancement](https://arxiv.org/abs/1808.04560)\nImage Source: [GLADNet: Low-Light Enhancement Network with Global Awareness](https://ieeexplore.ieee.org/document/8373911)", "variants": ["DICM"]}
{"id": "ActivityNet Captions", "title": "Dense-Captioning Events in Videos", "contents": "The **ActivityNet Captions** dataset is built on ActivityNet v1.3 which includes 20k YouTube untrimmed videos with 100k caption annotations. The videos are 120 seconds long on average. Most of the videos contain over 3 annotated events with corresponding start/end time and human-written sentences, which contain 13.5 words on average. The number of videos in train/validation/test split is 10024/4926/5044, respectively.\r\n\r\nSource: [Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning](https://arxiv.org/abs/1804.00100)\r\nImage Source: [https://cs.stanford.edu/people/ranjaykrishna/densevid/](https://cs.stanford.edu/people/ranjaykrishna/densevid/)", "variants": ["ActivityNet Captions"]}
{"id": "ImageNet-C", "title": "", "contents": "**ImageNet-C** is an open source data set that consists of algorithmically generated corruptions (blur, noise) applied to the ImageNet test-set.\r\n\r\nSource: [Selective Brain Damage: Measuring the Disparate Impact of Model Pruning](https://arxiv.org/abs/1911.05248)\r\nImage Source: [https://arxiv.org/pdf/1807.01697.pdf](https://arxiv.org/pdf/1807.01697.pdf)", "variants": ["ImageNet-C"]}
{"id": "MedNLI", "title": "", "contents": "The **MedNLI** dataset consists of the sentence pairs developed by Physicians from the Past Medical History section of MIMIC-III clinical notes annotated for Definitely True, Maybe True and Definitely False. The dataset contains 11,232 training, 1,395 development and 1,422 test instances. This provides a natural language inference task (NLI) grounded in the medical history of patients.\n\nSource: [MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask Learning](https://arxiv.org/abs/2004.10220)\nImage Source: [https://arxiv.org/abs/1904.02181](https://arxiv.org/abs/1904.02181)", "variants": ["MedNLI"]}
{"id": "CID", "title": "Learning an Adaptive Model for Extreme Low-light Raw Image Processing", "contents": "The **CID** (**Campus Image Dataset**) is a dataset captured in low-light env with the help of Android programming. Its basic unit is group, which is named by capture time and contains 8 exposure-time-varying raw image shot in a burst.\n\nSource: [https://github.com/505030475/ExtremeLowLight](https://github.com/505030475/ExtremeLowLight)", "variants": ["CID"]}
{"id": "LeNER-Br", "title": "LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text", "contents": "LeNER-Br is a dataset for named entity recognition (NER) in Brazilian Legal Text.", "variants": ["LeNER-Br"]}
{"id": "ToTTo", "title": "ToTTo: A Controlled Table-To-Text Generation Dataset", "contents": "ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description.\r\n\r\nDuring the dataset creation process, tables from English Wikipedia are matched with (noisy) descriptions. Each table cell mentioned in the description is highlighted and the descriptions are iteratively cleaned and corrected to faithfully reflect the content of the highlighted cells.\r\n\r\nSource: [Google Research Datasets](https://github.com/google-research-datasets/totto)", "variants": ["ToTTo"]}
{"id": "Kumar", "title": "", "contents": "The **Kumar** dataset contains 30 1,000×1,000 image tiles from seven organs (6 breast, 6 liver, 6 kidney, 6 prostate, 2 bladder, 2 colon and 2 stomach) of The Cancer Genome Atlas (TCGA) database acquired at 40× magnification. Within each image, the boundary of each nucleus is fully annotated.\r\n\r\nSource: [Dense Steerable Filter CNNs for Exploiting Rotational Symmetry in Histology Images](https://arxiv.org/abs/2004.03037)", "variants": ["Kumar"]}
{"id": "OpenBookQA", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering", "contents": "**OpenBookQA** is a new kind of question-answering dataset modeled after open book exams for assessing human understanding of a subject. It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), which probe the understanding of a small “book” of 1,326 core science facts and the application of these facts to novel situations. For training, the dataset includes a mapping from each question to the core science fact it was designed to probe. Answering OpenBookQA questions requires additional broad common knowledge, not contained in the book. The questions, by design, are answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.\r\nAdditionally, the dataset includes a collection of 5,167 crowd-sourced common knowledge facts, and an expanded version of the train/dev/test questions where each question is associated with its originating core fact, a human accuracy score, a clarity score, and an anonymized crowd-worker ID.\r\n\r\nSource: [https://allenai.org/data/open-book-qa](https://allenai.org/data/open-book-qa)\r\nImage Source: [https://arxiv.org/pdf/1809.02789.pdf](https://arxiv.org/pdf/1809.02789.pdf)", "variants": ["OpenBookQA"]}
{"id": "arXiv", "title": "", "contents": "**Arxiv HEP-TH (high energy physics theory) citation graph** is from the e-print **arXiv** and covers all the citations within a dataset of 27,770 papers with 352,807 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this.\nThe data covers papers in the period from January 1993 to April 2003 (124 months).\n\nSource: [https://snap.stanford.edu/data/cit-HepTh.html](https://snap.stanford.edu/data/cit-HepTh.html)", "variants": ["arXiv", "arXiv-AstroPh 3-clique", "arXiv-GrQc 4-clique", "arXiv-Long Test", "arXiv-Long Val"]}
{"id": "EmoryNLP", "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks", "contents": "EmoryNLP comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)’s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral.", "variants": ["EmoryNLP"]}
{"id": "WHAMR!", "title": "", "contents": "**WHAMR!** is a dataset for noisy and reverberant speech separation. It extends [WHAM!](/dataset/wham) by introducing synthetic reverberation to the\r\nspeech sources in addition to the existing noise. Room impulse responses were generated and convolved using `pyroomacoustics`. Reverberation times were chosen to approximate domestic and classroom environments (expected to be similar to the restaurants and coffee shops where the WHAM! noise was collected), and\r\nfurther classified as high, medium, and low reverberation based on a\r\nqualitative assessment of the mixture’s noise recording.", "variants": ["WHAMR!"]}
{"id": "Abalone", "title": "", "contents": "Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.\n\nSource: [UCL Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Abalone)\nImage Source: [http://archive.ics.uci.edu/ml/datasets/Abalone](http://archive.ics.uci.edu/ml/datasets/Abalone)", "variants": ["Abalone"]}
{"id": "NetHack Learning Environment", "title": "The NetHack Learning Environment", "contents": "The **NetHack Learning Environment** (NLE) is a Reinforcement Learning environment based on NetHack 3.6.6. It is designed to provide a standard reinforcement learning interface to the game, and comes with tasks that function as a first step to evaluate agents on this new environment.\nNetHack is one of the oldest and arguably most impactful videogames in history, as well as being one of the hardest roguelikes currently being played by humans. It is procedurally generated, rich in entities and dynamics, and overall an extremely challenging environment for current state-of-the-art RL agents, while being much cheaper to run compared to other challenging testbeds. Through NLE, the authors wish to establish NetHack as one of the next challenges for research in decision making and machine learning.\n\nSource: [https://github.com/facebookresearch/nle](https://github.com/facebookresearch/nle)\nImage Source: [https://github.com/facebookresearch/nle](https://github.com/facebookresearch/nle)", "variants": ["NetHack Learning Environment"]}
{"id": "CVC-ClinicDB", "title": "", "contents": "**CVC-ClinicDB** is an open-access dataset of 612 images with a resolution of 384×288 from 31 colonoscopy sequences.It is used for medical image segmentation, in particular polyp detection in colonoscopy videos.\n\nSource: [ResUNet++: An Advanced Architecture for Medical Image Segmentation](https://arxiv.org/abs/1911.07067)\nImage Source: [https://polyp.grand-challenge.org/CVCClinicDB/](https://polyp.grand-challenge.org/CVCClinicDB/)", "variants": ["CVC-ClinicDB"]}
{"id": "ImageNet-R", "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization", "contents": "ImageNet-R(endition) contains art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes.\r\n\r\nImageNet-R has renditions of 200 ImageNet classes resulting in 30,000 images.\r\n\r\nSource: [The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](/paper/the-many-faces-of-robustness-a-critical)", "variants": ["ImageNet-R"]}
{"id": "ROCStories", "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", "contents": "**ROCStories** is a collection of commonsense short stories. The corpus consists of 100,000 five-sentence stories. Each story logically follows everyday topics created by Amazon Mechanical Turk workers. These stories contain a variety of commonsense causal and temporal relations between everyday events. Writers also develop an additional 3,742 Story Cloze Test stories which contain a four-sentence-long body and two candidate endings. The endings were collected by asking Mechanical Turk workers to write both a right ending and a wrong ending after eliminating original endings of given short stories. Both endings were required to make logical sense and include at least one character from the main story line. The published ROCStories dataset is constructed with ROCStories as a training set that includes 98,162 stories that exclude candidate wrong endings, an evaluation set, and a test set, which have the same structure (1 body + 2 candidate endings) and a size of 1,871.\r\n\r\nSource: [Incorporating Structured Commonsense Knowledge in Story Completion](https://arxiv.org/abs/1811.00625)\r\nImage Source: [https://cs.rochester.edu/nlp/rocstories/](https://cs.rochester.edu/nlp/rocstories/)", "variants": ["Story Cloze Test", "ROCStories"]}
{"id": "ePillID", "title": "ePillID Dataset: A Low-Shot Fine-Grained Benchmark for Pill Identification", "contents": "**ePillID** is a benchmark for developing and evaluating computer vision models for pill identification. The ePillID benchmark is designed as a low-shot fine-grained benchmark, reflecting real-world challenges for developing image-based pill identification systems.\nThe characteristics of the ePillID benchmark include:\n* Reference and consumer images: The reference images are taken with controlled lighting and backgrounds, and with professional equipment. The consumer images are taken with real-world settings including different lighting, backgrounds, and equipment. For most of the pills, one image per side (two images per pill type) is available from the NIH Pillbox dataset.\n* Low-shot and fine-grained setting: 13k images representing 9804 appearance classes (two sides for 4902 pill types). For most of the appearance classes, there exists only one reference image, making it a challenging low-shot recognition setting.\n\nSource: [https://github.com/usuyama/ePillID-benchmark](https://github.com/usuyama/ePillID-benchmark)\nImage Source: [https://github.com/usuyama/ePillID-benchmark](https://github.com/usuyama/ePillID-benchmark)", "variants": ["ePillID"]}
{"id": "AViD", "title": "AViD Dataset: Anonymized Videos from Diverse Countries", "contents": "Is a collection of action videos from many different countries. The motivation is to create a public dataset that would benefit training and pretraining of action recognition models for everybody, rather than making it useful for limited countries.\r\n\r\nSource: [AViD Dataset: Anonymized Videos from Diverse Countries](/paper/avid-dataset-anonymized-videos-from-diverse)", "variants": ["AViD"]}
{"id": "SVG-Icons8", "title": "DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation", "contents": "A new large-scale dataset along with an open-source library for SVG manipulation.\r\n\r\nSource: [DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation](/paper/deepsvg-a-hierarchical-generative-network-for)", "variants": ["SVG-Icons8"]}
{"id": "Binarized MNIST", "title": "", "contents": "A binarized version of MNIST.\r\n\r\nSource: [Binarized MNIST](http://www.dmi.usherb.ca/~larocheh/mlpython/_modules/datasets/binarized_mnist.html)", "variants": ["Binarized MNIST"]}
{"id": "Sprites", "title": "Deep Visual Analogy-Making", "contents": "The **Sprites** dataset contains 60 pixel color images of animated characters (sprites). There are 672 sprites, 500 for training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images, so the full dataset has 120K images in total. There are many changes in the appearance of the sprites, they differ in their body shape, gender, hair, armor, arm type, greaves, and weapon.\r\n\r\nSource: [Challenges in Disentangling Independent Factors of Variation](https://arxiv.org/abs/1711.02245)", "variants": ["Colored dSprites", "Sprites"]}
{"id": "Skeleton-Mimetics", "title": "", "contents": "A dataset derived from the recently introduced Mimetics dataset.\r\n\r\nSource: [Quo Vadis, Skeleton Action Recognition ?](/paper/quo-vadis-skeleton-action-recognition)", "variants": ["Skeleton-Mimetics"]}
{"id": "Universal Dependencies", "title": "Universal Dependencies v1: A Multilingual Treebank Collection", "contents": "The **Universal Dependencies** (UD) project seeks to develop cross-linguistically consistent treebank annotation of morphology and syntax for multiple languages. The first version of the dataset was released in 2015 and consisted of 10 treebanks over 10 languages. Version 2.7 released in 2020 consists of 183 treebanks over 104 languages. The annotation consists of UPOS (universal part-of-speech tags), XPOS (language-specific part-of-speech tags), Feats (universal morphological features), Lemmas, dependency heads and universal dependency labels.\r\n\r\nSource: [Evaluating Contextualized Embeddings on 54 Languagesin POS Tagging, Lemmatization and Dependency Parsing](https://arxiv.org/abs/1908.07448)\r\nImage Source: [https://universaldependencies.org/introduction.html](https://universaldependencies.org/introduction.html)", "variants": ["UD", "Universal Dependencies", "Universal Dependency Treebank"]}
{"id": "UAVA", "title": "DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose Estimation via a Smooth Silhouette Loss", "contents": "The UAVA,<i>UAV-Assistant</i>, dataset is specifically designed for fostering applications which consider UAVs and humans as cooperative agents.\r\nWe employ a real-world 3D scanned dataset (<a href=\"https://niessner.github.io/Matterport/\">Matterport3D</a>), physically-based rendering, a gamiﬁed simulator for realistic drone navigation trajectory collection, to generate realistic multimodal data both from the user’s exocentric view of the drone, as well as the drone’s egocentric view.", "variants": ["UAVA"]}
{"id": "CMU Panoptic", "title": "Panoptic Studio: A Massively Multiview System for Social Motion Capture", "contents": "**CMU Panoptic** is a large scale dataset providing 3D pose annotations for multiple people engaging social activities. It contains 65 videos with multi-view annotations, but only 17 of them are in multi-person scenario and have the camera parameters.\r\n\r\nSource: [Single-Stage Multi-Person Pose Machines](https://arxiv.org/abs/1908.09220)\r\nImage Source: [http://domedb.perception.cs.cmu.edu/](http://domedb.perception.cs.cmu.edu/)", "variants": ["CMU Panoptic"]}
{"id": "ContactPose", "title": "ContactPose: A Dataset of Grasps with Object Contact and Hand Pose", "contents": "ContactPose is a dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images. \r\n\r\nSource: [ContactPose: A Dataset of Grasps with Object Contact and Hand Pose](/paper/contactpose-a-dataset-of-grasps-with-object)", "variants": ["ContactPose"]}
{"id": "ASSET", "title": "", "contents": "ASSET is a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations.\r\n\r\nSource: [ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations](https://arxiv.org/pdf/2005.00481v1.pdf)", "variants": ["ASSET"]}
{"id": "NJU2K", "title": "Depth saliency based on anisotropic center-surround difference", "contents": "**NJU2K** is a large RGB-D dataset containing 1,985 image pairs. The stereo images were collected from the Internet and 3D movies, while photographs were taken by a Fuji W3 camera.\n\nSource: [Bifurcated Backbone Strategy for RGB-D Salient Object Detection](https://arxiv.org/abs/2007.02713)\nImage Source: [Depth saliency based on anisotropic center-surround difference](https://doi.org/10.1109/ICIP.2014.7025222)", "variants": ["NJU2K"]}
{"id": "Cam2BEV", "title": "A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View", "contents": "The [dataset](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data) contains two subsets of synthetic, semantically segmented road-scene images, which have been created for developing and applying the methodology described in the paper **\"A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird’s Eye View\"** ([IEEE Xplore](https://ieeexplore.ieee.org/document/9294462), [arXiv](http://arxiv.org/abs/2005.04078), [YouTube](https://www.youtube.com/watch?v=TzXuwt56a0E))\r\n\r\nThe dataset can be used through the official code implementation of the Cam2BEV methodology described on [Github](https://github.com/ika-rwth-aachen/Cam2BEV).\r\n\r\n\r\n| Dataset | # Training Samples | # Validation Samples | # Vehicle Cameras | # Semantic Classes | Contained Images (examples) |\r\n| --- | --- | --- | --- | --- | --- |\r\n| [Dataset 1](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/tree/master/2_F): 360° Surround | 33199 | 3731 | 4 (front, rear, left, right) | 30 (CityScapes) | [front camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/front.png), [rear camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/rear.png), [left camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/left.png), [right camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/right.png), [bird's eye view](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/bev.png), [bird's eye view incl. occlusion](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples bev+occlusion.png), [homography view](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/homography.png) |\r\n| [Dataset 2](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/tree/master/2_F): Front Camera only | 32246 | 3172 | 1 (front) | 30 (CityScapes) | [front camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/2_F/examples/front.png), [bird's eye view](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/2_F/examples/bev.png), [bird's eye view incl. occlusion](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/2_F/examples/bev+occlusion.png), [homography view](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/2_F/examples/homography.png) |", "variants": ["Cam2BEV"]}
{"id": "UNSW-NB15", "title": "UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)", "contents": "**UNSW-NB15** is a network intrusion dataset. It contains nine different attacks, includes DoS, worms, Backdoors, and Fuzzers. The dataset contains raw network packets. The number of records in the training set is 175,341 records and the testing set is 82,332 records from the different types, attack and normal.\r\n\r\nSource: [Evaluation of Adversarial Training on Different Types of Neural Networks in Deep Learning-based IDSs](https://arxiv.org/abs/2007.04472)\r\nImage Source: [https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/)\r\nPaper: [UNSW-NB15: a comprehensive data set for network intrusion detection systems](https://doi.org/10.1109/MilCIS.2015.7348942)", "variants": ["UNSW-NB15"]}
{"id": "FarsTail", "title": "FarsTail: A Persian Natural Language Inference Dataset", "contents": "Natural Language Inference (NLI), also called Textual Entailment, is an important task in NLP with the goal of determining the inference relationship between a premise p and a hypothesis h. It is a three-class problem, where each pair (p, h) is assigned to one of these classes: \"ENTAILMENT\" if the hypothesis can be inferred from the premise, \"CONTRADICTION\" if the hypothesis contradicts the premise, and \"NEUTRAL\" if none of the above holds. There are large datasets such as SNLI, MNLI, and SciTail for NLI in English, but there are few datasets for poor-data languages like Persian. Persian (Farsi) language is a pluricentric language spoken by around 110 million people in countries like Iran, Afghanistan, and Tajikistan. **FarsTail** is the first relatively large-scale Persian dataset for NLI task. A total of 10,367 samples are generated from a collection of 3,539 multiple-choice questions. The train, validation, and test portions include 7,266, 1,537, and 1,564 instances, respectively.\n\nSource: [https://github.com/dml-qom/FarsTail](https://github.com/dml-qom/FarsTail)\nImage Source: [https://github.com/dml-qom/FarsTail](https://github.com/dml-qom/FarsTail)", "variants": ["FarsTail"]}
{"id": "RITE", "title": "", "contents": "The RITE (Retinal Images vessel Tree Extraction) is a database that enables comparative studies on segmentation or classification of arteries and veins on retinal fundus images, which is established based on the public available DRIVE database (Digital Retinal Images for Vessel Extraction).\r\n\r\nRITE contains 40 sets of images, equally separated into a training subset and a test subset, the same as DRIVE. The two subsets are built from the corresponding two subsets in DRIVE. For each set, there is a fundus photograph, a vessel reference standard, and a Arteries/Veins (A/V) reference standard. \r\n\r\n* The fundus photograph is inherited from DRIVE. \r\n* For the training set, the vessel reference standard is a modified version of 1st_manual from DRIVE. \r\n* For the test set, the vessel reference standard is 2nd_manual from DRIVE. \r\n* For the A/V reference standard, four types of vessels are labelled using four colors based on the vessel reference standard. \r\n* Arteries are labelled in red; veins are labelled in blue; the overlapping of arteries and veins are labelled in green; the vessels which are uncertain are labelled in white. \r\n* The fundus photograph is in tif format. And the vessel reference standard and the A/V reference standard are in png format.  \r\n\r\nThe dataset is described in more detail in our paper, which you will cite if you use the dataset in any way: \r\n\r\n Hu Q, Abràmoff MD, Garvin MK. Automated separation of binary overlapping trees in low-contrast color retinal images. Med Image Comput Comput Assist Interv. 2013;16(Pt 2):436-43. PubMed PMID: 24579170 https://doi.org/10.1007/978-3-642-40763-5_54", "variants": ["RITE"]}
{"id": "TVR", "title": "", "contents": "A new multimodal retrieval dataset. TVR requires systems to understand both videos and their associated subtitle (dialogue) texts, making it more realistic. The dataset contains 109K queries collected on 21.8K videos from 6 TV shows of diverse genres, where each query is associated with a tight temporal window. \r\n\r\nSource: [TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval](/paper/tvr-a-large-scale-dataset-for-video-subtitle)", "variants": ["TVR"]}
{"id": "DialogRE", "title": "Dialogue-Based Relation Extraction", "contents": "**DialogRE** is the first human-annotated dialogue-based relation extraction dataset, containing 1,788 dialogues originating from the complete transcripts of a famous American television situation comedy Friends. The are annotations for all occurrences of 36 possible relation types that exist between an argument pair in a dialogue. DialogRE is available in English and Chinese.\r\n\r\nSource: [DialogRE](https://dataset.org/dialogre/)", "variants": ["DialogRE"]}
{"id": "ToLD-Br", "title": "Toxic Language Detection in Social Media for Brazilian Portuguese: New Dataset and Multilingual Analysis", "contents": "The **Toxic Language Detection for Brazilian Portuguese** (**ToLD-Br**) is a dataset with tweets in Brazilian Portuguese annotated according to different toxic aspects.\n\nSource: [https://github.com/JAugusto97/ToLD-Br](https://github.com/JAugusto97/ToLD-Br)", "variants": ["ToLD-Br"]}
{"id": "DeepFix", "title": "DeepFix: Fixing Common C Language Errors by Deep Learning", "contents": "**DeepFix** consists of a program repair dataset (fix compiler errors in C programs). It enables research around automatically fixing programming errors using deep learning.", "variants": ["DeepFix"]}
{"id": "DUC 2004", "title": "", "contents": "The DUC2004 dataset is a dataset for document summarization. Is designed and used for testing only. It consists of 500 news articles, each paired with four human written summaries. Specifically it consists of 50 clusters of Text REtrieval Conference (TREC) documents, from the following collections: AP newswire, 1998-2000; New York Times newswire, 1998-2000; Xinhua News Agency (English version), 1996-2000. Each cluster contained on average 10 documents.\n\nSource: [Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction](https://arxiv.org/abs/2005.01791)\nImage Source: [https://duc.nist.gov/duc2004/](https://duc.nist.gov/duc2004/)", "variants": ["DUC 2004 Task 1", "DUC 2004"]}
{"id": "UT-Interaction", "title": "UT-Interaction Dataset, ICPR contest on Semantic Description of Human Activities (SDHA)", "contents": "The **UT-Interaction** dataset contains videos of continuous executions of 6 classes of human-human interactions: shake-hands, point, hug, push, kick and punch. Ground truth labels for these interactions are provided, including time intervals and bounding boxes. There is a total of 20 video sequences whose lengths are around 1 minute. Each video contains at least one execution per interaction, resulting in 8 executions of human activities per video on average. Several participants with more than 15 different clothing conditions appear in the videos. The videos are taken with the resolution of 720*480, 30fps, and the height of a person in the video is about 200 pixels.\r\n\r\nSource: [https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)\r\nImage Source: [https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)", "variants": ["UT-Interaction"]}
{"id": "eQASC", "title": "Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering", "contents": "This dataset contains 98k 2-hop explanations for questions in the QASC dataset, with annotations indicating if they are valid (~25k) or invalid (~73k) explanations.\r\n\r\nThis repository addresses the current lack of training data for distinguish valid multihop explanations from invalid, by providing three new datasets. The main one, eQASC, contains 98k explanation annotations for the multihop question answering dataset [QASC](https://allenai.org/data/qasc), and is the first that annotates multiple candidate explanations for each answer.\r\n\r\nThe second dataset, eQASC-perturbed, is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the [OBQA dataset](https://allenai.org/data/open-book-qa) to test generalization of models trained on eQASC.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/eqasc)", "variants": ["eQASC"]}
{"id": "ImageNet-LT", "title": "", "contents": "**ImageNet Long-Tailed** is a subset of [/dataset/imagenet](ImageNet) dataset consisting of 115.8K images from 1000 categories, with maximally 1280 images per class and minimally 5 images per class. The additional classes of images in ImageNet-2010 are used as the open set.\r\n\r\nSource: [Large-Scale Long-Tailed Recognition in an Open World](https://arxiv.org/pdf/1904.05160v2.pdf)", "variants": ["ImageNet-LT", "ImageNet-LT-d"]}
{"id": "Salinas", "title": "", "contents": "**Salinas Scene** is a hyperspectral dataset collected by the 224-band AVIRIS sensor over Salinas Valley, California, and is characterized by high spatial resolution (3.7-meter pixels). The area covered comprises 512 lines by 217 samples. 20 water absorption bands were discarder: [108-112], [154-167], 224. This image was available only as at-sensor radiance data. It includes vegetables, bare soils, and vineyard fields. Salinas groundtruth contains 16 classes.", "variants": ["Salinas Scene", "Salinas"]}
{"id": "XCOPA", "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "contents": "The Cross-lingual Choice of Plausible Alternatives (**XCOPA**) dataset is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning across languages. The dataset is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around the globe. The dataset is challenging as it requires both the command of world knowledge and the ability to generalise to new languages.\n\nSource: [https://github.com/cambridgeltl/xcopa](https://github.com/cambridgeltl/xcopa)", "variants": ["XCOPA"]}
{"id": "DebateSum", "title": "DebateSum: A large-scale argument mining and summarization dataset", "contents": "**DebateSum** consists of 187328 debate documents, arguments (also can be thought of as abstractive summaries, or queries), word-level extractive summaries, citations, and associated metadata organized by topic-year. This data is ready for analysis by NLP systems.\n\nSource: [https://github.com/Hellisotherpeople/DebateSum](https://github.com/Hellisotherpeople/DebateSum)", "variants": ["DebateSum"]}
{"id": "SEN12MS-CR", "title": "Multi-Sensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery", "contents": "Curates a large novel data set for training new cloud removal approaches and evaluate on two recently proposed performance metrics of image quality and diversity.\r\n\r\nSource: [Multi-Sensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery](/paper/multi-sensor-data-fusion-for-cloud-removal-in)", "variants": ["SEN12MS-CR"]}
{"id": "GoEmotions", "title": "GoEmotions: A Dataset of Fine-Grained Emotions", "contents": "**GoEmotions** is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.\r\n\r\n- Number of examples: 58,009.\r\n- Number of labels: 27 + Neutral.\r\n- Maximum sequence length in training and evaluation datasets: 30.\r\n\r\nOn top of the raw data, the dataset also includes a version filtered based on reter-agreement, which contains a train/test/validation split:\r\n\r\n- Size of training dataset: 43,410.\r\n- Size of test dataset: 5,427.\r\n- Size of validation dataset: 5,426.\r\n\r\nThe emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.\r\n\r\nSource: [Google Research](https://github.com/google-research/google-research/tree/master/goemotions)", "variants": ["GoEmotions"]}
{"id": "RecipeNLG", "title": "RecipeNLG: A Cooking Recipes Dataset for Semi-Structured Text Generation", "contents": "", "variants": ["RecipeNLG"]}
{"id": "WHO-COVID19 Dataset", "title": "", "contents": "COVID19 Data from the World Health Organization", "variants": ["WHO-COVID19 Dataset"]}
{"id": "DDRel", "title": "DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues", "contents": "**DDRel** is a dataset for interpersonal relation classification in dyadic dialogues. It consists of 6,300 dyadic dialogue sessions between 694 pairs of speakers with 53,126 utterances in total. It is constructed by crawling movie scripts from IMSDb and annotating the relation labels for each session according to 13 pre-defines relationships.\n\nSource: [https://github.com/JiaQiSJTU/DialogueRelationClassification](https://github.com/JiaQiSJTU/DialogueRelationClassification)", "variants": ["DDRel"]}
{"id": "HateXplain", "title": "HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection", "contents": "Covers multiple aspects of the issue. Each post in the dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based.\r\n\r\nSource: [HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection](/paper/hatexplain-a-benchmark-dataset-for)", "variants": ["HateXplain"]}
{"id": "RECCON", "title": "Recognizing Emotion Cause in Conversations", "contents": "RECCON is a dataset for the task of recognizing emotion cause in conversations.\r\n\r\nSource: [Recognizing Emotion Cause in Conversations](https://arxiv.org/pdf/2012.11820.pdf)", "variants": ["RECCON"]}
{"id": "Kennedy Space Center", "title": "", "contents": "**Kennedy Space Center** is a dataset for the classification of wetland vegetation at the Kennedy Space Center, Florida using hyperspectral imagery. Hyperspectral data were acquired over KSC on March 23, 1996 using JPL's Airborne Visible/Infrared Imaging Spectrometer.", "variants": ["Kennedy Space Center"]}
{"id": "Multi-PIE", "title": "Multi-PIE", "contents": "The **Multi-PIE** (Multi Pose, Illumination, Expressions) dataset consists of face images of 337 subjects taken under different pose, illumination and expressions. The pose range contains 15 discrete views, capturing a face profile-to-profile. Illumination changes were modeled using 19 flashlights located in different places of the room.\r\n\r\nSource: [Hybrid VAE: Improving Deep Generative Models using Partial Observations](https://arxiv.org/abs/1711.11566)", "variants": ["Multi-PIE"]}
{"id": "The Pile", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "contents": "The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together.", "variants": ["The Pile"]}
{"id": "S2ORC", "title": "S2ORC: The Semantic Scholar Open Research Corpus", "contents": "A large corpus of 81.1M English-language academic papers spanning many academic disciplines. Rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. Aggregated papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/s2orc)", "variants": ["S2ORC"]}
{"id": "CDD Dataset (season-varying)", "title": "CHANGE DETECTION IN REMOTE SENSING IMAGES USING CONDITIONAL ADVERSARIAL NETWORKS", "contents": "Source: [CHANGE DETECTION IN REMOTE SENSING IMAGES USING CONDITIONAL ADVERSARIAL NETWORKS](https://pdfs.semanticscholar.org/ae15/e5ccccaaff44ab542003386349ef1d3b7511.pdf)", "variants": ["CDD Dataset (season-varying)"]}
{"id": "ICT-3DHP", "title": "", "contents": "ICT-3DHP is collected using the Microsoft Kinect sensor and contains RGB images and depth maps of about 14k frames, divided in 10 sequences. The image resolution is 640 × 480 pixels. An hardware sensor (Polhemus Fastrack) is exploited to generate the ground truth annotation. The device is placed on a white cap worn by each subject, visible in both RGB and depth frames.\r\n\r\nSource: [POSEidon: Face-from-Depth for Driver Pose Estimation](https://arxiv.org/pdf/1611.10195v3.pdf)", "variants": ["ICT-3DHP"]}
{"id": "UCY", "title": "Crowds by Example", "contents": "The **UCY** dataset consist of real pedestrian trajectories with rich multi-human interaction scenarios captured at 2.5 Hz (Δt=0.4s). It is composed of three sequences (Zara01, Zara02, and UCY), taken in public spaces from top-view.\r\n\r\nSource: [Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data](https://arxiv.org/abs/2001.03093)\r\nImage Source: [http://trajnet.stanford.edu/data.php?n=1](http://trajnet.stanford.edu/data.php?n=1)", "variants": ["UCY"]}
{"id": "IAM", "title": "The IAM-database: an English sentence database for offline handwriting recognition", "contents": "The **IAM** database contains 13,353 images of handwritten lines of text created by 657 writers. The texts those writers transcribed are from the Lancaster-Oslo/Bergen Corpus of British English. It includes contributions from 657 writers making a total of 1,539 handwritten pages comprising of 115,320 words and is categorized as part of modern collection. The database is labeled at the sentence, line, and word levels.\r\n\r\nSource: [Measuring Human Perception to Improve Handwritten Document Transcription](https://arxiv.org/abs/1904.03734)\r\nImage Source: [https://fki.tic.heia-fr.ch/databases/iam-handwriting-database](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database)", "variants": ["IAM"]}
{"id": "MemeTracker", "title": "Meme-tracking and the dynamics of the news cycle", "contents": "The Memetracker corpus contains articles from mainstream media and blogs from August 1 to October 31, 2008 with about 1 million documents per day. It has 10,967 hyperlink cascades among 600 media sites.\n\nSource: [Marked Temporal Dynamics Modeling based on Recurrent Neural Network](https://arxiv.org/abs/1701.03918)\nImage Source: [http://blog.fabric.ch/index.php?/archives/292-Memetracker-Tracking-News-Phrases-over-the-Web.html](http://blog.fabric.ch/index.php?/archives/292-Memetracker-Tracking-News-Phrases-over-the-Web.html)", "variants": ["MemeTracker"]}
{"id": "English Web Treebank", "title": "English web treebank", "contents": "**English Web Treebank** is a dataset containing 254,830 word-level tokens and 16,624 sentence-level tokens of webtext in 1174 files annotated for sentence- and word-level tokenization, part-of-speech, and syntactic structure. The data is roughly evenly divided across five genres: weblogs, newsgroups, email, reviews, and question-answers. The files were manually annotated following the sentence-level tokenization guidelines for web text and the word-level tokenization guidelines developed for English treebanks in the DARPA GALE project. Only text from the subject line and message body of posts, articles, messages and question-answers were collected and annotated.\r\n\r\nSource: [https://catalog.ldc.upenn.edu/LDC2012T13](https://catalog.ldc.upenn.edu/LDC2012T13)", "variants": ["English Web Treebank"]}
{"id": "INRIA Person", "title": "Histograms of Oriented Gradients for Human Detection", "contents": "The **INRIA Person** dataset is a dataset of images of persons used for pedestrian detection. It consists of 614 person detections for training and 288 for testing.\r\n\r\nSource: [http://pascal.inrialpes.fr/data/human/](http://pascal.inrialpes.fr/data/human/)\r\nImage Source: [https://www.researchgate.net/figure/Some-human-examples-of-the-INRIA-person-dataset-4-Though-the-examples-are-aligned_fig2_224135181](https://www.researchgate.net/figure/Some-human-examples-of-the-INRIA-person-dataset-4-Though-the-examples-are-aligned_fig2_224135181)", "variants": ["INRIA Person"]}
{"id": "Office-Caltech-10", "title": "", "contents": "**Office-Caltech-10** a standard benchmark for domain adaptation, which consists of Office 10 and Caltech 10 datasets. It contains the 10 overlapping categories between the Office dataset and Caltech256 dataset. SURF BoW historgram features, vector quantized to 800 dimensions are also available for this dataset.\n\nSource: [Impact of ImageNet Model Selection on Domain Adaptation](https://arxiv.org/abs/2002.02559)\nImage Source: [https://arxiv.org/abs/1409.5241](https://arxiv.org/abs/1409.5241)", "variants": ["Office-Caltech-10"]}
{"id": "WASABI", "title": "", "contents": "The **WASABI** Song Corpus is a large corpus of songs enriched with metadata extracted from music databases on the Web, and resulting from the processing of song lyrics and from audio analysis.\r\nMore specifically, given that lyrics encode an important part of the semantics of a song, the authors focus on the description of the methods they proposed to extract relevant information from the lyrics, such as their structure segmentation, their topics, the explicitness of the lyrics content, the salient passages of a song and the emotions conveyed.\r\nThe corpus contains 1.73M songs with lyrics (1.41M unique lyrics) annotated at different levels with the output of the above mentioned methods. Such corpus labels and the provided methods can be exploited by music search engines and music professionals (e.g. journalists, radio presenters) to better handle large collections of lyrics, allowing an intelligent browsing, categorization and segmentation recommendation of songs.\r\n\r\nSource: [https://github.com/micbuffa/WasabiDataset](https://github.com/micbuffa/WasabiDataset)\nImage Source: [https://arxiv.org/abs/1912.02477](https://arxiv.org/abs/1912.02477)", "variants": ["WASABI"]}
{"id": "Multilingual Reuters", "title": "Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization", "contents": "The **Multilingual Reuters** Collection dataset comprises over 11,000 articles from six classes in five languages, i.e., English (E), French (F), German (G), Italian (I), and Spanish (S).\n\nSource: [Multi-source Heterogeneous Domain Adaptation with Conditional Weighting Adversarial Network](https://arxiv.org/abs/2008.02714)\nImage Source: [https://papers.nips.cc/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf](https://papers.nips.cc/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf)", "variants": ["Multilingual Reuters"]}
{"id": "Pan+ChiPhoto", "title": "", "contents": "**Pan+ChiPhoto** dataset is a Chinese character dataset. It is built by the combination of two datasets: ChiPhoto and Pan_Chinese_Character dataset. The images in this dataset are mainly captured at outdoors in Beijing and Shanghai, China, which involve various scenes like signs, boards, advertisements, banners, objects with texts printed on their surfaces.\n\nSource: [Boosting Scene Character Recognition by Learning Canonical Forms of Glyphs](https://arxiv.org/abs/1907.05577)\nImage Source: [https://www.researchgate.net/publication/318679069_Multi-order_Co-occurrence_Activations_Encoded_with_Fisher_Vector_for_Scene_Character_Recognition](https://www.researchgate.net/publication/318679069_Multi-order_Co-occurrence_Activations_Encoded_with_Fisher_Vector_for_Scene_Character_Recognition)", "variants": ["Pan+ChiPhoto"]}
{"id": "INRIA DLFD", "title": "", "contents": "The **INRIA Dense Light Field** Dataset (DLFD) is a dataset for testing depth estimation methods in a light field. DLFD contains 39 scenes with disparity range [-4,4] pixels. The light fields are of spatial resolution 512 x 512 and angular resolution 9 x 9.\n\nSource: [http://clim.inria.fr/Datasets/InriaSynLF/index.html](http://clim.inria.fr/Datasets/InriaSynLF/index.html)\nImage Source: [http://clim.inria.fr/Datasets/InriaSynLF/index.html](http://clim.inria.fr/Datasets/InriaSynLF/index.html)", "variants": ["INRIA DLFD"]}
{"id": "AIDS Antiviral Screen", "title": "IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning", "contents": "The **AIDS Antiviral Screen** dataset is a dataset of screens checking tens of thousands of compounds for evidence of anti-HIV activity. The available screen results are chemical graph-structured data of these various compounds.\n\nSource: [Graph Neural Processes Towards Bayesian Graph Neural Networks](https://arxiv.org/abs/1902.10042)", "variants": ["AIDS Antiviral Screen"]}
{"id": "Retinal Microsurgery", "title": "Real-time localization of articulated surgical instruments in retinal microsurgery", "contents": "The **Retinal Microsurgery** dataset is a dataset for surgical instrument tracking. It consists of 18 in-vivo sequences, each with 200 frames of resolution 1920 × 1080 pixels. The dataset is further classified into four instrument-dependent subsets. The annotated tool joints are n=3 and semantic classes c=2 (tool and background).\n\nSource: [Concurrent Segmentation and Localization for Tracking of Surgical Instruments](https://arxiv.org/abs/1703.10701)\nImage Source: [https://sites.google.com/site/sznitr/research/retinalmicrosurgery](https://sites.google.com/site/sznitr/research/retinalmicrosurgery)", "variants": ["Retinal Microsurgery"]}
{"id": "Daimler Monocular Pedestrian Detection", "title": "", "contents": "The **Daimler Monocular Pedestrian Detection** dataset is a dataset for pedestrian detection in urban environments. The training set contains 15560 pedestrian samples (image cut-outs at 48×96 resolution) and 6744 additional full images without pedestrians for extracting negative samples. The test set contains an independent sequence with more than 21790 images and 56492 pedestrian labels (fully visible or partially occluded), captured from a vehicle during a 27 min driving through the urban traffic.\n\nSource: [A Large Scale Urban Surveillance Video Dataset for Multiple-Object Tracking and Behavior Analysis](https://arxiv.org/abs/1904.11784)\nImage Source: [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html)", "variants": ["Daimler Monocular Pedestrian Detection"]}
{"id": "L-Bird", "title": "", "contents": "The **L-Bird** (**Large-Bird**) dataset contains nearly 4.8 million images which are obtained by searching images of a total of 10,982 bird species from the Internet.\n\nSource: [Fine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data](https://arxiv.org/abs/1807.10916)\nImage Source: [https://arxiv.org/pdf/1511.06789.pdf](https://arxiv.org/pdf/1511.06789.pdf)", "variants": ["L-Bird"]}
{"id": "Extended BBC Pose", "title": "", "contents": "**Extended BBC Pose** is a pose estimation dataset which extends the BBC Pose dataset with 72 additional training videos. Combined with the original BBC TV dataset, the dataset contains 92 videos (82 training, 5 validation and 5 testing), i.e. around 7 million frames. The frames of the new 72 videos are automatically assigned joint locations (used as ground truth for training) with the tracker of Charles et al. IJCV'13.\n\nSource: [https://www.robots.ox.ac.uk/~vgg/data/pose/](https://www.robots.ox.ac.uk/~vgg/data/pose/)\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/pose/](https://www.robots.ox.ac.uk/~vgg/data/pose/)", "variants": ["Extended BBC Pose"]}
{"id": "Short BBC Pose", "title": "", "contents": "**Short BBC Pose** contains five one-hour-long videos with sign language signers each with different sleeve length (in contrast to the BBC pose and Extended BBC Pose, which only contain signers with moderately long sleeves). Each of the five videos has 200 test frames (which have been manually annotated with joint locations), amounting to 1,000 test frames in total. Test frames were selected by the authors to contain a diverse range of poses.\n\nSource: [https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation](https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation)\nImage Source: [https://www.robots.ox.ac.uk/~vgg/publications/2013/Charles13/charles13.pdf](https://www.robots.ox.ac.uk/~vgg/publications/2013/Charles13/charles13.pdf)", "variants": ["Short BBC Pose"]}
{"id": "ChaLearn Pose", "title": "Multi-modal Gesture Recognition Challenge 2013: Dataset and Results", "contents": "**ChaLearn Pose** is a subset of the ChaLearn 2013 Multi-modal gesture dataset from Escalera et al. ICMI'13, which contains 23 hours of Kinect data of 27 persons performing 20 Italian gestures. The data includes RGB, depth, foreground segmentations and full body skeletons. In this dataset, both the training and testing labels are noisy (from Kinect).\n\nSource: [https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation](https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation)\nImage Source: [http://sunai.uoc.edu/chalearnLAP/](http://sunai.uoc.edu/chalearnLAP/)", "variants": ["ChaLearn Pose"]}
{"id": "VCTK", "title": "CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit", "contents": "This CSTR **VCTK** Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald & Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856. The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf. All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed.\r\n\r\nSource: [CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)](https://datashare.is.ed.ac.uk/handle/10283/3443)", "variants": ["Voice Bank corpus (VCTK)", "VCTK Multi-Speaker", "VCTK"]}
{"id": "VoxForge", "title": "", "contents": "**VoxForge** is an open speech dataset that was set up to collect transcribed speech for use with Free and Open Source Speech Recognition Engines (on Linux, Windows and Mac).\nImage Source: [http://www.voxforge.org/home](http://www.voxforge.org/home)", "variants": ["VoxForge American-Canadian", "VoxForge Commonwealth", "VoxForge European", "VoxForge Indian", "VoxForge"]}
{"id": "UMIST", "title": "", "contents": "The Sheffield (previously **UMIST**) Face Database consists of 564 images of 20 individuals (mixed race/gender/appearance). Each individual is shown in a range of poses from profile to frontal views – each in a separate directory labelled 1a, 1b, … 1t and images are numbered consecutively as they were taken. The files are all in PGM format, approximately 220 x 220 pixels with 256-bit grey-scale.\n\nSource: [https://www.visioneng.org.uk/datasets/](https://www.visioneng.org.uk/datasets/)\nImage Source: [https://www.visioneng.org.uk/datasets/](https://www.visioneng.org.uk/datasets/)", "variants": ["UMist", "UMIST"]}
{"id": "SEED", "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks", "contents": "The **SEED** dataset contains subjects' EEG signals when they were watching films clips. The film clips are carefully selected so as to induce different types of emotion, which are positive, negative, and neutral ones.\r\n\r\nSource: [http://bcmi.sjtu.edu.cn/home/seed/index.html](http://bcmi.sjtu.edu.cn/home/seed/index.html)\r\nImage Source: [http://bcmi.sjtu.edu.cn/home/seed/index.html](http://bcmi.sjtu.edu.cn/home/seed/index.html)", "variants": ["SEED-IV", "　SEED", "SEED"]}
{"id": "SHREC", "title": "A comparison of 3D shape retrieval methods based on a large-scale benchmark supporting multimodal queries", "contents": "The **SHREC** dataset contains 14 dynamic gestures performed by 28 participants (all participants are right handed) and captured by the Intel RealSense short range depth camera. Each gesture is performed between 1 and 10 times by each participant in two way: using one finger and the whole hand. Therefore, the dataset is composed by 2800 sequences captured. The depth image, with a resolution of 640x480, and the coordinates of 22 joints (both in the 2D depth image space and in the 3D world space) are saved for each frame of each sequence in the dataset.\n\nSource: [Exploiting Recurrent Neural Networks and Leap Motion Controller for Sign Language and Semaphoric Gesture Recognition](https://arxiv.org/abs/1803.10435)\nImage Source: [http://tosca.cs.technion.ac.il/book/shrec.html](http://tosca.cs.technion.ac.il/book/shrec.html)", "variants": ["SHREC 2017 track on 3D Hand Gesture Recognition", "SHREC15", "SHREC 2017", "SHREC", "SHREC11, Split16-4"]}
{"id": "Florence3D", "title": "Recognizing Actions from Depth Cameras as Weakly Aligned Multi-part Bag-of-Poses", "contents": "The dataset collected at the University of Florence during 2012, has been captured using a Kinect camera. It includes 9 activities: wave, drink from a bottle, answer phone,clap, tight lace, sit down, stand up, read watch, bow. During acquisition, 10 subjects were asked to perform the above actions for 2/3 times. This resulted in a total of 215 activity samples.\r\n\r\nSource: [https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/](https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/)\r\nImage Source: [https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/](https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/)", "variants": ["Florence 3D", "Florence3D"]}
{"id": "STRING", "title": "STRING v10: protein-protein interaction networks, integrated over the tree of life", "contents": "**STRING** is a collection of protein-protein interaction (PPI) networks.", "variants": ["STRING"]}
{"id": "Foursquare", "title": "Participatory Cultural Mapping Based on Collective Behavior Data in Location-Based Social Networks", "contents": "The **Foursquare** dataset consists of check-in data for different cities. One subset contains check-ins in NYC and Tokyo collected for about 10 month (from 12 April 2012 to 16 February 2013). It contains 227,428 check-ins in New York city and 573,703 check-ins in Tokyo. Each check-in is associated with its time stamp, its GPS coordinates and its semantic meaning (represented by fine-grained venue-categories).\nAnother subset contains long-term (about 18 months from April 2012 to September 2013) global-scale check-in data collected from Foursquare. It contains 33,278,683 checkins by 266,909 users on 3,680,126 venues (in 415 cities in 77 countries). Those 415 cities are the most checked 415 cities by Foursquare users in the world, each of which contains at least 10K check-ins.\n\nSource: [https://sites.google.com/site/yangdingqi/home/foursquare-dataset](https://sites.google.com/site/yangdingqi/home/foursquare-dataset)", "variants": ["Foursquare"]}
{"id": "PeerRead", "title": "A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications", "contents": "PearRead is a dataset of scientific peer reviews available to help researchers study this important artifact. The dataset consists of over 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR, as well as over 10K textual peer reviews written by experts for a subset of the papers.\n\nSource: [https://github.com/allenai/PeerRead](https://github.com/allenai/PeerRead)", "variants": ["PeerRead"]}
{"id": "Mindboggle", "title": "Mindboggling morphometry of human brains", "contents": "**Mindboggle** is a large publicly available dataset of manually labeled brain MRI. It consists of 101 subjects collected from different sites, with cortical meshes varying from 102K to 185K vertices. Each brain surface contains 32 manually labeled parcels.\n\nSource: [Graph Convolutions on Spectral Embeddings: Learning of Cortical Surface Data](https://arxiv.org/abs/1803.10336)\nImage Source: [https://mindboggle.info/data.html](https://mindboggle.info/data.html)", "variants": ["Mindboggle"]}
{"id": "Learning to Rank Challenge", "title": "Yahoo! Learning to Rank Challenge Overview", "contents": "The Yahoo! **Learning to Rank Challenge** dataset consists of 709,877 documents encoded in 700 features and sampled from query logs of the Yahoo! search engine, spanning 29,921 queries.\r\n\r\nSource: [Ranking for Relevance and Display Preferencesin Complex Presentation Layouts](https://arxiv.org/abs/1805.02404)", "variants": ["Learning to Rank Challenge"]}
{"id": "AMiner", "title": "ArnetMiner: extraction and mining of academic social networks", "contents": "The **AMiner** Dataset is a collection of different relational datasets. It consists of a set of relational networks such as citation networks, academic social networks or topic-paper-autor networks among others.", "variants": ["AMiner"]}
{"id": "IMDB-BINARY", "title": "", "contents": "**IMDB-BINARY** is a movie collaboration dataset that consists of the ego-networks of 1,000 actors/actresses who played roles in movies in IMDB. In each graph, nodes represent actors/actress, and there is an edge between them if they appear in the same movie. These graphs are derived from the Action and Romance genres.\r\n\r\nSource: [A simple yet effective baseline for non-attributed graph classification](https://arxiv.org/abs/1811.03508)", "variants": ["IMDb-B", "IMDB-BINARY"]}
{"id": "NCBI Disease", "title": "NCBI disease corpus: A resource for disease name recognition and concept normalization", "contents": "The **NCBI Disease** corpus consists of 793 PubMed abstracts, which are separated into training (593), development (100) and test (100) subsets. The NCBI Disease corpus is annotated with disease mentions, using concept identifiers from either MeSH or OMIM.\r\n\r\nSource: [A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization](https://arxiv.org/abs/1812.06081)", "variants": ["NCBI-disease", "NCBI Disease"]}
{"id": "MSLR-WEB10K", "title": "", "contents": "The **MSLR-WEB10K** dataset consists of 10,000 search queries over the documents from search results. The data also contains the values of 136 features and a corresponding user-labeled relevance factor on a scale of one to five with respect to each query-document pair. It is a subset of the MSLR-WEB30K dataset.\r\n\r\nSource: [Dueling Bandits with Qualitative Feedback](https://arxiv.org/abs/1809.05274)", "variants": ["MSLR-WEB10K"]}
{"id": "BeerAdvocate", "title": "", "contents": "BeerAdvocate is a dataset that consists of beer reviews from beeradvocate. The data span a period of more than 10 years, including all ~1.5 million reviews up to November 2011. Each review includes ratings in terms of five \"aspects\": appearance, aroma, palate, taste, and overall impression. Reviews include product and user information, followed by each of these five ratings, and a plaintext review.\r\n\r\nSource: [https://snap.stanford.edu/data/web-BeerAdvocate.html](https://snap.stanford.edu/data/web-BeerAdvocate.html)", "variants": ["BeerAdvocate"]}
{"id": "Epinion", "title": "eTrust: understanding trust evolution in an online world", "contents": "The **Epinion**s dataset is trust network dataset. For each user, it contains his profile, his ratings and his trust relations. For each rating, it has the product name and its category, the rating score, the time point when the rating is created, and the helpfulness of this rating.\n\nSource: [https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm](https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm)", "variants": ["Epinions-Extend", "Epinion"]}
{"id": "Orkut", "title": "", "contents": "**Orkut** is a social network dataset consisting of friendship social network and ground-truth communities from Orkut.com on-line social network where users form friendship each other.\r\n\r\nEach connected component in a group is regarded as a separate ground-truth community. The ground-truth communities which have less than 3 nodes are removed. The dataset also provides the top 5,000 communities with highest quality and the largest connected component of the network.\r\n\r\nSource: [https://snap.stanford.edu/data/com-Orkut.html](https://snap.stanford.edu/data/com-Orkut.html)", "variants": ["Orkut"]}
{"id": "MQ2008", "title": "", "contents": "The **MQ2008** dataset is a dataset for Learning to Rank. It contains 800 queries with labelled documents.", "variants": ["MQ2008"]}
{"id": "IMDB-MULTI", "title": "", "contents": "**IMDB-MULTI** is a relational dataset that consists of a network of 1000 actors or actresses who played roles in movies in IMDB. A node represents an actor or actress, and an edge connects two nodes when they appear in the same movie. In IMDB-MULTI, the edges are collected from three different genres: Comedy, Romance and Sci-Fi.\r\n\r\nSource: [Learning metrics for persistence-based summaries and applications for graph classification](https://arxiv.org/abs/1904.12189)", "variants": ["IMDb-M", "IMDB-MULTI"]}
{"id": "REDDIT-12K", "title": "", "contents": "Reddit12k contains 11929 graphs each corresponding to an online discussion thread where nodes represent users, and an edge represents the fact that one of the two users responded to the comment of the other user. There is 1 of 11 graph labels associated with each of these 11929 discussion graphs, representing the category of the community.\r\n\r\nSource: [Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity](https://arxiv.org/abs/1904.01098)", "variants": ["REDDIT-12K"]}
{"id": "REDDIT-BINARY", "title": "", "contents": "**REDDIT-BINARY** consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them respond to the other’s comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answer-based community or a discussion-based community.\r\n\r\nSource: [A simple yet effective baseline for non-attributed graph classification](https://arxiv.org/abs/1811.03508)", "variants": ["REDDIT-BINARY"]}
{"id": "MQ2007", "title": "Introducing LETOR 4.0 Datasets", "contents": "The **MQ2007** dataset consists of queries, corresponding retrieved documents and labels provided by human experts. The possible relevance labels for each document are “relevant”, “partially relevant”, and “not relevant”.\r\n\r\nSource: [ARSM GRADIENT ESTIMATOR FOR SUPERVISED LEARNING TO RANK](https://arxiv.org/abs/1911.00465)", "variants": ["MQ2007"]}
{"id": "REDDIT-5K", "title": "Deep Graph Kernels", "contents": "Reddit-5K is a relational dataset extracted from Reddit.", "variants": ["REDDIT-MULTI-5k", "REDDIT-5K"]}
{"id": "Arcade Learning Environment", "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "contents": "The **Arcade Learning Environment** (ALE) is an object-oriented framework that allows researchers to develop AI agents for Atari 2600 games. It is built on top of the Atari 2600 emulator Stella and separates the details of emulation from agent design.\r\n\r\nSource: [https://github.com/mgbellemare/Arcade-Learning-Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)\r\nImage Source: [https://github.com/muupan/async-rl/blob/master/README.md](https://github.com/muupan/async-rl/blob/master/README.md)", "variants": ["Atari 2600 Alien", "Atari 2600 Amidar", "Atari 2600 Berzerk", "Atari 2600 Bowling", "Atari 2600 Gopher", "Atari 2600 HERO", "Atari 2600 Krull", "Atari 2600 Phoenix", "Atari 2600 Pong", "Atari 2600 Pooyan", "Atari 2600 Q*Bert", "Atari 2600 Skiing", "Atari 2600 Tennis", "Atari 2600 Venture", "Atari 2600 Zaxxon", "Atari-57", "Atari 2600 Assault", "Atari 2600 Asteroids", "Atari 2600 Atlantis", "Atari 2600 Bank Heist", "Atari 2600 Battle Zone", "Atari 2600 Beam Rider", "Atari 2600 Carnival", "Atari 2600 Centipede", "Atari 2600 Chopper Command", "Atari 2600 Crazy Climber", "Atari 2600 Demon Attack", "Atari 2600 Double Dunk", "Atari 2600 Elevator Action", "Atari 2600 Fishing Derby", "Atari 2600 Freeway", "Atari 2600 Frostbite", "Atari 2600 Gravitar", "Atari 2600 Ice Hockey", "Atari 2600 James Bond", "Atari 2600 Journey Escape", "Atari 2600 Kangaroo", "Atari 2600 Kung-Fu Master", "Montezuma ’s Revenge", "Atari 2600 Montezuma s Revenge", "Atari 2600 Montezuma's Revenge", "Atari 2600 Ms. Pacman", "Atari 2600 Name This Game", "Atari 2600 Pitfall!", "Atari 2600 Pitfall ", "Atari 2600 Asterix", "Atari 2600 Solaris", "Atari 2600 Breakout", "Atari 2600 Defender", "Atari 2600 Ms  Pacman", "Atari 2600 Star Gunner", "Atari 2600 Boxing", "Atari 2600 Enduro", "Atari 2600 Private Eye", "Atari 2600 River Raid", "Atari 2600 Road Runner", "Atari 2600 Robotank", "Atari 2600 Seaquest", "Atari 2600 Space Invaders", "Atari 2600 Surround", "Atari 2600 Time Pilot", "Atari 2600 Tutankham", "Atari 2600 Up and Down", "Atari 2600 Video Pinball", "Atari 2600 Wizard of Wor", "Atari 2600 Yars Revenge", "Arcade Learning Environment"]}
{"id": "MedleyDB", "title": "MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research", "contents": "**MedleyDB**, is a dataset of annotated, royalty-free multitrack recordings. It was curated primarily to support research on melody extraction. For each song melody f₀ annotations are provided as well as instrument activations for evaluating automatic instrument recognition. The original dataset consists of 122 multitrack songs out of which 108 include melody annotations.\r\n\r\nThe songs in MedleyDB were obtained from the following sources:\r\n\r\n* Independent Artists (30 songs)\r\n* NYU's Dolan Recording Studio (32 songs)\r\n* Weathervane Music (25 songs)\r\n* Music Delta (35 songs)\r\n\r\nMedleyDB contains songs of a variety of musical genres: Singer/Songwriter, Classical, Rock, World/Folk, Fusion, Jazz, Pop, Musical Theatre, Rap. For each song three types of audio content are given: a mix, stems, and raw audio. All types of audio files are .wav files with a sample rate of 44.1 kHz and a bit depth of 16.\r\n\r\nSource: [https://medleydb.weebly.com/](https://medleydb.weebly.com/)\r\nImage Source: [https://medleydb.weebly.com/](https://medleydb.weebly.com/)\r\nAudio Source: [https://zenodo.org/record/1438309](https://zenodo.org/record/1438309)", "variants": ["MedleyDB"]}
{"id": "MIR-1K", "title": "On the Improvement of Singing Voice Separation for Monaural Recordings Using the MIR-1K Dataset", "contents": "**MIR-1K** (Multimedia Information Retrieval lab, 1000 song clips) is a dataset designed for singing voice separation. It contains:\r\n\r\n* 1000 song clips with the music accompaniment and the singing voice recorded as left and right channels, respectively,\r\n* Manual annotations of pitch contours in semitone, indices and types for unvoiced frames, lyrics, and vocal/non-vocal segments,\r\n* The speech recordings of the lyrics by the same person who sang the songs.\r\n\r\nThe duration of each clip ranges from 4 to 13 seconds, and the total length of the dataset is 133 minutes. These clips are extracted from 110 karaoke songs which contain a mixture track and a music accompaniment track. These songs are freely selected from 5000 Chinese pop songs and sung by researchers from MIR lab (8 females and 11 males). Most of the singers are amateur and do not have professional music training.\r\n\r\nSource: [https://sites.google.com/site/unvoicedsoundseparation/mir-1k](https://sites.google.com/site/unvoicedsoundseparation/mir-1k)\r\nAudio Source: [https://sites.google.com/site/unvoicedsoundseparation/sounddemosforjournal](https://sites.google.com/site/unvoicedsoundseparation/sounddemosforjournal)", "variants": ["MIR-1K"]}
{"id": "MagnaTagATune", "title": "Evaluation of Algorithms Using Games: The Case of Music Tagging", "contents": "**MagnaTagATune** dataset contains 25,863 music clips. Each clip is a 29-seconds-long excerpt belonging to one of the 5223 songs, 445 albums and 230 artists. The clips span a broad range of genres like Classical, New Age, Electronica, Rock, Pop, World, Jazz, Blues, Metal, Punk, and more. Each audio clip is supplied with a vector of binary annotations of 188 tags. These annotations are obtained by humans playing the two-player online TagATune game. In this game, the two players are either presented with the same or a different audio clip. Subsequently, they are asked to come up with tags for their specific audio clip. Afterward, players view each other’s tags and are asked to decide whether they were presented the same audio clip. Tags are only assigned when more than two players agreed. The annotations include tags like ’singer’, ’no singer’, ’violin’, ’drums’, ’classical’, ’jazz’. The top 50 most popular tags are typically used for evaluation to ensure that there is enough training data for each tag. There are 16 parts, and researchers comonnly use parts 1-12 for training, part 13 for validation and parts 14-16 for testing.\r\n\r\nSource: [Brains on Beats](https://arxiv.org/abs/1606.02627)\r\nAudio Source: [http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset](http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset)", "variants": ["MagnaTagATune"]}
{"id": "Lakh MIDI Dataset", "title": "Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching", "contents": "The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files). Around 10% of all MIDI files include timestamped lyrics events with lyrics are often transcribed at the word, syllable or character level.\r\n\r\nLMD-full denotes the whole dataset. LMD-matched is the subset of LMD-full that consists of MIDI files matched with the Million Song Dataset entries. LMD-aligned contains all the files of LMD-matched, aligned to preview MP3s from the Million Song Dataset.\r\n\r\nA lakh is a unit of measure used in the Indian number system which signifies 100,000.\r\n\r\nSource: [https://colinraffel.com/projects/lmd/](https://colinraffel.com/projects/lmd/)\r\nAudio Source: [https://colinraffel.com/projects/lmd/](https://colinraffel.com/projects/lmd/)", "variants": ["Lakh MIDI Dataset"]}
{"id": "CCMixter", "title": "Kernel Additive Models for Source Separation", "contents": "**CCMixter** is a singing voice separation dataset consisting of 50 full-length stereo tracks from [ccMixter](www.ccmixter.org) featuring many different musical genres. For each song there are three WAV files available: the background music, the voice signal, and their sum.\n\nSource: [Kernel Additive Models for Source Separation](https://doi.org/10.1109/TSP.2014.2332434)\nAudio Source: [https://members.loria.fr/ALiutkus/kam/](https://members.loria.fr/ALiutkus/kam/)", "variants": ["CCMixter"]}
{"id": "GoodSounds", "title": "A real-time system for measuring sound goodness in instrumental sounds", "contents": "**GoodSounds** dataset contains around 28 hours of recordings of single notes and scales played by 15 different professional musicians, all of them holding a music degree and having some expertise in teaching. 12 different instruments (flute, cello, clarinet, trumpet, violin, alto sax alto, tenor sax, baritone sax, soprano sax, oboe, piccolo and bass) were recorded using one or up to 4 different microphones. For all the instruments the whole set of playable semitones in the instrument is recorded several times with different tonal characteristics. Each note is recorded into a separate monophonic audio file of 48kHz and 32 bits. Rich annotations of the recordings are available, including details on recording environment and rating on tonal qualities of the sound (“good-sound”, “bad”, “scale-good”, “scale-bad”).\n\nSource: [A real-time system for measuring sound goodness in instrumental sounds](http://mtg.upf.edu/node/3197)\nImage Source: [A real-time system for measuring sound goodness in instrumental sounds](http://mtg.upf.edu/node/3197)\nAudio Source: [https://zenodo.org/record/820937](https://zenodo.org/record/820937)", "variants": ["GoodSounds"]}
{"id": "ForeDeCk", "title": "", "contents": "**ForeDeCk** is a time series database compiled at the National Technical University of Athens that contains 900,000 continuous time series, built from multiple, diverse and publicly accessible sources. ForeDeCk emphasizes business forecasting applications, including series from relevant domains such as industries, services, tourism, imports & exports, demographics, education, labor & wage, government, households, bonds, stocks, insurances, loans, real estate, transportation, and natural resources & environment.\n\nSource: [Are forecasting competitions data representative of the reality?](https://www.sciencedirect.com/science/article/abs/pii/S0169207019300159)", "variants": ["ForeDeCk"]}
{"id": "M4", "title": "The M4 competition: Results, findings, conclusion and way forward", "contents": "The **M4** dataset is a collection of 100,000 time series used for the fourth edition of the Makridakis forecasting Competition. The M4 dataset consists of time series of yearly, quarterly, monthly and other (weekly, daily and hourly) data, which are divided into training and test sets. The minimum numbers of observations in the training test are 13 for yearly, 16 for quarterly, 42 for monthly, 80 for weekly, 93 for daily and 700 for hourly series. The participants were asked to produce the following numbers of forecasts beyond the available data that they had been given: six for yearly, eight for quarterly, 18 for monthly series, 13 for weekly series and 14 and 48 forecasts respectively for the daily and hourly ones.\r\n\r\nThe M4 dataset was created by selecting a random sample of 100,000 time series from the ForeDeCk database. The selected series were then scaled to prevent negative observations and values lower than 10, thus avoiding possible problems when calculating various error measures. The scaling was performed by simply adding a constant to the series so that their minimum value was equal to 10 (29 occurrences across the whole dataset). In addition, any information that could possibly lead to the identification of the original series was removed so as to ensure the objectivity of the results. This included the starting dates of the series, which did not become available to the participants until the M4 had ended.\r\n\r\nSource: [The M4 competition: Results, findings, conclusion and way forward](https://doi.org/10.1016/j.ijforecast.2018.06.001)\r\nImage Source: [Are forecasting competitions data representative of the reality?](https://www.sciencedirect.com/science/article/abs/pii/S0169207019300159)", "variants": ["M4"]}
{"id": "MUSDB18-HQ", "title": "", "contents": "**MUSDB18-HQ** is a high-quality version of the MUSDB18 music tracks dataset. The high-quality dataset consists of the same 150 songs, but instead of MP4 files (compressed with Advanced Audio Coding encoder at 256kbps, with bandwidth limited to 16kHz), the songs are provided as raw WAV files.\nImage Source: [https://sigsep.github.io/datasets/musdb.html](https://sigsep.github.io/datasets/musdb.html)", "variants": ["MUSDB18-HQ"]}
{"id": "Slakh2100", "title": "Cutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity", "contents": "The Synthesized Lakh (Slakh) Dataset is a dataset for audio source separation that is synthesized from the Lakh MIDI Dataset v0.1 using professional-grade sample-based virtual instruments. This first release of Slakh, called **Slakh2100**, contains 2100 automatically mixed tracks and accompanying MIDI files synthesized using a professional-grade sampling engine. The tracks in Slakh2100 are split into training (1500 tracks), validation (375 tracks), and test (225 tracks) subsets, totaling 145 hours of mixtures.\n\nSource: [http://www.slakh.com/](http://www.slakh.com/)\nImage Source: [http://www.slakh.com/](http://www.slakh.com/)\nAudio Source: [http://www.slakh.com/](http://www.slakh.com/)", "variants": ["Slakh2100"]}
{"id": "GuitarSet", "title": "GuitarSet: A Dataset for Guitar Transcription", "contents": "**GuitarSet** is a dataset of high-quality guitar recordings and rich annotations. It contains 360 excerpts 30 seconds in length. The 360 excerpts are the result of the following combinations:\n\n* 6 players,\n* 2 versions: comping and soloing,\n* 5 styles: Rock, Singer-Songwriter, Bossa Nova, Jazz, and Funk,\n* 3 progressions: 12 Bar Blues, Autumn Leaves, and Pachelbel Canon,\n* 2 tempi: slow and fast.\n\nEach excerpt is annotated with 6 pitch contour and midi note annotations (one per string), 2 chord annotations (instructed and performed), beat and tempo annotations.\n\nSource: [https://guitarset.weebly.com/](https://guitarset.weebly.com/)\nAudio Source: [https://zenodo.org/record/3371780](https://zenodo.org/record/3371780)", "variants": ["GuitarSet"]}
{"id": "Mixing Secrets", "title": "", "contents": "**Mixing Secrets** is an instrument recognition dataset containing 258 multi-track recordings sourced from the [Mixing Secrets for The Small Studio]( https://www.cambridge-mt.com/ms/mtk/) website. The dataset was labelled to be consistent with MedleyDB format.\n\nSource: [Mixing secrets: a multi-track dataset for instrument recognition in polyphonic music](None)\nImage Source: [Mixing secrets: a multi-track dataset for instrument recognition in polyphonic music](None)\nAudio Source: [https://multitracksearch.cambridge-mt.com/ms-mtk-search.htm](https://multitracksearch.cambridge-mt.com/ms-mtk-search.htm)", "variants": ["Mixing Secrets"]}
{"id": "CAL500exp", "title": "Towards time-varying music auto-tagging based on CAL500 expansion", "contents": "The **CAL500 Expansion** (**CAL500exp**) dataset is an enriched version of the CAL500 music information retrieval dataset. CAL500exp is designed to facilitate music auto-tagging in a smaller temporal scale. The dataset consists of the same songs split into 3,223 acoustically homogenous segments of 3 to 16 seconds. The tag labels are annotated in the segment level instead of track level. The annotations were obtained from annotators with strong music background.\n\nSource: [Towards time-varying music auto-tagging based on CAL500 expansion](https://doi.org/10.1109/ICME.2014.6890290)\nImage Source: [Towards time-varying music auto-tagging based on CAL500 expansion](https://doi.org/10.1109/ICME.2014.6890290)\nAudio Source: [http://calab1.ucsd.edu/~datasets/cal500/cal500data/](http://calab1.ucsd.edu/~datasets/cal500/cal500data/)", "variants": ["CAL500exp"]}
{"id": "MuseScore", "title": "Multitask learning for frame-level instrument recognition", "contents": "The **MuseScore** dataset is a collection of 344,166 audio and MIDI pairs downloaded from [MuseScore](https://musescore.org/) website. The audio is usually synthesized by the MuseScore synthesizer. The audio clips have diverse musical genres and are about two mins long on average.\r\n\r\nDue to copyright issues the dataset is not publicly available, but can be collected and processed with the provided source code.\r\n\r\nSource: [Multitask learning for frame-level instrument recognition](https://arxiv.org/pdf/1811.01143.pdf)\r\nImage Source: [https://biboamy.github.io/instrument-demo/demo.html](https://biboamy.github.io/instrument-demo/demo.html)\r\nAudio Source: [Somewhere over the rainbow](https://biboamy.github.io/instrument-demo/demo.html)", "variants": ["MuseScore"]}
{"id": "LibriCount", "title": "LibriCount, a dataset for speaker count estimation (Version v1.0.0)", "contents": "**LibriCount** is a synthetic dataset for speaker count estimation. The dataset contains a simulated cocktail party environment of 0 to 10 speakers, mixed with 0dB SNR from random utterances of different speakers from the LibriSpeech `CleanTest` dataset. All recordings are of 5s durations, and all speakers are active for the most part of the recording.\n\nSource: [https://faroit.com/#libricount](https://faroit.com/#libricount)\nImage Source: [https://faroit.com/#libricount](https://faroit.com/#libricount)\nAudio Source: [https://zenodo.org/record/1216072](https://zenodo.org/record/1216072)", "variants": ["LibriCount"]}
{"id": "MultiWOZ", "title": "MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling", "contents": "The **Multi-domain Wizard-of-Oz** (**MultiWOZ**) dataset is a large-scale human-human conversational corpus spanning over seven domains, containing 8438 multi-turn dialogues, with each dialogue averaging 14 turns. Different from existing standard datasets like WOZ and DSTC2, which contain less than 10 slots and only a few hundred values, MultiWOZ has 30 (domain, slot) pairs and over 4,500 possible values. The dialogues span seven domains: restaurant, hotel, attraction, taxi, train, hospital and police.\r\n\r\nSource: [Contents](https://arxiv.org/abs/1905.07687)\r\n\r\nImage Source: [Zhang et al](https://www.researchgate.net/figure/Example-of-the-difference-between-dialogue-state-annotation-in-MultiWOZ-21-and-MultiWOZ_fig2_343022084)", "variants": ["MULTIWOZ 2.0", "MULTIWOZ 2.1", "MultiWOZ"]}
{"id": "MPQA Opinion Corpus", "title": "Annotating Expressions of Opinions and Emotions in Language", "contents": "The **MPQA Opinion Corpus** contains 535 news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).\r\n\r\nSource: [http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf](http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf)\r\nImage Source: [https://mpqa.cs.pitt.edu/](https://mpqa.cs.pitt.edu/)", "variants": ["MPQA", "MPQA Opinion Corpus"]}
{"id": "DROP", "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "contents": "**Discrete Reasoning Over Paragraphs** **DROP** is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. The questions consist of passages extracted from Wikipedia articles. The dataset is split into a training set of about 77,000 questions, a development set of around 9,500 questions and a hidden test set similar in size to the development set.\r\n\r\nSource: [https://allennlp.org/drop](https://allennlp.org/drop)\r\nImage Source: [DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs](https://paperswithcode.com/paper/drop-a-reading-comprehension-benchmark/)", "variants": ["DROP Test", "DROP"]}
{"id": "WMT 2016", "title": "", "contents": "**WMT 2016** is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.\r\n\r\nThe conference featured ten shared tasks:\r\n\r\n- a news translation task,\r\n- an IT domain translation task,\r\n- a biomedical translation task,\r\n- an automatic post-editing task,\r\n- a metrics task (assess MT quality given reference translation).\r\n- a quality estimation task (assess MT quality without access to any reference),\r\n- a tuning task (optimize a given MT system),\r\n- a pronoun translation task,\r\n- a bilingual document alignment task,\r\n- a multimodal translation task.\r\n\r\nSource: [http://www.statmt.org/wmt16/index.html](http://www.statmt.org/wmt16/index.html)", "variants": ["WMT2016 English--Romanian", "WMT2016 English-German", "WMT2016 English-Romanian", "WMT2016 German-English", "WMT2016 Romanian-English", "WMT2016 Czech-English", "WMT2016 English-Czech", "WMT2016 English-French", "WMT2016 English-Russian", "WMT2016 Finnish-English", "WMT2016 Russian-English", "WMT 2016", "WMT2016 En-Ro"]}
{"id": "WMT 2016 News", "title": "", "contents": "News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, Romanian, Russian, Turkish) and additional 1500 sentences from each of the 5 languages translated to English. For Romanian a third of the test set were released as a development set instead. For Turkish additional 500 sentence development set was released. The sentences were selected from dozens of news websites and translated by professional translators.\nThe training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.\nSome training corpora were identical from WMT 2015 (Europarl, United Nations, French-English 10⁹ corpus, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (CzEng v1.6pre, News Commentary v11, monolingual news data). Additionally, the following new corpora were added: Romanian Europarl, SETIMES2 from OPUS for Romanian-English and Turkish-English, Monolingual data sets from CommonCrawl.\n\nSource: [https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/](https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/)\nImage Source: [https://www.aclweb.org/anthology/W16-2301.pdf](https://www.aclweb.org/anthology/W16-2301.pdf)", "variants": ["WMT 2016 News"]}
{"id": "WMT 2016 IT", "title": "", "contents": "The IT Translation Task is a shared task introduced in the First Conference on Machine Translation. Compared to WMT 2016 News, this task brought several novelties to WMT:\r\n\r\n*  4 out of the 7 langauges of the IT task are new in WMT,\r\n* adaptation to the IT domain with its specifics such as frequent named entities (mostly menu items, names of products and companies) and technical jargon,\r\n* adaptation to translation of answers in helpdesk service setting (many of the sentences are instructions with imperative verbs, which is very rare in the News translation task).\r\n\r\nThe test set consisted of 1000 answers from the Batch 3 of the QTLeap Corpus. The in-domain training data contained 2000 answers from the Batches 1 and 2 and also localization files from several open-source projects (LibreOffice, KDE, VLC) and bilingual dictionaries of IT-related terms extracted from Wikipedia. The out-of-domain training data contained all the corpora from the WMT 2016 News, plus PaCo2-EuEn Basque-English corpus and SETimes with Bulgarian-English parallel sentences. “Constrained” systems were restricted to use only these training data provided by the organizers.\r\n\r\nThe task was evaluated on the following language pairs:\r\n\r\n* English → Bulgarian\r\n* English → Czech\r\n* English → German\r\n* English → Spanish\r\n* English → Basque\r\n* English → Dutch\r\n* English → Portuguese\r\n\r\nSource: [http://www.statmt.org/wmt16/index.html](http://www.statmt.org/wmt16/index.html)\r\nImage Source: [Bojar et al](https://www.aclweb.org/anthology/W16-2301.pdf)", "variants": ["WMT 2016 IT"]}
{"id": "WMT 2016 Biomedical", "title": "Findings of the 2016 Conference on Machine Translation", "contents": "The Biomedical Translation Shared Task was first introduced at the First Conference of Machine Translation. The task aims to evaluate systems for the translation of biomedical titles and abstracts from scientific publications. The data includes three language pairs (English ↔ Portuguese, English  ↔ Spanish, English  ↔ French) and two sub-domains of biological sciences and health sciences.\n\nThe training data consists mainly of the Scielo corpus, a parallel collection of scientific publications composed of either titles, abstracts or title and abstracts which were retrieved from the Scielo database. For the Scielo corpus, a parallel documents are provided for all language pairs in the two sub-domains, except for the English  ↔ French, where only health was considered, as there were inadequate parallel documents available for biology in that pair. The training data was aligned using the GMA alignment tool. Additionally, a corpus of parallel titles from MEDLINEⓇ for all three language pairs were provided as well as monolingual documents for the four languages, retrieved from the Scielo database. These consist of documents in the Scielo database which have no corresponding document in another language.\n\nThe test set consisted of 500 documents (title and abstract) for each of the two directions of each language pair. None of the test documents was included in the training data and there is no overlap of documents between the test sets for any language pair, translation direction and sub-domain.\n\nSource: [http://www.statmt.org/wmt16/index.html](http://www.statmt.org/wmt16/index.html)\nImage Source: [https://www.aclweb.org/anthology/W16-2301.pdf](https://www.aclweb.org/anthology/W16-2301.pdf)", "variants": ["WMT 2016 Biomedical"]}
{"id": "OHSUMED", "title": "OHSUMED: An Interactive Retrieval Evaluation and New Large Test Collection for Research", "contents": "The Ohsumed (**Oregon Health Sciences University’s MEDLINE Data Collection**) is a dataset consisting of a set of MEDLINE documents spanning the years from 1987 to 1991. Each entry consists of summary information relative to a paper published on one of 270 medical journals. The available fields are title, abstract, MeSH indexing terms, author, source, and publication type.\r\n\r\nSource: [Word-Class Embeddings for Multiclass Text Classification](https://arxiv.org/abs/1911.11506)\r\nImage Source: [https://pubmed.ncbi.nlm.nih.gov/21142435/](https://pubmed.ncbi.nlm.nih.gov/21142435/)", "variants": ["Ohsumed", "OHSUMED"]}
{"id": "WMT 2014", "title": "", "contents": "**WMT 2014** is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation. The workshop featured four tasks:\r\n\r\n* a news translation task,\r\n* a quality estimation task,\r\n* a metrics task,\r\n* a medical text translation task.\r\n\r\nSource: [https://www.aclweb.org/anthology/W14-3302.pdf](https://www.aclweb.org/anthology/W14-3302.pdf)", "variants": ["WMT2014 English-French", "WMT2014 French-English", "WMT2014 English-German", "WMT 2014 EN-DE", "WMT 2014 EN-FR", "WMT2014 English-Czech", "WMT2014 German-English", "WMT 2014"]}
{"id": "WMT 2014 News", "title": "", "contents": "News translation is a recurring WMT shared task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, French, German, Hindi, Russian) and additional 1500 sentences from each of the 5 languages translated to English. The sentences were selected from dozens of news websites and translated by professional translators.\n\nThe training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.\nSome training corpora were identical from WMT 2013 (Europarl, United Nations, French-English 10⁹ corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU) and some were update (Russian-English parallel data provided by Yandex, News Commentary, monolingual data). Additionally, a new Hindi-English Wikipedia Headline corpus was added.\n\nSource: [https://www.aclweb.org/anthology/W14-3302.pdf](https://www.aclweb.org/anthology/W14-3302.pdf)\nImage Source: [https://www.aclweb.org/anthology/W14-3302.pdf](https://www.aclweb.org/anthology/W14-3302.pdf)", "variants": ["WMT 2014 News"]}
{"id": "WMT 2015", "title": "", "contents": "**WMT 2015** is a collection of datasets used in shared tasks of the Tenth Workshop on Statistical Machine Translation. The workshop featured four tasks:\r\n\r\n* a news translation task,\r\n* a metrics task,\r\n* a tuning task,\r\n* a quality estimation task,\r\n* an automatic post-editing task.\r\n\r\nSource: [https://www.aclweb.org/anthology/W15-3001.pdf](https://www.aclweb.org/anthology/W15-3001.pdf)", "variants": ["WMT2015 English-German", "WMT2015 English-Russian", "WMT 2015"]}
{"id": "SHAPES", "title": "Neural Module Networks", "contents": "**SHAPES** is a dataset of synthetic images designed to benchmark systems for understanding of spatial and logical relations among multiple objects. The dataset consists of complex questions about arrangements of colored shapes. The questions are built around compositions of concepts and relations, e.g. Is there a red shape above a circle? or Is a red shape blue?. Questions contain between two and four attributes, object types, or relationships. There are 244 questions and 15,616 images in total, with all questions having a yes and no answer (and corresponding supporting image). This eliminates the risk of learning biases.\r\n\r\nEach image is a 30×30 RGB image depicting a 3×3 grid of objects. Each object is characterized by shape (circle, square, triangle), colour (red, green, blue) and size (small, big).\r\n\r\nSource: [Visual Question Answering: A Survey of Methods and Datasets](https://arxiv.org/abs/1607.05910)\r\nImage Source: [https://github.com/ronghanghu/n2nmn#train-and-evaluate-on-the-shapes-dataset](https://github.com/ronghanghu/n2nmn#train-and-evaluate-on-the-shapes-dataset)", "variants": ["Shapes", "SHAPES"]}
{"id": "QUASAR-S", "title": "", "contents": "**QUASAR-S** is a large-scale dataset aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. It consists of 37,362 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow. The posts and comments on the website serve as the background corpus for answering the cloze questions. The answer to each question is restricted to be another software entity, from an output vocabulary of 4874 entities.\n\nSource: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)\nImage Source: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)", "variants": ["QUASAR-S"]}
{"id": "QUASAR-T", "title": "", "contents": "**QUASAR-T** is a large-scale dataset aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. It consists of 43,013 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 serves as the background corpus for extracting these answers. The answers to these questions are free-form spans of text, though most are noun phrases.\r\n\r\nSource: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)\r\nImage Source: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)", "variants": ["Quasar", "Quasart-T", "QUASAR-T"]}
{"id": "WMT 2018", "title": "", "contents": "**WMT 2018** is a collection of datasets used in shared tasks of the Third Conference on Machine Translation. The conference builds on a series of twelve previous annual workshops and conferences on Statistical Machine Translation.\r\n\r\nThe conference featured ten shared tasks:\r\n\r\n- a news translation task,\r\n- a biomedical translation task,\r\n- a multimodal machine translation task,\r\n- a metrics task,\r\n- a quality estimation task,\r\n- an automatic post-editing task,\r\n- a parallel corpus filtering task.\r\n\r\nSource: [http://www.statmt.org/wmt18/](http://www.statmt.org/wmt18/)", "variants": ["WMT 2018 English-Finnish", "WMT 2018 Finnish-English", "WMT 2018", "WMT 2018 English-Estonian", "WMT 2018 Estonian-English"]}
{"id": "ArxivPapers", "title": "", "contents": "The **ArxivPapers** dataset is an unlabelled collection of over 104K papers related to machine learning and published on arXiv.org between 2007–2020. The dataset includes around 94K papers (for which LaTeX source code is available) in a structured form in which paper is split into a title, abstract, sections, paragraphs and references. Additionally, the dataset contains over 277K tables extracted from the LaTeX papers.\n\nDue to the papers license the dataset is published as a metadata and open-source pipeline that can be used to obtain and convert the papers.\n\nSource: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)\nImage Source: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)", "variants": ["ArxivPapers"]}
{"id": "SegmentedTables", "title": "", "contents": "The **SegmentedTables** dataset is a collection of almost 2,000 tables extracted from 352 machine learning papers. Each table consists of rich text content, layout and caption. Tables are annotated with types (leaderboard, ablation, irrelevant) and cells of relevant tables are annotated with semantic roles (such as “paper model”, “competing model”, “dataset”, “metric”).\n\nDue to the license of source papers the dataset is published as a metadata, all annotations and open-source pipeline that can be used to extract the tables.\n\nSource: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)\nImage Source: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)", "variants": ["SegmentedTables"]}
{"id": "LinkedResults", "title": "", "contents": "The **LinkedResults** dataset contains around 1,600 results capturing performance of machine learning models from tables of 239 papers. All tables come from a subset of SegmentedTables dataset. Each result is a tuple of form (task, dataset, metric name, metric value) and is linked to a particular table, row and cell it originates from.\n\nSource: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)", "variants": ["LinkedResults"]}
{"id": "PWC Leaderboards", "title": "AxCell: Automatic Extraction of Results from Machine Learning Papers", "contents": "The **Papers with Code Leaderboards** dataset is a collection of over 5,000 results capturing performance of machine learning models. Each result is a tuple of form (task, dataset, metric name, metric value). The data was collected using the Papers with Code review interface.\n\nSource: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)\nImage Source: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)", "variants": ["PWC Leaderboards (restricted)", "PWC Leaderboards"]}
{"id": "UBIRIS.v2", "title": "The UBIRIS.v2: A database of visible wavelength images captured on-the-move and at-a-distance", "contents": "The **UBIRIS.v2** iris dataset contains 11,102 iris images from 261 subjects with 10 images each subject. The images were captured under unconstrained conditions (at-a-distance, on-the-move and on the visible wavelength), with realistic noise factors.\n\nSource: [Constrained Design of Deep Iris Networks](https://arxiv.org/abs/1905.09481)\nImage Source: [https://arxiv.org/pdf/1905.09481.pdf](https://arxiv.org/pdf/1905.09481.pdf)", "variants": ["UBIRIS.v2"]}
{"id": "VIVA", "title": "", "contents": "The **VIVA** challenge’s dataset is a multimodal dynamic hand gesture dataset specifically designed with difficult settings of cluttered background, volatile illumination, and frequent occlusion for studying natural human activities in real-world driving settings. This dataset was captured using a Microsoft Kinect device, and contains 885 intensity and depth video sequences of 19 different dynamic hand gestures performed by 8 subjects inside a vehicle.\n\nSource: [Short-Term Temporal Convolutional Networks for Dynamic Hand Gesture Recognition](https://arxiv.org/abs/2001.05833)\nImage Source: [http://www.site.uottawa.ca/research/viva/projects/hand_detection/index.html](http://www.site.uottawa.ca/research/viva/projects/hand_detection/index.html)", "variants": ["VIVA Hand Gestures Dataset", "VIVA"]}
{"id": "GRID Dataset", "title": "", "contents": "The QMUL underGround Re-IDentification (**GRID**) dataset contains 250 pedestrian image pairs. Each pair contains two images of the same individual seen from different camera views. All images are captured from 8 disjoint camera views installed in a busy underground station. The figures beside show a snapshot of each of the camera views of the station and sample images in the dataset. The dataset is challenging due to variations of pose, colours, lighting changes; as well as poor image quality caused by low spatial resolution.\r\n\r\nSource: [https://personal.ie.cuhk.edu.hk/~ccloy/downloads_qmul_underground_reid.html](https://personal.ie.cuhk.edu.hk/~ccloy/downloads_qmul_underground_reid.html)", "variants": ["GRID corpus (mixed-speech)", "GRID Dataset"]}
{"id": "BDD100K", "title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning", "contents": "**BDD100K** is a semantic segmentation diverse driving video dataset with 10K images with full-frame instance segmentation annotations collected from distinct locations in the United States. It contains a diverse set of driving videos under various weather conditions, time, and scene types. It has a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.\r\n\r\nSource: [Variational Adversarial Active Learning](https://arxiv.org/abs/1904.00370)\r\nImage Source: [https://bdd-data.berkeley.edu/](https://bdd-data.berkeley.edu/)", "variants": ["BDD100k", "BDD100K"]}
{"id": "COIL-20", "title": "", "contents": "The Columbia Object Image Library (**COIL-20**) dataset contains 20 objects, and each of them has 72 images captured every 5 degrees along a viewing circle. All images consist of the smallest patch (of size 128×128 ) that contains the object, i.e. the background has been discarded.\n\nSource: [Factorization of View-Object Manifolds for Joint Object Recognition and Pose Estimation1footnote 11footnote 1This is an extension of the paper entitled ”Joint Object and Pose Recognition Using Homeomorphic Manifold Analysis” ZhangAAAI13 that was presented at the 27th AAAI Conference on Artificial Intelligence (AAAI-13) held July 14–-18, 2013 in Bellevue, Washington, USA.](https://arxiv.org/abs/1503.06813)\nImage Source: [https://www.researchgate.net/publication/249322334_Fast_Discriminative_Stochastic_Neighbor_Embedding_Analysis](https://www.researchgate.net/publication/249322334_Fast_Discriminative_Stochastic_Neighbor_Embedding_Analysis)", "variants": ["Coil-20", "COIL-20"]}
{"id": "COIL-100", "title": "", "contents": "**COIL-100** is a dataset comprising 7200 128×128 color images of 100 object classes. There are 72 images of each object in different poses.\r\n\r\nSource: [Puzzle-AE: Novelty Detection in Images through Solving Puzzles](https://arxiv.org/abs/2008.12959)\r\nImage Source: [https://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php](https://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php)", "variants": ["coil-100", "COIL-100"]}
{"id": "ICVL Hand Posture", "title": "Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture", "contents": "The ICVL dataset is a hand pose estimation dataset that consists of 330K training frames and 2 testing sequences with each 800 frames. The dataset is collected from 10 different subjects with 16 hand joint annotations for each frame.\n\nSource: [AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation](https://arxiv.org/abs/2007.09590)\nImage Source: [Tang et al.; Latent Regression Forest: Structured Estimation of 3D Hand Poses](https://alykhantejani.github.io/pdfs/LRF_PAMI_DRAFT.pdf)", "variants": ["ICVL Hand Posture"]}
{"id": "SegTrack-v2", "title": "Video Segmentation by Tracking Many Figure-Ground Segments", "contents": "SegTrack v2 is a video segmentation dataset with full pixel-level annotations on multiple objects at each frame within each video.\r\n\r\nSource: [https://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html](https://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html)\r\nImage Source: [https://www.researchgate.net/publication/325842926_Semantic_Video_Segmentation_A_Review_on_Recent_Approaches](https://www.researchgate.net/publication/325842926_Semantic_Video_Segmentation_A_Review_on_Recent_Approaches)", "variants": ["SegTrack v2", "SegTrack-v2"]}
{"id": "FAUST", "title": "", "contents": "The **FAUST** dataset is a dataset of real 3D scans of humans. It contains 10 scanned human shapes in 10 different poses, resulting in a total of 100 non-watertight meshes with 6,890 nodes each.\r\n\r\nSource: [SpiralNet++: A Fast and Highly Efficient Mesh Convolution Operator](https://arxiv.org/abs/1911.05856)\r\nImage Source: [http://faust.is.tue.mpg.de/overview](http://faust.is.tue.mpg.de/overview)", "variants": ["Faust", "FAUST"]}
{"id": "Wireframe", "title": "Learning to Parse Wireframes in Images of Man-Made Environments", "contents": "The **Wireframe** dataset consists of 5,462 images (5,000 for training, 462 for test) of indoor and outdoor man-made scenes.\r\n\r\nSource: [MCMLSD: A Probabilistic Algorithm and Evaluation Framework for Line Segment Detection](https://arxiv.org/abs/2001.01788)\r\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_to_Parse_CVPR_2018_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_to_Parse_CVPR_2018_paper.pdf)", "variants": ["wireframe dataset", "Wireframe"]}
{"id": "VisDA-2017", "title": "VisDA: The Visual Domain Adaptation Challenge", "contents": "**VisDA-2017** is a simulation-to-real dataset for domain adaptation with over 280,000 images across 12 categories in the training, validation and testing domains. The training images are generated from the same object under different circumstances, while the validation images are collected from MSCOCO..\r\n\r\nSource: [Gradually Vanishing Bridge for Adversarial Domain Adaptation](https://arxiv.org/abs/2003.13183)\r\nImage Source: [http://ai.bu.edu/visda-2017/](http://ai.bu.edu/visda-2017/)", "variants": ["VisDA2017", "VisDA-2017"]}
{"id": "ImageNet-32", "title": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets", "contents": "Imagenet32 is a huge dataset made up of small images called the down-sampled version of Imagenet. Imagenet32 is composed of 1,281,167 training data and 50,000 test data with 1,000 labels.\r\n\r\nSource: [Self-supervised Knowledge Distillation Using Singular Value Decomposition](https://arxiv.org/abs/1807.06819)\r\nImage Source: [https://arxiv.org/pdf/1707.08819v3.pdf](https://arxiv.org/pdf/1707.08819v3.pdf)", "variants": ["ImageNet32", "ImageNet-32"]}
{"id": "MVTecAD", "title": "MVTec AD -- A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection", "contents": "MVTec AD is a dataset for benchmarking anomaly detection methods with a focus on industrial inspection. It contains over 5000 high-resolution images divided into fifteen different object and texture categories. Each category comprises a set of defect-free training images and a test set of images with various kinds of defects as well as images without defects.\r\n\r\nThere are two common metrics: Detection AUROC and Segmentation (or pixelwise) AUROC\r\n\r\nDetection (or, classification) methods output single float (anomaly score) per input test image. \r\n\r\nSegmentation methods output anomaly probability for each pixel. \r\n\"To assess segmentation performance, we evaluate the relative per-region overlap of the segmentation with the ground truth. To get an additional performance measure that is independent of the determined threshold, we compute the area under the receiver operating characteristic curve (ROC AUC). We define the true positive rate as the percentage of pixels that were correctly classified as anomalous\" [1]\r\nLater segmentation metric was improved to balance regions with small and large area, see PRO-AUC and other in [2]\r\nSource: [MVTEC ANOMALY DETECTION DATASET](https://www.mvtec.com/company/research/datasets/mvtec-ad/)\r\nImage Source: [https://www.mvtec.com/company/research/datasets/mvtec-ad/](https://www.mvtec.com/company/research/datasets/mvtec-ad/)\r\n[1] Paul Bergmann et al, \"MVTec AD — A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection\"\r\n[2] [Bergmann, P., Batzner, K., Fauser, M. et al. The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. Int J Comput Vis (2021). https://doi.org/10.1007/s11263-020-01400-4](https://link.springer.com/article/10.1007/s11263-020-01400-4)", "variants": ["MVTec AD", "MVTecAD"]}
{"id": "Kvasir", "title": "KVASIR: A Multi-Class Image Dataset for Computer Aided Gastrointestinal Disease Detection", "contents": "The KVASIR Dataset was released as part of the medical multimedia challenge presented by MediaEval. It is based on images obtained from the GI tract via an endoscopy procedure. The dataset is composed of images that are annotated and verified by medical doctors, and captures 8 different classes. The classes are based on three anatomical landmarks (z-line, pylorus, cecum), three pathological findings (esophagitis, polyps, ulcerative colitis) and two other classes (dyed and lifted polyps, dyed resection margins) related to the polyp removal process. Overall, the dataset contains 8,000 endoscopic images, with 1,000 image examples per class.\r\n\r\nSource: [Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data Analysis](https://arxiv.org/abs/2007.05914)\r\nImage Source: [https://datasets.simula.no/kvasir/](https://datasets.simula.no/kvasir/)", "variants": ["Kvasir-SEG", "Kvasir"]}
{"id": "PASCAL VOC", "title": "Location-aware Single Image Reflection Removal", "contents": "The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The **PASCAL VOC** dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set.\r\n\r\nSource: [Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey](https://arxiv.org/abs/1902.06162)\r\nImage Source: [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/images/sheep_06.jpg](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/images/sheep_06.jpg)", "variants": ["PASCAL VOC 2007", "PASCAL VOC 2011", "PASCAL VOC 2011 test", "PASCAL VOC 2012 25% labeled", "PASCAL VOC 2012 test", "PASCAL VOC 2012 val", "PASCAL VOC 2012, 60 proposals per image", "Pascal VOC 2007 count-test", "Pascal VOC 2012 1% labeled", "Pascal VOC 2012 12.5% labeled", "Pascal VOC 2012 2% labeled", "Pascal VOC 2012 5% labeled", "PASCAL VOC", "PASCAL VOC 2012", "VOC12"]}
{"id": "LibriSpeech", "title": "Librispeech: An ASR corpus based on public domain audio books", "contents": "The **LibriSpeech** corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ’clean’ and ’other’ categories, respectively, depending upon how well or challening Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.\r\n\r\nSource: [State-of-the-art Speech Recognition using Multi-stream Self-attention with Dilated 1D Convolutions](https://arxiv.org/abs/1910.00716)", "variants": ["LibriSpeech test-clean", "LibriSpeech test-other", "Librispeech", "LibriSpeech", "LibriSpeechLibri-Light test-othertest-other"]}
{"id": "Set14", "title": "On Single Image Scale-Up Using Sparse-Representations", "contents": "The **Set14** dataset is a dataset consisting of 14 images commonly used for testing performance of Image Super-Resolution models.\r\nImage Source: [https://www.ece.rice.edu/~wakin/images/](https://www.ece.rice.edu/~wakin/images/)", "variants": ["Set14 - 2x upscaling", "Set14 - 3x upscaling", "Set14 - 4x upscaling", "Set14 - 8x upscaling", "Set14"]}
{"id": "NYUv2", "title": "", "contents": "The **NYU-Depth V2** data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect. It features:\r\n\r\n* 1449 densely labeled pairs of aligned RGB and depth images\r\n* 464 new scenes taken from 3 cities\r\n* 407,024 new unlabeled frames\r\n* Each object is labeled with a class and an instance number.\r\nThe dataset has several components:\r\n* Labeled: A subset of the video data accompanied by dense multi-class labels. This data has also been preprocessed to fill in missing depth labels.\r\n* Raw: The raw RGB, depth and accelerometer data as provided by the Kinect.\r\n* Toolbox: Useful functions for manipulating the data and labels.\r\n\r\nSource: [https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)\r\nImage Source: [https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)", "variants": ["NYU-Depth V2", "NYU Depth v2", "NYU-Depth V2 Surface Normals", "NYUv2"]}
{"id": "Urban100", "title": "Single Image Super-Resolution From Transformed Self-Exemplars", "contents": "The **Urban100** dataset contains 100 images of urban scenes. It commonly used as a test set to evaluate the performance of super-resolution models.\r\nImage Source: [http://vllab.ucmerced.edu/wlai24/LapSRN/](http://vllab.ucmerced.edu/wlai24/LapSRN/)", "variants": ["Urban100 - 16x upscaling", "Urban100 - 2x upscaling", "Urban100 - 3x upscaling", "Urban100 - 4x upscaling", "Urban100 - 8x upscaling", "Urban100 sigma10", "urban100 sigma15", "Urban100 sigma15", "Urban100 sigma25", "Urban100 sigma30", "Urban100 sigma50", "Urban100 sigma70", "Urban100"]}
{"id": "Helen", "title": "Interactive Facial Feature Localization", "contents": "The HELEN dataset is composed of 2330 face images of 400×400 pixels with labeled facial components generated through manually-annotated contours along eyes, eyebrows, nose, lips and jawline.\r\n\r\nSource: [Face Parsing via a Fully-Convolutional Continuous CRF Neural Network](https://arxiv.org/abs/1708.03736)\r\nImage Source: [http://www.ifp.illinois.edu/~vuongle2/helen/](http://www.ifp.illinois.edu/~vuongle2/helen/)", "variants": ["Helen"]}
{"id": "Extended Yale B", "title": "From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose", "contents": "The **Extended Yale B** database contains 2414 frontal-face images with size 192×168 over 38 subjects and about 64 images per subject. The images were captured under different lighting conditions and various facial expressions.\r\n\r\nSource: [Learning Locality-Constrained Collaborative Representation for Robust Face Recognition](https://arxiv.org/abs/1210.1316)\r\nImage Source: [http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html](http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html)", "variants": ["Extended Yale-B", "Extended Yale B"]}
{"id": "KTH", "title": "Recognizing Human Actions: A Local SVM Approach", "contents": "The efforts to create a non-trivial and publicly available dataset for action recognition was initiated at the **KTH** Royal Institute of Technology in 2004. The KTH dataset is one of the most standard datasets, which contains six actions: walk, jog, run, box, hand-wave, and hand clap. To account for performance nuance, each action is performed by 25 different individuals, and the setting is systematically altered for each action per actor. Setting variations include: outdoor (s1), outdoor with scale variation (s2), outdoor with different clothes (s3), and indoor (s4). These variations test the ability of each algorithm to identify actions independent of the background, appearance of the actors, and the scale of the actors.\r\n\r\nSource: [Review of Action Recognition and Detection Methods](https://arxiv.org/abs/1610.06906)", "variants": ["KTH"]}
{"id": "MoCap", "title": "", "contents": "Collection of various motion capture recordings (walking, dancing, sports, and others) performed by over 140 subjects. The database contains free motions which you can download and use. There is a zip file of all asf/amc's on the FAQs page.\n\nSource: [https://www.re3data.org/repository/r3d100012183](https://www.re3data.org/repository/r3d100012183)", "variants": ["MoCap"]}
{"id": "USF", "title": "The HumanID Gait Challenge Problem: Data Sets, Performance, and Analysis", "contents": "The **USF** **Human ID Gait Challenge Dataset** is a dataset of videos for gait recognition. It has videos from 122 subjects in up to 32 possible combinations of variations in factors.\n\nSource: [http://www.eng.usf.edu/cvprg/Gait_Data.html](http://www.eng.usf.edu/cvprg/Gait_Data.html)", "variants": ["USF"]}
{"id": "Oxford5k", "title": "", "contents": "Oxford5K is the **Oxford Buildings** Dataset, which contains 5062 images collected from Flickr. It offers a set of 55 queries for 11 landmark buildings, five for each landmark.\r\n\r\nSource: [Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN 1 Corresponding Author](https://arxiv.org/abs/1907.05793)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/](https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/)", "variants": ["Oxford5k"]}
{"id": "CBSD68", "title": "", "contents": "**Color BSD68** dataset for image denoising benchmarks is part of The Berkeley Segmentation Dataset and Benchmark. It is used for measuring image denoising algorithms performance. It contains 68 images.\r\n\r\nSource: [https://github.com/clausmichele/CBSD68-dataset](https://github.com/clausmichele/CBSD68-dataset)", "variants": ["CBSD68", "CBSD68 sigma15", "CBSD68 sigma25", "CBSD68 sigma35", "CBSD68 sigma50", "CBSD68 sigma75", "CBSD68 sigma10", "CBSD68 sigma20", "CBSD68 sigma30", "CBSD68 sigma40", "CBSD68 sigma45", "CBSD68 sigma5", "CBSD68 sigma55", "CBSD68 sigma60", "CBSD68 sigma65", "CBSD68 sigma70"]}
{"id": "Oxford105k", "title": "Object retrieval with large vocabularies and fast spatial matching", "contents": "**Oxford105k** is the combination of the Oxford5k dataset and 99782 negative images crawled from Flickr using 145 most popular tags. This dataset is used to evaluate search performance for object retrieval (reported as mAP) on a large scale.\r\n\r\nSource: [Multiple Measurements and Joint Dimensionality Reduction for Large Scale Image Search with Short Vectors Extended Version](https://arxiv.org/abs/1504.03285)", "variants": ["Oxford105k"]}
{"id": "Permuted MNIST", "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks", "contents": "**Permuted MNIST** is an MNIST variant that consists of 70,000 images of handwritten digits from 0 to 9, where 60,000 images are used for training, and 10,000 images for test. The difference of this dataset from the original MNIST is that each of the ten tasks is the multi-class classification of a different random permutation of the input pixels.\r\n\r\nSource: [Lifelong Learning with Dynamically Expandable Networks](https://arxiv.org/abs/1708.01547)\r\nImage Source: [https://arxiv.org/pdf/1810.12488.pdf](https://arxiv.org/pdf/1810.12488.pdf)", "variants": ["Permuted MNIST"]}
{"id": "SceneNet", "title": "SceneNet: Understanding Real World Indoor Scenes With Synthetic Data", "contents": "**SceneNet** is a dataset of labelled synthetic indoor scenes. There are several labeled indoor scenes, including:\r\n\r\n- 11 Bedroom scenes with 428 objects\r\n- 15 Office scenes with 1,203 objects\r\n- 11 Kitchen scenes with 797 objects\r\n- 10 Living Room scenes with 715 objects\r\n- 10 Bathrooms with 556 objects\r\n\r\nSource: [https://robotvault.bitbucket.io/](https://robotvault.bitbucket.io/)\r\nImage Source: [https://robotvault.bitbucket.io/big_scene.html](https://robotvault.bitbucket.io/big_scene.html)", "variants": ["SceneNet"]}
{"id": "SceneNet RGB-D", "title": "", "contents": "SceneNet-RGBD is a synthetic dataset containing large-scale photorealistic renderings of indoor scene trajectories with pixel-level annotations. Random sampling permits virtually unlimited scene configurations, and the dataset creators provide a set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses. Each layout also has random lighting, camera trajectories, and textures. The scale of this dataset is well suited for pre-training data-driven computer vision techniques from scratch with RGB-D inputs, which previously has been limited by relatively small labelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for investigating 3D scene labelling tasks by providing perfect camera poses and depth data as proxy for a SLAM system.\n\nSource: [ViewAL: Active Learning with Viewpoint Entropy for Semantic Segmentation](https://arxiv.org/abs/1911.11789)\nImage Source: [https://robotvault.bitbucket.io/scenenet-rgbd.html](https://robotvault.bitbucket.io/scenenet-rgbd.html)", "variants": ["SceneNet RGB-D"]}
{"id": "BMS-26", "title": "", "contents": "The  **Berkeley Motion Segmentation** Dataset (**BMS-26**) is a dataset for motion segmentation, which consists of 26 video sequences with pixel-accurate segmentation annotation of moving objects. A total of 189 frames is annotated. 12 of the sequences are taken from the Hopkins 155 dataset and new annotation is added.\n\nSource: [https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)", "variants": ["BMS-26"]}
{"id": "Freiburg Campus 3D Scan", "title": "", "contents": "The **Freiburg Campus 3D Scan** dataset consists of 3D area maps from the Freiburg campus that were scanned with 3D lasers. Areas include corridors, the outdoor campus, and some of the colleges and buildings.\n\nSource: [http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/](http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/)\nImage Source: [http://ais.informatik.uni-freiburg.de/projects/datasets/octomap/](http://ais.informatik.uni-freiburg.de/projects/datasets/octomap/)", "variants": ["Freiburg Campus 3D Scan"]}
{"id": "Plant Centroids", "title": "", "contents": "**Plant Centroids** is a dataset for stem emerging points (SEP) detection in RGB and NIR image data. The dataset is meant to aid the construction of agricultural robots, where detecting SEPs is an important perception task (to position weeding or fertilizing tools at the plant’s center and finding natural landmarks in the field environment). The dataset contains annotations for ~2000 image sets with a broad variance of plant species and growth stages.\n\nSource: [http://plantcentroids.cs.uni-freiburg.de/](http://plantcentroids.cs.uni-freiburg.de/)\nImage Source: [http://plantcentroids.cs.uni-freiburg.de/](http://plantcentroids.cs.uni-freiburg.de/)", "variants": ["Plant Centroids"]}
{"id": "Freiburg Across Seasons", "title": "", "contents": "**Freiburg Across Seasons** captures long-term perceptual changes across a span of 3 years. Image sequences were recorded with a forward facing bumblebee stereo camera mounted on a car. During summer, the camera was mounted outside the car where as during winters the camera was inside the car. The image sequences are recorded at relatively low frame rates of 1Hz and 4Hz. All the images have a resolution of 1024 × 768 (width×height) and are JPEG compressed. In total, there are ground truth matchings for 8,133 images for localization based on GPS position.\n\nSource: [http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/](http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/)\nImage Source: [http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/](http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/)", "variants": ["Freiburg Across Seasons"]}
{"id": "Freiburg Block Tasks", "title": "", "contents": "**Freiburg Block Tasks** is a dataset for robot skill learning. It consists of two datasets.\r\nThe first data set consisted of three simulated robot tasks: stacking (A), color pushing (B) and color stacking (C). The data set contains 300 multi-view demonstration videos per task. The tasks are simulated with PyBullet. Of these 300 demonstrations, 150 represent unsuccessful executions of the different tasks. The authors found it helpful to add unsuccessful demonstrations in the training of the embedding to enable training RL agents on it. Without fake examples, the distances in the embedding space for states not seen during training might be noisy. The test set contains the manipulation of blocks. Within the validation set, the blocks are replaced by cylinders of different colors.\r\nThe second data set includes real-world human executions of the simulated robot tasks (A, B and C), as well as demonstrations for a task where one has to first separate blocks in order to stack them (D). For each task, there are 60 multi-view demonstration videos, corresponding to 24 minutes of interaction. In contrast to the simulated data set, the real demonstrations contain no unsuccessful executions and are of varying length. The test set contains blocks of unseen sizes and textures, as well as unknown backgrounds.\r\n\r\nSource: [http://robotskills.cs.uni-freiburg.de/](http://robotskills.cs.uni-freiburg.de/)\nImage Source: [http://robotskills.cs.uni-freiburg.de/](http://robotskills.cs.uni-freiburg.de/)", "variants": ["Freiburg Block Tasks"]}
{"id": "Cityscapes-Motion", "title": "", "contents": "The **Cityscapes-Motion** dataset is a supplement to the semantic annotations provided by the Cityscapes dataset, containing 2975 training images and 500 validation images. The dataset creators provide manually annotated motion labels for the category of cars. The images are of resolution 2048×1024 pixels. The task to learn is not just semantic segmentation but also the motion status of the objects.\n\nSource: [http://deepmotion.cs.uni-freiburg.de/](http://deepmotion.cs.uni-freiburg.de/)\nImage Source: [http://deepmotion.cs.uni-freiburg.de/](http://deepmotion.cs.uni-freiburg.de/)", "variants": ["Cityscapes-Motion"]}
{"id": "KITTI-Motion", "title": "", "contents": "The **KITTI-Motion** dataset contains pixel-wise semantic class labels and moving object annotations for 255 images taken from the KITTI Raw dataset. The images are of resolution 1280×384 pixels and contain scenes of freeways, residential areas and inner-cities. The task is not just to semantically segment objects but also to identify their motion status.\n\nSource: [http://deepmotion.cs.uni-freiburg.de/](http://deepmotion.cs.uni-freiburg.de/)\nImage Source: [http://deepmotion.cs.uni-freiburg.de/](http://deepmotion.cs.uni-freiburg.de/)", "variants": ["KITTI-Motion"]}
{"id": "DeepLocCross", "title": "", "contents": "**DeepLocCross** is a localization dataset that contains RGB-D stereo images captured at 1280 x 720 pixels at a rate of 20 Hz. The ground-truth pose labels are generated using a LiDAR-based SLAM system. In addition to the 6-DoF localization poses of the robot, the dataset additionally contains tracked detections of the observable dynamic objects. Each tracked object is identified using a unique track ID, spatial coordinates, velocity and orientation angle. Furthermore, as the dataset contains multiple pedestrian crossings, labels at each intersection indicating its safety for crossing are provided.\r\nThis dataset consists of seven training sequences with a total of 2264 images, and three testing sequences with a total of 930 images. The dynamic nature of the surrounding environment at which the dataset was captured renders the tasks of localization and visual odometry estimation extremely challenging due to the varying weather conditions, presence of shadows and motion blur caused by the movement of the robot platform. Furthermore, the presence of multiple dynamic objects often results in partial and full occlusions to the informative regions of the image. Moreover, the presence of repeated structures render the pose estimation task more challenging. Overall this dataset covers a wide range of perception related tasks such as loop closure detection, semantic segmentation, visual odometry estimation, global localization, scene flow estimation and behavior prediction.\r\n\r\nSource: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)\nImage Source: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)", "variants": ["DeepLocCross"]}
{"id": "Freiburg Lighting Adaptable Map Tracking", "title": "", "contents": "**Freiburg Lighting Adaptable Map Tracking** is a dataset for camera trajectory estimation. The dataset consists of two subdatasets, each consisting of a Lighting Adaptable Map and three camera trajectories recorded under varying lighting conditions. The map meshes are stored in PLY format with custom properties and elements. The trajectories contain synchronized RGB-D images, exposure times and gains, ground-truth light settings and camera poses, as well as the camera tracking results presented in the paper.\n\nSource: [http://tracklam.informatik.uni-freiburg.de/](http://tracklam.informatik.uni-freiburg.de/)\nImage Source: [http://tracklam.informatik.uni-freiburg.de/](http://tracklam.informatik.uni-freiburg.de/)", "variants": ["Freiburg Lighting Adaptable Map Tracking"]}
{"id": "Freiburg Poking", "title": "", "contents": "The **Freiburg Poking** dataset is a dataset for learning intuitive physics from physical interaction. It consists of 40K of interaction data with a KUKA LBR iiwa manipulator and a fixed Azure Kinect RGB-D camera. The dataset creators built an arena of styrofoam with walls for preventing objects from falling down. At any given time there were 3-7 objects randomly chosen from a set of 34 distinct objects present on the arena. The objects differed from each other in shape, appearance, material, mass and friction.\n\nSource: [http://hind4sight.cs.uni-freiburg.de/](http://hind4sight.cs.uni-freiburg.de/)\nImage Source: [http://hind4sight.cs.uni-freiburg.de/](http://hind4sight.cs.uni-freiburg.de/)", "variants": ["Freiburg Poking"]}
{"id": "7-Scenes", "title": "", "contents": "The **7-Scenes** dataset is a collection of tracked RGB-D camera frames. The dataset may be used for evaluation of methods for different applications such as dense tracking and mapping and relocalization techniques.\nAll scenes were recorded from a handheld Kinect RGB-D camera at 640×480 resolution. The dataset creators use an implementation of the KinectFusion system to obtain the ‘ground truth’ camera tracks, and a dense 3D model. Several sequences were recorded per scene by different users, and split into distinct training and testing sequence sets.\n\nSource: [https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/)\nImage Source: [https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/)", "variants": ["7-Scenes"]}
{"id": "George Washington", "title": "Lexicon-free handwritten word spotting using character HMMs", "contents": "The **George Washington** dataset contains 20 pages of letters written by George Washington and his associates in 1755 and thereby categorized into historical collection. The images are annotated at word level and contain approximately 5,000 words.\r\n\r\nSource: [HWNet v2: An Efficient Word Image Representation for Handwritten Documents.](https://arxiv.org/abs/1802.06194)\r\nImage Source: [https://www.loc.gov/resource/mgw1a.002/?sp=2](https://www.loc.gov/resource/mgw1a.002/?sp=2)", "variants": ["George Washington"]}
{"id": "Parzival", "title": "Language Model Integration for the Recognition of Handwritten Medieval Documents", "contents": "The **Parzival** dataset consists of 47 pages by three writers. These pages were taken from a medieval German manuscript from the 13th century that contains the epic poem Parzival by Wolfram von Eschenbach. The image size is 2000 x 3000 pixels. 24 pages are selected as training set; 14 pages are selected as test set; 2 pages are selected as validation set.\n\nSource: [https://diuf.unifr.ch/main/hisdoc/divadia](https://diuf.unifr.ch/main/hisdoc/divadia)\nImage Source: [https://diuf.unifr.ch/main/hisdoc/divadia](https://diuf.unifr.ch/main/hisdoc/divadia)", "variants": ["Parzival"]}
{"id": "EgoDexter", "title": "", "contents": "The **EgoDexter** dataset provides both 2D and 3D pose annotations for 4 testing video sequences with 3190 frames. The videos are recorded with body-mounted camera from egocentric viewpoints and contain cluttered backgrounds, fast camera motion, and complex interactions with various objects. Fingertip positions were manually annotated for 1485 out of 3190 frames.\n\nSource: [Hand Pose Estimation via Latent 2.5D Heatmap Regression](https://arxiv.org/abs/1804.09534)\nImage Source: [https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm](https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm)", "variants": ["EgoDexter"]}
{"id": "Washington RGB-D Scenes v2", "title": "", "contents": "The RGB-D Scenes Dataset v2 consists of 14 scenes containing furniture (chair, coffee table, sofa, table) and a subset of the objects in the RGB-D Object Dataset (bowls, caps, cereal boxes, coffee mugs, and soda cans). Each scene is a point cloud created by aligning a set of video frames using Patch Volumes Mapping.\r\n\r\nSource: [https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/](https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/)\nImage Source: [https://arxiv.org/abs/1904.02530](https://arxiv.org/abs/1904.02530)", "variants": ["Washington RGB-D Scenes v2"]}
{"id": "Washington RGB-D Scenes", "title": "", "contents": "The RGB-D Scenes Dataset contains 8 scenes annotated with objects that belong to the Washington RGB-D Object Dataset. Each scene is a single video sequence consisting of multiple RGB-D frames.\n\nSource: [https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/](https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/)\nImage Source: [https://arxiv.org/abs/1904.02530](https://arxiv.org/abs/1904.02530)", "variants": ["Washington RGB-D Scenes"]}
{"id": "Freiburg RGB-D People", "title": "", "contents": "The **Freiburg RGB-D People** dataset contains 3000+ RGB-D frames acquired in a university hall from three vertically mounted Kinect sensors. The data contains mostly upright walking and standing persons seen from different orientations and with different levels of occlusions.\n\nSource: [http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html](http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html)\nImage Source: [http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html](http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html)", "variants": ["Freiburg RGB-D People"]}
{"id": "PAVIS RGB-D", "title": "Re-identification with RGB-D Sensors", "contents": "**PAVIS RGB-D** is a dataset for person re-identification using depth information. The main motivation is that techniques such as  SDALF fail when the individuals change their clothing, therefore they cannot be used for long-term video surveillance. Depth information is the solution to deal with this problem because it stays constant for a longer period of time. The dataset is composed by four different groups of data collected using the Kinect. The first group of data has been obtained by recording 79 people with a frontal view, walking slowly, avoiding occlusions and with stretched arms (\"Collaborative\"). This happened in an indoor scenario, where the people were at least 2 meters away from the camera. The second (\"Walking1\") and third (\"Walking2\") groups of data are composed by frontal recordings of the same 79 people walking normally while entering the lab where they normally work. The fourth group (\"Backwards\") is a back view recording of the people walking away from the lab.\nThe dataset creators provide 5 synchronized information for each person: 1) a set of 5 RGB images, 2) the foreground masks, 3) the skeletons, 4) the 3d mesh (ply), 5) the estimated floor.\n\nSource: [https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset](https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset)\nImage Source: [https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset](https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset)", "variants": ["PAVIS RGB-D"]}
{"id": "Couples Therapy", "title": "", "contents": "The **Couples Therapy** corpus contains audio, video recordings and manual transcriptions of conversations between 134 real-life couples attending marital therapy. In each session, one person selected a topic that was discussed over 10 minutes with the spouse. At the end of the session, both speakers were rated separately on 33 “behavior codes” by multiple annotators based on the Couples Interaction and Social Support Rating Systems. Each behavior was rated on a Likert scale from 1, indicating absence, to 9, indicating strong presence. A session-level rating was obtained for each speaker by averaging the annotator ratings. This process was repeated for the spouse, resulting in 2 sessions per couple at a time. The total number of sessions per couple varied between 2 and 6.\n\nSource: [Modeling Interpersonal Influence of Verbal Behaviorin Couples Therapy Dyadic Interactions](https://arxiv.org/abs/1805.09436)", "variants": ["Couples Therapy"]}
{"id": "AI2-THOR", "title": "AI2-THOR: An Interactive 3D Environment for Visual AI", "contents": "AI2-Thor is an interactive environment for embodied AI. It contains four types of scenes, including kitchen, living room, bedroom and bathroom, and each scene includes 30 rooms, where each room is unique in terms of furniture placement and item types. There are over 2000 unique objects for AI agents to interact with.\r\n\r\nSource: [Learning Object Relation Graph andTentative Policy for Visual Navigation](https://arxiv.org/abs/2007.11018)\r\nImage Source: [https://ai2thor.allenai.org/](https://ai2thor.allenai.org/)", "variants": ["AI2-THOR"]}
{"id": "DeepMind Control Suite", "title": "DeepMind Control Suite", "contents": "The **DeepMind Control Suite** (DMCS) is a set of simulated continuous control environments with a standardized structure and interpretable rewards. The tasks are written and powered by the MuJoCo physics engine, making them easy to identify. Control Suite tasks include Pendulum, Acrobot, Cart-pole, Cart-k-pole, Ball in cup, Point-mass, Reacher, Finger, Hooper, Fish, Cheetah, Walker, Manipulator, Manipulator extra, Stacker, Swimmer, Humanoid, Humanoid_CMU and LQR.\r\n\r\nSource: [Unsupervised Learning of Object Structure and Dynamics from Videos](https://arxiv.org/abs/1906.07889)\r\nImage Source: [https://arxiv.org/abs/1801.00690](https://arxiv.org/abs/1801.00690)", "variants": ["DeepMind Control Suite", "DeepMind Cartpole Balance (Images)", "DeepMind Cartpole Swingup (Images)", "DeepMind Cheetah Run (Images)", "DeepMind Cup Catch (Images)", "DeepMind Finger Spin (Images)", "DeepMind Walker Walk (Images)"]}
{"id": "Mario AI", "title": "The 2009 Mario AI Competition", "contents": "**Mario AI** was a benchmark environment for reinforcement learning. The gameplay in Mario AI, as in the original Nintendo’s version, consists in moving the controlled character, namely Mario, through two-dimensional levels, which are viewed sideways. Mario can walk and run to the right and left, jump, and (depending on which state he is in) shoot fireballs. Gravity acts on Mario, making it necessary to jump over cliffs to get past them. Mario can be in one of three states: Small, Big (can kill enemies by jumping onto them), and Fire (can shoot fireballs).\n\nSource: [https://github.com/zerg000000/mario-ai](https://github.com/zerg000000/mario-ai)\nImage Source: [http://julian.togelius.com/Karakovskiy2012The.pdf](http://julian.togelius.com/Karakovskiy2012The.pdf)", "variants": ["Mario AI"]}
{"id": "D4RL", "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning", "contents": "**D4RL** is a collection of environments for offline reinforcement learning. These environments include Maze2D, AntMaze, Adroit, Gym, Flow, FrankKitchen and CARLA.\r\n\r\nSource: [https://sites.google.com/view/d4rl/home](https://sites.google.com/view/d4rl/home)\r\nImage Source: [https://sites.google.com/view/d4rl/home](https://sites.google.com/view/d4rl/home)", "variants": ["D4RL"]}
{"id": "Griddly", "title": "Griddly: A platform for AI research in games", "contents": "**Griddly** is an environment for grid-world based research.  Griddly provides a highly optimized game state and rendering engine with a flexible high-level interface for configuring environments. Not only does Griddly offer simple interfaces for single, multi-player and RTS games, but also multiple methods of rendering, configurable partial observability and interfaces for procedural content generation.\r\n\r\nSource: [https://griddly.readthedocs.io/en/latest/about/introduction.html](https://griddly.readthedocs.io/en/latest/about/introduction.html)\r\nImage Source: [https://griddly.readthedocs.io/en/latest/](https://griddly.readthedocs.io/en/latest/)", "variants": ["Griddly"]}
{"id": "SParC", "title": "SParC: Cross-Domain Semantic Parsing in Context", "contents": "**SParC** is a large-scale dataset for complex, cross-domain, and context-dependent (multi-turn) semantic parsing and text-to-SQL task (interactive natural language interfaces for relational databases).\r\n\r\nSource: [https://github.com/taoyds/sparc](https://github.com/taoyds/sparc)\r\nImage Source: [https://arxiv.org/pdf/1906.02285.pdf](https://arxiv.org/pdf/1906.02285.pdf)", "variants": ["SParC"]}
{"id": "Panlex", "title": "PanLex: Building a Resource for Panlingual Lexical Translation", "contents": "PanLex translates words in thousands of languages. Its database is panlingual (emphasizes coverage of every language) and lexical (focuses on words, not sentences).\r\n\r\nSource: [PANLEX](https://panlex.org/)\r\nImage Source: [http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf)", "variants": ["Panlex"]}
{"id": "EVALution", "title": "EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models", "contents": "**EVALution** dataset is evenly distributed among the three classes (hypernyms, co-hyponyms and random) and involves three types of parts of speech (noun, verb, adjective). The full dataset contains a total of 4,263 distinct terms consisting of 2,380 nouns, 958 verbs and 972 adjectives.\r\n\r\nSource: [Network Features Based Co-hyponymy Detection](https://arxiv.org/abs/1802.04609)\r\nImage Source: [https://www.aclweb.org/anthology/W15-4208.pdf](https://www.aclweb.org/anthology/W15-4208.pdf)", "variants": ["EVALution"]}
{"id": "RoboCup", "title": "Learning to sportscast: a test of grounded language acquisition", "contents": "**RoboCup** is an initiative in which research groups compete by enabling their robots to play football matches. Playing football requires solving several challenging tasks, such as vision, motion, and team coordination. Framing the research efforts onto football attracts public interest (and potential research funding) in robotics, which may otherwise be less entertaining to non-experts.\r\n\r\nSource: [Robots as Actors in a Film: No War, A Robot Story](https://arxiv.org/abs/1910.12294)\r\nImage Source: [https://www.robocup.org/](https://www.robocup.org/)", "variants": ["RoboCup"]}
{"id": "Social IQA", "title": "Social IQa: Commonsense Reasoning about Social Interactions", "contents": "**Social Interaction QA**, a new question-answering benchmark for testing social commonsense intelligence. Contrary to many prior benchmarks that focus on physical or taxonomic knowledge, Social IQa focuses on reasoning about people’s actions and their social implications. For example, given an action like \"Jesse saw a concert\" and a question like \"Why did Jesse do this?\", humans can easily infer that Jesse wanted \"to see their favorite performer\" or \"to enjoy the music\", and not \"to see what's happening inside\" or \"to see if it works\". The actions in Social IQa span a wide variety of social situations, and answer candidates contain both human-curated answers and adversarially-filtered machine-generated candidates. Social IQa contains over 37,000 QA pairs for evaluating models’ abilities to reason about the social implications of everyday events and situations.\r\n\r\nSource: [Social IQA](https://leaderboard.allenai.org/socialiqa/submissions/get-started)\r\nImage Source: [https://arxiv.org/pdf/1904.09728.pdf](https://arxiv.org/pdf/1904.09728.pdf)", "variants": ["Social IQA"]}
{"id": "WikiSum", "title": "", "contents": "**WikiSum** is a dataset based on English Wikipedia and suitable for a task of multi-document abstractive summarization. In each instance, the input is comprised of a Wikipedia topic (title of article) and a collection of non-Wikipedia reference documents, and the target is the Wikipedia article text. The dataset is restricted to the articles with at least one crawlable citation. The official split divides the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples respectively.\r\n\r\nSource: [Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/pdf/1801.10198.pdf)\r\nImage Source: [https://arxiv.org/pdf/1801.10198.pdf](https://arxiv.org/pdf/1801.10198.pdf)", "variants": ["WikiSum"]}
{"id": "EmotionLines", "title": "EmotionLines: An Emotion Corpus of Multi-Party Conversations", "contents": "**EmotionLines** contains a total of 29245 labeled utterances from 2000 dialogues. Each utterance in dialogues is labeled with one of seven emotions, six Ekman’s basic emotions plus the neutral emotion. Each labeling was accomplished by 5 workers, and for each utterance in a label, the emotion category with the highest votes was set as the label of the utterance. Those utterances voted as more than two different emotions were put into the non-neutral category. Therefore the dataset has a total of 8 types of emotion labels, anger, disgust, fear, happiness, sadness, surprise, neutral, and non-neutral.\r\n\r\nSource: [Bridging Dialogue Generation and Facial Expression Synthesis](https://arxiv.org/abs/1905.11240)\r\nImage Source: [https://arxiv.org/pdf/1802.08379.pdf](https://arxiv.org/pdf/1802.08379.pdf)", "variants": ["EmotionPush", "EmotionLines"]}
{"id": "Chinese Gigaword", "title": "LDC Catalog No.: LDC2003T09, ISBN", "contents": "**Chinese Gigaword** corpus consists of 2.2M of headline-document pairs of news stories covering over 284 months from two Chinese newspapers, namely the Xinhua News Agency of China (XIN) and the Central News Agency of Taiwan (CNA).\r\n\r\nSource: [Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification](https://arxiv.org/abs/1709.05475)\r\nImage Source: [https://catalog.ldc.upenn.edu/desc/addenda/LDC2011T13.jpg](https://catalog.ldc.upenn.edu/desc/addenda/LDC2011T13.jpg)", "variants": ["Chinese Gigaword"]}
{"id": "MuST-C", "title": "MuST-C: a Multilingual Speech Translation Corpus", "contents": "**MuST-C** currently represents the largest publicly available multilingual corpus (one-to-many) for speech translation. It covers eight language directions, from English to German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian. The corpus consists of audio, transcriptions and translations of English TED talks, and it comes with a predefined training, validation and test split.\r\n\r\nSource: [One-to-Many Multilingual End-to-End Speech Translation](https://arxiv.org/abs/1910.03320)\r\nImage Source: [https://ict.fbk.eu/must-c/](https://ict.fbk.eu/must-c/)", "variants": ["MuST-C"]}
{"id": "FakeNewsNet", "title": "", "contents": "**FakeNewsNet** is collected from two fact-checking websites: GossipCop and PolitiFact containing news contents with labels annotated by professional journalists and experts, along with social context information.\n\nSource: [Leveraging Multi-Source Weak Social Supervision for Early Detection of Fake News](https://arxiv.org/abs/2004.01732)\nImage Source: [https://arxiv.org/pdf/1809.01286.pdf](https://arxiv.org/pdf/1809.01286.pdf)", "variants": ["FakeNewsNet"]}
{"id": "STS 2014", "title": "SemEval-2014 Task 10: Multilingual Semantic Textual Similarity", "contents": "STS-2014 is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes.\r\n\r\nSource: [Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering](https://arxiv.org/abs/1806.04330)\r\nImage Source: [https://www.aclweb.org/anthology/S14-2010.pdf](https://www.aclweb.org/anthology/S14-2010.pdf)", "variants": ["STS 2014"]}
{"id": "ASPEC", "title": "ASPEC: Asian Scientific Paper Excerpt Corpus", "contents": "**ASPEC**, Asian Scientific Paper Excerpt Corpus, is constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT). It consists of a Japanese-English paper abstract corpus of 3M parallel sentences (ASPEC-JE) and a Japanese-Chinese paper excerpt corpus of 680K parallel sentences (ASPEC-JC). This corpus is one of the achievements of the Japanese-Chinese machine translation project which was run in Japan from 2006 to 2010.\r\n\r\nSource: [ASPEC](http://lotus.kuee.kyoto-u.ac.jp/ASPEC/)\r\nImage Source: [https://www.aclweb.org/anthology/L16-1350.pdf](https://www.aclweb.org/anthology/L16-1350.pdf)", "variants": ["ASPEC"]}
{"id": "OMICS", "title": "Common Sense Data Acquisition for Indoor Mobile Robots", "contents": "**OMICS** is an extensive collection of knowledge for indoor service robots gathered from internet users. Currently, it contains 48 tables capturing different sorts of knowledge. Each tuple of the Help table maps a user desire to a task that may meet the desire (e.g., ⟨ “feel thirsty”, “by offering drink” ⟩). Each tuple of the Tasks/Steps table decomposes a task into several steps (e.g., ⟨ “serve a drink”, 0. “get a glass”, 1. “get a bottle”, 2. “fill class from bottle”, 3. “give class to person” ⟩). Given this, OMICS offers useful knowledge about hierarchism of naturalistic instructions, where a high-level user request (e.g., “serve a drink”) can be reduced to lower-level tasks (e.g., “get a glass”, ⋯). Another feature of OMICS is that elements of any tuple in an OMICS table are semantically related according to a predefined template. This facilitates the semantic interpretation of the OMICS tuples.\n\nSource: [Understanding User Instructions by Utilizing Open Knowledge for Service Robots](https://arxiv.org/abs/1606.02877)\nImage Source: [https://www.aaai.org/Papers/AAAI/2004/AAAI04-096.pdf](https://www.aaai.org/Papers/AAAI/2004/AAAI04-096.pdf)", "variants": ["OMICS"]}
{"id": "ISEAR", "title": "Evidence for universality and cultural variation of differential emotion response patterning", "contents": "Over a period of many years during the 1990s, a large group of psychologists all over the world collected data in the **ISEAR** project, directed by Klaus R. Scherer and Harald Wallbott. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of 7 major emotions (joy, fear, anger, sadness, disgust, shame, and guilt). In each case, the questions covered the way they had appraised the situation and how they reacted. The final data set thus contained reports on seven emotions each by close to 3000 respondents in 37 countries on all 5 continents.\r\n\r\nSource: [https://www.unige.ch/cisa/research/materials-and-online-research/research-material/](https://www.unige.ch/cisa/research/materials-and-online-research/research-material/)\r\nImage Source: [https://www.unige.ch/cisa/research/materials-and-online-research/research-material/](https://www.unige.ch/cisa/research/materials-and-online-research/research-material/)", "variants": ["ISEAR"]}
{"id": "CMRC", "title": "", "contents": "CMRC is a dataset is annotated by human experts with near 20,000 questions as well as a challenging set which is composed of the questions that need reasoning over multiple clues.\r\n\r\nSource: [A Span-Extraction Dataset for Chinese Machine Reading Comprehension](https://www.aclweb.org/anthology/D19-1600.pdf)\r\nImage Source: [https://www.aclweb.org/anthology/D19-1600.pdf](https://www.aclweb.org/anthology/D19-1600.pdf)", "variants": ["CMRC 2018 (Simplified Chinese)", "CMRC"]}
{"id": "NSIDES", "title": "Data-driven prediction of drug effects and interactions", "contents": "Drug side effects and drug-drug interactions were mined from publicly available data. Offsides is a database of drug side-effects that were found, but are not listed on the official FDA label. Twosides is the only comprehensive database drug-drug-effect relationships. Over 3,300 drugs and 63,000 combinations connected to millions of potential adverse reactions.\n\nSource: [http://tatonettilab.org/offsides/](http://tatonettilab.org/offsides/)\nImage Source: [http://doi.org/10.1126/scitranslmed.3003377](http://doi.org/10.1126/scitranslmed.3003377)", "variants": ["NSIDES"]}
{"id": "DDI", "title": "Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013)", "contents": "The **DDI**Extraction 2013 task relies on the DDI corpus which contains MedLine abstracts on drug-drug interactions as well as documents describing drug-drug interactions from the DrugBank database.\r\n\r\nSource: [DDIExtraction 2013](https://www.cs.york.ac.uk/semeval-2013/task9/)\r\nImage Source: [https://www.aclweb.org/anthology/S13-2056.pdf](https://www.aclweb.org/anthology/S13-2056.pdf)", "variants": ["DDI extraction 2013 corpus", "DDI"]}
{"id": "CRIM13", "title": "", "contents": "The Caltech Resident-Intruder Mouse dataset (**CRIM13**) consists of 237x2 videos (recorded with synchronized top and side view) of pairs of mice engaging in social behavior, catalogued into thirteen different actions. Each video lasts ~10min, for a total of 88 hours of video and 8 million frames. A team of behavior experts annotated each video on a frame-by-frame basis for a state-of-the-art study of the neurophysiological mechanisms involved in aggression and courtship in mice.\n\nSource: [https://pdollar.github.io/research.html](https://pdollar.github.io/research.html)\nImage Source: [https://authors.library.caltech.edu/104600/1/2020.07.26.222299v1.full.pdf](https://authors.library.caltech.edu/104600/1/2020.07.26.222299v1.full.pdf)", "variants": ["CRIM13"]}
{"id": "Imagewoof", "title": "", "contents": "**Imagewoof** is a subset of 10 dog breed classes from Imagenet. The breeds are: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog.\n\nSource: [https://github.com/fastai/imagenette](https://github.com/fastai/imagenette)\nImage Source: [https://medium.com/@lessw/how-we-beat-the-fastai-leaderboard-score-by-19-77-a-cbb2338fab5c](https://medium.com/@lessw/how-we-beat-the-fastai-leaderboard-score-by-19-77-a-cbb2338fab5c)", "variants": ["Imagewoof"]}
{"id": "Imagenette", "title": "", "contents": "**Imagenette** is a subset of 10 easily classified classes from Imagenet (bench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).\n\nSource: [https://github.com/fastai/imagenette](https://github.com/fastai/imagenette)\nImage Source: [https://docs.fast.ai/tutorial.imagenette.html](https://docs.fast.ai/tutorial.imagenette.html)", "variants": ["Imagenette"]}
{"id": "Stanford-ECM", "title": "Jointly Learning Energy Expenditures and Activities Using Egocentric Multimodal Signals", "contents": "**Stanford-ECM** is an egocentric multimodal dataset which comprises about 27 hours of egocentric video augmented with heart rate and acceleration data. The lengths of the individual videos cover a diverse range from 3 minutes to about 51 minutes in length. A mobile phone was used to collect egocentric video at 720x1280 resolution and 30 fps, as well as triaxial acceleration at 30Hz. The mobile phone was equipped with a wide-angle lens, so that the horizontal field of view was enlarged from 45 degrees to about 64 degrees. A wrist-worn heart rate sensor was used to capture the heart rate every 5 seconds. The phone and heart rate monitor was time-synchronized through Bluetooth, and all data was stored in the phone’s storage. Piecewise cubic polynomial interpolation was used to fill in any gaps in heart rate data. Finally, data was aligned to the millisecond level at 30 Hz.\n\nSource: [http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html](http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html)\nImage Source: [http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html](http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html)", "variants": ["Stanford-ECM"]}
{"id": "BSD", "title": "A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics", "contents": "**BSD** is a dataset used frequently for image denoising and super-resolution. Of the subdatasets, BSD100 is aclassical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. BSD100 is the testing set of the Berkeley segmentation dataset BSD300.\r\n\r\nSource: [A Deep Journey into Super-resolution: A Survey](https://arxiv.org/abs/1904.07523)\r\nImage Source: [https://www.slideshare.net/jbhuang/single-image-super-resolution-from-transformed-selfexemplars-cvpr-2015](https://www.slideshare.net/jbhuang/single-image-super-resolution-from-transformed-selfexemplars-cvpr-2015)", "variants": ["BSD100 - 16x upscaling", "BSD100 - 2x upscaling", "BSD100 - 3x upscaling", "BSD100 - 4x upscaling", "BSD100 - 8x upscaling", "BSD300 Noise Level 30%", "BSD300 Noise Level 50%", "BSD300 Noise Level 70%", "BSD300 sigma30", "BSD300 sigma50", "BSD300 sigma70", "BSD", "BSD200 - 2x upscaling", "BSD200 sigma10", "BSD200 sigma30", "BSD200 sigma50", "BSD200 sigma70", "BSD68 sigma10", "BSD68 sigma15", "BSD68 sigma25", "BSD68 sigma30", "BSD68 sigma35", "BSD68 sigma5", "BSD68 sigma50", "BSD68 sigma70", "BSD68 sigma75", "BSDS100 - 2x upscaling", "BSDS100 - 4x upscaling", "BSDS100 - 8x upscaling", "BSDS300", "BSD68 CS=50%", "BSD68 sigma20", "BSD68 sigma40", "BSD68 sigma45", "BSD68 sigma55", "BSD68 sigma60", "BSD68 sigma65"]}
{"id": "MSRA Hand", "title": "Realtime and Robust Hand Tracking from Depth", "contents": "**MSRA Hand**s is a dataset for hand tracking. In total 6 subjects' right hands are captured using Intel's Creative Interactive Gesture Camera. Each subject is asked to make various rapid gestures in a 400-frame video sequence. To account for different hand sizes, a global hand model scale is specified for each subject: 1.1, 1.0, 0.9, 0.95, 1.1, 1.0 for subject 1~6, respectively.\nThe camera intrinsic parameters are: principle point = image center(160, 120), focal length = 241.42. The depth image is 320x240, each *.bin file stores the depth pixel values in row scanning order, which are 320*240 floats. The unit is millimeters. The bin file is binary and needs to be opened with std::ios::binary flag.\njoint.txt file stores 400 frames x 21 hand joints per frame. Each line has 3 * 21 = 63 floats for 21 3D points in (x, y, z) coordinates. The 21 hand joints are: wrist, index_mcp, index_pip, index_dip, index_tip, middle_mcp, middle_pip, middle_dip, middle_tip, ring_mcp, ring_pip, ring_dip, ring_tip, little_mcp, little_pip, little_dip, little_tip, thumb_mcp, thumb_pip, thumb_dip, thumb_tip.\nThe corresponding *.jpg file is just for visualization of depth and ground truth joints.\n\nSource: [https://jimmysuen.github.io/txt/cvpr14_MSRAHandTrackingDB_readme.txt](https://jimmysuen.github.io/txt/cvpr14_MSRAHandTrackingDB_readme.txt)\nImage Source: [https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf)", "variants": ["MSRA Hands", "MSRA Hand"]}
{"id": "MSRA10K", "title": "", "contents": "**MSRA10K** is a dataset for salient object detection that contains 10,000 images with pixel-level saliency labeling for 10K images from the MSRA salient object detection dataset. The original MRSA database provides salient object annotation in terms of bounding boxes provided by 3-9 users.\n\nSource: [https://mmcheng.net/msra10k/](https://mmcheng.net/msra10k/)\nImage Source: [https://mmcheng.net/msra10k/](https://mmcheng.net/msra10k/)", "variants": ["MSRA10K"]}
{"id": "ARC", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge", "contents": "The AI2’s Reasoning Challenge (**ARC**) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ARC includes a supporting KB of 14.3M unstructured text passages.\r\n\r\nSource: [Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering](https://arxiv.org/abs/1911.07176)\r\nImage Source: [https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457)", "variants": ["ARC (Easy)", "ARC (Challenge)", "ARC"]}
{"id": "Color FERET", "title": "The FERET Evaluation Methodology for Face-Recognition Algorithms", "contents": "The color FERET database is a dataset for face recognition. It contains 11,338 color images of size 512×768 pixels captured in a semi-controlled environment with 13 different poses from 994 subjects.\r\n\r\nSource: [A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition](https://arxiv.org/abs/1606.02894)\r\nImage Source: [https://www.researchgate.net/figure/Sample-results-taken-from-Color-FERET-data-set-testing-using-LBP-algorithm_fig4_308836179](https://www.researchgate.net/figure/Sample-results-taken-from-Color-FERET-data-set-testing-using-LBP-algorithm_fig4_308836179)", "variants": ["Color FERET (Online Open Set)", "Color FERET"]}
{"id": "Replay-Mobile", "title": "The Replay-Mobile Face Presentation-Attack Database", "contents": "The **Replay-Mobile** Database for face spoofing consists of 1190 video clips of photo and video attack attempts to 40 clients, under different lighting conditions. These videos were recorded with current devices from the market -- an iPad Mini2 (running iOS) and a LG-G4 smartphone (running Android). This Database was produced at the Idiap Research Institute (Switzerland) within the framework of collaboration with Galician Research and Development Center in Advanced Telecommunications - Gradiant (Spain).\r\n\r\nSource: [Replay-Mobile](https://www.idiap.ch/dataset/replay-mobile)\r\nImage Source: [https://core.ac.uk/download/pdf/148024307.pdf](https://core.ac.uk/download/pdf/148024307.pdf)", "variants": ["Replay Mobile", "Replay-Mobile"]}
{"id": "Recipe1M+", "title": "Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images", "contents": "**Recipe1M+** is a dataset which contains one million structured cooking recipes with 13M associated images.\r\n\r\nSource: [Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images](http://im2recipe.csail.mit.edu/)\r\nImage Source: [http://im2recipe.csail.mit.edu/](http://im2recipe.csail.mit.edu/)", "variants": ["Recipe1M", "Recipe1M+"]}
{"id": "ICDAR 2003", "title": "ICDAR 2003 Robust Reading Competitions", "contents": "The ICDAR2003 dataset is a dataset for scene text recognition. It contains 507 natural scene images (including 258 training images and 249 test images) in total. The images are annotated at character level. Characters and words can be cropped from the images.\r\n\r\nSource: [Robust Scene Text Recognition Using Sparse Coding based Features](https://arxiv.org/abs/1512.08669)\r\nImage Source: [https://www.researchgate.net/figure/The-results-of-text-localization-and-extraction-on-ICDAR-2003-dataset_fig3_290070044](https://www.researchgate.net/figure/The-results-of-text-localization-and-extraction-on-ICDAR-2003-dataset_fig3_290070044)", "variants": ["ICDAR 2003"]}
{"id": "CASIA-FASD", "title": "A face antispoofing database with diverse attacks", "contents": "**CASIA-FASD** is a small face anti-spoofing dataset  containing 50 subjects.\r\n\r\nSource: [Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing](https://arxiv.org/abs/1901.05602)\r\nImage Source: [https://arxiv.org/abs/1511.06316](https://arxiv.org/abs/1511.06316)", "variants": ["CASIA-FASD"]}
{"id": "TAC 2010", "title": "", "contents": "**TAC 2010** is a dataset for summarization that consists of 44 topics, each of which is associated with a set of 10 documents. The test dataset is composed of approximately 44 topics, divided into five categories: Accidents and Natural Disasters, Attacks, Health and Safety, Endangered Resources, Investigations and Trials.\n\nSource: [Better Summarization Evaluation with Word Embeddings for ROUGE](https://arxiv.org/abs/1508.06034)\nImage Source: [https://tac.nist.gov//2010/Summarization/Guided-Summ.2010.guidelines.html](https://tac.nist.gov//2010/Summarization/Guided-Summ.2010.guidelines.html)", "variants": ["TAC2010", "TAC 2010"]}
{"id": "DUC 2005", "title": "", "contents": "The **DUC 2005** data set is a dataset for summarization which consists of 50 document collections of 25 documents each; each document collection includes a human-written query. Each document collection additionally has five human-written “reference” summaries (250 words long, each) that serve as the gold standard\n\nSource: [Search-based Structured Prediction](https://arxiv.org/abs/0907.0786)\nImage Source: [https://duc.nist.gov/duc2005/tasks.html](https://duc.nist.gov/duc2005/tasks.html)", "variants": ["DUC 2005"]}
{"id": "NIST SD 19", "title": "", "contents": "NIST Special Database 19 contains NIST's entire corpus of training materials for handprinted document and character recognition. It publishes Handprinted Sample Forms from 3600 writers, 810,000 character images isolated from their forms, ground truth classifications for those images, reference forms for further data collection, and software utilities for image management and handling.\n\nSource: [https://www.nist.gov/srd/nist-special-database-19](https://www.nist.gov/srd/nist-special-database-19)\nImage Source: [https://www.nist.gov/srd/nist-special-database-19](https://www.nist.gov/srd/nist-special-database-19)", "variants": ["NIST SD 19"]}
{"id": "PRImA", "title": "", "contents": "The Prima head pose dataset consists of 2790 images of 15 persons recorded twice. Pitch values lie in the interval [−60∘,60∘], and yaw values lie in the interval [−90∘,90∘] with a 15∘ step. Thus, there are 93 poses available for each person. All the recordings were achieved with the same background. One interesting feature of this dataset is the pose space is uniformly sampled. The dataset is annotated such that a face bounding box (manually annotated) and the corresponding yaw and pitch angle values are provided for each sample.\n\nSource: [Robust Head-Pose Estimation Based on Partially-Latent Mixture of Linear Regressions](https://arxiv.org/abs/1603.09732)\nImage Source: [http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html](http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html)", "variants": ["PRImA"]}
{"id": "TAU Urban Acoustic Scenes 2019", "title": "A multi-device dataset for urban acoustic scene classification", "contents": "**TAU Urban Acoustic Scenes 2019** development dataset consists of 10-seconds audio segments from 10 acoustic scenes: airport, indoor shopping mall, metro station, pedestrian street, public square, street with medium level of traffic, travelling by a tram, travelling by a bus, travelling by an underground metro and urban park. Each acoustic scene has 1440 segments (240 minutes of audio). The dataset contains in total 40 hours of audio.\n\nSource: [https://zenodo.org/record/2589280](https://zenodo.org/record/2589280)\nImage Source: [http://dcase.community/challenge2019/task-acoustic-scene-classification#citation](http://dcase.community/challenge2019/task-acoustic-scene-classification#citation)", "variants": ["TAU Urban Acoustic Scenes 2019"]}
{"id": "TAU Spatial Sound Events 2019 - Ambisonic", "title": "", "contents": "The **TAU Spatial Sound Events 2019 - Ambisonic** dataset contains recordings from a scene (along with the Microphone Array sister dataset). It provides four-channel First-Order Ambisonic (FOA) recordings. The recordings consist of stationary point sources from multiple sound classes each associated with a temporal onset and offset time, and DOA coordinate represented using azimuth and elevation angle.\nThe development set consists of 400, one minute long recordings sampled at 48000 Hz, and divided into four cross-validation splits of 100 recordings each. These recordings were synthesized using spatial room impulse response (IRs) collected from five indoor locations, at 504 unique combinations of azimuth-elevation-distance. Furthermore, in order to synthesize the recordings, the collected IRs were convolved with isolated sound events dataset from DCASE 2016 task 2. Finally, to create a realistic sound scene recording, natural ambient noise collected in the IR recording locations was added to the synthesized recordings such that the average SNR of the sound events was 30 dB.\n\nSource: [https://zenodo.org/record/2580091](https://zenodo.org/record/2580091)\nImage Source: [http://dcase.community/challenge2019/task-sound-event-localization-and-detection#audio-dataset](http://dcase.community/challenge2019/task-sound-event-localization-and-detection#audio-dataset)", "variants": ["TAU Spatial Sound Events 2019 - Ambisonic"]}
{"id": "TAU Spatial Sound Events 2019 – Microphone Array", "title": "", "contents": "The **TAU Spatial Sound Events 2019 – Microphone Array** dataset contains recordings from a scene (along with the Ambisonic sister dataset). It provides four-channel directional microphone recordings from a tetrahedral array configuration. The recordings consist of stationary point sources from multiple sound classes each associated with a temporal onset and offset time, and DOA coordinate represented using azimuth and elevation angle.\nThe development set consists of 400, one minute long recordings sampled at 48000 Hz, and divided into four cross-validation splits of 100 recordings each. These recordings were synthesized using spatial room impulse response (IRs) collected from five indoor locations, at 504 unique combinations of azimuth-elevation-distance. Furthermore, in order to synthesize the recordings, the collected IRs were convolved with isolated sound events dataset from DCASE 2016 task 2. Finally, to create a realistic sound scene recording, natural ambient noise collected in the IR recording locations was added to the synthesized recordings such that the average SNR of the sound events was 30 dB.\n\nSource: [https://zenodo.org/record/2580091](https://zenodo.org/record/2580091)\nImage Source: [http://dcase.community/challenge2019/task-sound-event-localization-and-detection#audio-dataset](http://dcase.community/challenge2019/task-sound-event-localization-and-detection#audio-dataset)", "variants": ["TAU Spatial Sound Events 2019 – Microphone Array"]}
{"id": "KTH Multiview Football II", "title": "", "contents": "KTI Multiview Football II consists of images of professional footballers during a match of the Allsvenskan league. It consists of two parts: one with ground truth pose in 2D and one with ground truth pose in both 2D and 3D. The 3D dataset has 800 time frames, captured from 3 views (2400 images). Views are calibrated and synchronized. 3D ground truth pose and orthographic camera matrices are provided for each frame. There are 14 annotated joints. Lastly, there are 2 different players and two sequences per player.\n\nSource: [http://www.csc.kth.se/~vahidk/football_data.html](http://www.csc.kth.se/~vahidk/football_data.html)\nImage Source: [http://www.csc.kth.se/cvap/cvg/?page=footballdataset2](http://www.csc.kth.se/cvap/cvg/?page=footballdataset2)", "variants": ["KTH Multiview Football II"]}
{"id": "KTH Multiview Football I", "title": "", "contents": "KTI Multiview Football I is a dataset of football players with annotated joints that can be used for multi-view reconstruction. The dataset includes 771 images of football players, images taken from 3 views at 257 time instances, and 14 annotated body joints.\n\nSource: [http://www.csc.kth.se/~vahidk/football_data.html](http://www.csc.kth.se/~vahidk/football_data.html)\nImage Source: [http://www.csc.kth.se/~vahidk/football_data.html](http://www.csc.kth.se/~vahidk/football_data.html)", "variants": ["KTH Multiview Football I"]}
{"id": "ICL-NUIM", "title": "", "contents": "The **ICL-NUIM** dataset aims at benchmarking RGB-D, Visual Odometry and SLAM algorithms. Two different scenes (the living room and the office room scene) are provided with ground truth. Living room has 3D surface ground truth together with the depth-maps as well as camera poses and as a result perfectly suits not just for benchmarking camera trajectory but also reconstruction. Office room scene comes with only trajectory data and does not have any explicit 3D model with it.\n\nAll data is compatible with the evaluation tools available for the TUM RGB-D dataset, and if your system can take TUM RGB-D format PNGs as input, the authors’ TUM RGB-D Compatible data will also work (given the correct camera parameters).\n\nSource: [https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html](https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html)\nImage Source: [https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html](https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html)", "variants": ["ICL-NUIM"]}
{"id": "EuRoC MAV", "title": "", "contents": "**EuRoC MAV** is a visual-inertial datasets collected on-board a Micro Aerial Vehicle (MAV). The dataset contains stereo images, synchronized IMU measurements, and accurate motion and structure ground-truth. The datasets facilitates the design and evaluation of visual-inertial localization algorithms on real flight data\n\nSource: [https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets](https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets)\nImage Source: [https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets](https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets)", "variants": ["EuRoC MAV"]}
{"id": "Sugar Beets 2016", "title": "", "contents": "**Sugar Beets 2016** is a robot dataset for plant classification as well as localization and mapping that covers the relevant stages for robotic intervention and weed control. It contains around 5TB of data recorded from a robot with a 4-channel multi-spectral camera and a RGB-D sensor to capture detailed information about the plantation.\n\nSource: [https://www.ipb.uni-bonn.de/data/sugarbeets2016/](https://www.ipb.uni-bonn.de/data/sugarbeets2016/)\nImage Source: [https://www.ipb.uni-bonn.de/data/sugarbeets2016/](https://www.ipb.uni-bonn.de/data/sugarbeets2016/)", "variants": ["Sugar Beets 2016"]}
{"id": "HDM05", "title": "", "contents": "**HDM05** is a MoCap (motion capture) dataset. It contains more than three hours of systematically recorded and well-documented motion capture data in the C3D as well as in the ASF/AMC data format. HDM05 contains almost 2337 sequences with 130 motion classes performed by 5 different actors.\n\nSource: [http://resources.mpi-inf.mpg.de/HDM05/](http://resources.mpi-inf.mpg.de/HDM05/)\nImage Source: [https://arxiv.org/pdf/1908.05750.pdf](https://arxiv.org/pdf/1908.05750.pdf)", "variants": ["HDM05"]}
{"id": "USYD CAMPUS", "title": "", "contents": "**USYD CAMPUS** is a driving dataset collected by Zhou et al at the University of Sydney (USyd) campus and surroundings. This USYD Campus Dataset contains more than 60 weeks of drives and is continuously updated. It includes multiple sensor modalities (camera, lidar, GPS, IMU, wheel encoder, steering angle, etc.) and covers various environmental conditions as well as diverse changes to illumination, scene structure, and pedestrian/vehicle traffic volumes.\n\nSource: [http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/](http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/)\nImage Source: [http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/](http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/)", "variants": ["USYD CAMPUS"]}
{"id": "TRECVID", "title": "", "contents": "**TRECVID** is a yearly set of competitions centered on video retrieval and indexing, hosting a variety of video data sets.\n\nSource: [YouTube-BoundingBoxes: A Large High-PrecisionHuman-Annotated Data Set for Object Detection in Video](https://arxiv.org/abs/1702.00824)\nImage Source: [https://www-nlpir.nist.gov/projects/tv2016/tv2016.html](https://www-nlpir.nist.gov/projects/tv2016/tv2016.html)", "variants": ["TRECVID"]}
{"id": "Musk v1", "title": "", "contents": "The Musk dataset describes a set of molecules, and the objective is to detect musks from non-musks. This dataset describes a set of 92 molecules of which 47 are judged by human experts to be musks and the remaining 45 molecules are judged to be non-musks. There are 166 features available that describe the molecules based on the shape of the molecule.\n\nSource: [Estimation of Dimensions Contributing to Detected Anomalies with Variational Autoencoders](https://arxiv.org/abs/1811.04576)", "variants": ["Musk v1"]}
{"id": "Musk v2", "title": "Solving the multiple instance problem with axis-parallel rectangles", "contents": "The Musk2 dataset is a set of 102 molecules of which 39 are judged by human experts to be musks and the remaining 63 molecules are judged to be non-musks. Each instance corresponds to a possible configuration of a molecule. The 166 features that describe these molecules depend upon the exact shape, or conformation, of the molecule.\n\nSource: [Confidence-Constrained Maximum Entropy Framework for Learning from Multi-Instance Data](https://arxiv.org/abs/1603.01901)", "variants": ["Musk v2"]}
{"id": "NYU Hand", "title": "", "contents": "The **NYU Hand** pose dataset contains 8252 test-set and 72757 training-set frames of captured RGBD data with ground-truth hand-pose information. For each frame, the RGBD data from 3 Kinects is provided: a frontal view and 2 side views. The training set contains samples from a single user only (Jonathan Tompson), while the test set contains samples from two users (Murphy Stein and Jonathan Tompson). A synthetic re-creation (rendering) of the hand pose is also provided for each view.\n\nSource: [https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm](https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm)\nImage Source: [https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm](https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm)", "variants": ["NYU Hands", "NYU Hand"]}
{"id": "Friedman1", "title": "Multivariate adaptive regression splines", "contents": "The friedman1 data set is commonly used to test semi-supervised regression methods.\r\n\r\nSource: [http://search.r-project.org/library/ssr/html/friedman1.html](http://search.r-project.org/library/ssr/html/friedman1.html)", "variants": ["Friedman1"]}
{"id": "DCASE 2016", "title": "", "contents": "**DCASE 2016** is a dataset for sound event detection. It consists of 20 short mono sound files for each of 11 sound classes (from office environments, like clearthroat, drawer, or keyboard), each file containing one sound event instance. Sound files are annotated with event on- and offset times, however silences between actual physical sounds (like with a phone ringing) are not marked and hence “included” in the event.\r\n\r\nSource: [The NIGENS General Sound Events Database](https://arxiv.org/abs/1902.08314)\r\nImage Source: [https://arxiv.org/pdf/1911.06878.pdf](https://arxiv.org/pdf/1911.06878.pdf)", "variants": ["DCASE 2016"]}
{"id": "DSTC7 Task 1", "title": "", "contents": "The **DSTC7 Task 1** dataset is a dataset and task for goal-oriented dialogue. The data originates from human-human conversations, which is built from online resources, specifically the Ubuntu Internet Relay Chat (IRC) channel and an Advising dataset from the University of Michigan.\r\n\r\nSource: [Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems](https://arxiv.org/abs/1907.01166)\r\nImage Source: [https://www.aclweb.org/anthology/W19-4107.pdf](https://www.aclweb.org/anthology/W19-4107.pdf)", "variants": ["DSTC7 Ubuntu", "DSTC7 Task 1"]}
{"id": "DSTC7 Task 2", "title": "", "contents": "DSTC Task 2 is a dataset and task for end-to-end conversation modeling. The goal is to generate conversational responses that go beyond trivial chitchat by injecting informative responses that are grounded in external knowledge. The data consists of conversational data from Reddit, and contextually-relevant “facts” taken from the website that started the Reddit conversation. That is the setup is grounded, as each conversation in the data is about a specific web page that was linked at the start of the conversation.\n\nSource: [http://workshop.colips.org/dstc7/](http://workshop.colips.org/dstc7/)\nImage Source: [http://workshop.colips.org/dstc7/](http://workshop.colips.org/dstc7/)", "variants": ["DSTC7 Task 2"]}
{"id": "Music21", "title": "Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data", "contents": "**Music21** is an untrimmed video dataset crawled by keyword query from Youtube. It contains music performances belonging to 21 categories. This dataset is relatively clean and collected for the purpose of training and evaluating visual sound source separation models.\r\n\r\nSource: [Music Gesture for Visual Sound Separation](https://arxiv.org/abs/2004.09476)\r\nImage Source: [https://towardsdatascience.com/midi-music-data-extraction-using-music21-and-word2vec-on-kaggle-cb383261cd4e](https://towardsdatascience.com/midi-music-data-extraction-using-music21-and-word2vec-on-kaggle-cb383261cd4e)", "variants": ["Music21"]}
{"id": "RWC", "title": "RWC Music Database: Popular, Classical and Jazz Music Databases", "contents": "The **RWC** (Real World Computing) Music Database is a copyright-cleared music database (DB) that is available to researchers as a common foundation for research. It contains around 100 complete songs with manually labeled section boundaries. For the 50 instruments, individual sounds at half-tone intervals were captured with several variations of playing styles, dynamics, instrument manufacturers and musicians.\r\n\r\nSource: [https://staff.aist.go.jp/m.goto/RWC-MDB/](https://staff.aist.go.jp/m.goto/RWC-MDB/)\r\nImage Source: [http://www.cs.tut.fi/sgn/arg/matti/demos/basstrans/](http://www.cs.tut.fi/sgn/arg/matti/demos/basstrans/)", "variants": ["RWC"]}
{"id": "DCASE 2013", "title": "", "contents": "**DCASE 2013** is a dataset for sound event detection. It consists of audio-only recordings where individual sound events are prominent in an acoustic scene.\n\nSource: [http://dcase.community/challenge2013/index](http://dcase.community/challenge2013/index)\nImage Source: [https://link.springer.com/article/10.1186/s13636-018-0138-4](https://link.springer.com/article/10.1186/s13636-018-0138-4)", "variants": ["DCASE 2013"]}
{"id": "TUT Acoustic Scenes 2017", "title": "", "contents": "The **TUT Acoustic Scenes 2017** dataset is a collection of recordings from various acoustic scenes all from distinct locations. For each recording location 3-5 minute long audio recordings are captured and are split into 10 seconds which act as unit of sample for this task. All the audio clips are recorded with 44.1 kHz sampling rate and 24 bit resolution.\n\nSource: [Ensemble of deep neural networks for acoustic scene classification](https://arxiv.org/abs/1708.05826)\nImage Source: [https://www.mathworks.com/help/audio/ug/acoustic-scene-recognition-using-late-fusion.html;jsessionid=95c969bc690c06fe42a7ed17f57e](https://www.mathworks.com/help/audio/ug/acoustic-scene-recognition-using-late-fusion.html;jsessionid=95c969bc690c06fe42a7ed17f57e)", "variants": ["TUT Acoustic Scenes 2017"]}
{"id": "Bach Chorales", "title": "", "contents": "Bach chorales is a univariate time series based on chorales, where the task is to learn generative grammar. The dataset consists of single-line melodies of 100 Bach chorales (originally 4 voices). The melody line can be studied independently of other voices. The grand challenge is to learn a generative grammar for stylistically valid chorales.\n\nSource: [https://archive.ics.uci.edu/ml/datasets/Bach+Chorales](https://archive.ics.uci.edu/ml/datasets/Bach+Chorales)\nImage Source: [https://arxiv.org/pdf/1612.01010.pdf](https://arxiv.org/pdf/1612.01010.pdf)", "variants": ["Bach Chorales"]}
{"id": "BirdVox-full-night", "title": "Birdvox-Full-Night: A Dataset and Benchmark for Avian Flight Call Detection", "contents": "The **BirdVox-full-night** dataset contains 6 audio recordings, each about ten hours in duration. These recordings come from ROBIN autonomous recording units, placed near Ithaca, NY, USA during the fall 2015. They were captured on the night of September 23rd, 2015, by six different sensors, originally numbered 1, 2, 3, 5, 7, and 10.\nAndrew Farnsworth used the Raven software to pinpoint every avian flight call in time and frequency. He found 35402 flight calls in total. He estimates that about 25 different species of passerines (thrushes, warblers, and sparrows) are present in this recording. Species are not labeled in BirdVox-full-night, but it is possible to tell apart thrushes from warblers and sparrrows by looking at the center frequencies of their calls. The annotation process took 102 hours.\n\nSource: [https://wp.nyu.edu/birdvox/birdvox-full-night/](https://wp.nyu.edu/birdvox/birdvox-full-night/)\nImage Source: [https://wp.nyu.edu/birdvox/birdvox-full-night/](https://wp.nyu.edu/birdvox/birdvox-full-night/)", "variants": ["BirdVox-full-night"]}
{"id": "POP909", "title": "POP909: A Pop-song Dataset for Music Arrangement Generation", "contents": "**POP909** is a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, annotations are provided of tempo, beat, key, and chords, where the tempo curves are hand-labelled and others are done by MIR algorithms.\n\nSource: [https://arxiv.org/pdf/2008.07142.pdf](https://arxiv.org/pdf/2008.07142.pdf)\nImage Source: [https://arxiv.org/pdf/2008.07142.pdf](https://arxiv.org/pdf/2008.07142.pdf)", "variants": ["POP909"]}
{"id": "SINS", "title": "", "contents": "**SINS** is a database of continuous real-life audio recordings in a home environment. The home is a vacation home and one person lived there during the recording period of over on week. It was collected using a network of 13 microphone arrays distributed over the multiple rooms. Each microphone array consisted of 4 linearly arranged microphones. Recordings were annotated based on the level of daily activities performed in the environment.\n\nSource: [https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Dekkers_141.pdf](https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Dekkers_141.pdf)\nImage Source: [https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Dekkers_141.pdf](https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Dekkers_141.pdf)", "variants": ["SINS"]}
{"id": "Robbie Williams", "title": "", "contents": "**Robbie Williams** is a dataset of 65 songs by Robbie Williams. It consists of chords, keys and beats. The dataset does not include audio.\n\nSource: [A BI-DIRECTIONAL TRANSFORMER FOR MUSICAL CHORD RECOGNITION](https://arxiv.org/abs/1907.02698)\nImage Source: [https://www.rwdb.info/](https://www.rwdb.info/)", "variants": ["Robbie Williams"]}
{"id": "URBAN-SED", "title": "", "contents": "**URBAN-SED** is a dataset of 10,000 soundscapes with sound event annotations generated using the scraper library. The dataset includes 10,000 soundscapes, totals almost 30 hours and includes close to 50,000 annotated sound events. Every soundscape is 10 seconds long and has a background of Brownian noise resembling the typical “hum” often heard in urban environments. Every soundscape contains between 1-9 sound evnts from the following classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren and street_music.\nThe source material for the sound events are the clips from the UrbanSound8K dataset. URBAN-SED comes pre-sorted into three sets: train, validate and test. There are 6000 soundscapes in the training set, generated using clips from folds 1-6 in UrbanSound8K, 2000 soundscapes in the validation set, generated using clips from fold 7-8 in UrbanSound8K, and 2000 soundscapes in the test set, generated using clips from folds 9-10 in UrbanSound8K.\n\nSource: [http://urbansed.weebly.com/](http://urbansed.weebly.com/)\nImage Source: [http://urbansed.weebly.com/](http://urbansed.weebly.com/)", "variants": ["URBAN-SED"]}
{"id": "ISMIR Genre", "title": "", "contents": "ISMIR2004 is an audio dataset consisting of 6 genres with 729 excerpts of 30 seconds. It is a dataset used for musical genre classification. The training set consists of 320 classical music samples, 115 electronic music samples, 26 jazz blues samples, 45 metal/punk samples, 101 rock/pop samples and 122 world samples.\n\nSource: [http://ismir2004.ismir.net/genre_contest/index.html](http://ismir2004.ismir.net/genre_contest/index.html)\nImage Source: [http://ismir2004.ismir.net/genre_contest/index.html](http://ismir2004.ismir.net/genre_contest/index.html)", "variants": ["ISMIR Genre"]}
{"id": "CLO-43SD", "title": "", "contents": "**CLO-43SD** is a dataset for multi-class species identification in avian flight calls. It consists of 5,428 labeled audio clips of flight calls from 43 different species of North American woodwarblers (in the family Parulidae). The clips came from a variety of recording conditions, including clean recordings obtained using highly-directional shotgun microphones, recordings obtained from noisier field recordings using omnidirectional microphones, and recordings obtained from birds in captivity.\n\nSource: [https://wp.nyu.edu/birdvox/codedata/](https://wp.nyu.edu/birdvox/codedata/)\nImage Source: [https://www.allaboutbirds.org/a-rosetta-stone-for-identifying-warblers-migration-calls/](https://www.allaboutbirds.org/a-rosetta-stone-for-identifying-warblers-migration-calls/)", "variants": ["CLO-43SD"]}
{"id": "CLO-WTSP", "title": "", "contents": "**CLO-WTSP** is a dataset for species-specific flight call identification for the White-Throated Sparrow. 16,703 labeled audio clips captured by remote acoustic sensors deployed in Ithaca, NY and NYC over the fall 2014 and spring 2015 migration seasons. Each clip is labeled to indicate whether it contains a flight call from the target species White-Throated Sparrow (WTSP), a flight call from a non-target species, or no flight call at all.​\n\nSource: [https://wp.nyu.edu/birdvox/codedata/](https://wp.nyu.edu/birdvox/codedata/)\nImage Source: [https://en.wikipedia.org/wiki/White-throated_sparrow#/media/File:Sparrow,_White_throated.jpg](https://en.wikipedia.org/wiki/White-throated_sparrow#/media/File:Sparrow,_White_throated.jpg)", "variants": ["CLO-WTSP"]}
{"id": "CLO-SWTH", "title": "", "contents": "**CLO-SWTH** is a dataset for species-specific flight call identification for the Swainson’s Thrush. 179,111 labeled audio clips captured by remote acoustic sensors deployed in Ithaca, NY and NYC over the fall 2014 and spring 2015 migration seasons. Each clip is labeled to indicate whether it contains a flight call from the target species Swainson’s Thrush (SWTH), a flight call from a non-target species, or no flight call at all.\n\nSource: [https://wp.nyu.edu/birdvox/codedata/](https://wp.nyu.edu/birdvox/codedata/)\nImage Source: [https://commons.wikimedia.org/wiki/Category:Catharus_ustulatus#/media/File:A_Swainson's_thrush_perched_in_a_tree_(7d58595b-c495-4744-9f20-2b301fa1cc63).jpg](https://commons.wikimedia.org/wiki/Category:Catharus_ustulatus#/media/File:A_Swainson's_thrush_perched_in_a_tree_(7d58595b-c495-4744-9f20-2b301fa1cc63).jpg)", "variants": ["CLO-SWTH"]}
{"id": "DCASE 2018 Task 4", "title": "", "contents": "DCASE2018 Task 4 is a dataset for large-scale weakly labeled semi-supervised sound event detection in domestic environments. The data are YouTube video excerpts focusing on domestic context which could be used for example in ambient assisted living applications. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events...) and potential industrial applications.\nSpecifically, the task employs a subset of “Audioset: An Ontology And Human-Labeled Dataset For Audio Events” by Google. Audioset consists of an expanding ontology of 632 sound event classes and a collection of 2 million human-labeled 10-second sound clips (less than 21% are shorter than 10-seconds) drawn from 2 million Youtube videos. The ontology is specified as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds.\nTask 4 focuses on a subset of Audioset that consists of 10 classes of sound events: speech, dog, cat, alarm bell ringing, dishes, frying, blender, running water, vacuum cleaner, electric shaver toothbrush.\n\nSource: [http://dcase.community/challenge2018/index](http://dcase.community/challenge2018/index)\nImage Source: [http://dcase.community/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection](http://dcase.community/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection)", "variants": ["DCASE 2018 Task 4"]}
{"id": "freefield1010", "title": "", "contents": "Freefield1010 is a collection of 7,690 excerpts from field recordings around the world, gathered by the FreeSound project, and then standardised for research.\n\nSource: [http://dcase.community/challenge2018/task-bird-audio-detection](http://dcase.community/challenge2018/task-bird-audio-detection)\nImage Source: [https://arxiv.org/pdf/1309.5275.pdf](https://arxiv.org/pdf/1309.5275.pdf)", "variants": ["freefield1010"]}
{"id": "warblrb10k", "title": "", "contents": "**warblrb10k** is a collection of 10,000 smartphone audio recordings from around the UK, crowdsourced by users of Warblr the bird recognition app. The audio covers a wide distribution of UK locations and environments, and includes weather noise, traffic noise, human speech and even human bird imitations.\n\nSource: [http://dcase.community/challenge2018/task-bird-audio-detection](http://dcase.community/challenge2018/task-bird-audio-detection)\nImage Source: [https://www.warblr.co.uk/](https://www.warblr.co.uk/)", "variants": ["warblrb10k"]}
{"id": "Chernobyl", "title": "", "contents": "**Chernobyl** is a collection of 620 audio clips collected from unattended remote monitoring equipment in the Chernobyl Exclusion Zone (CEZ). This data was collected as part of the TREE (Transfer-Exposure-Effects) research project into the long-term effects of the Chernobyl accident on local ecology. The audio covers a range of birds and includes weather, large mammal and insect noise sampled across various CEZ environments, including abandoned village, grassland and forest areas.\n\nSource: [http://dcase.community/challenge2018/task-bird-audio-detection](http://dcase.community/challenge2018/task-bird-audio-detection)\nImage Source: [https://en.wikipedia.org/wiki/Effects_of_the_Chernobyl_disaster#/media/File:Chernobyl,_Ukraine.jpg](https://en.wikipedia.org/wiki/Effects_of_the_Chernobyl_disaster#/media/File:Chernobyl,_Ukraine.jpg)", "variants": ["Chernobyl"]}
{"id": "PolandNFC", "title": "", "contents": "**PolandNFC** is a collection of 4,000 recordings from Hanna Pamuła's PhD project of monitoring autumn nocturnal bird migration. The recordings were collected every night, from September to November 2016 on the Baltic Sea coast, Poland, using Song Meter SM2 units with microphones mounted on 3–5 m poles. A subset derived from 15 nights with different weather conditions and background noise including wind, rain, sea noise, insect calls, human voice and deer calls was used in DCASE 2018 Challenge.\r\n\r\nSource: [http://dcase.community/challenge2018/task-bird-audio-detection](http://dcase.community/challenge2018/task-bird-audio-detection)\nImage Source: [http://dcase.community/challenge2018/task-bird-audio-detection](http://dcase.community/challenge2018/task-bird-audio-detection)", "variants": ["PolandNFC"]}
{"id": "BirdVox-DCASE-20k", "title": "", "contents": "The **BirdVox-DCASE-20k** dataset contains 20,000 ten-second audio recordings. These recordings come from ROBIN autonomous recording units, placed near Ithaca, NY, USA during the fall 2015. They were captured on the night of September 23rd, 2015, by six different sensors, originally numbered 1, 2, 3, 5, 7, and 10.\nOut of these 20,000 recording, 10,017 (50.09%) contain at least one bird vocalization (either song, call, or chatter).\nThe dataset is a derivative work of the BirdVox-full-night dataset, containing almost as much data but formatted into ten-second excerpts rather than ten-hour full night recordings.\n\nSource: [https://zenodo.org/record/1208080](https://zenodo.org/record/1208080)\nImage Source: [http://dcase.community/challenge2018/task-bird-audio-detection](http://dcase.community/challenge2018/task-bird-audio-detection)", "variants": ["BirdVox-DCASE-20k"]}
{"id": "BirdCLEF 2019", "title": "", "contents": "BirdClef 2019 is a bird soundscape dataset. It contains around 350 hours of manually annotated soundscapes using 30 field recorders between January and June of 2017 in Ithaca, NY, USA. There are around 50,000 recordings in the dataset in total, with 659 classes. The dataset also contains species tags.\n\nSource: [https://www.imageclef.org/BirdCLEF2019](https://www.imageclef.org/BirdCLEF2019)\nImage Source: [http://dcase.community/challenge2018/task-bird-audio-detection](http://dcase.community/challenge2018/task-bird-audio-detection)", "variants": ["BirdCLEF 2019"]}
{"id": "BirdCLEF 2018", "title": "", "contents": "BirdClef 2018 is a bird soundscape dataset based on the contributions of the Xeno-canto network. The training set contains 36,496 recordings covering 1500 species of central and south America (the largest bioacoustic dataset in the literature). There are about 68 hours of recordings in total, with 1,500 classes and species tags.\n\nSource: [https://www.imageclef.org/BirdCLEF2019](https://www.imageclef.org/BirdCLEF2019)\nImage Source: [http://dcase.community/challenge2018/task-bird-audio-detection](http://dcase.community/challenge2018/task-bird-audio-detection)", "variants": ["BirdCLEF 2018"]}
{"id": "DBR", "title": "DBR dataset", "contents": "**DBR** dataset is an environmental audio dataset created for the Bachelor's Seminar in Signal Processing in Tampere University of Technology. The samples in the dataset were collected from the online audio database Freesound. The dataset consists of three classes, each containing 50 samples, and the classes are 'dog', 'bird', and 'rain' (hence the name DBR).\n\nSource: [https://zenodo.org/record/1069747](https://zenodo.org/record/1069747)\nImage Source: [https://medium.com/@anonyomous.ut.grad.student/building-an-audio-classifier-f7c4603aa989](https://medium.com/@anonyomous.ut.grad.student/building-an-audio-classifier-f7c4603aa989)", "variants": ["DBR"]}
{"id": "DESED", "title": "Sound event detection in domestic environments withweakly labeled data and soundscape synthesis", "contents": "The **DESED** dataset is a dataset designed to recognize sound event classes in domestic environments. The dataset is designed to be used for sound event detection (SED, recognize events with their time boundaries) but it can also be used for sound event tagging (SET, indicate presence of an event in an audio file).\r\nThe dataset is composed of 10 event classes to recognize in 10 second audio files. The classes are: Alarm/bell/ringing, Blender, Cat, Dog, Dishes,\r\nElectric shaver/toothbrush, Frying, Running water, Speech, Vacuum cleaner.\r\n\r\nSource: [https://project.inria.fr/desed/](https://project.inria.fr/desed/)\r\nImage Source: [https://project.inria.fr/desed/](https://project.inria.fr/desed/)", "variants": ["DESED"]}
{"id": "FSL4", "title": "", "contents": "The **FSL4** dataset contains ~4000 user-contributed loops uploaded to Freesound. Loops were selected by searching Freesound for sounds with the query terms loop and bpm, and then automatically parsing the returned sound filenames, tags and textual descriptions to identify tempo annotations made by users. For example, a sound containing the tag 120bpm is considered to have a ground truth of 120 BPM.\n\nSource: [https://zenodo.org/record/3685832](https://zenodo.org/record/3685832)\nImage Source: [https://archives.ismir.net/ismir2016/paper/000195.pdf](https://archives.ismir.net/ismir2016/paper/000195.pdf)", "variants": ["FSL4"]}
{"id": "FSD50K", "title": "FSD50K: an Open Dataset of Human-Labeled Sound Events", "contents": "Freesound Dataset 50k (or **FSD50K** for short) is an open dataset of human-labeled sound events containing 51,197 Freesound clips unequally distributed in 200 classes drawn from the AudioSet Ontology. FSD50K has been created at the Music Technology Group of Universitat Pompeu Fabra. It consists mainly of sound events produced by physical sound sources and production mechanisms, including human sounds, sounds of things, animals, natural sounds, musical instruments and more.\n\nSource: [https://zenodo.org/record/4060432](https://zenodo.org/record/4060432)\nImage Source: [https://labs.freesound.org/datasets/](https://labs.freesound.org/datasets/)", "variants": ["FSD50K"]}
{"id": "SimSceneTVB Learning", "title": "SimSceneTVB Learning", "contents": "SimSceneTVB is a dataset of 600 simulated sound scenes of 45s each representing urban sound environments, simulated using the simScene Matlab library. The dataset is divided in two parts with a train subset (400 scenes) and a test subset (200 scenes) for the development of learning-based models.\nEach scene is composed of three main sources (traffic, human voices and birds) according to an original scenario, which is composed semi-randomly conditionally to five ambiances: park, quiet street, noisy street, very noisy street and square. Separate channels for the contribution of each source are available. The base audio files used for simulation are obtained from Freesound (https://freesound.org) and LibriSpeech (http://www.openslr.org/12). The sound scenes are scaled according to a playback sound level in dB, which is drawn randomly but remains plausible according to the ambiance.\n\nSource: [https://zenodo.org/record/3248703](https://zenodo.org/record/3248703)\nImage Source: [https://hal.archives-ouvertes.fr/hal-01078098v2/document](https://hal.archives-ouvertes.fr/hal-01078098v2/document)", "variants": ["SimSceneTVB Learning"]}
{"id": "SimSceneTVB Perception", "title": "SimSceneTVB Perception", "contents": "**SimSceneTVB Perception**  is a corpus of 100 sound scenes of 45s each representing urban sound environments, including: 6 scenes recorded in Paris, 19 scenes simulated using simScene to replicate recorded scenarios, 75 scenes simulated using simScene with diverse new scenarios, containing traffic, human voices and bird sources.The base audio files used for simulation are obtained from Freesound (https://freesound.org) and LibriSpeech (http://www.openslr.org/12).\n\nSource: [https://zenodo.org/record/3248734](https://zenodo.org/record/3248734)\nImage Source: [https://hal.archives-ouvertes.fr/hal-01078098v2/document](https://hal.archives-ouvertes.fr/hal-01078098v2/document)", "variants": ["SimSceneTVB Perception"]}
{"id": "Sound Events for Surveillance Applications", "title": "Sound Events for Surveillance Applications (Version 1.0.0)", "contents": "The **Sound Events for Surveillance Applications** (SESA) dataset files were obtained from Freesound. The dataset was divided between train (480 files) and test (105 files) folders. All audio files are WAV, Mono-Channel, 16 kHz, and 8-bit with up to 33 seconds. # Classes: 0 - Casual (not a threat) 1 - Gunshot 2 - Explosion 3 - Siren (also contains alarms).\n\nSource: [https://zenodo.org/record/3519845](https://zenodo.org/record/3519845)\nImage Source: [https://labs.freesound.org/datasets/](https://labs.freesound.org/datasets/)", "variants": ["Sound Events for Surveillance Applications"]}
{"id": "TUT Rare Sound Events 2017", "title": "TUT Rare sound events, Development dataset", "contents": "TUT Rare Sound events 2017, development dataset consists of source files for creating mixtures of rare sound events (classes baby cry, gun shot, glass break) with background audio, as well a set of readily generated mixtures and recipes for generating them. The \"source\" part of the dataset consists of two subsets: (a) background recordings from 15 different acoustic scenes, (b) recordings with the target rare sound events from three classes, accompanied by annotations of their temporal occurrences, (c) a set of meta files providing the cross-validation setup: lists of background and target event recordings split into training and test subsets (called \"devtrain\" and \"devtest\", respectively, indicating they are provided as the development dataset, as opposed to the evaluation dataset released separately).\nThe mixture set consists of two subsets (training and testing), each containing ~1500 mixtures (~500 per target class in each subset, with half of the mixtures not containing any target class events).\nDiment, Aleksandr, Mesaros, Annamaria, Heittola, Toni, & Virtanen, Tuomas. (2017). TUT Rare sound events, Development dataset [Data set]. Zenodo. http://doi.org/10.5281/zenodo.401395\n\nSource: [https://zenodo.org/record/401395](https://zenodo.org/record/401395)\nImage Source: [http://dcase.community/challenge2017/task-rare-sound-event-detection](http://dcase.community/challenge2017/task-rare-sound-event-detection)", "variants": ["TUT Rare Sound Events 2017"]}
{"id": "UrbanSound8K", "title": "A Dataset and Taxonomy for Urban Sound Research", "contents": "Urban Sound 8K is an audio dataset that contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. All excerpts are taken from field recordings uploaded to www.freesound.org.\r\n\r\nSource: [https://zenodo.org/record/401395](https://zenodo.org/record/401395)\r\nImage Source: [https://urbansounddataset.weebly.com/urbansound8k.html](https://urbansounddataset.weebly.com/urbansound8k.html)", "variants": ["UrbanSound8k", "UrbanSound8K"]}
{"id": "VocalImitationSet", "title": "", "contents": "The **VocalImitationSet** is a collection of crowd-sourced vocal imitations of a large set of diverse sounds collected from Freesound (https://freesound.org/), which were curated based on Google's AudioSet ontology (https://research.google.com/audioset/).\n\nSource: [https://zenodo.org/record/1340763](https://zenodo.org/record/1340763)\nImage Source: [https://www.researchgate.net/publication/332799163_VOCAL_IMITATION_SET_A_DATASET_OF_VOCALLY_IMITATED_SOUND_EVENTS_USING_THE_AUDIOSET_ONTOLOGY](https://www.researchgate.net/publication/332799163_VOCAL_IMITATION_SET_A_DATASET_OF_VOCALLY_IMITATED_SOUND_EVENTS_USING_THE_AUDIOSET_ONTOLOGY)", "variants": ["VocalImitationSet"]}
{"id": "TUT Sound Events 2018", "title": "TUT Sound Events 2018 - Ambisonic, Reverberant and Real-life Impulse Response Dataset", "contents": "The TUT Sounds Event 2018 dataset consists of real-life first order Ambisonic (FOA) format recordings with stationary point sources each associated with a spatial coordinate. The dataset was generated by collecting impulse responses (IR) from a real environment using the Eigenmike spherical microphone array. The measurement was done by slowly moving a Genelec G Two loudspeaker continuously playing a maximum length sequence around the array in circular trajectory in one elevation at a time. The playback volume was set to be 30 dB greater than the ambient sound level. The recording was done in a corridor inside the university with classrooms around it during work hours. The IRs were collected at elevations −40 to 40 with 10-degree increments at 1 m from the Eigenmike and at elevations −20 to 20 with 10-degree increments at 2 m.\n\nSource: [https://zenodo.org/record/1237793](https://zenodo.org/record/1237793)\nImage Source: [https://www.cs.tut.fi/~mesaros/pubs/mesaros_eusipco2016-dcase.pdf](https://www.cs.tut.fi/~mesaros/pubs/mesaros_eusipco2016-dcase.pdf)", "variants": ["TUT Sound Events 2018"]}
{"id": "aGender", "title": "A Database of Age and Gender Annotated Telephone Speech", "contents": "The **aGender** corpus contains audio recordings of predefined utterances and free speech produced by humans of different age and gender. Each utterance is labeled as one of four age groups: Child, Youth, Adult, Senior, and as one of three gender classes: Female, Male and Child.\n\nSource: [Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data](https://arxiv.org/abs/1602.05875)\nImage Source: [http://www.lrec-conf.org/proceedings/lrec2010/pdf/262_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2010/pdf/262_Paper.pdf)", "variants": ["aGender"]}
{"id": "TUT Sound Events 2017", "title": "TUT database for acoustic scene classification and sound event detection", "contents": "The **TUT Sound Events 2017** dataset contains 24 audio recordings in a street environment and contains 6 different classes. These classes are: brakes squeaking, car, children, large vehicle, people speaking, and people walking.\n\nSource: [Language Modelling for Sound Event Detection with Teacher Forcing and Scheduled Sampling](https://arxiv.org/abs/1907.08506)\nImage Source: [https://hal.inria.fr/hal-02067935/document](https://hal.inria.fr/hal-02067935/document)", "variants": ["TUT Sound Events 2017"]}
{"id": "DCASE 2014", "title": "Detection and Classification of Acoustic Scenes and Events", "contents": "DCASE2014 is an audio classification benchmark.\n\nSource: [Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization](https://arxiv.org/abs/1807.00230)", "variants": ["DCASE 2014"]}
{"id": "LOCATA", "title": "The LOCATA Challenge Data Corpus for Acoustic Source Localization and Tracking", "contents": "The **LOCATA** dataset is a dataset for acoustic source localization. It consists of real-world ambisonic speech recordings with optically tracked azimuth-elevation labels.\n\nSource: [Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks](https://arxiv.org/abs/1904.08452)\nImage Source: [https://www.locata.lms.tf.fau.de/files/2018/05/LOCATA_Paper_SAM_Workshop_2018.pdf](https://www.locata.lms.tf.fau.de/files/2018/05/LOCATA_Paper_SAM_Workshop_2018.pdf)", "variants": ["LOCATA"]}
{"id": "PPMI", "title": "The parkinson progression marker initiative (ppmi)", "contents": "The **Parkinson’s Progression Markers Initiative** (**PPMI**) dataset originates from an observational clinical and longitudinal study comprising evaluations of people with Parkinson’s disease (PD), those people with high risk, and those who are healthy.\r\n\r\nSource: [Time-Guided High-Order Attention Model of Longitudinal Heterogeneous Healthcare Data](https://arxiv.org/abs/1912.00773)\r\nImage Source: [https://www.ppmi-info.org/2013/08/imaging-inventory-whats-in-the-ppmi-database-2/](https://www.ppmi-info.org/2013/08/imaging-inventory-whats-in-the-ppmi-database-2/)", "variants": ["OASIS+ADIBE+ADHD200+MCIC+PPMI+HABS+HarvardGSP", "PPMI"]}
{"id": "BraTS 2018", "title": "", "contents": "**BraTS 2018** is a dataset which provides multimodal 3D brain MRIs and ground truth brain tumor segmentations annotated by physicians, consisting of 4 MRI modalities per case (T1, T1c, T2, and FLAIR). Annotations include 3 tumor subregions—the enhancing tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core. The annotations were combined into 3 nested subregions—whole tumor (WT), tumor core (TC), and enhancing tumor (ET). The data were collected from 19 institutions, using various MRI scanners\r\n\r\nSource: [End-to-End Boundary Aware Networks forMedical Image Segmentation](https://arxiv.org/abs/1908.08071)\r\nImage Source: [https://www.med.upenn.edu/sbia/brats2018/tasks.html](https://www.med.upenn.edu/sbia/brats2018/tasks.html)", "variants": ["BRATS 2018", "BRATS 2018 val", "BraTS 2018"]}
{"id": "ISIC 2017 Task 1", "title": "", "contents": "The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 1 challenge dataset for lesion segmentation contains 2,000 images for training with ground truth segmentations (2000 binary mask images).\n\nSource: [https://challenge.isic-archive.com/landing/2017/42](https://challenge.isic-archive.com/landing/2017/42)\nImage Source: [https://challenge.isic-archive.com/landing/2017/42](https://challenge.isic-archive.com/landing/2017/42)", "variants": ["ISIC 2017 Task 1"]}
{"id": "ISIC 2017 Task 2", "title": "", "contents": "The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 2 challenge dataset for lesion dermoscopic feature extraction contains the original lesion image, a corresponding superpixel mask, and superpixel-mapped expert annotations of the presence and absence of the following features: (a) network, (b) negative network, (c) streaks and (d) milia-like cysts.\n\nSource: [https://challenge.isic-archive.com/landing/2017/43](https://challenge.isic-archive.com/landing/2017/43)\nImage Source: [https://challenge.isic-archive.com/landing/2017/43](https://challenge.isic-archive.com/landing/2017/43)", "variants": ["ISIC 2017 Task 2"]}
{"id": "ISIC 2017 Task 3", "title": "", "contents": "The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 3 challenge dataset for lesion classification contains 2,000 images for training including 374 melanoma, 254 seborrheic keratosis and the remainder as benign nevi (1372).\n\nSource: [https://challenge.isic-archive.com/landing/2017/42](https://challenge.isic-archive.com/landing/2017/42)\nImage Source: [https://challenge.isic-archive.com/landing/2017/44](https://challenge.isic-archive.com/landing/2017/44)", "variants": ["ISIC 2017 Task 3"]}
{"id": "ISIC 2018 Task 1", "title": "", "contents": "The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. This Task 1 dataset is the challenge on lesion segmentation. It includes 2594 images.\n\nSource: [Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions](https://arxiv.org/abs/1909.00166)\nImage Source: [https://challenge2018.isic-archive.com/task1/](https://challenge2018.isic-archive.com/task1/)", "variants": ["ISIC 2018 Task 1"]}
{"id": "ISIC 2018 Task 2", "title": "", "contents": "The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 2 dataset is the challenge on lesion attribute detection. It includes 2594 images. The task is to detect the following dermoscopic attributes: pigment network, negative network, streaks, mila-like cysts and globules (including dots).\n\nSource: [Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions](https://arxiv.org/abs/1909.00166)\nImage Source: [https://challenge2018.isic-archive.com/task2/](https://challenge2018.isic-archive.com/task2/)", "variants": ["ISIC 2018 Task 2"]}
{"id": "ISIC 2018 Task 3", "title": "", "contents": "The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 3 dataset is the challenge on lesion classification. It includes 2594 images. The task is to classify the dermoscopic images into one of the following categories: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowen’s disease, benign keratosis, dermatofibroma, and vascular lesion.\n\nSource: [Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions](https://arxiv.org/abs/1909.00166)\nImage Source: [https://challenge2018.isic-archive.com/task3/](https://challenge2018.isic-archive.com/task3/)", "variants": ["ISIC 2018 Task 3"]}
{"id": "NeuB1", "title": "", "contents": "**NeuB1** is a microscopic neuronal image dataset for retinal vessel segmentation, which contains 112 images of size 512 x 152. The train/test split is 37/75.\nImage Source: [https://web.bii.a-star.edu.sg/~zhaoh/Jaydeep_Tracing/](https://web.bii.a-star.edu.sg/~zhaoh/Jaydeep_Tracing/)", "variants": ["NeuB1"]}
{"id": "BraTS 2017", "title": "", "contents": "The BRATS2017 dataset. It contains 285 brain tumor MRI scans, with four MRI modalities as T1, T1ce, T2, and Flair for each scan. The dataset also provides full masks for brain tumors, with labels for ED, ET, NET/NCR. The segmentation evaluation is based on three tasks: WT, TC and ET segmentation.\r\n\r\nSource: [Scribble-based Hierarchical Weakly Supervised Learning for Brain Tumor Segmentation](https://arxiv.org/abs/1911.02014)\r\nImage Source: [https://www.google.com/search?q=A+Modified+U-Net+Convolutional+Network+Featuring+a+Nearest-neighbor+Re-sampling-based+Elastic-Transformation+for+Brain+Tissue+Characterization+and+Segmentation&oq=A+Modified+U-Net+Convolutional+Network+Featuring+a+Nearest-neighbor+Re-sampling-based+Elastic-Transformation+for+Brain+Tissue+Characterization+and+Segmentation&aqs=chrome..69i57j69i64l3.296j0j4&sourceid=chrome&ie=UTF-8](https://www.google.com/search?q=A+Modified+U-Net+Convolutional+Network+Featuring+a+Nearest-neighbor+Re-sampling-based+Elastic-Transformation+for+Brain+Tissue+Characterization+and+Segmentation&oq=A+Modified+U-Net+Convolutional+Network+Featuring+a+Nearest-neighbor+Re-sampling-based+Elastic-Transformation+for+Brain+Tissue+Characterization+and+Segmentation&aqs=chrome..69i57j69i64l3.296j0j4&sourceid=chrome&ie=UTF-8)", "variants": ["BRATS-2017 val", "BraTS 2017"]}
{"id": "BraTS 2015", "title": "", "contents": "The **BraTS 2015** dataset is a dataset for brain tumor image segmentation. It consists of 220 high grade gliomas (HGG) and 54 low grade gliomas (LGG) MRIs. The four MRI modalities are T1, T1c, T2, and T2FLAIR. Segmented “ground truth” is provide about four intra-tumoral classes, viz. edema, enhancing tumor, non-enhancing tumor, and necrosis.\r\n\r\nSource: [Brain MRI Tumor Segmentation with Adversarial Networks](https://arxiv.org/abs/1910.02717)\r\nImage Source: [https://sites.google.com/site/braintumorsegmentation/home/brats2015](https://sites.google.com/site/braintumorsegmentation/home/brats2015)", "variants": ["BRATS-2015", "BraTS 2015"]}
{"id": "BraTS 2013", "title": "", "contents": "BRATS 2013 is a brain tumor segmentation dataset consists of synthetic and real images, where each of them is further divided into high-grade gliomas (HG) and low-grade gliomas (LG). There are 25 patients with both synthetic HG and LG images and 20 patients with real HG and 10 patients with real LG images. For each patient, FLAIR, T1, T2, and post-Gadolinium T1 magnetic resonance (MR) image sequences are available.\r\n\r\nSource: [Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization](https://arxiv.org/abs/1908.06965)\r\nImage Source: [https://arxiv.org/pdf/1708.00377.pdf](https://arxiv.org/pdf/1708.00377.pdf)", "variants": ["BRATS-2013", "BRATS-2013 leaderboard", "BraTS 2013"]}
{"id": "ISRUC-Sleep", "title": "", "contents": "**ISRUC-Sleep** is a polysomnographic (PSG) dataset. The data were obtained from human adults, including healthy subjects, and subjects with sleep disorders under the effect of sleep medication. The dataset, which is structured to support different research objectives, comprises three groups of data: (a) data concerning 100 subjects, with one recording session per subject, (b) data gathered from 8 subjects; two recording sessions were performed per subject, which are useful for studies involving changes in the PSG signals over time, (c) data collected from one recording session related to 10 healthy subjects, which are useful for studies involving comparison of healthy subjects with the patients suffering from sleep disorders.\n\nSource: [https://sleeptight.isr.uc.pt/](https://sleeptight.isr.uc.pt/)\nImage Source: [https://sleeptight.isr.uc.pt/](https://sleeptight.isr.uc.pt/)", "variants": ["ISRUC-Sleep"]}
{"id": "NIH-LN", "title": "A new 2.5 D representation for lymph node detection in CT. The Cancer Imaging Archive.", "contents": "**NIH-Lymph Node** (**NIH-LN**) contains 388 mediastinal LNs in 90 CT scans and 595 abdominal LNs in 86 scans.\n\nSource: [https://sleeptight.isr.uc.pt/](https://sleeptight.isr.uc.pt/)", "variants": ["NIH-LN"]}
{"id": "BraTS 2014", "title": "", "contents": "BRATS 2014 is a brain tumor segmentation dataset.\n\nSource: [Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization](https://arxiv.org/abs/1908.06965)\nImage Source: [http://people.csail.mit.edu/menze/papers/proceedings_miccai_brats_2014.pdf](http://people.csail.mit.edu/menze/papers/proceedings_miccai_brats_2014.pdf)", "variants": ["BRATS-2014", "BraTS 2014"]}
{"id": "MedDialog", "title": "MedDialog: Two Large-scale Medical Dialogue Datasets", "contents": "The MedDialog dataset (Chinese) contains conversations (in Chinese) between doctors and patients. It has 1.1 million dialogues and 4 million utterances. The data is continuously growing and more dialogues will be added. The raw dialogues are from haodf.com. All copyrights of the data belong to haodf.com.\r\n\r\nSource: [GitHub](https://github.com/UCSD-AI4H/Medical-Dialogue-System)", "variants": ["MedDialog"]}
{"id": "Country211", "title": "", "contents": "Country211 is an internal OpenAI dataset designed to assess the geolocation capability of visual representations. It filters the YFCC100m dataset (Thomee et al., 2016) to find 211 countries (defined as having an ISO-3166 country code) that have at least 300 photos with GPS coordinates. OpenAI built a balanced dataset with 211 categories, by sampling 200 photos for training and 100 photos for testing, for each country.", "variants": ["Country211"]}
{"id": "Hateful Memes", "title": "", "contents": "The Hateful Memes data set is a multimodal dataset for hateful meme detection (image + text) that contains 10,000+ new multimodal examples created by Facebook AI. Images were licensed from Getty Images so that researchers can use the data set to support their work.", "variants": ["Hateful Memes"]}
{"id": "Rendered SST2", "title": "", "contents": "The **Rendered SST2** dataset is an internal OpenAI dataset that measures the optical character recognition capability of visual representations.\r\nIt uses sentences from the [Stanford Sentiment Treebank](/dataset/sst) dataset and renders them into images, with black texts on a white background, in a\r\n448×448 resolution.", "variants": ["Rendered SST2"]}
{"id": "AccentDB", "title": "AccentDB: A Database of Non-Native English Accents to Assist Neural Speech Recognition", "contents": "AccentDB is a database that contains samples of 4 Indian-English accents, and a compilation of samples from 4 native-English, and a metropolitan Indian-English accent.", "variants": ["AccentDB"]}
{"id": "CREMA-D", "title": "", "contents": "**CREMA-D** is an emotional multimodal actor data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified).\r\n\r\nActors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).\r\n\r\nParticipants rated the emotion and emotion levels based on the combined audiovisual presentation, the video alone, and the audio alone. Due to the large number of ratings needed, this effort was crowd-sourced and a total of 2443 participants each rated 90 unique clips, 30 audio, 30 visual, and 30 audio-visual. 95% of the clips have more than 7 ratings.", "variants": ["CREMA-D"]}
{"id": "DementiaBank", "title": "", "contents": "DementiaBank is a shared database of multimedia interactions for the study of communication in dementia. The dataset contains 117 people diagnosed with Alzheimer Disease, and 93 healthy people, reading a description of an image. The principal task and benchmark is to classify each group.", "variants": ["DementiaBank"]}
{"id": "FUSS", "title": "What's All the FUSS About Free Universal Sound Separation Data?", "contents": "The **Free Universal Sound Separation (FUSS)** dataset is a database of arbitrary sound mixtures and source-level references, for use in experiments on arbitrary sound separation. FUSS is based on FSD50K corpus.", "variants": ["FUSS"]}
{"id": "gtzan", "title": "", "contents": "The **gtzan8** audio dataset contains 1000 tracks of 30 second length. There are 10 genres, each containing 100 tracks which are all 22050Hz Mono 16-bit audio files in .wav format. The genres are:\r\n\r\n- blues\r\n- classical\r\n- country\r\n- disco\r\n- hiphop\r\n- jazz\r\n- metal\r\n- pop\r\n- reggae\r\n- rock", "variants": ["gtzan"]}
{"id": "gtzan_music_speech", "title": "", "contents": "**gtzan_music_speech** is a dataset for music/speech discrimination. It consists of 120 tracks of 30 second length. Each class (music/speech) has 60 samples. The tracks are all 22050Hz Mono 16-bit audio files in .wav format.", "variants": ["gtzan_music_speech"]}
{"id": "iVQA", "title": "", "contents": "A new open-ended VideoQA benchmark that aims to: i) provide a well-defined evaluation by including five correct answer annotations per question and ii) avoid questions which can be answered without the video. \r\n\r\niVQA videos are obtained by randomly sampling 7-30 sec. video clips from the HowTo100M dataset. Each clip is manually annotated with one question and five answers on Amazon Mechanical Turk.\r\n\r\nSource: [Just Ask: Learning to Answer Questions from Millions of Narrated Videos](https://www.di.ens.fr/willow/research/just-ask/)", "variants": ["iVQA"]}
{"id": "LibriTTS", "title": "", "contents": "**LibriTTS** is a multi-speaker English corpus of approximately 585 hours of read English speech at 24kHz sampling rate, prepared by Heiga Zen with the assistance of Google Speech and Google Brain team members. The LibriTTS corpus is designed for TTS research. It is derived from the original materials (mp3 audio files from LibriVox and text files from Project Gutenberg) of the LibriSpeech corpus. The main differences from the LibriSpeech corpus are listed below:\r\n\r\n- The audio files are at 24kHz sampling rate.\r\n- The speech is split at sentence breaks.\r\n- Both original and normalized texts are included.\r\n- Contextual information (e.g., neighbouring sentences) can be extracted.\r\n- Utterances with significant background noise are excluded.", "variants": ["LibriTTS"]}
{"id": "SAVEE", "title": "", "contents": "The **Surrey Audio-Visual Expressed Emotion (SAVEE)** dataset was recorded as a pre-requisite for the development of an automatic emotion recognition system. The database consists of recordings from 4 male actors in 7 different emotions, 480 British English utterances in total. The sentences were chosen from the standard TIMIT corpus and phonetically-balanced for each emotion. The data were recorded in a visual media lab with high quality audio-visual equipment, processed and labeled. To check the quality of performance, the recordings were evaluated by 10 subjects under audio, visual and audio-visual conditions. Classification systems were built using standard features and classifiers for each of the audio, visual and audio-visual modalities, and speaker-independent recognition rates of 61%, 65% and 84% achieved respectively.", "variants": ["SAVEE"]}
{"id": "FSDD", "title": "", "contents": "**Free Spoken Digit Dataset (FSDD)** is a simple audio/speech dataset consisting of recordings of spoken digits in wav files at 8kHz. The recordings are trimmed so that they have near minimal silence at the beginnings and ends. It contains data from 6 speakers, 3,000 recordings (50 of each digit per speaker), and English pronunciations.", "variants": ["FSDD"]}
{"id": "SQA69M", "title": "Just Ask: Learning to Answer Questions from Millions of Narrated Videos", "contents": "A dataset of 69,270,581 video clip, question and answer triplets (v, q, a). It has fewer triplets than 136M video clips in HowTo100M as adjacent clips are integrated so that they correspond to the transcribed sentences in the generation pipeline. SQA69M is two orders of magnitude larger than all current VideoQA datasets.\r\n\r\nSQA69M contains over 16M unique answers, where over 2M of them appear more than once and over 300K of them appear more than ten times, demonstrating the large diversity of the answers.\r\n\r\nSource: [Just Ask: Learning to Answer Questions from Millions of Narrated Videos](https://arxiv.org/pdf/2012.00451v1.pdf)", "variants": ["SQA69M"]}
{"id": "Yesno", "title": "", "contents": "**Yesno** is an audio dataset consisting of 60 recordings of one individual saying yes or no in Hebrew; each recording is eight words long. It was created for the Kaldi audio project by an author who wishes to remain anonymous.", "variants": ["Yesno"]}
{"id": "AbstractReasoning", "title": "", "contents": "**AbstractReasoning** is a dataset for abstract reasoning, where the goal is to infer the correct answer from the context panels based on abstract reasoning.\r\n\r\nImage Source: [Barrett et al](https://arxiv.org/pdf/1807.04225.pdf)", "variants": ["AbstractReasoning"]}
{"id": "BCCD", "title": "", "contents": "**BCCD** is a small-scale dataset for blood cells detection.", "variants": ["BCCD"]}
{"id": "How2R", "title": "", "contents": "Amazon Mechanical Turk (AMT) is used to collect annotations on HowTo100M videos. 30k 60-second clips are randomly sampled from 9,421 videos and present each clip to the turkers, who are asked to select a video segment containing a single, self-contained scene. After this segment selection step, another group of workers are asked to write descriptions for each displayed segment. Narrations are not provided to the workers to ensure that their written queries are based on visual content only. These final video segments are 10-20 seconds long on average, and the length of queries ranges from 8 to 20 words. From this process, 51,390 queries are collected for 24k 60-second clips from 9,371 videos in HowTo100M, on average 2-3 queries per clip. The video clips and its associated queries are split into 80% train, 10% val and 10% test.\r\n\r\nSource: [HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training](https://arxiv.org/pdf/2005.00200v2.pdf)", "variants": ["How2R"]}
{"id": "How2QA", "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training", "contents": "To collect How2QA for video QA task, the same set of selected video clips are presented to another group of AMT workers for multichoice QA annotation. Each worker is assigned with one video segment and asked to write one question with four answer candidates (one correctand three distractors). Similarly, narrations are hidden from the workers to ensure the collected QA pairs are not biased by subtitles. Similar to TVQA, the start and end points are provided for the relevant moment for each question. After filtering low-quality annotations, the final dataset contains 44,007 QA pairs for 22k 60-second clips selected from 9035 videos.\r\n\r\nSource: [HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training](https://arxiv.org/pdf/2005.00200v2.pdf)", "variants": ["How2QA"]}
{"id": "CLIC", "title": "", "contents": "**CLIC** is a dataset for learned image compression. The dataset contains both RGB and grayscale images.", "variants": ["CLIC"]}
{"id": "SemEval 2013", "title": "", "contents": "", "variants": ["SemEval 2013 Task 12"]}
{"id": "PixelHelp", "title": "", "contents": "PixelHelp includes 187 multi-step instructions of 4 task categories deined in https://support.google.com/pixelphone and annotated by human. This dataset includes 88 general tasks, such as configuring accounts, 38 Gmail tasks, 31 Chrome tasks, and 30 Photos related tasks. This dataset is an updated opensource version of the original PixelHelp dataset, which was used for testing the end-to-end grounding quality of the model in paper \"Mapping Natural Language Instructions to Mobile UI Action Sequences\". The similar accuracy is acquired on this version of the dataset.\r\n\r\nSource: [PixelHelp](https://github.com/google-research-datasets/seq2act/tree/master/data/pixel_help)", "variants": ["PixelHelp"]}
{"id": "RicoSCA", "title": "", "contents": "Rico is a public UI corpus with 72K Android UI screens mined from 9.7K Android apps (Deka et al., 2017). Each screen in Rico comes with a screenshot image and a view hierarchy of a collection of UI objects. Authors manually removed screens whose view hierarchies do not match their screenshots by asking annotators to visually verify whether the bounding boxes of view hierarchy leaves match each UI object on the corresponding screenshot image. This filtering results in 25K unique screens.\r\n\r\nIn total, RICOSCA contains 295,476 single-step synthetic commands for operating 177,962 different target objects across 25,677 Android screens.\r\n\r\nSource: [Google Research](https://github.com/google-research/google-research/blob/master/seq2act/data_generation/README.md)", "variants": ["RicoSCA"]}
{"id": "AndroidHowTo", "title": "Mapping Natural Language Instructions to Mobile UI Action Sequences", "contents": "AndroidHowTo contains 32,436 data points from 9,893 unique How-To instructions and split into training (8K), validation (1K) and test (900). All test examples have perfect agreement across all three annotators for the entire sequence. In total, there are 190K operation spans, 172K object spans, and 321 input spans labeled. The lengths of the instructions range from 19 to 85 tokens, with median of 59. They describe a sequence of actions from one to 19 steps, with a median of 5.\r\n\r\nSource: [Google Research](https://github.com/google-research/google-research/blob/master/seq2act/data_generation/README.md)", "variants": ["AndroidHowTo"]}
{"id": "MIND", "title": "MIND: A Large-scale Dataset for News Recommendation", "contents": "MIcrosoft News Dataset (MIND) is a large-scale dataset for news recommendation research. It was collected from anonymized behavior logs of Microsoft News website. The mission of MIND is to serve as a benchmark dataset for news recommendation and facilitate the research in news recommendation and recommender systems area.\r\n\r\nMIND contains about 160k English news articles and more than 15 million impression logs generated by 1 million users. Every news article contains rich textual content including title, abstract, body, category and entities. Each impression log contains the click events, non-clicked events and historical news click behaviors of this user before this impression. To protect user privacy, each user was de-linked from the production system when securely hashed into an anonymized ID.\r\n\r\nSource: [MIND](https://msnews.github.io/)", "variants": ["MIND"]}
{"id": "SciREX", "title": "SciREX: A Challenge Dataset for Document-Level Information Extraction", "contents": "SCIREX is a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. The dataset is annotated by integrating automatic and human annotations, leveraging existing scientific knowledge resources.\r\n\r\nSource: [SCIREX: A Challenge Dataset for Document-Level Information Extraction](https://arxiv.org/pdf/2005.00512v1.pdf)", "variants": ["SciREX"]}
{"id": "CH-SIMS", "title": "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality", "contents": "CH-SIMS is a Chinese single- and multimodal sentiment analysis dataset which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.\r\n\r\nSource: [CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality](https://www.aclweb.org/anthology/2020.acl-main.343.pdf)", "variants": ["CH-SIMS"]}
{"id": "WCEP", "title": "A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal", "contents": "The WCEP dataset for multi-document summarization (MDS) consists of short, human-written summaries about news events, obtained from the Wikipedia Current Events Portal (WCEP), each paired with a cluster of news articles associated with an event. These articles consist of sources cited by editors on WCEP, and are extended with articles automatically obtained from the Common Crawl News dataset. \r\n\r\nSource: [WCEP](https://github.com/complementizer/wcep-mds-dataset)", "variants": ["WCEP"]}
{"id": "MATINF", "title": "MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization", "contents": "Maternal and Infant (MATINF) Dataset is a large-scale dataset jointly labeled for classification, question answering and summarization in the domain of maternity and baby caring in Chinese. An entry in the dataset includes four fields: question (Q), description (D), class (C) and answer (A).\r\n\r\nNearly two million question-answer pairs are collected with fine-grained human-labeled classes from a large Chinese maternity and baby caring QA site. Authors conduct both automatic and manual data cleansing and remove: (1) classes with insufficient samples; (2) entries in which the length of the description filed is less than the length of the question field; (3) data with any field longer than 256 characters; (4) human-spotted ill-formed data. After the data cleansing, MATINF is constructed with the remaining 1.07 million entries\r\n\r\nSource: [MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization](https://arxiv.org/pdf/2004.12302v2.pdf)", "variants": ["MATINF"]}
{"id": "FOBIE", "title": "In Layman's Terms: Semi-Open Relation Extraction from Scientific Texts", "contents": "The Focused Open Biology Information Extraction (FOBIE) dataset aims to support IE from Computer-Aided Biomimetics. The dataset contains ~1,500 sentences from scientific biological texts. These sentences are annotated with TRADE-OFFS and syntactically similar relations between unbounded arguments, as well as argument-modifiers.\r\n\r\nThe FOBIE dataset has been used to explore Semi-Open Relation Extraction (SORE). The code for this and instructions can be found inside the SORE folder Readme.md, or in the ReadTheDocs documentations.\r\n\r\nSource: [FOBIE](https://github.com/rubenkruiper/FOBIE)", "variants": ["FOBIE"]}
{"id": "CODA-19", "title": "CODA-19: Using a Non-Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the COVID-19 Open Research Dataset", "contents": "CODA-19 is a human-annotated dataset that denotes the Background, Purpose, Method, Finding/Contribution, and Other for 10,966 English abstracts in the COVID-19 Open Research Dataset.\r\n\r\nCODA-19 was created by 248 crowd workers from Amazon Mechanical Turk collectively within ten days. Each abstract was annotated by nine different workers, and the final labels were obtained by majority voting.\r\n\r\nCODA-19's labels have an accuracy of 82% and an inter-annotator agreement (Cohen's kappa) of 0.74 when compared against expert labels on 129 abstracts.\r\n\r\nSource: [CODA-19](https://github.com/windx0303/CODA-19)", "variants": ["CODA-19"]}
{"id": "COVID-Q", "title": "What Are People Asking About COVID-19? A Question Classification Dataset", "contents": "COVID-Q consists of COVID-19 questions which have been annotated into a broad category (e.g. Transmission, Prevention) and a more specific class such that questions in the same class are all asking the same thing.\r\n\r\nSource: [COVID-Q](https://github.com/JerryWei03/COVID-Q)", "variants": ["COVID-Q"]}
{"id": "WT-WT", "title": "Will-They-Won't-They: A Very Large Dataset for Stance Detection on Twitter", "contents": "Will-They-Won't-They (WT-WT) is a large dataset of English tweets targeted at stance detection for the rumor verification task. The dataset is constructed based on tweets that discuss five recent merger and acquisition (M&A) operations of US companies, mainly from the healthcare sector.\r\n\r\nAll the annotations are carried out by domain experts; therefore, the dataset constitutes a high-quality and reliable benchmark for future research in stance detection.\r\n\r\nSource: [WT-WT](https://github.com/cambridge-wtwt/acl2020-wtwt-tweets)", "variants": ["WT-WT"]}
{"id": "KLEJ", "title": "", "contents": "The KLEJ benchmark (Kompleksowa Lista Ewaluacji Językowych) is a set of nine evaluation tasks for the Polish language understanding task.\r\n\r\nKey benchmark features:\r\n\r\n- It contains a diverse set of tasks from different domains and with different objectives.\r\n- Most tasks are created from existing datasets but the authors also released the new sentiment analysis dataset from an e-commerce domain.\r\n- It includes tasks which have relatively small datasets and require extensive external knowledge to solve them. It promotes the usage of transfer learning instead of training separate models from scratch.\r\n\r\nThe name KLEJ (English: GLUE) is an abbreviation for Kompleksowa Lista Ewaluacji Językowych (English: Comprehensive List of Language Evaluations) and refers to the [GLUE benchmark](/dataset/glue).\r\n\r\nSource: [KLEJ](https://klejbenchmark.com/)", "variants": ["KLEJ"]}
{"id": "Microsoft Research Multimodal Aligned Recipe Corpus", "title": "A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks", "contents": "To construct the MICROSOFT RESEARCH MULTIMODAL ALIGNED RECIPE CORPUS the authors first extract a large number of text and video recipes from the web. The goal is to find joint alignments between multiple text recipes and multiple video recipes for the same dish. The task is challenging, as different recipes vary in their order of instructions and use of ingredients. Moreover, video instructions can be noisy, and text and video instructions include different levels of specificity in their descriptions.\r\n\r\nSource: [A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks](https://arxiv.org/pdf/2005.09606v1.pdf)", "variants": ["Microsoft Research Multimodal Aligned Recipe Corpus"]}
{"id": "ClarQ", "title": "ClarQ: A large-scale and diverse dataset for Clarification Question Generation", "contents": "ClarQ, consists of ∼2M examples distributed across 173 domains of stackexchange. This dataset is meant for training and evaluation of Clarification Question Generation Systems.\r\n\r\n\r\nSource: [ClarQ: A large-scale and diverse dataset for Clarification Question Generation](https://arxiv.org/pdf/2006.05986v2.pdf)", "variants": ["ClarQ"]}
{"id": "Refer360°", "title": "Refer360$^\\circ$: A Referring Expression Recognition Dataset in 360$^\\circ$ Images", "contents": "Refer360° is a novel large-scale referring expression recognition dataset consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360° scenes.\r\n\r\nSource: [Refer360° : A Referring Expression Recognition Dataset in 360° Images](https://www.aclweb.org/anthology/2020.acl-main.644.pdf)", "variants": ["Refer360°"]}
{"id": "MUStARD", "title": "Towards Multimodal Sarcasm Detection (An \\_Obviously\\_ Perfect Paper)", "contents": "We release the MUStARD dataset which is a multimodal video corpus for research in automated sarcasm discovery. The dataset is compiled from popular TV shows including Friends, The Golden Girls, The Big Bang Theory, and Sarcasmaholics Anonymous. MUStARD consists of audiovisual utterances annotated with sarcasm labels. Each utterance is accompanied by its context, which provides additional information on the scenario where the utterance occurs.\r\n\r\nSource: [MUStARD](https://github.com/soujanyaporia/MUStARD)", "variants": ["MUStARD"]}
{"id": "CONAN", "title": "CONAN - COunter NArratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech", "contents": "COunter NArratives through Nichesourcing (CONAN) is a dataset that consists of 4,078 pairs over the 3 languages. Additionally, 3 types of metadata are provided: expert demographics, hate speech sub-topic and counter-narrative type. The dataset is augmented through translation (from Italian/French to English) and paraphrasing, which brought the total number of pairs to 14.988.\r\n\r\nSource: [CONAN](https://github.com/marcoguerini/CONAN)\r\nImage Source: [https://www.aclweb.org/anthology/P19-1271](https://www.aclweb.org/anthology/P19-1271)", "variants": ["CONAN"]}
{"id": "OQGend", "title": "", "contents": "Dataset OQRanD and OQGenD for paper \"Asking the crowd: Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums\" by Zi Chai, Xinyu Xing, Xiaojun Wan and Bo Huang. This paper is accepted by ACL'19.\r\n\r\nThe OQGenD dataset can be viewed at \"OQGenD.xml\". Each data (NQ-pairs) contains a certain piece of news with multiple related open-answered questions.\r\n\r\nThe OQRanD dataset can be viewed at \"OQRanD.xml\". Each data (Question Pairs) contains two questions, Q2 has more answers than Q1.\r\n\r\nSource: [GitHub](https://github.com/ChaiZ-pku/OQRanD-and-OQGenD)", "variants": ["OQGend"]}
{"id": "OQRanD", "title": "Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums", "contents": "Dataset OQRanD and OQGenD for paper \"Asking the crowd: Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums\" by Zi Chai, Xinyu Xing, Xiaojun Wan and Bo Huang. This paper is accepted by ACL'19.\r\n\r\nThe OQGenD dataset can be viewed at \"OQGenD.xml\". Each data (NQ-pairs) contains a certain piece of news with multiple related open-answered questions.\r\n\r\nThe OQRanD dataset can be viewed at \"OQRanD.xml\". Each data (Question Pairs) contains two questions, Q2 has more answers than Q1.\r\n\r\nSource: [GitHub](https://github.com/ChaiZ-pku/OQRanD-and-OQGenD)", "variants": ["OQRanD"]}
{"id": "WikiCREM", "title": "WikiCREM: A Large Unsupervised Corpus for Coreference Resolution", "contents": "An unsupervised dataset for co-reference resolution. Presented in the publication: Kocijan et. al, WikiCREM: A Large Unsupervised Corpus for Coreference Resolution, presented at EMNLP 2019.\r\n\r\nSource: [WikiCREM](https://ora.ox.ac.uk/objects/uuid:c83e94bb-7584-41a1-aef9-85b0e764d9e3)\r\nImage Source: [https://arxiv.org/pdf/1908.08025v3.pdf](https://arxiv.org/pdf/1908.08025v3.pdf)", "variants": ["WikiCREM"]}
{"id": "TyDi QA", "title": "", "contents": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 200K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology — the set of linguistic features that each language expresses — such that the authors expect models performing well on this set to generalize across a large number of the languages in the world.\r\n\r\nSource: [Google Research](https://ai.google.com/research/tydiqa)", "variants": ["TyDi QA", "TyDiQA-GoldP"]}
{"id": "OLPBENCH", "title": "Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction", "contents": "OLPBENCH is a large Open Link Prediction benchmark, which was derived from the state-of-the-art Open Information Extraction corpus OPIEC (Gashteovski et al., 2019). OLPBENCH contains 30M open triples, 1M distinct open relations and 2.5M distinct mentions of approximately 800K entities. \r\n\r\nOpen Link Prediction is defined as follows: Given an Open Knowledge Graph and a question consisting of an entity mention and an open relation, predict mentions as answers. A predicted mention is correct if it is a mention of the correct answer entity. For example, given the question (“NBC-TV”, “has office in”, ?), correct answers include “NYC” and “New York”.\r\n\r\nSource: [OLPBENCH](https://www.uni-mannheim.de/dws/research/resources/olpbench/)", "variants": ["OLPBENCH"]}
{"id": "Business Scene Dialogue", "title": "Designing the Business Conversation Corpus", "contents": "The Japanese-English business conversation corpus, namely **Business Scene Dialogue** corpus, was constructed in 3 steps:\r\n\r\n1. selecting business scenes,\r\n2. writing monolingual conversation scenarios according to the selected scenes, and\r\n3. translating the scenarios into the other language.\r\n\r\nHalf of the monolingual scenarios were written in Japanese and the other half were written in English. The whole construction process was supervised by a person who satisfies the following conditions to guarantee the conversations to be natural:\r\n\r\n- has the experience of being engaged in language learning programs, especially for business conversations\r\n- is able to smoothly communicate with others in various business scenes both in Japanese and English\r\n- has the experience of being involved in business\r\n\r\nThe BSD corpus is split into balanced training, development and evaluation sets. The documents in these sets are balanced in terms of scenes and original languages. In this repository we publicly share the full development and evaluation sets and a part of the training data set.\r\n\r\nSource: [BSD](https://github.com/tsuruoka-lab/BSD)", "variants": ["Business Scene Dialogue JA-EN", "Business Scene Dialogue EN-JA", "Business Scene Dialogue"]}
{"id": "ProofWriter", "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language", "contents": "The ProofWriter dataset contains many small rulebases of facts and rules, expressed in English. Each rulebase also has a set of questions (English statements) which can either be proven true or false using proofs of various depths, or the answer is “Unknown” (in open-world setting, OWA) or assumed negative (in closed-world setting, CWA).\r\n\r\nThe dataset includes full proofs with intermediate conclusions, which models can try to reproduce.\r\n\r\nThe dataset supports various tasks:\r\n\r\n- Given rulebase + question, what is answer + proof (w/intermediates)?\r\n- Given rulebase, what are all the provable implications?\r\n- Given rulebase + question without proof, what single fact can be added to make the question true?\r\n\r\nSource: [Allen AI](https://allenai.org/data/proofwriter)", "variants": ["ProofWriter"]}
{"id": "Open PI", "title": "A Dataset for Tracking Entities in Open Domain Procedural Text", "contents": "**Open PI** is the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. The dataset comprises 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from WikiHow.com.\r\nThe state tracking task assumes new formulation in which just the text is provided, from which a set of state changes (entity, attribute, before, after) is generated for each step, where the entity, attribute, and values must all be predicted from an open vocabulary.\r\n\r\nSource: [Allen Institute of AI](https://allenai.org/data/openpi)", "variants": ["Open PI"]}
{"id": "hasPart KB", "title": "Do Dogs have Whiskers? A New Knowledge Base of hasPart Relations", "contents": "This dataset is a new knowledge-base (KB) of hasPart relationships, extracted from a large corpus of generic statements. Complementary to other resources available, it is the first which is all three of: accurate (90% precision), salient (covers relationships a person may mention), and has high coverage of common terms (approximated as within a 10 year old’s vocabulary), as well as having several times more hasPart entries than in the popular ontologies ConceptNet and WordNet. In addition, it contains information about quantifiers, argument modifiers, and links the entities to appropriate concepts in Wikipedia and WordNet.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/haspartkb)", "variants": ["hasPart KB"]}
{"id": "SciDocs", "title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers", "contents": "SciDocs evaluation framework consists of a suite of evaluation tasks designed for document-level tasks.\r\n\r\nSource: [Allen Institute for AI](https://github.com/allenai/scidocs)", "variants": ["SciDocs"]}
{"id": "GenericsKB", "title": "GenericsKB: A Knowledge Base of Generic Statements", "contents": "The **GenericsKB** contains 3.4M+ generic sentences about the world, i.e., sentences expressing general truths such as \"Dogs bark,\" and \"Trees remove carbon dioxide from the atmosphere.\" Generics are potentially useful as a knowledge source for AI systems requiring general world knowledge. The GenericsKB is the first large-scale resource containing naturally occurring generic sentences (as opposed to extracted or crowdsourced triples), and is rich in high-quality, general, semantically complete statements. Generics were primarily extracted from three large text sources, namely the Waterloo Corpus, selected parts of Simple Wikipedia, and the ARC Corpus. A filtered, high-quality subset is also available in GenericsKB-Best, containing 1,020,868 sentences.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/genericskb)", "variants": ["GenericsKB"]}
{"id": "CORD-19", "title": "CORD-19: The COVID-19 Open Research Dataset", "contents": "CORD-19 is a free resource of tens of thousands of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses for use by the global research community.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/cord-19)", "variants": ["CORD-19"]}
{"id": "ScienceExamCER", "title": "ScienceExamCER: A High-Density Fine-Grained Science-Domain Corpus for Common Entity Recognition", "contents": "ScienceExamCER is a collection of resources for studying explanation-centered inference, including explanation graphs for 1,680 questions, with 4,950 tablestore rows, and other analyses of the knowledge required to answer elementary and middle-school science questions.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/explanationbank)", "variants": ["ScienceExamCER"]}
{"id": "Countix", "title": "Counting Out Time: Class Agnostic Video Repetition Counting in the Wild", "contents": "Countix is a real world dataset of repetition videos collected in the wild (i.e.YouTube) covering a wide range of semantic settings with significant challenges such as camera and object motion, diverse set of periods and counts, and changes in the speed of repeated actions. Countix include repeated videos of workout activities (squats, pull ups, battle rope training, exercising arm), dance moves (pirouetting, pumping fist), playing instruments (playing ukulele), using tools repeatedly (hammer hitting objects, chainsaw cutting wood, slicing onion), artistic performances (hula hooping, juggling soccer ball), sports (playing ping pong and tennis) and many others. Figure 6 illustrates some examples from the dataset as well as the distribution of repetition counts and period lengths.\r\n\r\nSource: [Counting Out Time: Class Agnostic Video Repetition Counting in the Wild](https://arxiv.org/pdf/2006.15418v1.pdf)", "variants": ["Countix"]}
{"id": "RL Unplugged", "title": "RL Unplugged: A Collection of Benchmarks for Offline Reinforcement Learning", "contents": "RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL Unplugged is designed around the following considerations: to facilitate ease of use, the datasets are provided with a unified API which makes it easy for the practitioner to work with all data in the suite once a general pipeline has been established. This is a dataset accompanying the paper RL Unplugged: Benchmarks for Offline Reinforcement Learning.\r\n\r\nIn this suite of benchmarks, the authors try to focus on the following problems:\r\n\r\n- High dimensional action spaces, for example the locomotion humanoid domains, there are 56 dimensional actions.\r\n- High dimensional observations.\r\n- Partial observability, observations have egocentric vision.\r\n- Difficulty of exploration, using states of the art algorithms and imitation to generate data for difficult environments.\r\n- Real world challenges.\r\n\r\nSource: [DeepMind](https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged)", "variants": ["RL Unplugged"]}
{"id": "Dakshina", "title": "Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset", "contents": "The Dakshina dataset is a collection of text in both Latin and native scripts for 12 South Asian languages. For each language, the dataset includes a large collection of native script Wikipedia text, a romanization lexicon which consists of words in the native script with attested romanizations, and some full sentence parallel data in both a native script of the language and the basic Latin alphabet.\r\n\r\nSource: [Google Research](https://github.com/google-research-datasets/dakshina)", "variants": ["Dakshina"]}
{"id": "ChrEn", "title": "ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization", "contents": "Cherokee-English Parallel Dataset is a low-resource dataset of 14,151 pairs of sentences with around\r\n313K English tokens and 206K Cherokee tokens. The parallel corpus is accompanied by a monolingual Cherokee dataset of 5,120 sentences. Both datasets are mostly derived from Cherokee monolingual books.", "variants": ["ChrEn"]}
{"id": "C4", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "contents": "**C4** is a colossal, cleaned version of Common Crawl's web crawl corpus. It was based on Common Crawl dataset: https://commoncrawl.org. It was used to train the T5 text-to-text Transformer models.\r\n\r\nThe dataset can be downloaded in a pre-processed form from [allennlp](https://github.com/allenai/allennlp/discussions/5056).", "variants": ["C4"]}
{"id": "WikiTableT", "title": "Generating Wikipedia Article Sections from Diverse Data Sources", "contents": "WikiTableT contains Wikipedia article sections and their corresponding tabular data and various metadata. WikiTableT contains millions of instances while covering a broad range of topics and a variety of kinds of generation tasks.\r\n\r\nSource: [WikiTableT](https://github.com/mingdachen/WikiTableT)", "variants": ["WikiTableT"]}
{"id": "AutoWeakS", "title": "Recommending Courses in MOOCs for Jobs: An Auto Weak Supervision Approach", "contents": "Collects all the courses from XuetangX5, one of the largest MOOCs in China, and this results in 1951 courses. The collected courses involve seven areas: computer science, economics, engineering, foreign language, math, physics, and social science. Each course contains 131 words in its descriptions on average. Contains 706 job postings from the recruiting website operated by JD.com (JD) and 2,456 job postings from the website owned by Tencent corporation (Tencent). The collected job postings involve six areas: technical post, financial post, product post, design post, market post, supply chain and engineering post. \r\n\r\nSource: [AutoWeakS](https://github.com/jerryhao66/AutoWeakS)", "variants": ["AutoWeakS"]}
{"id": "GazeFollow", "title": "Where are they looking?", "contents": "GazeFollow is a large-scale dataset annotated with the location of where people in images are looking. It uses several major datasets that contain people as a source of images: 1, 548 images from SUN, 33, 790 images from MS COCO, 9, 135 images from Actions 40, 7, 791 images from PASCAL, 508 images from the ImageNet detection challenge and 198, 097 images from the Places dataset. This concatenation results in a challenging and large image collection of people performing diverse activities in many everyday scenarios.\r\n\r\nSource: [GazeFollow](http://gazefollow.csail.mit.edu/index.html)", "variants": ["GazeFollow"]}
{"id": "iQIYI-VID-2019", "title": "", "contents": "iQIYI-VID-2019 dataset is the first video dataset for multi-model person identification. This dataset aims to encourage the research of multi-modal based person identification. To get close to real applications, video clips are extracted from real online videos of extensive types. All the clips are labeled by human annotators, and use automatic algorithms to accelerate the collection and labeling process. The iQIYI-VID-2019 dataset is more challenging comparing to the iQIYI-VID-2018 dataset, since most hard examples are selected from iQIYI-VID-2018 while more person ids is added. The dataset contains 100K~200K video clips, divided into three parts, 40% for training, 30% for validation, and 30% for test. The dataset contains about 10, 000 identities, 5,000 of which come from the iQIYI celebrity database and mainly extracts from iQIYI-VID-2018.\r\n\r\nSource: [2019 iQIYI Celebrity Video Identification Challenge](http://challenge.ai.iqiyi.com/detail?raceId=5c767dc41a6fa0ccf53922e7)", "variants": ["iQIYI-VID-2019"]}
{"id": "ELFW", "title": "Extended Labeled Faces in-the-Wild (ELFW): Augmenting Classes for Face Segmentation", "contents": "Extended Labeled Faces in-the-Wild (ELFW) is a dataset supplementing with additional face-related categories —and also additional faces— the originally released semantic labels in the vastly used Labeled Faces in-the-Wild (LFW) dataset. Additionally, two object-based data augmentation techniques are deployed to synthetically enrich under-represented categories which, in benchmarking experiments, reveal that not only segmenting the augmented categories improves, but also the remaining ones benefit.\r\n\r\nSource: [Extended Labeled Faces in-the-Wild ](https://multimedia-eurecat.github.io/2020/06/22/extended-faces-in-the-wild.html)\r\nImage Source: [https://multimedia-eurecat.github.io/2020/06/22/extended-faces-in-the-wild.html](https://multimedia-eurecat.github.io/2020/06/22/extended-faces-in-the-wild.html)", "variants": ["ELFW"]}
{"id": "KANFace", "title": "Investigating Bias in Deep Face Analysis: The KANFace Dataset and Empirical Study", "contents": "KANFace consists of 40K still images and 44K sequences (14.5M video frames in total) captured in unconstrained, real-world conditions from 1,045 subjects. The dataset is manually annotated in terms of identity, exact age, gender and kinship.\r\n\r\nSource: [KANFace Dataset](https://sites.google.com/view/kanface-dataset)", "variants": ["KANFace"]}
{"id": "BAVL", "title": "", "contents": "Blind Audio-Visual Localization (BAVL) Dataset consists of 20 audio-visual recordings of  sound sources, which could be talking faces or music instruments. Most audio-visual recordings (19) are videos from Youtube except V8, which is from [1]. Besides, the video V7 was also used in[2][3], and V16 used in [3]. All 20 videos are annotated by ourselves in a uniform manner. Details of the video sequences are listed in Table 1. \r\n\r\nThe videos in the dataset have average duration of 10 seconds, and they are all recorded by one camera and one microphone. The audio files (.wav) was sampled at a 16 kHz for V7, V8, V16, and 44.1 kHz for the rest. The video frames contain the sound-making object (sound source) and distracting objects (e.g. pedestrian on the street), while the audio signals  consists of the sound produced by the sound source (human speech or instrumental music), environmental noise and sometimes other sounds. The distracting objects and other irrelevant noise/sounds do not exist in all videos. The primary usage of the dataset is to evaluate the performance of sound source localization method, in the presence of distracting motions and noise.\r\n\r\n[1] Kidron, Einat, Yoav Y. Schechner, and Michael Elad. \"Pixels that sound.\"Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. Vol. 1. IEEE, 2005.\r\n\r\n[2] Izadinia, Hamid, Imran Saleemi, and Mubarak Shah. \"Multimodal analysis for identification and segmentation of moving-sounding objects.\"IEEE Transactions on Multimedia 15.2 (2013): 378-390.\r\n\r\n[3] Li, Kai, Jun Ye, and Kien A. Hua. \"What's making that sound?.\"Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014.\r\n\r\nSource: [Sound of Pixels](https://ibug.doc.ic.ac.uk/resources/SOP/)", "variants": ["BAVL"]}
{"id": "Fabrics Dataset", "title": "", "contents": "The Fabrics Dataset consists of about 2000 samples of garments and fabrics. A small patch of each surface has been captured under 4 different illumination conditions using a custom made, portable photometric stereo sensor. All images have been acquired \"in the field\" (at clothes shops) and the dataset reflects the distribution of fabrics in real world, hence it is not balanced. The majority of clothes are made of specific fabrics, such as cotton and polyester, while some other fabrics, such as silk and linen, are more rare. Also, a large number of clothes are not composed of a single fabric but two or more fabrics are used to give the garment the desired properties (blended fabrics). For every garment there is information (attributes) about its material composition from the manufacturer label and its type (pants, shirt, skirt etc.).\r\n\r\nSource: [Fabrics Dataset](https://ibug.doc.ic.ac.uk/resources/fabrics/)", "variants": ["Fabrics Dataset"]}
{"id": "LSFM", "title": "", "contents": "The Large Scale Facial Model (LSFM) is a 3D statistical model of facial shape built from nearly 10,000 individuals.\r\n\r\nSource: [LSFM](https://ibug.doc.ic.ac.uk/resources/lsfm/)", "variants": ["LSFM"]}
{"id": "AgeDB", "title": "", "contents": "AgeDB contains 16, 488 images of various famous people, such as actors/actresses, writers, scientists, politicians, etc. Every image is annotated with respect to the identity, age and gender attribute. There exist a total of 568 distinct subjects. The average number of images per subject is 29. The minimum and maximum age is 1 and 101, respectively. The average age range for each subject is 50.3 years.\r\n\r\nSource: [AgeDB](https://ibug.doc.ic.ac.uk/resources/agedb/)", "variants": ["AgeDB"]}
{"id": "AFEW-VA", "title": "", "contents": "The AFEW-VA databaset is a collection of highly accurate per-frame annotations levels of valence and arousal, along with per-frame annotations of 68 facial landmarks for 600 challenging video clips. These clips are extracted from feature films and were also annotated in terms of discrete emotion categories in the form of the AFEW database (that can be obtained [there](https://cs.anu.edu.au/few/AFEW.html)).\r\n\r\nSource: [AFEW-VA](https://ibug.doc.ic.ac.uk/resources/afew-va-database/)", "variants": ["AFEW-VA"]}
{"id": "KILT", "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks", "contents": "KILT (Knowledge Intensive Language Tasks) is a benchmark consisting of 11 datasets representing 5 types of tasks:\r\n\r\n* Fact-checking (FEVER),\r\n* Entity linking (AIDA CoNLL-YAGO, WNED-WIKI, WNED-CWEB),\r\n* Slot filling (T-Rex, Zero Shot RE),\r\n* Open domain QA (Natural Questions, HotpotQA, TriviaQA, ELI5),\r\n* Dialog generation (Wizard of Wikipedia).\r\n\r\nAll these datasets have been grounded in a single pre-processed wikipedia snapshot, allowing for fairer and more consistent evaluation as well as enabling new task setups such as multitask and transfer learning.\r\n\r\nSource: [KILT Benchmarking](https://ai.facebook.com/tools/kilt/)", "variants": ["KILT", "KILT: AIDA-YAGO2", "KILT: ELI5", "KILT: FEVER", "KILT: HotpotQA", "KILT: Natural Questions", "KILT: T-REx", "KILT: TriviaQA", "KILT: WNED-CWEB", "KILT: WNED-WIKI", "KILT: Wizard of Wikipedia", "KILT: Zero Shot RE"]}
{"id": "SOREL-20M", "title": "SOREL-20M: A Large Scale Benchmark Dataset for Malicious PE Detection", "contents": "SOREL-20M is a large-scale dataset consisting of nearly 20 million files with pre-extracted features and metadata, high-quality labels derived from multiple sources, information about vendor detections of the malware samples at the time of collection, and additional “tags” related to each malware sample to serve as additional targets.\r\n\r\nSource: [SOREL-20M](https://github.com/sophos-ai/SOREL-20M)", "variants": ["SOREL-20M"]}
{"id": "ORVS", "title": "Transfer Learning Through Weighted Loss Function and Group Normalization for Vessel Segmentation from Retinal Images", "contents": "The ORVS dataset has been newly established as a collaboration between the computer science and visual-science departments at the University of Calgary.\r\n\r\nThis dataset contains 49 images (42 training and seven testing images) collected from a clinic in Calgary-Canada. All images were acquired with a Zeiss Visucam 200 with 30 degrees field of view (FOV). The image size is 1444×1444 with 24 bits per pixel. Images and are stored in JPEG format with low compression, which is common in ophthalmology practice. All images were manually traced by an expert who a has been working in the field of retinal-image analysis and went through training. The expert was asked to label all pixels belonging to retinal vessels. The Windows Paint 3D tool was used to manually label the images.\r\n\r\nSource: [Transfer Learning Through Weighted Loss Function and Group Normalization for Vessel Segmentation from Retinal Images](https://arxiv.org/pdf/2012.09250v1.pdf)", "variants": ["ORVS"]}
{"id": "DR HAGIS", "title": "", "contents": "The DR HAGIS database has been created to aid the development of vessel extraction algorithms suitable for retinal screening programmes. Researchers are encouraged to test their segmentation algorithms using this database.\r\n\r\nAll thirty-nine fundus images were obtained from a diabetic retinopathy screening programme in the UK. Hence, all images were taken from diabetic patients. Since patients attending these screening programmes exhibit other co-morbidities, the DR HAGIS database consists of following four co-morbidity subgroups:\r\n\r\n- Images 1-10: Glaucoma subgroup\r\n- Images 11-20: Hypertension subgroup\r\n- Images 21-30: Diabetic retinopathy subgroup\r\n- Images 31-40: Age-related macular degeneration subgroup\r\n\r\nBesides the fundus images, the manual segmentation of the retinal surface vessels is provided by an expert grader. These manually segmented images can be used as the ground truth to compare and assess the automatic vessel extraction algorithms. Masks of the FOV are provided as well to quantify the accuracy of vessel extraction within the FOV only.\r\n\r\nThe images were acquired in different screening centers, therefore reflecting the range of image resolutions, digital cameras and fundus cameras used in the clinic. The fundus images were captured using a Topcon TRC-NW6s, Topcon TRC-NW8 or a Canon CR DGi fundus camera with a horizontal 45 degree field-of-view (FOV). The images are 4752x3168 pixels, 3456x2304 pixels, 3126x2136 pixels, 2896x1944 pixels or 2816x1880 pixels in size.\r\n\r\nThe fundus images are saved as compressed JPEG files with 8 bits per colour plane. The ground truth and mask images are saved as binary PNG files.\r\n\r\nSource: [DR HAGIS](https://personalpages.manchester.ac.uk/staff/niall.p.mcloughlin/)", "variants": ["DR HAGIS"]}
{"id": "ARIA", "title": "", "contents": "This data set was collected in 2004 to 2006 in the United Kingdom. Subjects were adult males and females, some of whom were healthy (control group), some with age-related macular degeneration (AMD group), and some were diabetic patients (diabetic group). Unfortunately, no other information from this time exists about this subjects.\r\n\r\nSource: [ARIA](http://www.damianjjfarnell.com/?page_id=276)", "variants": ["ARIA"]}
{"id": "VICAVR", "title": "", "contents": "The VICAVR database is a set of retinal images used for the computation of the A/V Ratio. The database currently includes 58 images. The images have been acquired with a TopCon non-mydriatic camera NW-100 model and are optic disc centered with a resolution of 768x584. The database includes the caliber of the vessels measured at different radii from the optic disc as well as the vessel type (artery/vein) labelled by three experts.\r\n\r\nSource: [VICAVR](http://www.varpa.es/research/ophtalmology.html#vicavr)", "variants": ["VICAVR"]}
{"id": "CLOUD", "title": "", "contents": "The CLOUD dataset is a set of Optical Coherence Tomography of the Anterior Segment images (AS-OCT) used to the automatic identification and representation of the cornea-contact lens relationship. The dataset includes 112 AS-OCT images that were captured from 16 different patients. In particular, the images were obtained by an OCT Cirrus 500 scanner model of Carl Zeiss Meditec with an anterior segment module for users of scleral contact lens (SCL).\r\n\r\nSource: [CLOUD Dataset](http://www.varpa.es/research/ophtalmology.html#cloud)", "variants": ["CLOUD"]}
{"id": "MESSIDOR", "title": "", "contents": "The Messidor database has been established to facilitate studies on computer-assisted diagnoses of diabetic retinopathy. The research community is welcome to test its algorithms on this database. In this section, you will find instructions on how to download the database.\r\n\r\nSource: [MESSIDOR](http://www.adcis.net/en/third-party/messidor/)", "variants": ["Messidor", "MESSIDOR"]}
{"id": "DIARETDB1", "title": "", "contents": "The database consists of 89 colour fundus images of which 84 contain at least mild non-proliferative signs (Microaneurysms) of the diabetic retinopathy, and 5 are considered as normal which do not contain any signs of the diabetic retinopathy according to all experts who participated in the evaluation. Images were captured using the same 50 degree field-of-view digital fundus camera with varying imaging settings. The data correspond to a good (not necessarily typical) practical situation, where the images are comparable, and can be used to evaluate the general performance of diagnostic methods. This data set is referred to as \"calibration level 1 fundus images\".\r\n\r\nSource: [DIARETDB1](http://www2.it.lut.fi/project/imageret/diaretdb1/)", "variants": ["DIARETDB1"]}
{"id": "UDA-CH", "title": "An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork Recognition in Cultural Sites", "contents": "UDA-CH contains 16 objects that cover a variety of artworks which can be found in a museum like sculptures, paintings and books. Specifically, the dataset has been collected inside the cultural site “Galleria Regionale di Palazzo Bellomo” located in Siracusa, Italy.\r\n\r\nSource: [An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork Recognition in Cultural Sites](https://arxiv.org/pdf/2008.01882.pdf)", "variants": ["UDA-CH"]}
{"id": "MAP", "title": "", "contents": "**Maybe Ambiguous Pronoun** is a dataset similar to [GAP](/dataset/gap-coreference-dataset) dataset, but without binary gender constraints.\r\n\r\nSource: [Toward Gender-Inclusive Coreference Resolution](https://www.aclweb.org/anthology/2020.acl-main.418.pdf)", "variants": ["MAP"]}
{"id": "ImageNet-P", "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations", "contents": "**ImageNet-P** consists of noise, blur, weather, and digital distortions. The dataset has validation perturbations; has difficulty levels; has CIFAR-10, Tiny ImageNet, ImageNet 64 × 64, standard, and Inception-sized editions; and has been designed for benchmarking not training networks. ImageNet-P departs from ImageNet-C by having perturbation sequences generated from each ImageNet validation image. Each sequence contains more than 30 frames, so to counteract an increase in dataset size and evaluation time only 10 common perturbations are used.\r\n\r\nSource: [Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://arxiv.org/pdf/1903.12261.pdf)", "variants": ["ImageNet-P"]}
{"id": "Combinatorial 3D Shape Dataset", "title": "Combinatorial 3D Shape Generation via Sequential Assembly", "contents": "The combinatorial 3D shape dataset is composed of 406 instances of 14 classes. Specifically, each object in the dataset is considered equivalent to a sequence of primitive placement.\r\n\r\nSource: [Combinatorial 3D Shape Generation via Sequential Assembly](https://arxiv.org/pdf/2004.07414.pdf)", "variants": ["Combinatorial 3D Shape Dataset"]}
{"id": "Chart2Text", "title": "Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model", "contents": "Chart2Text is a dataset that was crawled from 23,382 freely accessible pages from statista.com in early March of 2020, yielding a total of 8,305 charts, and associated summaries. For each chart, the chart image, the underlying data table, the title, the axis labels, and a human-written summary describing the statistic was downloaded.\r\n\r\nSource: [Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model](https://arxiv.org/pdf/2010.09142.pdf)", "variants": ["Chart2Text"]}
{"id": "DENSE", "title": "Learning Monocular Dense Depth from Events", "contents": "DENSE (Depth Estimation oN Synthetic Events) is a new dataset with synthetic events and perfect ground truth.\r\n\r\nSource: [Learning Monocular Dense Depth from Events](http://rpg.ifi.uzh.ch/docs/3DV20_Hidalgo.pdf)", "variants": ["Dense-Haze", "DENSE"]}
{"id": "VLEP", "title": "What is More Likely to Happen Next? Video-and-Language Future Event Prediction", "contents": "VLEP contains 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips. Each example (see Figure 1) consists of a Premise Event (a short video clip with dialogue), a Premise Summary (a text summary of the premise event), and two potential natural language Future Events (along with Rationales) written by people. These clips are on average 6.1 seconds long and are harvested from diverse event-rich sources, i.e., TV show and YouTube Lifestyle Vlog videos.\r\n\r\nSource: [What is More Likely to Happen Next? Video-and-Language Future Event Prediction](https://arxiv.org/pdf/2010.07999.pdf)", "variants": ["VLEP"]}
{"id": "UCC", "title": "Six Attributes of Unhealthy Conversation", "contents": "The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.\r\n\r\nEach comment is labelled as either 'healthy' or 'unhealthy', in addition to binary labels for the presence of six potentially 'unhealthy' sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation. Each label also has an associated confidence score.\r\n\r\nThe UCC contributes further high quality data on attributes like sarcasm, hostility, and condescension, adding to existing datasets on these and related attributes, and provides the first dataset of this scale with labels for dismissiveness, unfair generalisations, antagonistic behavior, and overall assessments of whether those comments fall within 'healthy' conversation.\r\n\r\nSource: [UCC ](https://github.com/conversationai/unhealthy-conversations)", "variants": ["UCC"]}
{"id": "Satire Dataset", "title": "A Multi-Modal Method for Satire Detection using Textual and Visual Cues", "contents": "The satire dataset is a new multi-modal dataset of satirical and regular news articles. The satirical news is collected from four websites that explicitly declare themselves to be satire, and the regular news is collected from six mainstream news websites. Specifically, the satirical news websites the articles were collected from are The Babylon Bee, Clickhole, Waterford Whisper News, and The DailyER. The regular news websites are Reuters, The Hill, Politico, New York Post, Huffington Post, and Vice News. The headlines\r\nand the thumbnail images of the latest 1000 articles for each of the publications are collected. The dataset contains a total of 4000 satirical and 6000 regular news articles.\r\n\r\nSource: [A Multi-Modal Method for Satire Detection using Textual and Visual Cues](https://www.aclweb.org/anthology/2020.nlp4if-1.4.pdf)", "variants": ["Satire Dataset"]}
{"id": "OCNLI", "title": "OCNLI: Original Chinese Natural Language Inference", "contents": "OCNLI stands for Original Chinese Natural Language Inference. It is corpus for Chinese Natural Language Inference, collected following closely the procedures of MNLI, but with enhanced strategies aiming for more challenging inference pairs. No human/machine translation is used in creating the dataset, and thus the Chinese texts are original and not translated.\r\n\r\nOCNLI has roughly 50k pairs for training, 3k for development and 3k for test. Only the test data is released but not its labels.\r\n\r\nOCNLI is part of the CLUE benchmark.\r\n\r\nSource: [OCNLI](https://github.com/CLUEbenchmark/OCNLI)", "variants": ["OCNLI"]}
{"id": "QReCC", "title": "Open-Domain Question Answering Goes Conversational via Question Rewriting", "contents": "QReCC contains 14K conversations with 81K question-answer pairs. QReCC is built on questions from TREC CAsT, QuAC and Google Natural Questions. While TREC CAsT and QuAC datasets contain multi-turn conversations, Natural Questions is not a conversational dataset. Questions in NQ dataset were used as prompts to create conversations explicitly balancing types of context-dependent questions, such as anaphora (co-references) and ellipsis.\r\n\r\nFor each query the authors collect query rewrites by resolving references, the resulting query rewrite is a context-independent version of the original (context-dependent) question. The rewritten query is then used to with a search engine to answer the question. Each query is also annotated with answer, link to the web page that used to produce the answer.\r\n\r\nEach conversation in the dataset contains a unique Conversation_no, Turn_no unique within a conversation, the original Question, Context, Rewrite and Answer with Answer_URL.\r\n\r\nSource: [QReCC](https://github.com/apple/ml-qrecc)", "variants": ["QReCC"]}
{"id": "VAST", "title": "Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations", "contents": "VAST consists of a large range of topics covering broad themes, such as politics (e.g., ‘a Palestinian state’), education (e.g., ‘charter schools’), and public health (e.g., ‘childhood vaccination’). In addition, the data includes a wide range of similar expressions (e.g., ‘guns on campus’ versus ‘firearms on campus’). This variation captures how humans might realistically describe the same topic and contrasts with the lack of variation in existing datasets.\r\n\r\nSource: [Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations](https://arxiv.org/pdf/2010.03640v1.pdf)", "variants": ["VAST"]}
{"id": "Silent Speech EMG", "title": "Digital Voicing of Silent Speech", "contents": "Facial electromyography recordings during both silent and vocalized speech.\r\n\r\nSource: [Silent Speech EMG](https://github.com/dgaddy/silent_speech)", "variants": ["Silent Speech EMG"]}
{"id": "SMOT", "title": "Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images", "contents": "The SMOT dataset, Single sequence-Multi Objects Training, is collected to represent a practical scenario of collecting training images of new objects in the real world, i.e. a mobile robot with an RGB-D camera collects a sequence of frames while driving around a table to learning multiple objects and tries to recognize objects in different locations.\r\n\r\nSource: [SMOT](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/smot/)", "variants": ["SMOT"]}
{"id": "3DNet", "title": "", "contents": "The 3DNet dataset is a free resource for object class recognition and 6DOF pose estimation from point cloud data. 3DNet provides a large-scale hierarchical CAD-model databases with increasing numbers of classes and difficulty with 10, 60 and 200 object classes together with evaluation datasets that contain thousands of scenes captured with an RGB-D sensor.\r\n\r\nSource: [3DNet](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/3dnet-dataset/)", "variants": ["3DNet"]}
{"id": "LfED-6D", "title": "", "contents": "The LfED-6D dataset is a collection of 6D grasp annotations acquired through experience (with a robot platform) or by human demonstration. For known objects, the annotated grasps can be directly applied given the pose of the object model is correctly computed. For unknown objects, the grasps can be generalized using methods for shape matching, for example the Dense Geometrical Correspondence Network.\r\n\r\nSource: [LfED-6D](https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/lfed-6d-dataset/)", "variants": ["LfED-6D"]}
{"id": "NYU-VP", "title": "", "contents": "NYU-VP is a new dataset for multi-model fitting, vanishing point (VP) estimation in this case. Each image is annotated with up to eight vanishing points, and pre-extracted line segments are provided which act as data points for a robust estimator. Due to its size, the dataset is the first to allow for supervised learning of a multi-model fitting task.\r\n\r\nSource: [CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus](https://arxiv.org/pdf/2001.02643.pdf)", "variants": ["NYU-VP"]}
{"id": "NText", "title": "", "contents": "NText is an eight million words dataset extracted and preprocessed from nuclear research papers and thesis.\r\n\r\nSource: [NukeBERT: A Pre-trained language model for Low Resource Nuclear Domain](https://arxiv.org/pdf/2003.13821.pdf)", "variants": ["NText"]}
{"id": "EHR-Rel", "title": "Biomedical Concept Relatedness -- A large EHR-based benchmark", "contents": "EHR-RelB is a benchmark dataset for biomedical concept relatedness, consisting of 3630 concept pairs sampled from electronic health records (EHRs). EHR-RelA is a smaller dataset of 111 concept pairs, which are mainly unrelated.\r\n\r\nSource: [EHR-Rel](https://github.com/babylonhealth/EHR-Rel)", "variants": ["EHR-Rel"]}
{"id": "MLGESTURE DATASET", "title": "Low-latency hand gesture recognition with a low resolution thermal imager", "contents": "MlGesture is a dataset for hand gesture recognition tasks, recorded in a car with 5 different sensor types at two different viewpoints. The dataset contains over 1300 hand gesture videos from 24 participants and features 9 different hand gesture symbols. One sensor cluster with five different cameras is mounted in front of the driver in the center of the dashboard. A second sensor cluster is mounted on the ceiling looking straight down.\r\n\r\nSource: [MLGESTURE DATASET](https://iiw.kuleuven.be/onderzoek/eavise/mlgesture/home)", "variants": ["MLGESTURE DATASET"]}
{"id": "NYT-H", "title": "Towards Accurate and Consistent Evaluation: A Dataset for Distantly-Supervised Relation Extraction", "contents": "NYT-H is a dataset for distantly-supervised relation extraction, in which DS-labelled training data is used and several annotators to label test data are hired. NYT-H can serve as a benchmark of distantly-supervised relation extraction.\r\n\r\nSource: [Towards Accurate and Consistent Evaluation: A Dataset for Distantly-Supervised Relation Extraction](https://www.aclweb.org/anthology/2020.coling-main.566.pdf)", "variants": ["NYT-H"]}
{"id": "CSAW-S", "title": "Adding Seemingly Uninformative Labels Helps in Low Data Regimes", "contents": "CSAW-S is a dataset of mammography images which includes expert annotations of tumors and non-expert annotations of breast anatomy and artifacts in the image.\r\n\r\nSource: [CSAW-S](https://arxiv.org/pdf/2008.00807.pdf)", "variants": ["CSAW-S"]}
{"id": "UNDD", "title": "What's There in the Dark", "contents": "UNDD consists of 7125 unlabelled day and night images; additionally, it has 75 night images with pixel-level annotations having classes equivalent to Cityscapes dataset.\r\n\r\nSource: [UNDD](https://github.com/sauradip/night_image_semantic_segmentation)\r\nImage Source: [https://github.com/sauradip/night_image_semantic_segmentation](https://github.com/sauradip/night_image_semantic_segmentation)", "variants": ["UNDD"]}
{"id": "Pesteh-Set", "title": "Detecting and Counting Pistachios based on Deep Learning", "contents": "Pesteh-Set is made of two parts. The first part includes 423 images with ground truth. The pistachios are sorted into two classes: Open-mouth and closed-mouth. The ground truth of the images is a CSV file that consists of the bounding boxes of the two classes of pistachios in the images. There are between 1 to 27 pistachios in each image, and 3927 pistachios in total. The second part includes 6 videos with a total length of 167 seconds and 561 moving pistachios.\r\n\r\nSource: [Pesteh-Set](https://github.com/mr7495/Pesteh-Set)", "variants": ["Pesteh-Set"]}
{"id": "CelebAGaze", "title": "Dual In-painting Model for Unsupervised Gaze Correction and Animation in the Wild", "contents": "CelebAGaze consists of 25283 high-resolution celebrity images that are collected from CelebA and the Internet. It consists of 21832 face images with eyes staring at the camera and 3451 face images with eyes staring somewhere else. All images (256 × 256) are cropped and the eye mask region by dlib is computed. Specifically, dlib is used to extract 68 facial landmarks and calculate the mean of 6 points near the eye region, which will be the center point of the mask. The size of the mask is fixed to 30×50. As described above, 300 samples from domain Y are randomly selected, 100 samples from domain X as the test set, the remaining as the training set. Note that this dataset is unpaired and it is not labeled with the specific eye angle or the head pose information.\r\n\r\nSource: [Dual In-painting Model for Unsupervised Gaze Correction and Animation in the Wild](https://arxiv.org/pdf/2008.03834.pdf)", "variants": ["CelebAGaze"]}
{"id": "EVE", "title": "Towards End-to-end Video-based Eye-Tracking", "contents": "EVE (End-to-end Video-based Eye-tracking) is a dataset for eye-tracking. It is collected from 54 participants and consists of 4 camera views, over 12 million frames and 1327 unique visual stimuli (images, video, text), adding up to approximately 105 hours of video data in total.\r\n\r\nOfficial competition on Codalab: [https://competitions.codalab.org/competitions/28954](https://competitions.codalab.org/competitions/28954)", "variants": ["EVE"]}
{"id": "ForecastQA", "title": "ForecastQA: A Question Answering Challenge for Event Forecasting", "contents": "ForecastQA is a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. The forecasting problem for this dataset is formulated as a restricted-domain, multiple-choice, question-answering (QA) task that simulates the forecasting scenario.\r\n\r\nSource: [ForecastQA: A Question Answering Challenge for Event Forecasting](https://arxiv.org/abs/2005.00792)", "variants": ["ForecastQA"]}
{"id": "TSU", "title": "", "contents": "Toyota Smarthome Untrimmed (TSU) is a dataset for activity detection in long untrimmed videos. The dataset contains 536 videos with an average duration of 21 mins. Since this dataset is based on the same footage video as Toyota Smarthome Trimmed version, it features the same challenges and introduces additional ones. The dataset is annotated with 51 activities.\r\n\r\nThe dataset has been recorded in an apartment equipped with 7 Kinect v1 cameras. It contains common daily living activities of 18 subjects. The subjects are senior people in the age range 60-80 years old. The dataset has a resolution of 640×480 and offers 3 modalities: RGB + Depth + 3D Skeleton. The 3D skeleton joints were extracted from RGB. For privacy-preserving reasons, the face of the subjects is blurred.\r\n\r\nSource: [Toyota Smarthome](https://project.inria.fr/toyotasmarthome/)", "variants": ["TSU"]}
{"id": "AUTSL", "title": "AUTSL: A Large Scale Multi-modal Turkish Sign Language Dataset and Baseline Methods", "contents": "The Ankara University Turkish Sign Language Dataset (AUTSL) is a large-scale, multimode dataset that contains isolated Turkish sign videos. It contains 226 signs that are performed by 43 different signers. There are 38,336 video samples in total. The samples are recorded using Microsoft Kinect v2 in RGB, depth and skeleton formats. The videos are provided at a resolution of 512×512. The skeleton data contains spatial coordinates, i.e. (x, y), of the 25 junction points on the signer body that are aligned with 512×512 data.\r\n\r\nSource: [AUTSL Dataset](http://cvml.ankara.edu.tr/datasets/)", "variants": ["AUTSL"]}
{"id": "DRealSR", "title": "Component Divide-and-Conquer for Real-World Image Super-Resolution", "contents": "DRealSR establishes a Super Resolution (SR) benchmark with diverse real-world degradation processes, mitigating the limitations of conventional simulated image degradation. \r\n\r\nIt has been collected from five DSLR cameras in natural scenes and cover indoor and outdoor scenes avoiding moving objects, e.g., advertising posters, plants, offices, buildings. The training images are cropped into 380×380, 272×272 and 192×192 patches, resulting in 31,970 patches.\r\n\r\n\r\nSource: [Component Divide-and-Conquer for Real-World Image Super-Resolution](https://arxiv.org/abs/2008.01928)", "variants": ["DRealSR"]}
{"id": "EDEN", "title": "EDEN: Multimodal Synthetic Dataset of Enclosed GarDEN Scenes", "contents": "EDEN (Enclosed garDEN) is a multimodal synthetic dataset, a dataset for nature-oriented applications. The dataset features more than 300K images captured from more than 100 garden models. Each image is annotated with various low/high-level vision modalities, including semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. \r\n\r\nSource: [EDEN: Multimodal Synthetic Dataset of Enclosed GarDEN Scenes](https://arxiv.org/abs/2011.04389)\r\nImage Source: [https://lhoangan.github.io/eden/](https://lhoangan.github.io/eden/)", "variants": ["EDEN"]}
{"id": "Wiki-CS", "title": "Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks", "contents": "Wiki-CS is a Wikipedia-based dataset for benchmarking Graph Neural Networks. The dataset is constructed from Wikipedia categories, specifically 10 classes corresponding to branches of computer science, with very high connectivity. The node features are derived from the text of the corresponding articles.  They were calculated as the average of pretrained GloVe word embeddings (Pennington et al., 2014), resulting in 300-dimensional node features.\r\n\r\nThe dataset has 11,701 nodes and 216,123 edges.\r\n\r\nSource: [Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks](https://arxiv.org/abs/2007.02901)\r\nImage Source: [https://arxiv.org/pdf/2007.02901v1.pdf](https://arxiv.org/pdf/2007.02901v1.pdf)", "variants": ["Wiki-CS"]}
{"id": "ChaosNLI", "title": "What Can We Learn from Collective Human Opinions on Natural Language Inference Data?", "contents": "Chaos NLI is a Natural Language Inference (NLI) dataset with 100 annotations per example (for a total of 464,500 annotations) for some existing data points in the development sets of SNLI, MNLI, and Abductive NLI. The dataset provides additional labels for NLI annotations that reflect the distribution of human annotators, instead of picking the majority label as the gold standard label.\r\n\r\nSource: [ChaosNLI Github Repository](https://github.com/easonnie/ChaosNLI)", "variants": ["ChaosNLI"]}
{"id": "VideoForensicsHQ", "title": "VideoForensicsHQ: Detecting High-quality Manipulated Face Videos", "contents": "VideoForensicsHQ is a benchmark dataset for face video forgery detection, providing high quality visual manipulations.  It is one of the first face video manipulation benchmark sets that also contains audio and thus complements existing datasets along a new challenging dimension. VideoForensicsHQ shows manipulations at much higher video quality and resolution, and shows manipulations that are provably much harder to detect by humans than videos in other datasets. \r\n\r\nVideoForensicsHQ contains 1,737 videos of speaking faces (44% male, 56% female), with 8 different emotions, most of them of “HD” resolution. The videos amount to 1,666,816 frames.\r\n\r\nSource: [VideoForensicsHQ: Detecting High-quality Manipulated Face Videos](https://arxiv.org/abs/2005.10360)", "variants": ["VideoForensicsHQ"]}
{"id": "SOLO", "title": "SOLO: A Corpus of Tweets for Examining the State of Being Alone", "contents": "The SOLO Corpus comprises over 4 million English tweets, each of which contains at least one of the following tokens: solitude, lonely, and loneliness. The corpus has been collected to analyze the language and emotions associated with the state of being alone in English tweets.\r\n\r\nTweets related to the state of being alone were collected by polling the Twitter API from August 28, 2018 to July 10, 2019 with the following query terms: loneliness, lonely, and solitude. Duplicate tweets, short tweets (containing less than three words), and tweets with external URLs were discarded. Further, only up to three tweets per user are kept. This minimizes the impact of prolific tweeters and bots on the corpus.\r\n\r\nSource: [SOLO: A Corpus of Tweets for Examining the State of Being Alone](https://arxiv.org/abs/2006.03096)", "variants": ["SOLO"]}
{"id": "EXPO-HD", "title": "EXPO-HD: Exact Object Perception usingHigh Distraction Synthetic Data", "contents": "The EXPO-HD Dataset is a dataset of Expo whiteboard markers for the purpose of instance segmentation. The dataset contains two subsets (both include instances segmentation labels):\r\n\r\n* Photorealistic synthetic image dataset with 5000 images.\r\n* Real image dataset with 200 images (used for validation and test).\r\n\r\nThe dataset can be used for testing domain adaptation techniques, as the training set consists of only synthetic images, and the validation and test sets consist of real images.\r\n\r\nSource: [EXPO-HD: Exact Object Perception using High Distraction Synthetic Data](https://arxiv.org/abs/2007.14354)", "variants": ["EXPO-HD"]}
{"id": "KACC", "title": "KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion", "contents": "The KACC benchmark consists of three subtasks that can be applied to knowledge graphs: knowledge abstraction, knowledge concretization and knowledge completion. \r\n\r\n- The **knowledge abstraction** subtask contains tasks of concept inference, schema prediction and concept graph completion on the two-view KG. \r\n- The **knowledge concretization** subtask requires models to do entity graph completion based on the two subgraphs. The concretization ability can be further examined by the results of long-tail entity link prediction. \r\n- The **knowledge completion** subtask consists of typical single-view knowledge graph completion tasks for each subgraph.\r\n\r\nKACC contains 999,902 entities in the entity graph, with 691 types of relations . The concept graph contains 21,293 concepts with 198 types of meta-relations. There are 2,367,971 cross-links between the two.\r\n\r\nSource: [KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion](https://arxiv.org/abs/2004.13631)", "variants": ["KACC"]}
{"id": "InterHand2.6M", "title": "InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image", "contents": "The InterHand2.6M dataset is a large-scale real-captured dataset with accurate GT 3D interacting hand poses, used for 3D hand pose estimation The dataset contains 2.6M labeled single and interacting hand frames.\r\n\r\nSource: [InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image](https://arxiv.org/abs/2008.09309)", "variants": ["InterHand2.6M"]}
{"id": "TaxiNLI", "title": "TaxiNLI: Taking a Ride up the NLU Hill", "contents": "TaxiNLI is a dataset collected based on the principles and categorizations of the aforementioned taxonomy. A subset of examples are curated from MultiNLI (Williams et al., 2018) by sampling uniformly based on the entailment label and the domain. The dataset is annotated with finegrained category labels.\r\n\r\nSource: [TaxiNLI: Taking a Ride up the NLU Hill](https://www.aclweb.org/anthology/2020.conll-1.4.pdf)", "variants": ["TaxiNLI"]}
{"id": "AllMusic Mood Subset", "title": "Mood Classification Using Listening Data", "contents": "The AllMusic Mood Subset (AMS) is a dataset for mood classification from songs. It is created by matching a subset of the Million Song Dataset (MSD), totalling 67k tracks, with expert annotations of 188 different moods collected from AllMusic.\r\n\r\nSince the AMS is a subset of the MSD, the audio data is gathered by obtaining the 7-digital 30 second previews associated with all MSD tracks. These are 128kbps mp3 stereo\r\nfiles sampled at 44.1kHz.\r\n\r\nSource: [Mood Classification Using Listening Data](https://arxiv.org/abs/2010.11512)", "variants": ["AllMusic Mood Subset"]}
{"id": "NISP", "title": "", "contents": "This dataset contains speech recordings along with speaker physical parameters (height, weight, shoulder size, age ) as well as regional information and linguistic information.\r\n\r\nThere are a total of 345 speakers (219 male and 126 female). The dataset contains sentences that are taken out from newspapers. Each speaker has contributed about 4-5 minutes of data that includes recordings in both English and their mother tongue. The transcript for the text is provided in UTF-8 format.\r\n\r\nSource: [NISP](https://github.com/iiscleap/NISP-Dataset)", "variants": ["NISP"]}
{"id": "EDUVSUM", "title": "Classification of Important Segments in Educational Videos using Multimodal Features", "contents": "EDUVSUM contains educational videos with subtitles from three popular e-learning platforms: Edx,YouTube, and TIB AV-Portal that cover the following topics: crash course on history of science and engineering, computer science, python and web programming, machine learning and computer vision, Internet of things (IoT), and software engineering. In total, the current version of the dataset contains 98 videos with ground truth values annotated by a user with an academic background in computer science.\r\n\r\nSource: [EDUVSUM](https://github.com/VideoAnalysis/EDUVSUM)", "variants": ["EDUVSUM"]}
{"id": "ADVANCE", "title": "Cross-Task Transfer for Geotagged Audiovisual Aerial Scene Recognition", "contents": "The AuDio Visual Aerial sceNe reCognition datasEt (ADVANCE) is a brand-new multimodal learning dataset, which aims to explore the contribution of both audio and conventional visual messages to scene recognition. This dataset in summary contains 5075 pairs of geotagged aerial images and sounds, classified into 13 scene classes, i.e., airport, sports land, beach, bridge, farmland, forest, grassland, harbor, lake, orchard, residential area, shrub land, and train station.\r\n\r\nSource: [](https://akchen.github.io/ADVANCE-DATASET/)", "variants": ["ADVANCE"]}
{"id": "Multi-Modal CelebA-HQ", "title": "TediGAN: Text-Guided Diverse Face Image Generation and Manipulation", "contents": "Multi-Modal-CelebA-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has high-quality segmentation mask, sketch, descriptive text, and image with transparent background.\r\n\r\nMulti-Modal-CelebA-HQ can be used to train and evaluate algorithms of text-to-image-generation, text-guided image manipulation, sketch-to-image generation, and GANs for face generation and editing.\r\n\r\nSource: [Multi-Modal CelebA-HQ Dataset](https://github.com/weihaox/Multi-Modal-CelebA-HQ-Dataset)\r\nImage Source: [Xia et al](https://arxiv.org/pdf/2012.03308.pdf)", "variants": ["Multi-Modal-CelebA-HQ", "Multi-Modal CelebA-HQ"]}
{"id": "Short Text Font Dataset", "title": "Let Me Choose: From Verbal Context to Font Selection", "contents": "The proposed dataset includes 1,309 short text instances from Adobe Spark. The dataset is a collection of publicly available sample texts created by different designers. It covers a variety of topics found in posters, flyers, motivational quotes and advertisements.\r\n\r\nSource: [Short Text Font Dataset](https://github.com/RiTUAL-UH/Font-prediction-dataset)", "variants": ["Short Text Font Dataset"]}
{"id": "Circa", "title": "\"I'd rather just go to bed\": Understanding Indirect Answers", "contents": "The Circa (meaning ‘approximately’) dataset aims to help machine learning systems to solve the problem of interpreting indirect answers to polar questions.\r\n\r\nThe dataset contains pairs of yes/no questions and indirect answers, together with annotations for the interpretation of the answer. The data is collected in 10 different social conversational situations (eg. food preferences of a friend). Examples:\r\n\r\n```\r\nQ: Are you vegan?\r\nA: I love burgers too much. [No]\r\n\r\nQ: Do you like spicy food?\r\nA: I put hot sauce on everything. [Yes] \r\n\r\nQ: Would you like to go see live music?\r\nA: If it’s not too crowded. [Yes, upon a condition]\r\n```\r\n\r\nCurrently, the Circa annotations focus on a few classes such as ‘yes’, ‘no’ and ‘yes, upon condition’. The data can be used to build machine learning models which can replicate these classes on new question-answer pairs, and allow evaluation of methods for doing so.\r\n\r\nSource: [Circa](https://github.com/google-research-datasets/circa)", "variants": ["Circa"]}
{"id": "NH-HAZE", "title": "NH-HAZE: An Image Dehazing Benchmark with Non-Homogeneous Hazy and Haze-Free Images", "contents": "NN-HAZE is an image dehazing dataset. Since in many real cases haze is not uniformly distributed NH-HAZE, a non-homogeneous realistic dataset with pairs of real hazy and corresponding haze-free images. This is the first non-homogeneous image dehazing dataset and contains 55 outdoor scenes. The non-homogeneous haze has been introduced in the scene using a professional haze generator that imitates the real conditions of hazy scenes.\r\n\r\nSource: [NH-HAZE: An Image Dehazing Benchmark with Non-Homogeneous Hazy and Haze-Free Images](https://data.vision.ee.ethz.ch/cvl/ntire20/nh-haze/)", "variants": ["NH-HAZE validation", "NH-HAZE"]}
{"id": "PVDN", "title": "Provident Vehicle Detection at Night: The PVDN Dataset", "contents": "PVDN is a dataset of vehicle detection at night, using light reflections caused by their headlamps. It contains 59,746 annotated grayscale images out of 346 different scenes in a rural environment at night. In these images, all oncoming vehicles, their corresponding light objects (e. g., headlamps), and their respective light reflections (e. g., light reflections on guardrails) are labeled. With this information, this dataset enables research into new methods of detecting oncoming vehicles based on the light reflections they cause, long before they are directly visible. \r\n\r\nSource: [Provident Vehicle Detection at Night: The PVDN Dataset](https://arxiv.org/abs/2012.15376)", "variants": ["PVDN"]}
{"id": "TrashCan", "title": "TrashCan: A Semantically-Segmented Dataset towards Visual Detection of Marine Debris", "contents": "The TrashCan dataset is an instance-segmentation dataset of underwater trash. It is comprised of annotated images (7,212 images) which contain observations of trash, ROVs, and a wide variety of undersea flora and fauna. The annotations in this dataset take the format of instance segmentation annotations: bitmaps containing a mask marking which pixels in the image contain each object. The imagery in TrashCan is sourced from the J-EDI (JAMSTEC E-Library of Deep-sea Images) dataset, curated by the Japan Agency of Marine Earth Science and Technology (JAMSTEC). \r\n\r\nSource: [TrashCan 1.0 An Instance-Segmentation Labeled Dataset of Trash Observations](https://conservancy.umn.edu/handle/11299/214865)\r\nImage Source: [https://conservancy.umn.edu/handle/11299/214865](https://conservancy.umn.edu/handle/11299/214865)", "variants": ["TrashCan"]}
{"id": "NeuralNews", "title": "Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News", "contents": "NeuralNews is a dataset for machine-generated news detection. It consists of human-generated and machine-generated articles. The human-generated articles are extracted from the GoodNews dataset, which is extracted from the New York Times. It contains 4 types of articles:\r\n\r\n- Real Articles and Real Captions\r\n- Real Articles and Generated Captions\r\n- Generated Articles and Real Captions\r\n- Generated Articles and Generated Captions\r\n\r\nIn total, it contains about 32K samples of each article type (resulting in about 128K total).\r\n\r\nSource: [Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News](https://arxiv.org/abs/2009.07698)", "variants": ["NeuralNews"]}
{"id": "Doc3DShade", "title": "Intrinsic Decomposition of Document Images In-the-Wild", "contents": "Doc3DShade extends Doc3D with realistic lighting and shading. Follows a similar synthetic rendering procedure using captured document 3D shapes but final image generation step combines real shading of different types of paper materials under numerous illumination conditions.\r\n\r\nSource: [Doc3DShade](https://github.com/cvlab-stonybrook/DocIIW)", "variants": ["Doc3DShade"]}
{"id": "GSL", "title": "A Comprehensive Study on Sign Language Recognition Methods", "contents": "## Dataset Description \r\n\r\nThe [Greek Sign Language (GSL)](https://arxiv.org/abs/2007.12530) is a large-scale RGB+D dataset, suitable for Sign Language Recognition (SLR) and Sign Language Translation (SLT). The video captures are conducted using an Intel RealSense D435 RGB+D camera at a rate of 30 fps. Both the RGB and the depth streams are acquired in the same spatial resolution of 848×480 pixels. To increase variability in the videos, the camera position and orientation is slightly altered within subsequent recordings. Seven different signers are employed to perform 5 individual and commonly met scenarios in different public services. The average length of each scenario is twenty sentences.\r\n\r\nThe dataset contains 10,290 sentence instances, 40,785 gloss instances, 310 unique glosses (vocabulary size) and 331 unique sentences, with 4.23 glosses per sentence on average. Each signer is asked to perform the pre-defined dialogues five consecutive times. In all cases, the simulation considers a deaf person communicating with a single public service employee. The involved signer performs the sequence of glosses of both agents in the discussion. For the annotation of each gloss sequence, GSL linguistic experts are involved. The given annotations are at individual gloss and gloss sequence level. A translation of the gloss sentences to spoken Greek is also provided.\r\n\r\n## Evaluation\r\n\r\nThe GSL dataset includes the 3 evaluation setups:\r\n\r\n- Signer-dependent continuous sign language recognition (GSL SD) – roughly 80% of videos are used for training, corresponding to 8,189 instances. The rest 1,063 (10%) were kept for validation and 1,043 (10%) for testing.\r\n\r\n- Signer-independent continuous sign language recognition (GSL SI) – the selected test gloss sequences are not used in the training set, while all the individual glosses exist in the training set. In GSL SI, the recordings of one signer are left out for validation and testing (588 and 881 instances, respectively). The rest 8821 instances are utilized for training.\r\n\r\n- Isolated gloss sign language recognition (GSL isol.) – The validation set consists of 2,231 gloss instances, the test set 3,500, while the remaining 34,995 are used for training. All 310 unique glosses are seen in the training set.\r\n\r\nFor more info and results, advice our [paper](https://arxiv.org/abs/2007.12530)\r\n\r\n## Paper Abstract: A Comprehensive Study on Sign Language Recognition Methods, Adaloglou et al. 2020\r\n\r\nIn this paper, a comparative experimental assessment of computer vision-based methods for sign language recognition is conducted. By implementing the most recent deep neural network methods in this field, a thorough evaluation on multiple publicly available datasets is performed. The aim of the present study is to provide insights on sign language recognition, focusing on mapping non-segmented video streams to glosses. For this task, two new sequence training criteria, known from the fields of speech and scene text recognition, are introduced. Furthermore, a\r\nplethora of pretraining schemes are thoroughly discussed. Finally, a new RGB+D dataset for the Greek sign language is created. To the best of our knowledge, this is the first sign language dataset where sentence and gloss level annotations are provided for every video capture.\r\n\r\n[Arxiv link](https://arxiv.org/abs/2007.12530)", "variants": ["GSL"]}
{"id": "XTREME", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation", "contents": "The **Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME)** benchmark was introduced to encourage more research on multilingual transfer learning,. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics.\r\n\r\nThe languages in XTREME are selected to maximize language diversity, coverage in existing tasks, and availability of training data. The languages in XTREME are selected to maximize language diversity, coverage in existing tasks, and availability of training data. Among these are many under-studied languages, such as the Dravidian languages Tamil (spoken in southern India, Sri Lanka, and Singapore), Telugu and Malayalam (spoken mainly in southern India), and the Niger-Congo languages Swahili and Yoruba, spoken in Africa.", "variants": ["XTREME", "XNLI", "BUCC", "BUCC Chinese-to-English", "BUCC French-to-English", "BUCC German-to-English", "BUCC Russian-to-English", "MLQA", "PAWS-X", "TyDi QA", "XNLI Chinese", "XNLI French", "XQuAD", "Wikiann", "TyDiQA-GoldP", "Tatoeba"]}
{"id": "WikiSuggest", "title": "Coarse-to-Fine Question Answering for Long Documents", "contents": "To collect WikiSuggest, Google Suggest API is used to harvest natural language questions and submit them to Google Search. Whenever Google Search returns a box with a short answer from Wikipedia, an example from the question, answer, and the Wikipedia document are created. If the answer string is missing from the document this often implies a spurious question-answer pair, such as (‘what time is half time in rugby’, ‘80 minutes, 40 minutes’). Question-answer pairs without the exact answer string are pruned. Fifty examples after filtering are examined and 54% were found to be well-formed question-answer pairs where answers in the document can be grounded, 20% contained answers without textual evidence in the document (the answer string exists in an irreleveant context), and 26% contain incorrect QA pairs.\r\n\r\nSource: [Coarse-to-Fine Question Answering for Long Documents](https://arxiv.org/pdf/1611.01839.pdf)", "variants": ["WikiSuggest"]}
{"id": "FineGym", "title": "FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding", "contents": "**FineGym** is an action recognition dataset build on top of gymnasium videos. Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a \"balance beam\" event will be annotated as a sequence of elementary sub-actions derived from five sets: \"leap-jumphop\", \"beam-turns\", \"flight-salto\", \"flight-handspring\", and \"dismount\", where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes.", "variants": ["FineGym"]}
{"id": "MovieNet", "title": "MovieNet: A Holistic Dataset for Movie Understanding", "contents": "**MovieNet** is a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92 K tags of cinematic style.", "variants": ["MovieNet"]}
{"id": "MessyTable", "title": "MessyTable: Instance Association in Multiple Camera Views", "contents": "**MessyTable** features a large number of scenes with messy tables captured from multiple camera views. Each scene in this dataset is highly complex, containing multiple object instances that could be identical, stacked and occluded by other instances. The key challenge is to associate all instances given the RGB image of all views. The seemingly simple task surprisingly fails many popular methods or heuristics. The dataset challenges existing methods in mining subtle appearance differences, reasoning based on contexts, and fusing appearance with geometric cues for establishing an association.\r\n\r\nThere are 50,211 images and 5,579 scenes in the dataset.", "variants": ["MessyTable"]}
{"id": "TweetQA", "title": "", "contents": "With social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effectiveness of many applications that rely on real-time knowledge. While previous question answering (QA) datasets have concentrated on formal text like news and Wikipedia, the first large-scale dataset for QA over social media data is presented. To make sure the tweets are meaningful and contain interesting information, tweets used by journalists to write news articles are gathered. Then human annotators are asked to write questions and answers upon these tweets. Unlike other QA datasets like SQuAD in which the answers are extractive, the answer are allowed to be abstractive. The task requires model to read a short tweet and a question and outputs a text phrase (does not need to be in the tweet) as the answer.\r\n\r\nSource: [TWEETQA: A Social Media Focused Question Answering Dataset](https://tweetqa.github.io/)", "variants": ["TweetQA"]}
{"id": "UCF Sports", "title": "", "contents": "The UCF Sports dataset consists of a set of actions collected from various sports which are typically featured on broadcast television channels such as the BBC and ESPN. The video sequences were obtained from a wide range of stock footage websites including BBC Motion gallery and GettyImages.\r\n\r\nThe dataset includes a total of 150 sequences with the resolution of 720 x 480. The collection represents a natural pool of actions featured in a wide range of scenes and viewpoints.\r\n\r\nSource: [UCF Sports Action](https://www.crcv.ucf.edu/data/UCF_Sports_Action.php)", "variants": ["UCF Sports"]}
{"id": "MSRA-B", "title": "", "contents": "The MSRA-B dataset is a dataset for salient object detection. It contains 5,000 images with a variety of image contents. Most of the images have a single salient object. There is a large variation among images including natural scenes, animals, indoor, outdoor, etc.\r\n\r\nSource: [Deep Contrast Learning for Salient Object Detection](https://arxiv.org/pdf/1603.01976.pdf)", "variants": ["MSRA-B"]}
{"id": "VOT2015", "title": "", "contents": "VOT2015 is a visual object tracking dataset. The dataset comprises 60 short sequences showing various objects in challenging backgrounds. The sequences were chosen from a large pool of sequences from different sources.\r\n\r\nSource: [VOT2015](https://www.votchallenge.net/vot2015/dataset.html)", "variants": ["VOT2015"]}
{"id": "UCF50", "title": "", "contents": "UCF50 is an action recognition data set with 50 action categories, consisting of realistic videos taken from youtube. This data set is an extension of YouTube Action data set (UCF11) which has 11 action categories.\r\n\r\nUCF50 data set's 50 action categories collected from youtube are: Baseball Pitch, Basketball Shooting, Bench Press, Biking, Biking, Billiards Shot,Breaststroke, Clean and Jerk, Diving, Drumming, Fencing, Golf Swing, Playing Guitar, High Jump, Horse Race, Horse Riding, Hula Hoop, Javelin Throw, Juggling Balls, Jump Rope, Jumping Jack, Kayaking, Lunges, Military Parade, Mixing Batter, Nun chucks, Playing Piano, Pizza Tossing, Pole Vault, Pommel Horse, Pull Ups, Punch, Push Ups, Rock Climbing Indoor, Rope Climbing, Rowing, Salsa Spins, Skate Boarding, Skiing, Skijet, Soccer Juggling, Swing, Playing Tabla, TaiChi, Tennis Swing, Trampoline Jumping, Playing Violin, Volleyball Spiking, Walking with a dog, and Yo Yo.\r\n\r\n\r\nSource: [UCF50](https://www.crcv.ucf.edu/data/UCF50.php)", "variants": ["UCF50"]}
{"id": "CASME II", "title": "", "contents": "The Chinese Academy of Sciences Micro-Expression dataset (CASME II) consists of 255 videos, elicited from 26 participants. The videos are recorded using Point Gray GRAS-03K2C camera which has a frame rate of 200fps. The average video length is 0.34s, equivalent to 68 frames. Each video’s emotion label is annotated by two coders, where the reliability is 0.846. \r\n\r\nAll the images are cropped to 170×140 pixels. The ground-truth information provided by the database include the emotion state, the action unit, the onset, apex and offset frame indices. The videos are grouped into seven categories: others (99 videos), disgust (63 videos), happiness (32 videos), repression (27 videos), surprise (25 videos), sadness (7 videos) and fear (2 videos).\r\n\r\nSource: [OFF-ApexNet on Micro-expression Recognition System](https://arxiv.org/pdf/1805.08699.pdf)", "variants": ["CASME II"]}
{"id": "VOT2013", "title": "", "contents": "The dataset comprises 16 short sequences showing various objects in challenging backgrounds. The sequences were chosen from a large pool of sequences using a methodology based on clustering visual features of object and background so that those 16 sequences sample evenly well the existing pool. The sequences were annotated by the VOT committee using axis-aligned bounding boxes.\r\n\r\n\r\nSource: [VOT2013](https://www.votchallenge.net/vot2013/dataset.html)", "variants": ["VOT2013"]}
{"id": "CrossNER", "title": "CrossNER: Evaluating Cross-Domain Named Entity Recognition", "contents": "CrossNER is a cross-domain NER (Named Entity Recognition) dataset, a fully-labeled collection of NER data spanning over five diverse domains (Politics, Natural Science, Music, Literature, and Artificial Intelligence) with specialized entity categories for different domains. Additionally, CrossNER also includes unlabeled domain-related corpora for the corresponding five domains. \r\n\r\nSource: [CrossNER](https://github.com/zliucr/CrossNER)", "variants": ["CrossNER"]}
{"id": "Placepedia", "title": "Placepedia: Comprehensive Place Understanding with Multi-Faceted Annotations", "contents": "**Placepedia** contains 240K places with 35M images from all over the world. Each place is associated with its district, city/town/village, state/province, country, continent, and a large amount of diverse photos. Both administrative areas and places have rich side information, e.g. discription, population, category, function. In addition, two cleaned subsets (Places-Coarse and Places-Fine) for experiments are provided.", "variants": ["Placepedia"]}
{"id": "2WikiMultiHopQA", "title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps", "contents": "Uses structured and unstructured data. The dataset introduces the evidence information containing a reasoning path for multi-hop questions.\r\n\r\nSource: [Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps](/paper/constructing-a-multi-hop-qa-dataset-for)", "variants": ["2WikiMultiHopQA"]}
{"id": "3DMAD", "title": "", "contents": "The 3D Mask Attack Database (3DMAD) is a biometric (face) spoofing database. It currently contains 76500 frames of 17 persons, recorded using Kinect for both real-access and spoofing attacks. Each frame consists of:\r\n\r\n- a depth image (640x480 pixels – 1x11 bits)\r\n- the corresponding RGB image (640x480 pixels – 3x8 bits)\r\n- manually annotated eye positions (with respect to the RGB image).\r\n\r\nSource: [3D Mask Attack Dataset](https://www.idiap.ch/dataset/3dmad)", "variants": ["3DMAD"]}
{"id": "3D-ZeF", "title": "3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset", "contents": "**3D-ZeF** dataset consists of eight sequences with a duration between 15-120 seconds and 1-10 free moving zebrafish. The videos have been annotated with a total of 86,400 points and bounding boxes.\r\n\r\nSource: [3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset](/paper/3d-zef-a-3d-zebrafish-tracking-benchmark-1)", "variants": ["3D-ZeF"]}
{"id": "4Seasons", "title": "4Seasons: A Cross-Season Dataset for Multi-Weather SLAM in Autonomous Driving", "contents": "4Seasons is adataset covering seasonal and challenging perceptual conditions for autonomous driving.\r\n\r\nSource: [4Seasons: A Cross-Season Dataset for Multi-Weather SLAM in Autonomous Driving](/paper/4seasons-a-cross-season-dataset-for-multi)\r\nImage Source: [Wenzel et al](https://arxiv.org/pdf/2009.06364v2.pdf)", "variants": ["4Seasons"]}
{"id": "A2D2", "title": "A2D2: Audi Autonomous Driving Dataset", "contents": "Audi Autonomous Driving Dataset (A2D2) consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus.\r\n\r\nSource: [A2D2: Audi Autonomous Driving Dataset](/paper/a2d2-audi-autonomous-driving-dataset)\r\nImage Source: [https://www.a2d2.audi/a2d2/en/dataset.html](https://www.a2d2.audi/a2d2/en/dataset.html)", "variants": ["A2D2"]}
{"id": "AAVE/SAE Paired Dataset", "title": "Investigating African-American Vernacular English in Transformer-Based Text Generation", "contents": "AAVE/SAE Paired Dataset contains 2019 intent-equivalent AAVE/SAE pairs. The AAVE (African-American Vernacular English) samples are sampled from Blodgett et. al. (2016)'s TwitterAAE, with their corresponding SAE (Standard American English) samples annotated by Amazon MTurk.\r\n\r\nSource: [https://github.com/sophiegroenwold/AAVE_SAE_dataset](https://github.com/sophiegroenwold/AAVE_SAE_dataset)", "variants": ["AAVE/SAE Paired Dataset"]}
{"id": "ACL ARC", "title": "", "contents": "ACL Anthology Reference Corpus (ACL ARC) is a collection of 10,920 academic papers from the ACL Anthology. ACL ARC is cleaned to remove:\r\n\r\n(a) files that look like not full papers, paper fragments, foreign-language papers (e.g., French), or pure junk.\r\n(b) headers (title and author information; NOT abstract).\r\n(c) footers (\"References\" line and the actual references).\r\n(d) some bad characters (spurious characters).\r\n(e) some page numbers (i.e., a single number appearing on a line, with nothing else attached to it).\r\n(f) significant foreign-language (e.g., French) content in an otherwise English paper.\r\n\r\nThe cleaned corpus has 10,628 documents.\r\n\r\nSource: [ACL ARC](https://web.eecs.umich.edu/~lahiri/acl_arc.html)", "variants": ["ACL-ARC", "ACL ARC"]}
{"id": "ACRONYM", "title": "ACRONYM: A Large-Scale Grasp Dataset Based on Simulation", "contents": "A dataset for robot grasp planning based on physics simulation. The dataset contains 17.7M parallel-jaw grasps, spanning 8872 objects from 262 different categories, each labeled with the grasp result obtained from a physics simulator. \r\n\r\nSource: [ACRONYM: A Large-Scale Grasp Dataset Based on Simulation](/paper/acronym-a-large-scale-grasp-dataset-based-on)", "variants": ["ACRONYM"]}
{"id": "Acronym Identification", "title": "What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation", "contents": "Is an acronym disambiguation (AD) dataset for scientific domain with 62,441 samples which is significantly larger than the previous scientific AD dataset.\r\n\r\nSource: [What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation](/paper/what-does-this-acronym-mean-introducing-a-new)", "variants": ["Acronym Identification"]}
{"id": "ActioNet", "title": "Actionet: An Interactive End-To-End Platform For Task-Based Data Collection And Augmentation In 3D Environment", "contents": "**ActioNet** is a video task-based dataset collected in a synthetic 3D environment. It contains 3,038 annotated videos and hierarchical task structures over 65 individual household tasks from 120 different scenes. Each task is annotated across three to five different scenes by 10 different annotators. The tasks can be broken down into four categories: living room, bedroom, bathroom, kitchen.\r\n\r\nSource: [https://github.com/SamsonYuBaiJian/actionet](https://github.com/SamsonYuBaiJian/actionet)", "variants": ["ActioNet"]}
{"id": "ADHA", "title": "", "contents": "ADHA: “Adverbs Describing Human Actions” is the first benchmark for a new problem — recognizing human action adverbs (HAA). This is the first step for computer vision to change over from pattern recognition to real AI. Some key features of ADHA are: a semantically complete set of adverbs describing human actions, a set of common, describable human actions, and an exhaustive labeling of simultaneously emerging actions in each video.\r\n\r\nSource: [ADHA](http://www.mvig.org/research/adha/adha.html)", "variants": ["ADHA"]}
{"id": "ADL Piano MIDI", "title": "Computer-Generated Music for Tabletop Role-Playing Games", "contents": "The **ADL Piano MIDI** is a dataset of 11,086 piano pieces from different genres. This dataset is based on the Lakh MIDI dataset, which is a collection on 45,129 unique MIDI files that have been matched to entries in the Million Song Dataset. Most pieces in the Lakh MIDI dataset have multiple instruments, so for each file the authors of ADL Piano MIDI dataset extracted only the tracks with instruments from the \"Piano Family\" (MIDI program numbers 1-8). This process generated a total of 9,021 unique piano MIDI files. Theses 9,021 files were then combined with other approximately 2,065 files scraped from publicly-available sources on the internet. All the files in the final collection were de-duped according to their MD5 checksum.\r\n\r\nSource: [ADL Piano MIDI](https://github.com/lucasnfe/adl-piano-midi)", "variants": ["ADL Piano MIDI"]}
{"id": "Affective Text", "title": "", "contents": "Affective Text (Test Corpus of SemEval 2007) by [Carlo Strapparava & Rada Mihalcea](https://www.aclweb.org/anthology/S07-1013/).\r\n\r\nSource: [Affective Text](https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets)", "variants": ["Affective Text"]}
{"id": "AfroMNIST", "title": "Afro-MNIST: Synthetic generation of MNIST-style datasets for low-resource languages", "contents": "A set of synthetic MNIST-style datasets for four orthographies used in Afro-Asiatic and Niger-Congo languages: Ge`ez (Ethiopic), Vai, Osmanya, and N'Ko. These datasets serve as \"drop-in\" replacements for MNIST. \r\n\r\nSource: [Afro-MNIST: Synthetic generation of MNIST-style datasets for low-resource languages](/paper/afro-mnist-synthetic-generation-of-mnist)", "variants": ["AfroMNIST"]}
{"id": "Alexa Point of View", "title": "Converting the Point of View of Messages Spoken to Virtual Assistants", "contents": "The **Alexa Point of View** dataset is point of view conversion dataset, a parallel corpus of messages spoken to a virtual assistant and the converted messages for delivery.\nThe dataset contains parallel corpus of input (input column) message and POV converted messages (output column). An example of a pair is `tell @CN@ that i'll be late [\\t] hi @CN@, @SCN@ would like you to know that they'll be late.` The input and pov-converted output pair is tab separated. `@CN@` tag is a placeholder for the contact name (receiver) and `@SCN@` tag is a placeholder for source contact name (sender).\nThe total dataset has 46563 pairs. This data is then test/train/dev split into 6985 pairs/32594 pairs/6985 pairs.\n\nSource: [https://github.com/alexa/alexa-point-of-view-dataset](https://github.com/alexa/alexa-point-of-view-dataset)", "variants": ["Alexa Point of View"]}
{"id": "ALGAD", "title": "Deep Learning of Individual Aesthetics", "contents": "Repository of a generative art dataset by computer artist Andy Lomas.\n\nSource: [https://github.com/SensiLab/Andy-Lomas-Generative-Art-Dataset](https://github.com/SensiLab/Andy-Lomas-Generative-Art-Dataset)\nImage Source: [https://github.com/SensiLab/Andy-Lomas-Generative-Art-Dataset](https://github.com/SensiLab/Andy-Lomas-Generative-Art-Dataset)", "variants": ["ALGAD"]}
{"id": "Allegro Reviews", "title": "KLEJ: Comprehensive Benchmark for Polish Language Understanding", "contents": "A comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. \r\n\r\nSource: [KLEJ: Comprehensive Benchmark for Polish Language Understanding](/paper/klej-comprehensive-benchmark-for-polish)", "variants": ["Allegro Reviews"]}
{"id": "AlloCine", "title": "", "contents": "A new dataset for sentiment analysis, scraped from Allociné.fr user reviews. It contains 100k positive and 100k negative reviews divided into 3 balanced splits: train (160k reviews), val (20k) and test (20k). \r\n\r\nSource: [AlloCine](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert)", "variants": ["AlloCine"]}
{"id": "ALT", "title": "", "contents": "The ALT project aims to advance the state-of-the-art Asian natural language processing (NLP) techniques through the open collaboration for developing and using ALT. It was first conducted by NICT and UCSY as described in Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch and Eiichiro Sumita (2016). Then, it was developed under ASEAN IVO as described in this Web page. The process of building ALT began with sampling about 20,000 sentences from English Wikinews, and then these sentences were translated into the other languages. ALT now has 13 languages: Bengali, English, Filipino, Hindi, Bahasa Indonesia, Japanese, Khmer, Lao, Malay, Myanmar (Burmese), Thai, Vietnamese, Chinese (Simplified Chinese).\r\n\r\nSource: [ALT](https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/)", "variants": ["ALT"]}
{"id": "AmbigQA", "title": "", "contents": "Is a new open-domain question answering task which involves predicting a set of question-answer pairs, where every plausible answer is paired with a disambiguated rewrite of the original question. A dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark.\r\n\r\nSource: [AmbigQA](https://nlp.cs.washington.edu/ambigqa/)", "variants": ["AmbigQA"]}
{"id": "AML Robot Cutting Dataset", "title": "Learning the Latent Space of Robot Dynamics for Cutting Interaction Inference", "contents": "The AML Robot Cutting Dataset consists of approximately 1500 seconds of real data collected on Kinova Jaco 2 robot retrofitted with a custom end-effector fixture and dremel performing cutting tasks on wood specimens for 5 materials and 5 thicknesses.\r\n\r\nSource: [AML Robot Cutting Dataset](https://github.com/sahandrez/aml_robot_cutting_dataset)", "variants": ["AML Robot Cutting Dataset"]}
{"id": "AO-CLEVr", "title": "A causal view of compositional zero-shot recognition", "contents": "**AO-CLEVr** is a new synthetic-images dataset containing images of \"easy\" Attribute-Object categories, based on the CLEVr. AO-CLEVr has attribute-object pairs created from 8 attributes: { red, purple, yellow, blue, green, cyan, gray, brown } and 3 object shapes {sphere, cube, cylinder}, yielding 24 attribute-object pairs. Each pair consists of 7500 images. Each image has a single object that consists of the attribute-object pair. The object is randomly assigned one of two sizes (small/large), one of two materials (rubber/metallic), a random position, and random lightning according to CLEVr defaults.\n\nSource: [https://github.com/nv-research-israel/causal_comp](https://github.com/nv-research-israel/causal_comp)\nImage Source: [https://github.com/nv-research-israel/causal_comp](https://github.com/nv-research-israel/causal_comp)", "variants": ["AO-CLEVr"]}
{"id": "ApartmenTour", "title": "Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision", "contents": "Contains a large number of online videos and subtitles. \r\n\r\nSource: [Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision](/paper/watch-and-learn-mapping-language-and-noisy)", "variants": ["ApartmenTour"]}
{"id": "APE", "title": "Learning Non-Monotonic Automatic Post-Editing of Translations from Human Orderings", "contents": "APE is useful to evaluate Machine Translation automatic post-editing (**APE**), which is the task of improving the output of a blackbox MT system by automatically fixing its mistakes. The act of post-editing text can be fully specified as a sequence of delete and insert actions in given positions.\r\n\r\nSource: [https://github.com/antoniogois/keystrokes_ape](https://github.com/antoniogois/keystrokes_ape)\r\nImage Source: [Gois et al](https://arxiv.org/pdf/2004.14120v1.pdf)", "variants": ["APE"]}
{"id": "AQUA", "title": "A Dataset and Baselines for Visual Question Answering on Art", "contents": "The question-answer (QA) pairs are automatically generated using state-of-the-art question generation methods based on paintings and comments provided in an existing art understanding dataset. The QA pairs are cleansed by crowdsourcing workers with respect to their grammatical correctness, answerability, and answers' correctness. The dataset inherently consists of visual (painting-based) and knowledge (comment-based) questions. \r\n\r\nSource: [A Dataset and Baselines for Visual Question Answering on Art](/paper/a-dataset-and-baselines-for-visual-question)", "variants": ["AQUA"]}
{"id": "aquamuse", "title": "AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization", "contents": "5,519 query-based summaries, each associated with an average of 6 input documents selected from an index of 355M documents from Common Crawl.\r\n\r\nSource: [AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization](/paper/aquamuse-automatically-generating-datasets)", "variants": ["aquamuse"]}
{"id": "Arabic Dataset for Commonsense Validation¬†", "title": "Is this sentence valid? An Arabic Dataset for Commonsense Validation", "contents": "A benchmark Arabic dataset for commonsense understanding and validation as well as a baseline research and models trained using the same dataset. \r\n\r\nSource: [Is this sentence valid? An Arabic Dataset for Commonsense Validation](/paper/is-this-sentence-valid-an-arabic-dataset-for)", "variants": ["Arabic Dataset for Commonsense Validation¬†"]}
{"id": "Arabic Handwritten Digits Dataset", "title": "", "contents": "Contain Arabic handwritten digits images (60000 training and 10000 testing images).\r\n\r\nSource: [Arabic Handwritten Digits Dataset](https://mloey.github.io/research/research1.html)", "variants": ["Arabic Handwritten Digits Dataset"]}
{"id": "ArabicWeb16", "title": "", "contents": "It includes 150M (150,211,934) Arabic Web pages.\r\n\r\nWeb pages in ArabicWeb16 are collected into files that conform to the WARC ISO 28500 version 0.18 standard (\"WARC files\").\r\n\r\nSource: [ArabicWeb16](https://sites.google.com/view/arabicweb16/download/arabicweb16?authuser=0)", "variants": ["ArabicWeb16"]}
{"id": "ArCOV-19", "title": "ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation Networks", "contents": "ArCOV-19 is an Arabic COVID-19 Twitter dataset that covers the period from 27th of January till 30th of April 2020. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes over 1M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and -liked).\r\n\r\nSource: [ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation Networks](/paper/texttt-arcov-19-the-first-arabic-covid-19)", "variants": ["ArCOV-19"]}
{"id": "ArCOV19-Rumors", "title": "ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation Detection", "contents": "ArCOV19-Rumors is an Arabic COVID-19 Twitter dataset for misinformation detection composed of tweets containing claims from 27th January till the end of April 2020. \r\n\r\nSource: [ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation Detection](/paper/arcov19-rumors-arabic-covid-19-twitter)", "variants": ["ArCOV19-Rumors"]}
{"id": "ARDIS", "title": "", "contents": "This is a new image-based handwritten historical digit dataset named ARDIS (Arkiv Digital Sweden). The images in ARDIS dataset are extracted from 15.000 Swedish church records which were written by different priests with various handwriting styles in the nineteenth and twentieth centuries. The constructed dataset consists of three single digit datasets and one digit strings dataset. The digit strings dataset includes 10.000 samples in Red-Green-Blue (RGB) color space, whereas, the other datasets contain 7.600 single digit images in different color spaces.\r\n\r\nSource: [ARDIS](https://ardisdataset.github.io/ARDIS/)", "variants": ["ARDIS"]}
{"id": "Armenian Paraphrase Detection Corpus", "title": "ARPA: Armenian Paraphrase Detection Corpus and Models", "contents": "This dataset contains 2,360 paraphrases in Armenian that can be used for paraphrase detection. The dataset is constructed by back-translating sentences from Armenian to English twice, and manually filtering the result.\n\nSource: [https://github.com/ivannikov-lab/arpa-paraphrase-corpus](https://github.com/ivannikov-lab/arpa-paraphrase-corpus)", "variants": ["Armenian Paraphrase Detection Corpus"]}
{"id": "ArraMon", "title": "ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments", "contents": "A dataset (in English; and also extended to Hindi) with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. \r\n\r\nSource: [ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments](/paper/arramon-a-joint-navigation-assembly)", "variants": ["ArraMon"]}
{"id": "ASAYAR", "title": "", "contents": "The first public dataset dedicated for Latin (French) and Arabic Scene Text Detection in Highway panels. It comprises more than 1800 well-annotated images. The dataset was colleted from Moroccan Highway and it has been manually annotated. ASAYAR data can be used to develop and evaluate traffic signs detection and French or Arabic text detection in different languages.\r\n\r\nSource: [ASAYAR](https://vcar.github.io/ASAYAR/)", "variants": ["ASAYAR"]}
{"id": "AskParents", "title": "", "contents": "**AskParents** is a dataset for advice classification extracted from Reddit. In this dataset, posts are annotated for whether they contain advice or not. It contains 8,701 samples for training, 802 for validation and 1,091 for testing.\n\nSource: [https://github.com/venkatasg/Advice-EMNLP2020](https://github.com/venkatasg/Advice-EMNLP2020)", "variants": ["AskParents"]}
{"id": "ASSET Corpus", "title": "ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations", "contents": "A crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations.\r\n\r\nSource: [ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations](/paper/asset-a-dataset-for-tuning-and-evaluation-of)", "variants": ["ASSET Corpus"]}
{"id": "ASSIN", "title": "", "contents": "ASSIN (Avaliação de Similaridade Semântica e INferência textual) is a dataset with semantic similarity score and entailment annotations. It was used in a shared task in the PROPOR 2016 conference.\r\n\r\nThe full dataset has 10,000 sentence pairs, half of which in Brazilian Portuguese and half in European Portuguese. Either language variant has 2,500 pairs for training, 500 for validation and 2,000 for testing. This is different from the split used in the shared task, in which the training set had 3,000 pairs and there was no validation set. The shared task training set can be reconstructed by simply merging both sets.\r\n\r\nSource: [ASSIN](http://nilc.icmc.usp.br/assin/)", "variants": ["ASSIN"]}
{"id": "ASSIN2", "title": "", "contents": "ASSIN 2 is the second Semantic Similarity Assessment and Textual Inference, and was a workshop held in conjunction with STIL 2019 .\r\n\r\nSource: [ASSIN2](https://sites.google.com/view/assin2)", "variants": ["ASSIN2"]}
{"id": "Astyx HiRes2019", "title": "", "contents": "A radar-centric automotive dataset based on radar, lidar and camera data for the purpose of 3D object detection.\r\n\r\nSource: [Automotive Radar Dataset for Deep Learning Based 3D Object Detection](https://www.astyx.com/fileadmin/redakteur/dokumente/Automotive_Radar_Dataset_for_Deep_learning_Based_3D_Object_Detection.PDF)", "variants": ["Astyx HiRes2019"]}
{"id": "AU-AIR", "title": "Integration of the 3D Environment for UAV Onboard Visual Object Tracking", "contents": "The **AU-AIR** is a multi-modal aerial dataset captured by a UAV. Having visual data, object annotations, and flight data (time, GPS, altitude, IMU sensor data, velocities), AU-AIR meets vision and robotics for UAVs.\n\nSource: [https://github.com/bozcani/auairdataset](https://github.com/bozcani/auairdataset)\nImage Source: [https://github.com/bozcani/auairdataset](https://github.com/bozcani/auairdataset)", "variants": ["AU-AIR"]}
{"id": "Automatic Keyphrase Extraction Dataset", "title": "", "contents": "Dataset for automatic keyphrase extraction task.\r\n\r\nSource: [Automatic Keyphrase Extraction Dataset](https://github.com/snkim/AutomaticKeyphraseExtraction)", "variants": ["Automatic Keyphrase Extraction Dataset"]}
{"id": "AuxAD", "title": "", "contents": "**AuxAD** is a a distantly supervised dataset for acronym disambiguation.\n\nSource: [https://github.com/PrimerAI/sdu-data](https://github.com/PrimerAI/sdu-data)", "variants": ["AuxAD"]}
{"id": "AVA-ActiveSpeaker", "title": "AVA-ActiveSpeaker: An Audio-Visual Dataset for Active Speaker Detection", "contents": "Contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio. \r\n\r\nSource: [AVA-ActiveSpeaker: An Audio-Visual Dataset for Active Speaker Detection](/paper/ava-activespeaker-an-audio-visual-dataset-for)", "variants": ["AVA-ActiveSpeaker"]}
{"id": "AVA-LAEO", "title": "", "contents": "Dataset to address the problem of detecting people Looking At Each Other (LAEO) in video sequences.\r\n\r\nSource: [LAEO-Net: revisiting people Looking At Each Other in videos](/paper/laeo-net-revisiting-people-looking-at-each-1)", "variants": ["AVA-LAEO"]}
{"id": "AVECL-UMons", "title": "AVECL-UMONS database for audio-visual event classification and localization", "contents": "A dataset for audio-visual event classification and localization in the context of office environments. The audio-visual dataset is composed of 11 event classes recorded at several realistic positions in two different rooms. Two types of sequences are recorded according to the number of events in the sequence. The dataset comprises 2662 unilabel sequences and 2724 multilabel sequences corresponding to a total of 5.24 hours. \r\n\r\nSource: [AVECL-UMONS database for audio-visual event classification and localization](/paper/avecl-umons-database-for-audio-visual-event)", "variants": ["AVECL-UMons"]}
{"id": "BanFakeNews", "title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla", "contents": "An annotated dataset of ~50K news that can be used for building automated fake news detection systems for a low resource language like Bangla. \r\n\r\nSource: [BanFakeNews: A Dataset for Detecting Fake News in Bangla](/paper/banfakenews-a-dataset-for-detecting-fake-news)", "variants": ["BanFakeNews"]}
{"id": "BanglaWriting", "title": "BanglaWriting: A multi-purpose offline Bangla handwriting dataset", "contents": "The **BanglaWriting** dataset contains single-page handwritings of 260 individuals of different personalities and ages. Each page includes bounding-boxes that bounds each word, along with the unicode representation of the writing. This dataset contains 21,234 words and 32,787 characters in total. Moreover, this dataset includes 5,470 unique words of Bangla vocabulary. Apart from the usual words, the dataset comprises 261 comprehensible overwriting and 450 incomprehensible overwriting. All of the bounding boxes and word labels are manually-generated. The dataset can be used for complex optical character/word recognition, writer identification, and handwritten word segmentation. Furthermore, this dataset is suitable for extracting age-based and gender-based variation of handwriting.\n\nSource: [https://github.com/QuwsarOhi/BanglaWriting](https://github.com/QuwsarOhi/BanglaWriting)\nImage Source: [https://github.com/QuwsarOhi/BanglaWriting](https://github.com/QuwsarOhi/BanglaWriting)", "variants": ["BanglaWriting"]}
{"id": "BAR", "title": "Learning from Failure: Training Debiased Classifier from Biased Classifier", "contents": "**Biased Action Recognition** (**BAR**) dataset is a real-world image dataset categorized as six action classes which are biased to distinct places. The authors settle these six action classes by inspecting imSitu, which provides still action images from Google Image Search with action and place labels. In detail, the authors choose action classes where images for each of these candidate actions share common place characteristics. At the same time, the place characteristics of action class candidates should be distinct in order to classify the action only from place attributes. The select pairs are six typical action-place pairs: (Climbing, RockWall), (Diving, Underwater), (Fishing, WaterSurface), (Racing, APavedTrack), (Throwing, PlayingField),and (Vaulting, Sky).\r\n\r\nSource: [https://github.com/alinlab/BAR](https://github.com/alinlab/BAR)\nImage Source: [https://github.com/alinlab/BAR](https://github.com/alinlab/BAR)", "variants": ["BAR"]}
{"id": "BBDB", "title": "", "contents": "A new large-scale baseball video dataset which is produced semi-automatically by using play-by-play texts available online. The BBDB contains 4200 hours of baseball game videos with 400k temporally annotated activity segments.\r\n\r\nSource: [BBDB](https://sites.google.com/site/eccv2018bbdb/)", "variants": ["BBDB"]}
{"id": "BD-4SK-ASR", "title": "Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset", "contents": "The **Basic Dataset for Sorani Kurdish Automatic Speech Recognition** (**BD-4SK-ASR**) is a dataset for automatic speech recognition for Sorani Kurdish.\n\nSource: [https://arxiv.org/abs/1911.13087](https://arxiv.org/abs/1911.13087)", "variants": ["BD-4SK-ASR"]}
{"id": "Bengali Hate Speech", "title": "Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network", "contents": "Introduces three datasets of expressing hate, commonly used topics, and opinions for hate speech detection, document classification, and sentiment analysis, respectively. \r\n\r\nSource: [Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network](/paper/classification-benchmarks-for-under-resourced)", "variants": ["Bengali Hate Speech"]}
{"id": "BIMCV COVID-19", "title": "BIMCV COVID-19+: a large annotated dataset of RX and CT images from COVID-19 patients", "contents": "BIMCV-COVID19+ dataset is a large dataset with chest X-ray images CXR (CR, DX) and computed tomography (CT) imaging of COVID-19 patients along with their radiographic findings, pathologies, polymerase chain reaction (PCR), immunoglobulin G (IgG) and immunoglobulin M (IgM) diagnostic antibody tests and radiographic reports from Medical Imaging Databank in Valencian Region Medical Image Bank (BIMCV). The findings are mapped onto standard Unified Medical Language System (UMLS) terminology and they cover a wide spectrum of thoracic entities, contrasting with the much more reduced number of entities annotated in previous datasets. Images are stored in high resolution and entities are localized with anatomical labels in a Medical Imaging Data Structure (MIDS) format. In addition, 23 images were annotated by a team of expert radiologists to include semantic segmentation of radiographic findings. Moreover, extensive information is provided, including the patient’s demographic information, type of projection and acquisition parameters for the imaging study, among others. These iterations of the database include 7,377 CR, 9,463 DX and 6,687 CT studies.\r\n\r\nSource: [https://github.com/BIMCV-CSUSP/BIMCV-COVID-19](https://github.com/BIMCV-CSUSP/BIMCV-COVID-19)\r\nImage Source: [https://github.com/BIMCV-CSUSP/BIMCV-COVID-19](https://github.com/BIMCV-CSUSP/BIMCV-COVID-19)", "variants": ["BIMCV COVID-19"]}
{"id": "BIOMRC", "title": "BIOMRC: A Dataset for Biomedical Machine Reading Comprehension", "contents": "A large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018).\r\n\r\nSource: [BIOMRC: A Dataset for Biomedical Machine Reading Comprehension](/paper/biomrc-a-dataset-for-biomedical-machine)", "variants": ["BIOMRC"]}
{"id": "Blended Skill Talk", "title": "Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills", "contents": "To analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. \r\n\r\nSource: [Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills](/paper/can-you-put-it-all-together-evaluating)", "variants": ["BlendedSkillTalk", "Blended Skill Talk"]}
{"id": "Blog Authorship Corpus", "title": "", "contents": "The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.  \r\n\r\nSource: [Blog Authorship Corpus](https://u.cs.biu.ac.il/~koppel/BlogCorpus.htm)", "variants": ["Blog Authorship Corpus"]}
{"id": "BRWAC", "title": "The brWaC Corpus: A New Open Resource for Brazilian Portuguese", "contents": "Composed by 2.7 billion tokens, and has been annotated with tagging and parsing information. \r\n\r\nSource: [The brWaC Corpus: A New Open Resource for Brazilian Portuguese](/paper/the-brwac-corpus-a-new-open-resource-for)", "variants": ["BRWAC"]}
{"id": "BSTLD", "title": "", "contents": "This dataset contains 13427 camera images at a resolution of 1280x720 pixels and contains about 24000 annotated traffic lights. The annotations include bounding boxes of traffic lights as well as the current state (active light) of each traffic light.\r\nThe camera images are provided as raw 12bit HDR images taken with a red-clear-clear-blue filter and as reconstructed 8-bit RGB color images. The RGB images are provided for debugging and can also be used for training. However, the RGB conversion process has some drawbacks. Some of the converted images may contain artifacts and the color distribution may seem unusual.\r\n\r\nSource: [BSTLD](https://hci.iwr.uni-heidelberg.de/node/6132)", "variants": ["BSTLD"]}
{"id": "CALFW", "title": "", "contents": "A renovation of Labeled Faces in the Wild (LFW), the de facto standard testbed for unconstraint face verification. \r\n\r\nSource: [CALFW](http://whdeng.cn/CALFW/index.html)", "variants": ["CALFW"]}
{"id": "Caltech Pedestrian Dataset", "title": "", "contents": "The Caltech Pedestrian Dataset consists of approximately 10 hours of 640x480 30Hz video taken from a vehicle driving through regular traffic in an urban environment. About 250,000 frames (in 137 approximately minute long segments) with a total of 350,000 bounding boxes and 2300 unique pedestrians were annotated. The annotation includes temporal correspondence between bounding boxes and detailed occlusion labels.\r\n\r\nSource: [Caltech Pedestrian Dataset](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)", "variants": ["Caltech Pedestrian Dataset"]}
{"id": "CARRADA", "title": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations", "contents": "CARRADA is a dataset of synchronized camera and radar recordings with range-angle-Doppler annotations. \r\n\r\nSource: [CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations](/paper/carrada-dataset-camera-and-automotive-radar)\r\nImage Source: [https://github.com/valeoai/carrada_dataset](https://github.com/valeoai/carrada_dataset)", "variants": ["CARRADA"]}
{"id": "caWaC", "title": "caWaC -- A web corpus of Catalan and its application to language modeling and machine translation", "contents": "The corpus represents the largest existing corpus of Catalan containing 687 million words, which is a significant increase given that until now the biggest corpus of Catalan, CuCWeb, counts 166 million words. \r\n\r\nSource: [caWaC -- A web corpus of Catalan and its application to language modeling and machine translation](/paper/cawac-a-web-corpus-of-catalan-and-its)", "variants": ["caWaC"]}
{"id": "CC-19", "title": "Blockchain-Federated-Learning and Deep Learning Models for COVID-19 detection using CT Imaging", "contents": "**CC-19** is a small new dataset related to the latest family of coronavirus i.e. COVID-19. The proposed dataset “CC-19” contains 34,006 CT scan slices (images) belonging to 98 subjects out of which 28,395 CT scan slices belong to positive COVID patients.\n\nSource: [https://github.com/abdkhanstd/COVID-19](https://github.com/abdkhanstd/COVID-19)\nImage Source: [https://github.com/abdkhanstd/COVID-19](https://github.com/abdkhanstd/COVID-19)", "variants": ["CC-19"]}
{"id": "CCD", "title": "Uncertainty-based Traffic Accident Anticipation with Spatio-Temporal Relational Learning", "contents": "**Car Crash Dataset** (**CCD**) is collected for traffic accident analysis. It contains real traffic accident videos captured by dashcam mounted on driving vehicles, which is critical to developing safety-guaranteed self-driving systems. CCD is distinguished from existing datasets for diversified accident annotations, including environmental attributes (day/night, snowy/rainy/good weather conditions), whether ego-vehicles involved, accident participants, and accident reason descriptions.\n\nSource: [https://github.com/Cogito2012/CarCrashDataset](https://github.com/Cogito2012/CarCrashDataset)\nImage Source: [https://github.com/Cogito2012/CarCrashDataset](https://github.com/Cogito2012/CarCrashDataset)", "variants": ["CCD"]}
{"id": "CCPD", "title": "Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline", "contents": "The **Chinese City Parking Dataset** (**CCPD**) is a dataset for license plate detection and recognition. It contains over 250k unique car images, with license plate location annotations.\r\n\r\nSource: [Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf)\r\nImage Source: [Xu et al](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf)", "variants": ["CCPD"]}
{"id": "Polish CDSCorpus", "title": "Polish evaluation dataset for compositional distributional semantics models", "contents": "Consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish.\r\n\r\nSource: [Polish evaluation dataset for compositional distributional semantics models](/paper/polish-evaluation-dataset-for-compositional)", "variants": ["Polish CDSCorpus"]}
{"id": "CelebA-Spoof", "title": "CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations", "contents": "CelebA-Spoof is a large-scale face anti-spoofing dataset with the following properties: \r\n\r\n1. Quantity: CelebA-Spoof comprises of 625,537 pictures of 10,177 subjects, significantly larger than the existing datasets. \r\n2. Diversity: The spoof images are captured from 8 scenes (2 environments * 4 illumination conditions) with more than 10 sensors. \r\n3. Annotation Richness: CelebA-Spoof contains 10 spoof type annotations, as well as the 40 attribute annotations inherited from the original CelebA dataset.\r\n\r\nSource: [CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations](/paper/celeba-spoof-large-scale-face-anti-spoofing)", "variants": ["CelebA-Spoof"]}
{"id": "CHECKED", "title": "CHECKED: Chinese COVID-19 Fake News Dataset", "contents": "Chinese dataset on COVID-19 misinformation. CHECKED provides ground-truth on credibility, carefully obtained by ensuring the specific sources are used. CHECKED includes microblogs related to COVID-19, identified by using a specific list of keywords, covering a total 2120 microblogs published from December 2019 to August 2020. The dataset contains a rich set of multimedia information for each microblog including ground-truth label, textual, visual, response, and social network information.\r\n\r\nSource: [CHECKED: Chinese COVID-19 Fake News Dataset](/paper/checked-chinese-covid-19-fake-news-dataset)", "variants": ["CHECKED"]}
{"id": "CIC", "title": "Multilingual Stance Detection: The Catalonia Independence Corpus", "contents": "The dataset is annotated with stance towards one topic, namely, the independence of Catalonia.\r\n\r\nSource: [Multilingual Stance Detection: The Catalonia Independence Corpus](/paper/multilingual-stance-detection-the-catalonia)", "variants": ["CIC-DDoS", "CIC-DoS", "CIC"]}
{"id": "CITIUS Video Database", "title": "", "contents": "This eye tracking video database can be used to validate visual attention models. This dataset includes 72 videos downloaded from Internet and some synthetic videos generated in the lab. The videos can be classified in four categories, natural and synthetic, with fixed or movement camera. It includes 27 synthetic videos with dynamic pop-out effects. The videos have been selected in order to minimize the influence of the top-down effects.\r\n\r\nSource: [CITIUS Video Database](https://wiki.citius.usc.es/aws-d:citius_video_database)", "variants": ["CITIUS Video Database"]}
{"id": "CITR Dataset", "title": "", "contents": "**CITR Dataset** consists of experimentally designed fundamental VCI scenarios (front, back, and lateral VCIs) and provides unique ID for each pedestrian, which is suitable for exploring a specific aspect of VCI. DUT dataset gives two ordinary and natural VCI scenarios in crowded university campus, which can be used for more general purpose VCI exploration.\r\n\r\nSource: [Top-view Trajectories: A Pedestrian Dataset of Vehicle-Crowd Interaction from Controlled Experiments and Crowded Campus](/paper/top-view-trajectories-a-pedestrian-dataset-of)\r\nImage Source: [Yang et al](https://arxiv.org/pdf/1902.00487v2.pdf)", "variants": ["CITR Dataset"]}
{"id": "Cityscapes Panoptic Parts", "title": "", "contents": "The Cityscapes-Panoptic-Parts dataset is an extension of the Cityscapes dataset with panoptic segmentation annotations and par-level labels for selected semantic classes.\n\nSource: [https://arxiv.org/pdf/2004.07944.pdf](https://arxiv.org/pdf/2004.07944.pdf)\nImage Source: [https://github.com/pmeletis/panoptic_parts](https://github.com/pmeletis/panoptic_parts)", "variants": ["Cityscapes Panoptic Parts"]}
{"id": "ClaimBuster", "title": "A Benchmark Dataset of Check-worthy Factual Claims", "contents": "Consist of 23,533 statements extracted from all U.S. general election presidential debates and annotated by human coders. The ClaimBuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. \r\n\r\nSource: [A Benchmark Dataset of Check-worthy Factual Claims](/paper/a-benchmark-dataset-of-check-worthy-factual)", "variants": ["ClaimBuster"]}
{"id": "ClariQ", "title": "ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue Systems (ClariQ)", "contents": "ClariQ is an extension of the Qulac dataset with additional new topics, questions, and answers in the training set. The test set is completely unseen and newly collected. Like Qulac, ClariQ consists of single-turn conversations (initial_request, followed by clarifying question and answer). In addition, it comes with synthetic multi-turn conversations (up to three turns). ClariQ features approximately 18K single-turn conversations, as well as 1.8 million multi-turn conversations. \r\n\r\nSource: [ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue Systems (ClariQ)](/paper/convai3-generating-clarifying-questions-for)", "variants": ["ClariQ"]}
{"id": "Climate Claims", "title": "Generating Fact Checking Summaries for Web Claims", "contents": "The Climate Change Claims dataset for generating fact checking summaries contains claims broadly related to climate change and global warming from climatefeedback.org. It contains 1k documents from 104 different claims from 97 different domains.\n\nSource: [https://arxiv.org/abs/2010.08570](https://arxiv.org/abs/2010.08570)", "variants": ["Climate Claims"]}
{"id": "CLIMATE-FEVER", "title": "CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims", "contents": "A new publicly available dataset for verification of climate change-related claims. \r\n\r\nSource: [CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims](/paper/climate-fever-a-dataset-for-verification-of)", "variants": ["CLIMATE-FEVER"]}
{"id": "CLIRMatrix", "title": "CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval", "contents": "CLIRMatrix is a large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval. It includes:\r\n\r\n* BI-139: A bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs,\r\n* MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages.\r\n\r\nIn total, 49 million unique queries and 34 billion (query, document, label) triplets were mined, making CLIRMatrix the largest and most comprehensive CLIR dataset to date.\r\n\r\nSource: [CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval](/paper/clirmatrix-a-massively-large-collection-of)", "variants": ["CLIRMatrix"]}
{"id": "CloudCast", "title": "CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds", "contents": "A satellite-based dataset called \"CloudCast\". It consists of 70080 images with 10 different cloud types for multiple layers of the atmosphere annotated on a pixel level. \r\n\r\nSource: [CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds](/paper/cloudcast-a-satellite-based-dataset-and)", "variants": ["CloudCast"]}
{"id": "ClovaCall", "title": "ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers", "contents": "**ClovaCall** is a new large-scale Korean call-based speech corpus under a goal-oriented dialog scenario from more than 11,000 people. The raw dataset of ClovaCall includes approximately 112,000 pairs of a short sentence and its corresponding spoken utterance in a restaurant reservation domain.\n\nSource: [https://github.com/ClovaAI/ClovaCall](https://github.com/ClovaAI/ClovaCall)", "variants": ["ClovaCall"]}
{"id": "CLUE", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark", "contents": "CLUE is a Chinese Language Understanding Evaluation benchmark. It consists of different NLU datasets. It is a community-driven project that brings together 9 tasks spanning several well-established single-sentence/sentence-pair classification tasks, as well as machine reading comprehension, all on original Chinese text.", "variants": ["ClueWeb09-B", "CLUE"]}
{"id": "CMD", "title": "Condensed Movies: Story Based Retrieval with Contextual Embeddings", "contents": "Consists of the key scenes from over 3K movies: each key scene is accompanied by a high level semantic description of the scene, character face-tracks, and metadata about the movie. The dataset is scalable, obtained automatically from YouTube, and is freely available for anybody to download and use. \r\n\r\nSource: [Condensed Movies: Story Based Retrieval with Contextual Embeddings](/paper/condensed-movies-story-based-retrieval-with)", "variants": ["CMD"]}
{"id": "CMRC 2019", "title": "A Sentence Cloze Dataset for Chinese Machine Reading Comprehension", "contents": "**CMRC 2019** is a Chinese Machine Reading Comprehension dataset that was used in The Third Evaluation Workshop on Chinese Machine Reading Comprehension. Specifically, CMRC 2019 is a sentence cloze-style machine reading comprehension dataset that aims to evaluate the sentence-level inference ability.\n\nSource: [http://ymcui.com/cmrc2019/](http://ymcui.com/cmrc2019/)", "variants": ["CMRC 2019"]}
{"id": "Coached Conversational Preference Elicitation", "title": "", "contents": "**Coached Conversational Preference Elicitation** is a dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language. It was collected using a Wizard-of-Oz methodology between two paid crowd-workers, where one worker plays the role of an 'assistant', while the other plays the role of a 'user'.\r\n\r\nImage Source: [https://www.aclweb.org/anthology/W19-5941](https://www.aclweb.org/anthology/W19-5941)", "variants": ["Coached Conversational Preference Elicitation"]}
{"id": "CoAID", "title": "CoAID: COVID-19 Healthcare Misinformation Dataset", "contents": "CoAID include diverse COVID-19 healthcare misinformation, including fake news on websites and social platforms, along with users' social engagement about such news. CoAID includes 4,251 news, 296,000 related user engagements, 926 social platform posts about COVID-19, and ground truth labels.\r\n\r\nSource: [CoAID: COVID-19 Healthcare Misinformation Dataset](/paper/coaid-covid-19-healthcare-misinformation)", "variants": ["CoAID"]}
{"id": "Coarse Discourse", "title": "", "contents": "A large corpus of discourse annotations and relations on ~10K forum threads.\r\n\r\nSource: [Coarse Discourse](https://github.com/google-research-datasets/coarse-discourse)", "variants": ["Coarse Discourse"]}
{"id": "CoarseWSD-20", "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation", "contents": "The **CoarseWSD-20** dataset is a coarse-grained sense disambiguation dataset built from Wikipedia (nouns only) targeting 2 to 5 senses of 20 ambiguous words. It was specifically designed to provide an ideal setting for evaluating Word Sense Disambiguation (WSD) models (e.g. no senses in test sets missing from training), both quantitively and qualitatively.\n\nSource: [https://github.com/danlou/bert-disambiguation](https://github.com/danlou/bert-disambiguation)", "variants": ["CoarseWSD-20"]}
{"id": "COCO-WholeBody", "title": "Whole-Body Human Pose Estimation in the Wild", "contents": "Extends COCO dataset with whole-body annotations. \r\n\r\nSource: [Whole-Body Human Pose Estimation in the Wild](/paper/whole-body-human-pose-estimation-in-the-wild)", "variants": ["COCO-WholeBody"]}
{"id": "CoDEx", "title": "CoDEx: A Comprehensive Knowledge Graph Completion Benchmark", "contents": "CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. \r\n\r\nSource: [CoDEx: A Comprehensive Knowledge Graph Completion Benchmark](/paper/codex-a-comprehensive-knowledge-graph)", "variants": ["CoDEx"]}
{"id": "COMETA", "title": "COMETA: A Corpus for Medical Entity Linking in the Social Media", "contents": "Consists of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph.\r\n\r\nSource: [COMETA: A Corpus for Medical Entity Linking in the Social Media](/paper/cometa-a-corpus-for-medical-entity-linking-in)", "variants": ["COMETA"]}
{"id": "CompGuessWhat?!", "title": "CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning", "contents": "CompGuessWhat?! extends the original GuessWhat?! datasets with a rich semantic representations in the form of scene graphs associated with every image used as reference scene for the guessing games.\r\n\r\nSource: [CompGuessWhat?!](https://compguesswhat.github.io/download/)", "variants": ["CompGuessWhat?!"]}
{"id": "CoNLL-2000", "title": "", "contents": "CoNLL-2000 is a dataset for dividing text into syntactically related non-overlapping groups of words, so-called text chunking. \r\n\r\nSource: [https://www.clips.uantwerpen.be/conll2000/chunking/](https://www.clips.uantwerpen.be/conll2000/chunking/)", "variants": ["CoNLL-2000"]}
{"id": "ContentWise Impressions", "title": "ContentWise Impressions: An Industrial Dataset with Impressions Included", "contents": "The **ContentWise Impressions** dataset is a collection of implicit interactions and impressions of movies and TV series from an Over-The-Top media service, which delivers its media contents over the Internet. The dataset is distinguished from other already available multimedia recommendation datasets by the availability of impressions, i.e., the recommendations shown to the user, its size, and by being open-source.\nThe items in the dataset represent the multimedia content that the service provided to the users and are represented by an anonymized numerical identifier. The items refer to television and cinema products belonging to four mutually exclusive categories: movies, movies and clips in series, TV movies or shows, and episodes of TV series.\nThe interactions represent the actions performed by users on items in the service and are associated with the timestamp when it occurred. Interactions contain the identifier of the impressions, except in those cases where the recommendations came from a row added by the service provider. The interactions are categorized in four different types: views, detail, ratings, and purchases.\nThe impressions refer to the recommended items that were presented to the user and are identified by their series. Impressions consist of a numerical identifier, the list position on the screen, the length of the recommendation list, and an ordered list of recommended series identifiers, where the most relevant item is in the first position.\n\nSource: [https://github.com/ContentWise/contentwise-impressions](https://github.com/ContentWise/contentwise-impressions)", "variants": ["ContentWise Impressions"]}
{"id": "Cookie", "title": "COOKIE: A Dataset for Conversational Recommendation over Knowledge Graphs in E-commerce", "contents": "The dataset is constructed from an Amazon review corpus by integrating both user-agent dialogue and custom knowledge graphs for recommendation.\r\n\r\nSource: [COOKIE: A Dataset for Conversational Recommendation over Knowledge Graphs in E-commerce](/paper/cookie-a-dataset-for-conversational)", "variants": ["Cookie"]}
{"id": "Cornell Movie-Dialogs Corpus", "title": "", "contents": "This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:\r\n\r\n- 220,579 conversational exchanges between 10,292 pairs of movie characters\r\n- involves 9,035 characters from 617 movies\r\n- in total 304,713 utterances\r\n- movie metadata included:\r\n    - genres\r\n    - release year\r\n    - IMDB rating\r\n    - number of IMDB votes\r\n    - IMDB rating\r\n- character metadata included:\r\n    - gender (for 3,774 characters)\r\n    - position on movie credits (3,321 characters)\r\n\r\nSource: [Cornell Movie-Dialogs Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)", "variants": ["Cornell Movie-Dialogs Corpus"]}
{"id": "COUGH", "title": "COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval", "contents": "A large challenging dataset, COUGH, for COVID-19 FAQ retrieval. Specifically, similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, User Query Bank and Annotated Relevance Set. FAQ Bank contains ~16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO).\r\n\r\nSource: [COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval](/paper/cough-a-challenge-dataset-and-models-for)", "variants": ["COUGH"]}
{"id": "COVERAGE", "title": "", "contents": "COVERAGE contains copymove forged (CMFD) images and their originals with similar but genuine objects (SGOs). COVERAGE is designed to highlight and address tamper detection ambiguity of popular methods, caused by self-similarity within natural images. In COVERAGE, forged–original pairs are annotated with (i) the duplicated and forged region masks, and (ii) the tampering factor/similarity metric. For benchmarking, forgery quality is evaluated using (i) computer vision-based methods, and (ii) human detection performance.\r\n\r\nSource: [COVERAGE](https://github.com/wenbihan/coverage)", "variants": ["COVERAGE"]}
{"id": "COVID19-Algeria-and-World-Dataset", "title": "COVID-19 Data Analysis and Forecasting: Algeria and the World", "contents": "A coronavirus dataset with 98 countries constructed from different reliable sources, where each row represents a country, and the columns represent geographic, climate, healthcare, economic, and demographic factors that may contribute to accelerate/slow the spread of the COVID-19. The assumptions for the different factors are as follows:\n\nSource: [https://github.com/SamBelkacem/COVID19-Algeria-and-World-Dataset](https://github.com/SamBelkacem/COVID19-Algeria-and-World-Dataset)\nImage Source: [https://github.com/SamBelkacem/COVID19-Algeria-and-World-Dataset](https://github.com/SamBelkacem/COVID19-Algeria-and-World-Dataset)", "variants": ["COVID19-Algeria-and-World-Dataset"]}
{"id": "COVID19-CountryImage", "title": "Country Image in COVID-19 Pandemic: A Case Study of China", "contents": "The Covid19-CountryImage dataset is a Twitter dataset which contains COVID-19-related tweets.\n\nSource: [https://github.com/thunlp/COVID19-CountryImage](https://github.com/thunlp/COVID19-CountryImage)", "variants": ["COVID19-CountryImage"]}
{"id": "COVID-19-CT-CXR", "title": "COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and CT image collection on COVID-19 from biomedical literature", "contents": "A public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. \r\n\r\nSource: [COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and CT image collection on COVID-19 from biomedical literature](/paper/covid-19-ct-cxr-a-freely-accessible-and)", "variants": ["COVID-19-CT-CXR"]}
{"id": "COVID-CQ", "title": "A Stance Data Set on Polarized Conversations on Twitter about the Efficacy of Hydroxychloroquine as a Treatment for COVID-19", "contents": "COVID-CQ is a stance data set of user-generated content on Twitter in the context of COVID-19.\r\n\r\nSource: [A Stance Data Set on Polarized Conversations on Twitter about the Efficacy of Hydroxychloroquine as a Treatment for COVID-19](/paper/a-stance-data-set-on-polarized-conversations)\r\nImage Source: [https://arxiv.org/pdf/2009.01188v2.pdf](https://arxiv.org/pdf/2009.01188v2.pdf)", "variants": ["COVID-CQ"]}
{"id": "COVIDGR", "title": "COVIDGR dataset and COVID-SDNet methodology for predicting COVID-19 based on Chest X-Ray images", "contents": "Under a close collaboration with an expert radiologist team of the Hospital Universitario San Cecilio, the **COVIDGR**-1.0 dataset of patients' anonymized X-ray images has been built. 852 images have been collected following a strict labeling protocol. They are categorized into 426 positive cases and 426 negative cases. Positive images correspond to patients who have been tested positive for COVID-19 using RT-PCR within a time span of at most 24h between the X-ray image and the test. Every image has been taken using the same type of equipment and with the same format: only the posterior-anterior view is considered.\n\nSource: [https://github.com/ari-dasci/covidgr](https://github.com/ari-dasci/covidgr)", "variants": ["COVIDGR"]}
{"id": "Covid-HeRA", "title": "Drink bleach or do what now? Covid-HeRA: A dataset for risk-informed health decision making in the presence of COVID19 misinformation", "contents": "**Covid-HeRA** is a dataset for health risk assessment and severity-informed decision making in the presence of COVID19 misinformation. It is a benchmark dataset for risk-aware health misinformation detection, related to the 2019 coronavirus pandemic. Social media posts (Twitter) are annotated based on the perceived likelihood of health behavioural changes and the perceived corresponding risks from following unreliable advice found online.\n\nSource: [https://github.com/TIMAN-group/covid19_misinformation](https://github.com/TIMAN-group/covid19_misinformation)", "variants": ["Covid-HeRA"]}
{"id": "CovidQA", "title": "Rapidly Bootstrapping a Question Answering Dataset for COVID-19", "contents": "The beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge. \r\n\r\nSource: [Rapidly Bootstrapping a Question Answering Dataset for COVID-19](/paper/rapidly-bootstrapping-a-question-answering)", "variants": ["CovidQA"]}
{"id": "CPCXR", "title": "Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned deep neural networks", "contents": "The **COVID-19 Posteroanterior Chest X-Ray fused** (**CPCXR**) dataset is generated by the fusion of three publicly available datasets: COVID-19 cxr image, Radiological Society of North America (RSNA), and U.S. national library of medicine (USNLM) collected Montgomery country - NLM(MC). The dataset consists of samples of diseases labeled as COVID-19, Tuberculosis, Other pneumonia (SARS, MERS, etc.), and Normal. The dataset can be utilized to train an evaulate deep learning and machine learning models as binary and multi-class classification problem.\n\nSource: [https://github.com/nspunn1993/COVID-19-PA-CXR-fused-dataset](https://github.com/nspunn1993/COVID-19-PA-CXR-fused-dataset)", "variants": ["CPCXR"]}
{"id": "CPLFW", "title": "", "contents": "A renovation of Labeled Faces in the Wild (LFW), the de facto standard testbed for unconstraint face verification. \r\n\r\nThere are three motivations behind the construction of CPLFW benchmark as follows:\r\n\r\n1.Establishing a relatively more difficult database to evaluate the performance of real world face verification so the effectiveness of several face verification methods can be fully justified.\r\n\r\n2.Continuing the intensive research on LFW with more realistic consideration on pose intra-class variation and fostering the research on cross-pose face verification in unconstrained situation. The challenge of CPLFW emphasizes pose difference to further enlarge intra-class variance. Also, negative pairs are deliberately selected to avoid different gender or race. CPLFW considers both the large intra-class variance and the tiny inter-class variance simultaneously.\r\n\r\n3.Maintaining the data size, the face verification protocol which provides a 'same/different' benchmark and the same identities in LFW, so one can easily apply CPLFW to evaluate the performance of face verification.\r\n\r\nSource: [CPLFW](http://whdeng.cn/CPLFW/index.html)", "variants": ["CPLFW"]}
{"id": "Common Crawl Domain Names", "title": "", "contents": "Corpus of domain names scraped from Common Crawl and manually annotated to add word boundaries (e.g. \"commoncrawl\" to \"common crawl\").\r\n\r\nSource: [Common Crawl Domain Names](https://github.com/google-research-datasets/common-crawl-domain-names)", "variants": ["Common Crawl Domain Names"]}
{"id": "CRD3", "title": "Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset", "contents": "The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction.\r\n\r\nSource: [Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset](/paper/storytelling-with-dialogue-a-critical-role)", "variants": ["CRD3"]}
{"id": "CRL-Person", "title": "Continual Representation Learning for Biometric Identification", "contents": "Provides two large-scale multi-step benchmarks for biometric identification, where the visual appearance of different classes are highly relevant.\r\n\r\nSource: [Continual Representation Learning for Biometric Identification](/paper/continual-representation-learning-for)", "variants": ["CRL-Person"]}
{"id": "CrowS-Pairs", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models", "contents": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups.\r\n\r\nSource: [CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models](/paper/crows-pairs-a-challenge-dataset-for-measuring)", "variants": ["CrowS-Pairs"]}
{"id": "CS", "title": "Writing Polishment with Simile: Task, Dataset and A Neural Approach", "contents": "This dataset is constructed and based on the online free-access fictions that are tagged with sci-fi, urban novel, love story, youth, etc. It is used for Writing Polishment with Smile (WPS) a task that aims to polish plain text with similes.\nAll similes are extracted by rich regular expression, and the extraction precision is estimated as 92% by labelling 500 random extracted samples.\nIt contains 5M samples for training and 2.5k for validation and test respectively.\n\nSource: [https://github.com/mrzjy/writing-polishment-with-simile](https://github.com/mrzjy/writing-polishment-with-simile)", "variants": ["CS"]}
{"id": "CTC", "title": "StacMR: Scene-Text Aware Cross-Modal Retrieval", "contents": "A dataset that allows exploration of cross-modal retrieval where images contain scene-text instances. \r\n\r\nSource: [StacMR: Scene-Text Aware Cross-Modal Retrieval](/paper/stacmr-scene-text-aware-cross-modal-retrieval)", "variants": ["CTC"]}
{"id": "CTPelvic1K", "title": "Deep Learning to Segment Pelvic Bones: Large-scale CT Datasets and Baseline Models", "contents": "Curates a large pelvic CT dataset pooled from multiple sources and different manufacturers, including 1, 184 CT volumes and over 320, 000 slices with different resolutions and a variety of the above-mentioned appearance variations.\r\n\r\nSource: [Deep Learning to Segment Pelvic Bones: Large-scale CT Datasets and Baseline Models](/paper/deep-learning-to-segment-pelvic-bones-large)", "variants": ["CTPelvic1K"]}
{"id": "Cube++", "title": "The Cube++ Illumination Estimation Dataset", "contents": "**Cube++** is a novel dataset for the color constancy problem that continues on the Cube+ dataset. It includes 4890 images of different scenes under various conditions. For calculating the ground truth illumination, a calibration object with known surface colors was placed in every scene.\n\nSource: [https://github.com/Visillect/CubePlusPlus](https://github.com/Visillect/CubePlusPlus)\nImage Source: [https://github.com/Visillect/CubePlusPlus](https://github.com/Visillect/CubePlusPlus)", "variants": ["Cube Engraving", "Cube++"]}
{"id": "Curation Corpus", "title": "", "contents": "The Curation Corpus is a collection of 40,000 professionally-written summaries of news articles, with links to the articles themselves. \r\n\r\nSource: [Curation Corpus](https://github.com/CurationCorp/curation-corpus)", "variants": ["Curation Corpus"]}
{"id": "Curiosity", "title": "Information Seeking in the Spirit of Learning: a Dataset for Conversational Curiosity", "contents": "The **Curiosity** dataset consists of 14K dialogs (with 181K utterances) with fine-grained knowledge groundings, dialog act annotations, and other auxiliary annotation. In this dataset users and virtual assistants converse about geographic topics like geopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages.\n\nSource: [https://github.com/facebookresearch/curiosity](https://github.com/facebookresearch/curiosity)\nImage Source: [https://www.pedro.ai/curiosity](https://www.pedro.ai/curiosity)", "variants": ["Curiosity"]}
{"id": "CzEng 2.0 Parallel Corpus", "title": "Announcing CzEng 2.0 Parallel Corpus with over 2 Gigawords", "contents": "Czech-English parallel corpus CzEng 2.0 consisting of over 2 billion words (2 \"gigawords\") in each language. The corpus contains document-level information and is filtered with several techniques to lower the amount of noise.\r\n\r\nSource: [Announcing CzEng 2.0 Parallel Corpus with over 2 Gigawords](/paper/announcing-czeng-2-0-parallel-corpus-with)", "variants": ["CzEng 2.0 Parallel Corpus"]}
{"id": "DAD", "title": "Driver Anomaly Detection: A Dataset and Contrastive Learning Approach", "contents": "Contains normal driving videos together with a set of anomalous actions in its training set. In the test set of the DAD dataset, there are unseen anomalous actions that still need to be winnowed out from normal driving. \r\n\r\nSource: [Driver Anomaly Detection: A Dataset and Contrastive Learning Approach](/paper/driver-anomaly-detection-a-dataset-and)", "variants": ["DAD"]}
{"id": "DAIS", "title": "Investigating representations of verb bias in neural language models", "contents": "A large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. \r\n\r\nSource: [Investigating representations of verb bias in neural language models](/paper/investigating-representations-of-verb-bias-in)", "variants": ["DAIS"]}
{"id": "DANBOORU2020", "title": "", "contents": "A large-scale anime image database with 4.2m+ images annotated with 130m+ tags; it can be useful for machine learning purposes such as image recognition and generation.\r\n\r\nSource: [DANBOORU2020](https://www.gwern.net/Danbooru2020)", "variants": ["DANBOORU2020"]}
{"id": "DaNE", "title": "DaNE: A Named Entity Resource for Danish", "contents": "Danish Dependency Treebank (DaNE) is a named entity annotation for the Danish Universal Dependencies treebank using the CoNLL-2003 annotation scheme.\r\n\r\nSource: [DaNE: A Named Entity Resource for Danish](/paper/dane-a-named-entity-resource-for-danish)", "variants": ["DaNE"]}
{"id": "DART", "title": "DART: Open-Domain Structured Data Record to Text Generation", "contents": "DART is a large dataset for open-domain structured data record to text generation. DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set.\r\n\r\nSource: [DART: Open-Domain Structured Data Record to Text Generation](/paper/dart-open-domain-structured-data-record-to)", "variants": ["DART"]}
{"id": "Da Vinci Dataset", "title": "", "contents": "A line drawing restoration dataset which consists of 71 line drawing sketches by Leonardo Da Vinci.\r\n\r\nSource: [Da Vinci Dataset](http://hi.cs.waseda.ac.jp/~esimo/en/data/davincidataset/)", "variants": ["Da Vinci Dataset"]}
{"id": "DAWN", "title": "DAWN: Vehicle Detection in Adverse Weather Nature Dataset", "contents": "DAWN emphasizes a diverse traffic environment (urban, highway and freeway) as well as a rich variety of traffic flow. The DAWN dataset comprises a collection of 1000 images from real-traffic environments, which are divided into four sets of weather conditions: fog, snow, rain and sandstorms. The dataset is annotated with object bounding boxes for autonomous driving and video surveillance scenarios. This data helps interpreting effects caused by the adverse weather conditions on the performance of vehicle detection systems.\r\n\r\nSource: [DAWN: Vehicle Detection in Adverse Weather Nature Dataset](/paper/dawn-vehicle-detection-in-adverse-weather)\r\nImage Source: [Kenk and Hassaballah](https://arxiv.org/pdf/2008.05402v1.pdf)", "variants": ["DAWN"]}
{"id": "DDD20", "title": "DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events with Deep Learning for Improved Steering Prediction", "contents": "The dataset was captured with a DAVIS camera that concurrently streams both dynamic vision sensor (DVS) brightness change events and active pixel sensor (APS) intensity frames. DDD20 is the longest event camera end-to-end driving dataset to date with 51h of DAVIS event+frame camera and vehicle human control data collected from 4000km of highway and urban driving under a variety of lighting conditions.\r\n\r\nSource: [DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events with Deep Learning for Improved Steering Prediction](/paper/ddd20-end-to-end-event-camera-driving-dataset)", "variants": ["DDD20"]}
{"id": "DeepFish", "title": "A Realistic Fish-Habitat Dataset to Evaluate Algorithms for Underwater Visual Analysis", "contents": "**DeepFish** as a benchmark suite with a large-scale dataset to train and test methods for several computer vision tasks. The dataset consists of approximately 40 thousand images collected underwater from 20 habitats in the marine environments of tropical Australia. It contains classification labels as well as point-level and segmentation labels to have a more comprehensive fish analysis benchmark. These labels enable models to learn to automatically monitor fish count, identify their locations, and estimate their sizes.\n\nSource: [https://github.com/alzayats/DeepFish](https://github.com/alzayats/DeepFish)\nImage Source: [https://github.com/alzayats/DeepFish](https://github.com/alzayats/DeepFish)", "variants": ["DeepFish"]}
{"id": "OST", "title": "Deep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks", "contents": "Is one of the largest egocentric datasets in the object search task with eyetracking information available\r\n\r\nSource: [Deep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Deep_Future_Gaze_CVPR_2017_paper.pdf)", "variants": ["OST"]}
{"id": "Dengue", "title": "", "contents": "Benchmark dataset for low-resource multiclass classification, with 4,015 training, 500 testing, and 500 validation examples, each labeled as part of five classes. Each sample can be a part of multiple classes. Collected as tweets and originally used in Livelo & Cheng (2018).\r\n\r\nSource: [Dengue](https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks)", "variants": ["Dengue"]}
{"id": "DeSMOG", "title": "Detecting Stance in Media on Global Warming", "contents": "A dataset of stance-labeled GW sentences.\r\n\r\nSource: [DeSMOG: Detecting Stance in Media On Global Warming](/paper/desmog-detecting-stance-in-media-on-global)", "variants": ["DeSMOG"]}
{"id": "DET", "title": "", "contents": "DET is a lane detection dataset that consists of the raw event data, accumulated images over 30ms and corresponding lane labels. Contains 17,103 lane instances, each of which is labeled pixel by pixel manually. \r\n\r\nSource: [DET](https://spritea.github.io/DET/)", "variants": ["DET"]}
{"id": "DHP19", "title": "", "contents": "DHP19 is the first human pose dataset with data collected from DVS event cameras. \r\n\r\nIt has recordings from 4 synchronized 346x260 pixel DVS cameras and marker positions in 3D space from Vicon motion capture system. The files have event streams and 3D positions recorded from 17 subjects each performing 33 movements.\r\n\r\nSource: [DHP19](https://sites.google.com/view/dhp19/home)", "variants": ["DHP19"]}
{"id": "Diabetes60", "title": "", "contents": "RGB-D images of 60 western dishes, home made. Data was recorded using a Microsoft Kinect V2.\r\n\r\nSource: [Diabetes60 Dataset](https://github.com/PatrickChrist/diabetes60)", "variants": ["Diabetes60"]}
{"id": "DialoGLUE", "title": "DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue", "contents": "DialoGLUE is a natural language understanding nenchmark for task-oriented dialogue designed to encourage dialogue research in representation-based transfer, domain adaptation, and sample-efficient task learning. It consisting of 7 task-oriented dialogue datasets covering 4 distinct natural language understanding tasks.\r\n\r\nSource: [DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue](/paper/dialoglue-a-natural-language-understanding)", "variants": ["DialoGLUE full", "DialoGLUE fewshot", "DialoGLUE"]}
{"id": "DIB-10K", "title": "The DongNiao International Birds 10000 Dataset", "contents": "Is a challenging image dataset which has more than 10 thousand different types of birds. It was created to enable the study of machine learning and also ornithology research.\r\n\r\nSource: [The DongNiao International Birds 10000 Dataset](/paper/the-dongniao-international-birds-10000)", "variants": ["DIB-10K"]}
{"id": "DLBCL-Morph", "title": "DLBCL-Morph: Morphological features computed using deep learning for an annotated digital DLBCL image set", "contents": "**DLBCL-Morph** is a dataset containing 42 digitally scanned high-resolution tissue microarray (TMA) slides accompanied by clinical, cytogenetic, and geometric features from 209 DLBCL cases.\n\nSource: [https://github.com/stanfordmlgroup/DLBCL-Morph](https://github.com/stanfordmlgroup/DLBCL-Morph)", "variants": ["DLBCL-Morph"]}
{"id": "dMelodies", "title": "dMelodies: A Music Dataset for Disentanglement Learning", "contents": "**dMelodies** is dataset of simple 2-bar melodies generated using 9 independent latent factors of variation where each data point represents a unique melody based on the following constraints:\n- Each melody will correspond to a unique scale (major, minor, blues, etc.).\n- Each melody plays the arpeggios using the standard I-IV-V-I cadence chord pattern.\n- Bar 1 plays the first 2 chords (6 notes), Bar 2 plays the second 2 chords (6 notes).\n- Each played note is an 8th note.\n\nSource: [https://github.com/ashispati/dmelodies_dataset](https://github.com/ashispati/dmelodies_dataset)", "variants": ["dMelodies"]}
{"id": "doc2dial", "title": "doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset", "contents": "A new dataset of goal-oriented dialogues that are grounded in the associated documents.\r\n\r\nSource: [doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset](/paper/doc2dial-a-goal-oriented-document-grounded)", "variants": ["doc2dial"]}
{"id": "DocBank", "title": "DocBank: A Benchmark Dataset for Document Layout Analysis", "contents": "A benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the \\LaTeX{} documents available on the arXiv.com.\r\n\r\nSource: [DocBank: A Benchmark Dataset for Document Layout Analysis](/paper/docbank-a-benchmark-dataset-for-document)", "variants": ["DocBank"]}
{"id": "DocVQA", "title": "DocVQA: A Dataset for VQA on Document Images", "contents": "DocVQA consists of 50,000 questions defined on 12,000+ document images.\r\n\r\nSource: [DocVQA: A Dataset for VQA on Document Images](/paper/docvqa-a-dataset-for-vqa-on-document-images)", "variants": ["DocVQA val", "DocVQA test", "DocVQA"]}
{"id": "DOGC", "title": "Parallel Data, Tools and Interfaces in OPUS", "contents": "Intended to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies.\r\n\r\nSource: [Parallel Data, Tools and Interfaces in OPUS](/paper/parallel-data-tools-and-interfaces-in-opus)", "variants": ["DOGC"]}
{"id": "DoQA", "title": "DoQA -- Accessing Domain-Specific FAQs via Conversational QA", "contents": "A dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing.\r\n\r\nSource: [DoQA -- Accessing Domain-Specific FAQs via Conversational QA](/paper/doqa-accessing-domain-specific-faqs-via)", "variants": ["DoQA"]}
{"id": "DramaQA", "title": "DramaQA: Character-Centered Video Story Understanding with Hierarchical QA", "contents": "The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an evaluation metric based on the cognitive developmental stages of human intelligence. 2) Character-centered video annotations to model local coherence of the story. The dataset is built upon the TV drama \"Another Miss Oh\" and it contains 17,983 QA pairs from 23,928 various length video clips, with each QA pair belonging to one of four difficulty levels.\r\n\r\nSource: [DramaQA: Character-Centered Video Story Understanding with Hierarchical QA](/paper/dramaqa-character-centered-video-story)", "variants": ["DramaQA"]}
{"id": "dSprites", "title": "", "contents": "dSprites is a dataset of 2D shapes procedurally generated from 6 ground truth independent latent factors. These factors are color, shape, scale, rotation, x and y positions of a sprite.\r\n\r\nAll possible combinations of these latents are present exactly once, generating N = 737280 total images.\r\n\r\nSource: [dSprites](https://github.com/deepmind/dsprites-dataset)", "variants": ["dSprites"]}
{"id": "DuRecDial", "title": "Towards Conversational Recommendation over Multi-Type Dialogs", "contents": "A human-to-human Chinese dialog dataset (about 10k dialogs, 156k utterances), which contains multiple sequential dialogs for every pair of a recommendation seeker (user) and a recommender (bot). \r\n\r\nSource: [Towards Conversational Recommendation over Multi-Type Dialogs](/paper/towards-conversational-recommendation-over)", "variants": ["DuRecDial"]}
{"id": "DWIE", "title": "DWIE: an entity-centric dataset for multi-task document-level information extraction", "contents": "The '**Deutsche Welle corpus for Information Extraction**' (**DWIE**) is a multi-task dataset that combines four main Information Extraction (IE) annotation sub-tasks: (i) Named Entity Recognition (NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv) Entity Linking. DWIE is conceived as an entity-centric dataset that describes interactions and properties of conceptual entities on the level of the complete document.\n\nSource: [https://arxiv.org/abs/2009.12626](https://arxiv.org/abs/2009.12626)", "variants": ["DWIE"]}
{"id": "DynaSent", "title": "DynaSent: A Dynamic Benchmark for Sentiment Analysis", "contents": "DynaSent is an English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-in-the-loop dataset creation. DynaSent has a total of 121,634 sentences, each validated by five crowdworkers.\r\n\r\nSource: [DynaSent: A Dynamic Benchmark for Sentiment Analysis](/paper/dynasent-a-dynamic-benchmark-for-sentiment)", "variants": ["DynaSent"]}
{"id": "E2E", "title": "Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge", "contents": "End-to-End NLG Challenge (E2E) aims to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena.\r\n\r\nSource: [Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge](/paper/evaluating-the-state-of-the-art-of-end-to-end)", "variants": ["E2E NLG Challenge", "Cleaned E2E NLG Challenge", "E2E"]}
{"id": "Edge-Map-345C", "title": "On Learning Semantic Representations for Million-Scale Free-Hand Sketches", "contents": "**Edge-Map-345C** is a large-scale edge-map dataset including 290,281 edge-maps corresponding to 345 object categories of QuickDraw dataset. In particular, these 345 categories are corresponding to the 345 free-hand sketch categories of Google QuickDraw dataset.\n\nSource: [https://github.com/PengBoXiangShang/EdgeMap345C_Dataset](https://github.com/PengBoXiangShang/EdgeMap345C_Dataset)\nImage Source: [https://github.com/PengBoXiangShang/EdgeMap345C_Dataset](https://github.com/PengBoXiangShang/EdgeMap345C_Dataset)", "variants": ["Edge-Map-345C"]}
{"id": "EGOK360", "title": "Egok360: A 360 Egocentric Kinetic Human Activity Video Dataset", "contents": "Contains annotations of human activity with different sub-actions, e.g., activity Ping-Pong with four sub-actions which are pickup-ball, hit, bounce-ball and serve. \r\n\r\nSource: [Egok360: A 360 Egocentric Kinetic Human Activity Video Dataset](/paper/egok360-a-360-egocentric-kinetic-human)", "variants": ["EGOK360"]}
{"id": "EiTB-ParCC", "title": "Handle with Care: A Case Study in Comparable Corpora Exploitation for Neural Machine Translation", "contents": "A large comparable corpus for Basque-Spanish was prepared, on the basis of independently-produced news by the Basque public broadcaster EiTB.\r\n\r\nSource: [Handle with Care: A Case Study in Comparable Corpora Exploitation for Neural Machine Translation](/paper/handle-with-care-a-case-study-in-comparable)", "variants": ["EiTB-ParCC"]}
{"id": "Elsevier OA CC-BY", "title": "Elsevier OA CC-By Corpus", "contents": "An open corpus of Scientific Research papers which has a representative sample from across scientific disciplines. This corpus not only includes the full text of the article, but also the metadata of the documents, along with the bibliographic information for each reference.\r\n\r\nSource: [Elsevier OA CC-By Corpus](/paper/elsevier-oa-cc-by-corpus)", "variants": ["Elsevier OA CC-BY"]}
{"id": "EMU", "title": "Edited Media Understanding: Reasoning About Implications of Manipulated Images", "contents": "48k question-answer pairs written in rich natural language.\r\n\r\nSource: [Edited Media Understanding: Reasoning About Implications of Manipulated Images](/paper/edited-media-understanding-reasoning-about)", "variants": ["EMU"]}
{"id": "EndoSLAM", "title": "EndoSLAM Dataset and An Unsupervised Monocular Visual Odometry and Depth Estimation Approach for Endoscopic Videos: Endo-SfMLearner", "contents": "The endoscopic SLAM dataset (**EndoSLAM**) is a dataset for depth estimation approach for endoscopic videos. It consists of both ex-vivo and synthetically generated data. The ex-vivo part of the dataset includes standard as well as capsule endoscopy recordings. The dataset is divided into 35 sub-datasets. Specifically, 18, 5 and 12 sub-datasets exist for colon, small intestine and stomach respectively.\n\nSource: [https://github.com/CapsuleEndoscope/EndoSLAM](https://github.com/CapsuleEndoscope/EndoSLAM)\nImage Source: [https://github.com/CapsuleEndoscope/EndoSLAM](https://github.com/CapsuleEndoscope/EndoSLAM)", "variants": ["EndoSLAM"]}
{"id": "ENT-DESC", "title": "ENT-DESC: Entity Description Generation by Exploring Knowledge Graph", "contents": "ENT-DESC involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions.\r\n\r\nSource: [ENT-DESC: Entity Description Generation by Exploring Knowledge Graph](/paper/knowledge-graph-empowered-entity-description)", "variants": ["ENT-DESC"]}
{"id": "EORSSD", "title": "Dense Attention Fluid Network for Salient Object Detection in Optical Remote Sensing Images", "contents": "The **Extended Optical Remote Sensing Saliency Detection** (**EORSSD**) dataset is an extension of the ORSSD dataset. This new dataset is larger and more varied than the original. It contains 2,000 images and corresponding pixel-wise ground truth, which includes many semantically meaningful but challenging images.\r\n\r\nSource: [https://github.com/rmcong/EORSSD-dataset](https://github.com/rmcong/EORSSD-dataset)\r\nImage Source: [Zhang et al](https://arxiv.org/pdf/2011.13144v1.pdf)", "variants": ["EORSSD"]}
{"id": "EPIE", "title": "EPIE Dataset: A Corpus For Possible Idiomatic Expressions", "contents": "Corpus containing 25206 sentences labelled with lexical instances of 717 idiomatic expressions. These spans also cover literal usages for the given set of idiomatic expressions. \r\n\r\nSource: [EPIE Dataset: A Corpus For Possible Idiomatic Expressions](/paper/epie-dataset-a-corpus-for-possible-idiomatic)", "variants": ["EPIE"]}
{"id": "ESAD", "title": "ESAD: Endoscopic Surgeon Action Detection Dataset", "contents": "A challenging dataset for surgeon action detection in real-world endoscopic videos. \r\n\r\nSource: [ESAD: Endoscopic Surgeon Action Detection Dataset](/paper/esad-endoscopic-surgeon-action-detection)", "variants": ["ESAD"]}
{"id": "eSports Sensors Dataset", "title": "Collection and Validation of Psycophysiological Data from Professional and Amateur Players: a Multimodal eSports Dataset", "contents": "The eSports Sensors dataset contains sensor data collected from 10 players in 22 matches in League of Legends. The sensor data collected includes:\n* Hand/head/chair movements.\n* Heart rate.\n* Muscle activity.\n* Gaze movement on the monitor.\n* Galvanic skin response(GSR).\n* Electroencephalography (EEG).\n* Mouse and keyboard activity.\n* Facial skin temperature.\n* Environmental data.\nThe data were collected for one team of 5 people simultaneously. In-game logs and meta information for each match are also provided for each match.\n\nSource: [https://github.com/asmerdov/eSports_Sensors_Dataset](https://github.com/asmerdov/eSports_Sensors_Dataset)", "variants": ["eSports Sensors Dataset"]}
{"id": "ETHICS", "title": "Aligning AI With Shared Human Values", "contents": "A new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality.\r\n\r\nSource: [Aligning AI With Shared Human Values](/paper/aligning-ai-with-shared-human-values)", "variants": ["Ethics", "ETHICS"]}
{"id": "ETHOS", "title": "ETHOS: an Online Hate Speech Detection Dataset", "contents": "**ETHOS** is a hate speck detection dataset. It is built from Youtube and Reddit comments validated through a crowdsourcing platform. It has two subsets, one for binary classification and the other for multi-label classification. The former contains 998 comments, while the latter contains fine-grained hate-speech annotations for 433 comments.\n\nSource: [https://arxiv.org/abs/2006.08328](https://arxiv.org/abs/2006.08328)", "variants": ["Ethos Binary", "Ethos MultiLabel", "ETHOS"]}
{"id": "ETH-XGaze", "title": "ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation", "contents": "Consists of over one million high-resolution images of varying gaze under extreme head poses. The dataset is collected from 110 participants with a custom hardware setup including 18 digital SLR cameras and adjustable illumination conditions, and a calibrated system to record ground truth gaze targets. \r\n\r\nSource: [ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation](/paper/eth-xgaze-a-large-scale-dataset-for-gaze)\r\n\r\nOfficial competition on Codalab: [https://competitions.codalab.org/competitions/28930](https://competitions.codalab.org/competitions/28930)", "variants": ["ETH-XGaze"]}
{"id": "eTRIMS Image Database", "title": "", "contents": "The database is comprised of two datasets, the 4-Class eTRIMS Dataset with 4 annotated object classes and the 8-Class eTRIMS Dataset with 8 annotated object classes.\r\n\r\nSource: [eTRIMS Image Database](http://www.ipb.uni-bonn.de/projects/etrims_db/)", "variants": ["eTRIMS Image Database"]}
{"id": "ETT", "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting", "contents": "The **Electricity Transformer Temperature** (**ETT**) is a crucial indicator in the electric power long-term deployment. This dataset consists of 2 years data from two separated counties in China. To explore the granularity on the Long sequence time-series forecasting (LSTF) problem, different subsets are created, {ETTh1, ETTh2} for 1-hour-level and ETTm1 for 15-minutes-level. Each data point consists of the target value ”oil temperature” and 6 power load features. The train/val/test is 12/4/4 months.\n\nSource: [https://arxiv.org/pdf/2012.07436.pdf](https://arxiv.org/pdf/2012.07436.pdf)\nImage Source: [https://github.com/zhouhaoyi/ETDataset](https://github.com/zhouhaoyi/ETDataset)", "variants": ["ETT"]}
{"id": "Europeana Newspapers", "title": "An Open Corpus for Named Entity Recognition in Historic Newspapers", "contents": "Europeana Newspapers consists of four datasets with 100 pages each for the languages Dutch, French, German (including Austrian) as part of the Europeana Newspapers project is expected to contribute to the further development and improvement of named entity recognition systems with a focus on historical content.\r\n\r\nSource: [An Open Corpus for Named Entity Recognition in Historic Newspapers](/paper/an-open-corpus-for-named-entity-recognition)", "variants": ["Europeana Newspapers"]}
{"id": "EventKG+Click", "title": "EventKG+Click: A Dataset of Language-specific Event-centric User Interaction Traces", "contents": "Builds upon the event-centric EventKG knowledge graph and language-specific information on user interactions with events, entities, and their relations derived from the Wikipedia clickstream.\r\n\r\nSource: [EventKG+Click: A Dataset of Language-specific Event-centric User Interaction Traces](/paper/eventkg-click-a-dataset-of-language-specific)", "variants": ["EventKG+Click"]}
{"id": "Event-QA", "title": "Event-QA: A Dataset for Event-Centric Question Answering over Knowledge Graphs", "contents": "Contains 1000 semantic queries and the corresponding English, German and Portuguese verbalizations for EventKG - an event-centric knowledge graph with more than 970 thousand events.\r\n\r\nSource: [Event-QA: A Dataset for Event-Centric Question Answering over Knowledge Graphs](/paper/event-qa-a-dataset-for-event-centric-question)", "variants": ["Event-QA"]}
{"id": "EXAMS", "title": "EXAMS: A Multi-Subject High School Examinations Dataset for Cross-Lingual and Multilingual Question Answering", "contents": "A new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. Collects more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others. EXAMS offers a fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of various models. \r\n\r\nSource: [EXAMS: A Multi-Subject High School Examinations Dataset for Cross-Lingual and Multilingual Question Answering](/paper/exams-a-multi-subject-high-school)", "variants": ["EXAMS"]}
{"id": "EXEQ-300k", "title": "", "contents": "The **EXEQ-300k** dataset contains 290,479 detailed questions with corresponding math headlines from Mathematics Stack Exchange. The dataset can be used to generate concise math headlines from detailed math questions.\n\nSource: [https://arxiv.org/pdf/1912.00839.pdf](https://arxiv.org/pdf/1912.00839.pdf)", "variants": ["EXEQ-300k"]}
{"id": "Explainable Abstract Trains", "title": "Explainable Abstract Trains Dataset", "contents": "An image dataset containing simplified representations of trains. It aims to provide a platform for the application and research of algorithms for justification and explanation extraction. The dataset is accompanied by an ontology that conceptualizes and classifies the depicted trains based on their visual characteristics, allowing for a precise understanding of how each train was labeled. Each image in the dataset is annotated with multiple attributes describing the trains' features and with bounding boxes for the train elements. \r\n\r\nSource: [Explainable Abstract Trains Dataset](/paper/explainable-abstract-trains-dataset)", "variants": ["Explainable Abstract Trains"]}
{"id": "ExPose", "title": "Monocular Expressive Body Regression through Body-Driven Attention", "contents": "Curates a dataset of SMPL-X fits on in-the-wild images.\r\n\r\nSource: [Monocular Expressive Body Regression through Body-Driven Attention](/paper/monocular-expressive-body-regression-through)", "variants": ["ExPose"]}
{"id": "Fashionpedia", "title": "Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset", "contents": "Fashionpedia consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, 294 fine-grained attributes and their relationships; (2) a dataset with everyday and celebrity event fashion images annotated with segmentation masks and their associated per-mask fine-grained attributes, built upon the Fashionpedia ontology. \r\n\r\nSource: [Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset](/paper/fashionpedia-ontology-segmentation-and-an)", "variants": ["Fashionpedia"]}
{"id": "FB15k-237-low", "title": "", "contents": "The **FB15k-237-low** dataset is a variation of the FB15k-237 dataset where relations with a low number of triplets are kept.\n\nSource: [https://arxiv.org/pdf/1911.03091.pdf](https://arxiv.org/pdf/1911.03091.pdf)", "variants": ["FB15k-237-low"]}
{"id": "FeathersV1", "title": "Feathers dataset for Fine-Grained Visual Categorization", "contents": "The FeatherV1 dataset is a dataset for fine-grained visual classification. It contains 28,272 images of feathers categorized by 595 bird species.\n\nSource: [https://github.com/feathers-dataset/feathersv1-dataset](https://github.com/feathers-dataset/feathersv1-dataset)\nImage Source: [https://github.com/feathers-dataset/feathersv1-dataset](https://github.com/feathers-dataset/feathersv1-dataset)", "variants": ["FeathersV1"]}
{"id": "FewGlue", "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "contents": "FewGLUE consists of a random selection of 32 training examples from the SuperGLUE training sets and up to 20,000 unlabeled examples for each SuperGLUE task.\r\n\r\nSource: [FewGlue](https://github.com/timoschick/fewglue)", "variants": ["FewGlue"]}
{"id": "FGADR", "title": "A Benchmark for Studying Diabetic Retinopathy: Segmentation, Grading, and Transferability", "contents": "This dataset has 1,842 images with pixel-level DR-related lesion annotations, and 1,000 images with image-level labels graded by six board-certified ophthalmologists with intra-rater consistency. The proposed dataset will enable extensive studies on DR diagnosis.\r\n\r\nSource: [A Benchmark for Studying Diabetic Retinopathy: Segmentation, Grading, and Transferability](/paper/a-benchmark-for-studying-diabetic-retinopathy)", "variants": ["FGADR"]}
{"id": "FIGRIM", "title": "", "contents": "This is a dataset of 9428 images, 1754 of which are target images with memorability scores.\r\nThe images span 21 scene categories from the SUN database. Each scene category was chosen to contain at least 300 images of size 700x700 or greater. All images were cropped to 700x700 pixels.\r\n\r\nSource: [FIGRIM](http://figrim.mit.edu/)", "variants": ["FIGRIM"]}
{"id": "FinChat", "title": "FinChat: Corpus and evaluation setup for Finnish chat conversations on everyday topics", "contents": "Finnish chat conversation corpus and includes unscripted conversations on seven topics from people of different ages. \r\n\r\nSource: [FinChat: Corpus and evaluation setup for Finnish chat conversations on everyday topics](/paper/finchat-corpus-and-evaluation-setup-for)", "variants": ["FinChat"]}
{"id": "FinnSentiment", "title": "FinnSentiment -- A Finnish Social Media Corpus for Sentiment Polarity Annotation", "contents": "FinnSentiment introduces a 27,000 sentence dataset (in Finnish) annotated independently with sentiment polarity by three native annotators. \r\n\r\nSource: [FinnSentiment -- A Finnish Social Media Corpus for Sentiment Polarity Annotation](/paper/finnsentiment-a-finnish-social-media-corpus)", "variants": ["FinnSentiment"]}
{"id": "FIW-MM", "title": "Families In Wild Multimedia (FIW-MM): A Multi-Modal Database for Recognizing Kinship", "contents": "A large-scale dataset for recognizing kinship in multimedia which extend FIW with multimedia data (i.e., video, audio, and contextual transcripts). \r\n\r\nSource: [Families In Wild Multimedia (FIW-MM): A Multi-Modal Database for Recognizing Kinship](/paper/families-in-wild-multimedia-fiw-mm-a-multi)", "variants": ["FIW-MM"]}
{"id": "FLAME", "title": "Aerial Imagery Pile burn detection using Deep Learning: the FLAME dataset", "contents": "FLAME is a fire image dataset collected by drones during a prescribed burning piled detritus in an Arizona pine forest. The dataset includes video recordings and thermal heatmaps captured by infrared cameras. The captured videos and images are annotated and labeled frame-wise to help researchers easily apply their fire detection and modeling algorithms.\r\n\r\nSource: [Aerial Imagery Pile burn detection using Deep Learning: the FLAME dataset](/paper/aerial-imagery-pile-burn-detection-using-deep)", "variants": ["FLAME"]}
{"id": "Flightmare Simulator", "title": "Flightmare: A Flexible Quadrotor Simulator", "contents": "Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. Those two components are totally decoupled and can run independently from each other. Flightmare comes with several desirable features: (i) a large multi-modal sensor suite, including an interface to extract the 3D point-cloud of the scene; (ii) an API for reinforcement learning which can simulate hundreds of quadrotors in parallel; and (iii) an integration with a virtual-reality headset for interaction with the simulated environment. Flightmare can be used for various applications, including path-planning, reinforcement learning, visual-inertial odometry, deep learning, human-robot interaction, etc.\r\n\r\nSource: [Flightmare Simulator](https://uzh-rpg.github.io/flightmare/)", "variants": ["Flightmare Simulator"]}
{"id": "FLoRes", "title": "The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali--English and Sinhala--English", "contents": "FLoRes is a benchmark dataset for machine translation between English and four low resource languages, Nepali, Sinhala, Khmer and Pashto, based on sentences translated from Wikipedia.", "variants": ["FLoRes"]}
{"id": "Florence 3D Faces", "title": "", "contents": "This dataset is being constructed specifically to support research on techniques that bridge the gap between 2D, appearance-based recognition techniques, and fully 3D approaches. It is designed to simulate, in a controlled fashion, realistic surveillance conditions and to probe the efficacy of exploiting 3D models in real scenarios.\r\n\r\nSource: [FLORENCE 3D FACES](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)", "variants": ["Florence 3D Faces"]}
{"id": "Fon-French Dataset", "title": "FFR v1.1: Fon-French Neural Machine Translation", "contents": "FFR Dataset is an ongoing project to collect, clean and store corpora of Fon and French sentences for machine translation from Fon-French. Fon (also called Fongbe) is an African-indigenous language spoken mostly in Benin, by about 1.7 million people. As training data is crucial to the high performance of a machine learning model, the aim of the project is to compile the largest set of training corpora for the research and design of translation and NLP models involving Fon. There are 117,029 parallel Fon-French sentences at the moment.\r\n\r\nSource: [FFR Dataset](https://github.com/bonaventuredossou/ffr-v1/tree/master/FFR-Dataset)", "variants": ["Fon-French Dataset"]}
{"id": "Fraxtil", "title": "", "contents": "**Fraxtil** is an audio dataset where given a raw audio track, the goal is to produce a choreography step chart, similar to those used in the Dance Dance Revolution video game. It contains 90 songs choreographed by a single author, with 450 charts for the 90 songs.\n\nSource: [https://arxiv.org/pdf/1703.06891.pdf](https://arxiv.org/pdf/1703.06891.pdf)\nImage Source: [https://github.com/chrisdonahue/ddc](https://github.com/chrisdonahue/ddc)", "variants": ["Fraxtil"]}
{"id": "FreebaseQA", "title": "FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase", "contents": "FreebaseQA is a data set for open-domain QA over the Freebase knowledge graph. The question-answer pairs in this data set are collected from various sources, including the TriviaQA data set and other trivia websites (QuizBalls, QuizZone, KnowQuiz), and are matched against Freebase to generate relevant subject-predicate-object triples that were further verified by human annotators. As all questions in FreebaseQA are composed independently for human contestants in various trivia-like competitions, this data set shows richer linguistic variation and complexity than existing QA data sets, making it a good test-bed for emerging KB-QA systems.\r\n\r\nSource: [FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase](/paper/freebaseqa-a-new-factoid-qa-data-set-matching)", "variants": ["FreebaseQA"]}
{"id": "French CASS dataset", "title": "STRASS: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings", "contents": "Composed of judgments from the French Court of cassation and their corresponding summaries.\r\n\r\nSource: [STRASS: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings](/paper/strass-a-light-and-effective-method-for)", "variants": ["French CASS dataset"]}
{"id": "FSOCO", "title": "FSOCO: The Formula Student Objects in Context Dataset", "contents": "**FSOCO** is a collaborative dataset for vision-based cone detection systems in Formula Student Driverless competitions. It contains human annotated ground truth labels for both bounding boxes and instance-wise segmentation masks. The data buy-in philosophy of FSOCO asks student teams to contribute to the database first before being granted access ensuring continuous growth. By providing clear labeling guidelines and tools for a sophisticated raw image selection, new annotations are guaranteed to meet the desired quality.\r\n\r\nSource: [https://github.com/fsoco/fsoco-dataset](https://github.com/fsoco/fsoco-dataset)\nImage Source: [https://github.com/fsoco/fsoco-dataset](https://github.com/fsoco/fsoco-dataset)", "variants": ["FSOCO"]}
{"id": "FT Speech", "title": "FT Speech: Danish Parliament Speech Corpus", "contents": "FT Speech is a speech corpus created from the recorded meetings of the Danish Parliament, otherwise known as the Folketing (FT). The corpus contains over 1,800 hours of transcribed speech by a total of 434 speakers. It is significantly larger in duration, vocabulary, and amount of spontaneous speech than the existing public speech corpora for Danish, which are largely limited to read-aloud and dictation data. \r\n\r\nSource: [FT Speech: Danish Parliament Speech Corpus](/paper/ft-speech-danish-parliament-speech-corpus)", "variants": ["FT Speech"]}
{"id": "Fusion 360 Gallery", "title": "Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD Reconstruction", "contents": "The **Fusion 360 Gallery** Dataset contains rich 2D and 3D geometry data derived from parametric CAD models. The dataset is produced from designs submitted by users of the CAD package Autodesk Fusion 360 to the Autodesk Online Gallery. The dataset provides valuable data for learning how people design, including sequential CAD design data, designs segmented by modelling operation, and design hierarchy and connectivity data.\n\nSource: [https://github.com/AutodeskAILab/Fusion360GalleryDataset](https://github.com/AutodeskAILab/Fusion360GalleryDataset)\nImage Source: [https://github.com/AutodeskAILab/Fusion360GalleryDataset](https://github.com/AutodeskAILab/Fusion360GalleryDataset)", "variants": ["Fusion 360 Gallery"]}
{"id": "G1020", "title": "G1020: A Benchmark Retinal Fundus Image Dataset for Computer-Aided Glaucoma Detection", "contents": "A large publicly available retinal fundus image dataset for glaucoma classification called G1020. The dataset is curated by conforming to standard practices in routine ophthalmology and it is expected to serve as standard benchmark dataset for glaucoma detection. This database consists of 1020 high resolution colour fundus images and provides ground truth annotations for glaucoma diagnosis, optic disc and optic cup segmentation, vertical cup-to-disc ratio, size of neuroretinal rim in inferior, superior, nasal and temporal quadrants, and bounding box location for optic disc. \r\n\r\nSource: [G1020: A Benchmark Retinal Fundus Image Dataset for Computer-Aided Glaucoma Detection](/paper/g1020-a-benchmark-retinal-fundus-image)", "variants": ["G1020"]}
{"id": "Gazeta", "title": "Dataset for Automatic Summarization of Russian News", "contents": "**Gazeta** is a dataset for automatic summarization of Russian news. The dataset consists of 63,435 text-summary pairs. To form training, validation, and test datasets, these pairs were sorted by time and the first 52,400 pairs are used as the training dataset, the proceeding 5,265 pairs as the validation dataset, and the remaining 5,770 pairs as the test dataset.\n\nSource: [https://github.com/IlyaGusev/gazeta](https://github.com/IlyaGusev/gazeta)", "variants": ["Gazeta"]}
{"id": "GDXray+", "title": "", "contents": "GDXray+ is a collection of more than 21.100 X-ray images for the development, testing, and evaluation of image analysis and computer vision algorithms.\r\n\r\nGDXray includes five groups of images:\r\n\r\n* Castings,\r\n* Welds,\r\n* Baggage,\r\n* Nature,\r\n* Settings.\r\n\r\nEach group has several series, and each series several X-ray images.\r\n\r\nSource: [GDXray](https://domingomery.ing.puc.cl/material/gdxray/)", "variants": ["GDXray+"]}
{"id": "GeoCoV19", "title": "GeoCoV19: A Dataset of Hundreds of Millions of Multilingual COVID-19 Tweets with Location Information", "contents": "GeoCoV19 is a large-scale Twitter dataset containing more than 524 million multilingual tweets. The dataset contains around 378K geotagged tweets and 5.4 million tweets with Place information. The annotations include toponyms from the user location field and tweet content and resolve them to geolocations such as country, state, or city level. In this case, 297 million tweets are annotated with geolocation using the user location field and 452 million tweets using tweet content.", "variants": ["GeoCoV19"]}
{"id": "GeoFaces", "title": "", "contents": "A large database of geotagged face images.\r\n\r\nSource: [GeoFaces](http://geofaces.csr.uky.edu/)", "variants": ["GeoFaces"]}
{"id": "Get it #OffMyChest", "title": "", "contents": "Corpus and annotations for the CL-Aff Shared Task - Get it #OffMyChest - from Nanyang Technological University Singapore.\r\n\r\nSource: [Get it #OffMyChest](https://github.com/kj2013/claff-offmychest)", "variants": ["Get it #OffMyChest"]}
{"id": "GGPONC", "title": "GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines", "contents": "German Guideline Program in Oncology NLP Corpus (GGPONC) is a German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions.\r\n\r\nSource: [GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines](/paper/ggponc-a-corpus-of-german-medical-text-with)\r\nImage Source: [https://arxiv.org/pdf/2007.06400.pdf](https://arxiv.org/pdf/2007.06400.pdf)", "variants": ["GGPONC"]}
{"id": "GiantMIDI-Piano", "title": "GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music", "contents": "**GiantMIDI-Piano** contains 10,854 unique piano solo pieces composed by 2,786 composers. GiantMIDI-Piano contains 34,504,873 transcribed notes, and contains metadata information of each music piece.\r\n\r\nSource: [GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music](/paper/giantmidi-piano-a-large-scale-midi-dataset)", "variants": ["GiantMIDI-Piano"]}
{"id": "Gigaword Entailment", "title": "Improving Truthfulness of Headline Generation", "contents": "The **Gigaword Entailment** dataset is a dataset for entailment prediction between an article and its headline. It is built from the Gigaword dataset.\n\nSource: [https://github.com/nlp-titech/headline-entailment](https://github.com/nlp-titech/headline-entailment)", "variants": ["Gigaword Entailment"]}
{"id": "GRAB", "title": "GRAB: A Dataset of Whole-Body Human Grasping of Objects", "contents": "**GRAB** is a dataset of full-body motions interacting and grasping 3D objects. It contains accurate finger and facial motions as well as the contact between the objects and body. It contains 5 male and 5 female participants and 4 different motion intents.\nThe GRAB dataset also contains binary contact maps between the body and objects.\n\nSource: [https://github.com/otaheri/GRAB](https://github.com/otaheri/GRAB)\nImage Source: [https://github.com/otaheri/GRAB](https://github.com/otaheri/GRAB)", "variants": ["GRAB"]}
{"id": "GRAL", "title": "RGB2LIDAR: Towards Solving Large-Scale Cross-Modal Visual Localization", "contents": "A new dataset containing over 550K pairs (covering 143 km^2 area) of RGB and aerial LIDAR depth images. \r\n\r\nSource: [RGB2LIDAR: Towards Solving Large-Scale Cross-Modal Visual Localization](/paper/rgb2lidar-towards-solving-large-scale-cross)", "variants": ["GRAL"]}
{"id": "Groningen Meaning Bank", "title": "The Groningen Meaning Bank", "contents": "Groningen Meaning Bank is a semantic resource that anyone can edit and that integrates various semantic phenomena, including predicate-argument structure, scope, tense, thematic roles, animacy, pronouns, and rhetorical relations.\r\n\r\nSource: [The Groningen Meaning Bank](https://www.aclweb.org/anthology/W13-3802.pdf)", "variants": ["Groningen Meaning Bank"]}
{"id": "GTA-IM Dataset", "title": "Long-term Human Motion Prediction with Scene Context", "contents": "The **GTA Indoor Motion** dataset (GTA-IM) that emphasizes human-scene interactions in the indoor environments. It consists of HD RGB-D image sequences of 3D human motion from a realistic game engine. The dataset has clean 3D human pose and camera pose annotations, and large diversity in human appearances, indoor environments, camera views, and human activities.\n\nSource: [https://github.com/ZheC/GTA-IM-Dataset](https://github.com/ZheC/GTA-IM-Dataset)\nImage Source: [https://github.com/ZheC/GTA-IM-Dataset](https://github.com/ZheC/GTA-IM-Dataset)", "variants": ["GTA-IM Dataset"]}
{"id": "Gutenberg Dialog Dataset", "title": "The Gutenberg Dialogue Dataset", "contents": "This is a high-quality dataset consisting of 14.8M utterances in English, extracted from processed dialogues from publicly available online books.\n\nSource: [https://github.com/ricsinaruto/gutenberg-dialog](https://github.com/ricsinaruto/gutenberg-dialog)", "variants": ["Gutenberg Dialog Dataset"]}
{"id": "Gutenberg Time Dataset", "title": "What time is it? Temporal Analysis of Novels", "contents": "A data set of hourly time phrases from 52,183 fictional books. \r\n\r\nSource: [What time is it? Temporal Analysis of Novels](/paper/what-time-is-it-temporal-analysis-of-novels)", "variants": ["Gutenberg Time Dataset"]}
{"id": "HAA500", "title": "HAA500: Human-Centric Atomic Action Dataset with Curated Videos", "contents": "HAA500 is a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591k labeled frames. Unlike existing atomic action datasets, where coarse-grained atomic actions were labeled with action-verbs, e.g., \"Throw\", HAA500 contains fine-grained atomic actions where only consistent actions fall under the same label, e.g., \"Baseball Pitching\" vs \"Free Throw in Basketball\", to minimize ambiguities in action classification. HAA500 has been carefully curated to capture the movement of human figures with less spatio-temporal label noises to greatly enhance the training of deep neural networks. \r\n\r\nSource: [HAA500: Human-Centric Atomic Action Dataset with Curated Videos](/paper/haa500-human-centric-atomic-action-dataset)", "variants": ["HAA500"]}
{"id": "HAM", "title": "Graph Neural Network Based Coarse-Grained Mapping Prediction", "contents": "**HAM** is a dataset for molecular graph partitioning. This dataset contains coarse-grained (CG) mappings of 1206 organic molecules with less than 25 heavy atoms. Each molecule was downloaded from the PubChem database as SMILES. One molecule was assigned to two annotators to compare the human agreement between CG mappings. Downloaded SMILES were hand-mapped. The completed annotations were reviewed by a third person, to identify and remove unreasonable mappings (eg: one bead mappings) which did not agree with the given guidelines. Hence, there are 1.68 annotations per molecule in the current database (16% removed).\n\nSource: [https://github.com/rochesterxugroup/HAM_dataset](https://github.com/rochesterxugroup/HAM_dataset)", "variants": ["HAM"]}
{"id": "HANS", "title": "", "contents": "The HANS (Heuristic Analysis for NLI Systems) dataset which contains many examples where the heuristics fail. \r\n\r\nSource: [Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference](https://arxiv.org/abs/1902.01007)", "variants": ["HANS"]}
{"id": "HarperValleyBank", "title": "HarperValleyBank: A Domain-Specific Spoken Dialog Corpus", "contents": "The data simulate simple consumer banking interactions, containing about 23 hours of audio from 1,446 human-human conversations between 59 unique speakers.\r\n\r\nSource: [HarperValleyBank: A Domain-Specific Spoken Dialog Corpus](/paper/harpervalleybank-a-domain-specific-spoken)", "variants": ["HarperValleyBank"]}
{"id": "Hateful Memes Challenge", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes", "contents": "A new challenge set for multimodal classification, focusing on detecting hate speech in multimodal memes.\r\n\r\nSource: [The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes](/paper/the-hateful-memes-challenge-detecting-hate)", "variants": ["Hateful Memes Challenge"]}
{"id": "HDR+ Burst Photography Dataset", "title": "", "contents": "The dataset consists of 3640 bursts (made up of 28461 images in total), organized into subfolders, plus the results of an image processing pipeline. Each burst consists of the raw burst input (in DNG format) and certain metadata not present in the images, as sidecar files.\r\n\r\nSource: [HDR+ Burst Photography Dataset](http://hdrplusdata.org/dataset.html)", "variants": ["HDR+ Burst Photography Dataset"]}
{"id": "HiEve", "title": "Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events", "contents": "A new large-scale dataset for understanding human motions, poses, and actions in a variety of realistic events, especially crowd & complex events. It contains a record number of poses (>1M), the largest number of action labels (>56k) for complex events, and one of the largest number of trajectories lasting for long terms (with average trajectory length >480). Besides, an online evaluation server is built for researchers to evaluate their approaches.\r\n\r\nSource: [Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events](/paper/human-in-events-a-large-scale-benchmark-for)", "variants": ["HiEve"]}
{"id": "HIGGS Data Set", "title": "", "contents": "The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.\r\n\r\nSource: [HIGGS Data Set](https://archive.ics.uci.edu/ml/datasets/HIGGS)", "variants": ["HIGGS Data Set"]}
{"id": "HindEnCorp", "title": "HindEnCorp - Hindi-English and Hindi-only Corpus for Machine Translation", "contents": "A parallel corpus of Hindi and English, and HindMonoCorp, a monolingual corpus of Hindi in their release version 0.5. Both corpora were collected from web sources and preprocessed primarily for the training of statistical machine translation systems. HindEnCorp consists of 274k parallel sentences (3.9 million Hindi and 3.8 million English tokens). HindMonoCorp amounts to 787 million tokens in 44 million sentences.\r\n\r\nSource: [HindEnCorp - Hindi-English and Hindi-only Corpus for Machine Translation](/paper/hindencorp-hindi-english-and-hindi-only)", "variants": ["HindEnCorp"]}
{"id": "HINT3", "title": "HINT3: Raising the bar for Intent Detection in the Wild", "contents": "**HINT3** is a dataset for intent detection. It consists of 3 different datasets each containing a diverse set of intents in a single domain - mattress products retail, fitness supplements retail and online gaming named SOFMattress, Curekart and Powerplay11.\n\nSource: [https://github.com/hellohaptik/HINT3](https://github.com/hellohaptik/HINT3)", "variants": ["HINT3"]}
{"id": "HJDataset", "title": "A Large Dataset of Historical Japanese Documents with Complex Layouts", "contents": "HJDataset is a large dataset of Historical Japanese Documents with Complex Layouts. It contains over 250,000 layout element annotations of seven types. In addition to bounding boxes and masks of the content regions, it also includes the hierarchical structures and reading orders for layout elements. The dataset is constructed using a combination of human and machine efforts. \r\n\r\nSource: [A Large Dataset of Historical Japanese Documents with Complex Layouts](/paper/a-large-dataset-of-historical-japanese)\r\nImage Source: [https://dell-research-harvard.github.io/HJDataset/](https://dell-research-harvard.github.io/HJDataset/)", "variants": ["HJDataset"]}
{"id": "Hong Kong Cantonese corpus", "title": "", "contents": "The Hong Kong Cantonese Corpus was collected from transcribed conversations that were recorded between March 1997 and August 1998. About 230,000 Chinese words were collected in the annotated corpus. It contains recordings of spontaneous speech (51 texts) and radio programmes (42 texts), which involve 2 to 4 speakers, with 1 text of monologue. The text were word-segmented, annotated with part-of-speech tagging and Cantonese pronunciation using the romanisation scheme of Linguistic Society of Hong Kong (LSHK).\r\n\r\nSource: [Hong Kong Cantonese corpus](http://compling.hss.ntu.edu.sg/hkcancor/)", "variants": ["Hong Kong Cantonese corpus"]}
{"id": "HLA-Chat", "title": "ALOHA: Artificial Learning of Human Attributes for Dialogue Agents", "contents": "Models character profiles and gives dialogue agents the ability to learn characters' language styles through their HLAs.\r\n\r\nSource: [ALOHA: Artificial Learning of Human Attributes for Dialogue Agents](/paper/follow-alice-into-the-rabbit-hole-giving)", "variants": ["HLA-Chat"]}
{"id": "HORAE", "title": "HORAE: an annotated dataset of books of hours", "contents": "A new dataset of annotated pages from books of hours, a type of handwritten prayer books owned and used by rich lay people in the late middle ages. The dataset was created for conducting historical research on the evolution of the religious mindset in Europe at this period since the book of hours represent one of the major sources of information thanks both to their rich illustrations and the different types of religious sources they contain. \r\n\r\nSource: [HORAE: an annotated dataset of books of hours](/paper/horae-an-annotated-dataset-of-books-of-hours)", "variants": ["HORAE"]}
{"id": "Houses3K", "title": "Next-Best View Policy for 3D Reconstruction", "contents": "**Houses3K** is a dataset of 3000 textured 3D house models. Houses3K is divided into twelve batches, each containing 50 unique house geometries. For each batch, five different textures were applied forming the sets (A, B, C, D, E).\n\nSource: [https://github.com/darylperalta/Houses3K](https://github.com/darylperalta/Houses3K)\nImage Source: [https://github.com/darylperalta/Houses3K](https://github.com/darylperalta/Houses3K)", "variants": ["Houses3K"]}
{"id": "HoVer", "title": "HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification", "contents": "Is a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is Supported or Not-Supported by the facts. In HoVer, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes.\r\n\r\nSource: [HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification](/paper/hover-a-dataset-for-many-hop-fact-extraction)", "variants": ["HoVer"]}
{"id": "HUMBI", "title": "HUMBI: A Large Multiview Dataset of Human Body Expressions", "contents": "A new large multiview dataset for human body expressions with natural clothing. The goal of HUMBI is to facilitate modeling view-specific appearance and geometry of gaze, face, hand, body, and garment from assorted people. 107 synchronized HD cameras are used to capture 772 distinctive subjects across gender, ethnicity, age, and physical condition. \r\n\r\nSource: [HUMBI: A Large Multiview Dataset of Human Body Expressions](/paper/humbi-10-human-multiview-behavioral-imaging)", "variants": ["HUMBI"]}
{"id": "HurricaneEmo", "title": "Detecting Perceived Emotions in Hurricane Disasters", "contents": "**HurricaneEmo** is an emotion dataset that contains 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria.\n\nSource: [https://github.com/shreydesai/hurricane](https://github.com/shreydesai/hurricane)", "variants": ["HurricaneEmo"]}
{"id": "HybridQA", "title": "HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data", "contents": "A new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. \r\n\r\nSource: [HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data](/paper/hybridqa-a-dataset-of-multi-hop-question)", "variants": ["HybridQA"]}
{"id": "Hypersim", "title": "Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding", "contents": "For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. **Hypersim** is a photorealistic synthetic dataset for holistic indoor scene understanding. It contains 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry.\n\nSource: [https://github.com/apple/ml-hypersim](https://github.com/apple/ml-hypersim)\nImage Source: [https://github.com/apple/ml-hypersim](https://github.com/apple/ml-hypersim)", "variants": ["Hypersim"]}
{"id": "IIIT-AR-13K", "title": "IIIT-AR-13K: A New Dataset for Graphical Object Detection in Documents", "contents": "IIIT-AR-13K is created by manually annotating the bounding boxes of graphical or page objects in publicly available annual reports. This dataset contains a total of 13k annotated page images with objects in five different popular categories - table, figure, natural image, logo, and signature. It is the largest manually annotated dataset for graphical object detection.\r\n\r\nSource: [IIIT-AR-13K: A New Dataset for Graphical Object Detection in Documents](/paper/iiit-ar-13k-a-new-dataset-for-graphical)\r\nImage Source: [http://cvit.iiit.ac.in/usodi/iiitar13k.php](http://cvit.iiit.ac.in/usodi/iiitar13k.php)", "variants": ["IIIT-AR-13K"]}
{"id": "IIRC", "title": "IIRC: A Dataset of Incomplete Information Reading Comprehension Questions", "contents": "Contains more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. \r\n\r\nSource: [IIRC: A Dataset of Incomplete Information Reading Comprehension Questions](/paper/iirc-a-dataset-of-incomplete-information)", "variants": ["IIRC"]}
{"id": "IIW", "title": "", "contents": "Intrinsic Images in the Wild is a large scale, public dataset for intrinsic image decompositions of real-world scenes selected from the OpenSurfaces dataset. Each image is annotated with crowdsourced pairwise comparisons of material properties. \r\n\r\nSource: [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/)", "variants": ["IIW"]}
{"id": "IKEA ASM", "title": "The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose", "contents": "A three million frame, multi-view, furniture assembly video dataset that includes depth, atomic actions, object segmentation, and human pose. \r\n\r\nSource: [The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose](/paper/the-ikea-asm-dataset-understanding-people)", "variants": ["IKEA ASM"]}
{"id": "Image Memorability", "title": "", "contents": "A database of images with measured probabilities that each picture will be remembered after a single view. \r\n\r\nSource: [Image Memorability](http://web.mit.edu/phillipi/Public/WhatMakesAnImageMemorable/)", "variants": ["Image Memorability"]}
{"id": "IMEMNET", "title": "Emotion-Based End-to-End Matching Between Image and Music in Valence-Arousal Space", "contents": "The **Image-MusicEmotion-Matching-Net** (IMEMNet) dataset is a dataset for continuous emotion-based image and music matching. It has over 140K image-music pairs.\n\nSource: [https://github.com/linkAmy/IMEMNet](https://github.com/linkAmy/IMEMNet)", "variants": ["IMEMNET"]}
{"id": "iMoCap", "title": "Motion Capture from Internet Videos", "contents": "A dataset that consists of 20 actions of various actors, such as tennis serves, yoga and Tai Chi. Take tennis serves as an example. The publicly available videos of some tennis players from YouTube are downloaded, and manually crop the videos roughly to obtain a set of video clips of serves for each player.\r\n\r\nSource: [Motion Capture from Internet Videos](/paper/motion-capture-from-internet-videos)", "variants": ["iMoCap"]}
{"id": "iNaturalist Fine-Grained Geolocation", "title": "", "contents": "The **iNaturalist Fine-Grained Geolocation** dataset is an extension of the iNaturalist dataset with complementary geolocation information.\n\nSource: [https://github.com/visipedia/fg_geo](https://github.com/visipedia/fg_geo)\nImage Source: [https://github.com/visipedia/fg_geo](https://github.com/visipedia/fg_geo)", "variants": ["iNaturalist Fine-Grained Geolocation"]}
{"id": "Incidents", "title": "Detecting natural disasters, damage, and incidents in the wild", "contents": "Contains 446,684 images annotated by humans that cover 43 incidents across a variety of scenes. \r\n\r\nSource: [Detecting natural disasters, damage, and incidents in the wild](/paper/detecting-natural-disasters-damage-and)", "variants": ["Incidents"]}
{"id": "IndicNLP Corpus", "title": "AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages", "contents": "The IndicNLP corpus is a large-scale, general-domain corpus containing 2.7 billion words for 10 Indian languages from two language families.\n\nSource: [https://arxiv.org/abs/2005.00085](https://arxiv.org/abs/2005.00085)", "variants": ["IndicNLP Corpus"]}
{"id": "IndoNLU Benchmark", "title": "IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding", "contents": "The IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for Bahasa Indonesia. It is a joint venture from many Indonesia NLP enthusiasts from different institutions such as Gojek, Institut Teknologi Bandung, HKUST, Universitas Multimedia Nusantara, Prosa.ai, and Universitas Indonesia.\r\n\r\nSource: [IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding](/paper/indonlu-benchmark-and-resources-for)", "variants": ["IndoNLU Benchmark"]}
{"id": "InfoTabS", "title": "INFOTABS: Inference on Tables as Semi-structured Data", "contents": "InfoTabS comprises of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes.\r\n\r\nSource: [INFOTABS: Inference on Tables as Semi-structured Data](/paper/infotabs-inference-on-tables-as-semi)", "variants": ["InfoTabS"]}
{"id": "INQUISITIVE", "title": "Inquisitive Question Generation for High Level Text Comprehension", "contents": "A dataset of ~19K questions that are elicited while a person is reading through a document.\r\n\r\nSource: [Inquisitive Question Generation for High Level Text Comprehension](/paper/inquisitive-question-generation-for-high)", "variants": ["INQUISITIVE"]}
{"id": "INRIA Holidays Dataset", "title": "", "contents": "The Holidays dataset is a set of images which mainly contains some of the authors' personal holidays photos. The remaining ones were taken on purpose to test the robustness to various attacks: rotations, viewpoint and illumination changes, blurring, etc. The dataset includes a very large variety of scene types (natural, man-made, water and fire effects, etc) and images are in high resolution. The dataset contains 500 image groups, each of which represents a distinct scene or object. The first image of each group is the query image and the correct retrieval results are the other images of the group.\r\n\r\nSource: [INRIA Holidays Dataset](http://lear.inrialpes.fr/~jegou/data.php#holidays)", "variants": ["INRIA Holidays Dataset"]}
{"id": "Inspired", "title": "INSPIRED: Toward Sociable Recommendation Dialog Systems", "contents": "A new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations.\r\n\r\nSource: [INSPIRED: Toward Sociable Recommendation Dialog Systems](/paper/inspired-toward-sociable-recommendation)", "variants": ["Inspired"]}
{"id": "IPOD", "title": "A Large-scale Industrial and Professional Occupation Dataset", "contents": "Comprises 192k job titles belonging to 56k LinkedIn users. \r\n\r\nSource: [A Large-scale Industrial and Professional Occupation Dataset](/paper/a-large-scale-industrial-and-professional)", "variants": ["IPOD"]}
{"id": "IS-A", "title": "", "contents": "The **IS-A** dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are related by the “is a” relation. For example, ‘acute leukemia’ is a ‘leukemia’. The dataset has 294,693 nodes with 356,541 edges between them.\n\nSource: [https://arxiv.org/pdf/1906.05939.pdf](https://arxiv.org/pdf/1906.05939.pdf)", "variants": ["IS-A"]}
{"id": "ISBDA", "title": "MSNet: A Multilevel Instance Segmentation Network for Natural Disaster Damage Assessment in Aerial Videos", "contents": "Consists of user-generated aerial videos from social media with annotations of instance-level building damage masks. This provides the first benchmark for quantitative evaluation of models to assess building damage using aerial videos.\r\n\r\nSource: [MSNet: A Multilevel Instance Segmentation Network for Natural Disaster Damage Assessment in Aerial Videos](/paper/msnet-a-multilevel-instance-segmentation)", "variants": ["ISBDA"]}
{"id": "ISIA Food-500", "title": "ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network", "contents": "Includes 500 categories from the list in the Wikipedia and 399,726 images, a more comprehensive food dataset that surpasses existing popular benchmark datasets by category coverage and data volume.\r\n\r\nSource: [ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network](/paper/isia-food-500-a-dataset-for-large-scale-food)", "variants": ["ISIA Food-500"]}
{"id": "ISR-UoL 3D Social Activity Dataset", "title": "", "contents": "This is a social interaction dataset between two subjects. This dataset consists of RGB and depth images, and tracked skeleton data (i.e. joints 3D coordinates and rotations) acquired by an RGB-D sensor. It includes 8 social activities: {handshake, greeting hug, help walk, help stand-up, fight, push, conversation, call attention}. Each activity was recorded in a period around 40 to 60 seconds of repetitions within the same session at a frame rate of 30 frames per second. The only exceptions are help walking (at a short distance) and help stand-up, which were recorded 4 times to the same session, regardless of the time spent on it.\r\n\r\nSource: [ISR-UoL 3D Social Activity Dataset](https://lcas.lincoln.ac.uk/wp/research/data-sets-software/isr-uol-3d-social-activity-dataset/)", "variants": ["ISR-UoL 3D Social Activity Dataset"]}
{"id": "JAMUL", "title": "", "contents": "A large-scale evaluation dataset for headlines of three different lengths composed by professional editors.\r\n\r\nSource: [A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation](/paper/a-large-scale-multi-length-headline-corpus)", "variants": ["JAMUL"]}
{"id": "JIT Dataset", "title": "", "contents": "The **Jejueo Interview Transcripts** (JIT) dataset is a parallel corpus containing 170k+ Jejueo-Korean sentences.\n\nSource: [https://arxiv.org/abs/1911.12071](https://arxiv.org/abs/1911.12071)", "variants": ["JIT Dataset"]}
{"id": "JW300", "title": "JW300: A Wide-Coverage Parallel Corpus for Low-Resource Languages", "contents": "A parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average.\r\n\r\nSource: [JW300: A Wide-Coverage Parallel Corpus for Low-Resource Languages](/paper/jw300-a-wide-coverage-parallel-corpus-for-low)", "variants": ["JW300"]}
{"id": "KAIST Multispectral Pedestrian Detection Benchmark", "title": "", "contents": "The KAIST Multispectral Pedestrian Dataset consists of 95k color-thermal pairs (640x480, 20Hz) taken from a vehicle. All the pairs are manually annotated (person, people, cyclist) for the total of 103,128 dense annotations and 1,182 unique pedestrians. The annotation includes temporal correspondence between bounding boxes like Caltech Pedestrian Dataset. \r\n\r\nSource: [KAIST Multispectral Pedestrian Detection Benchmark](https://soonminhwang.github.io/rgbt-ped-detection/)", "variants": ["KAIST Multispectral Pedestrian Detection Benchmark"]}
{"id": "KELM", "title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training", "contents": "KELM is a large-scale synthetic corpus of Wikidata KG as natural text.\r\n\r\nSource: [KELM](https://github.com/google-research-datasets/KELM-corpus)", "variants": ["KELM"]}
{"id": "K-EmoCon", "title": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations", "contents": "A multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations. The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue. \r\n\r\nSource: [K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations](/paper/k-emocon-a-multimodal-sensor-dataset-for)", "variants": ["K-EmoCon"]}
{"id": "KinectFaceDB", "title": "", "contents": "The Dataset consists of the multimodal facial images of 52 people (14 females, 38 males) obtained by Kinect. The data is captured in two sessions happened at different time period (about half month). In each session, the dataset provides the facial images of each person in 9 states of different facial expressions, different lighting and occlusion conditions: neutral, smile, open mouth, left profile, right profile, occlusion eyes, occlusion mouth, occlusion paper and light on [Figure 1]. All the images are provided in three sources of information: the RGB color image, the depth map (provided in two forms of the bitmap depth image and the text file containing the original depth levels sensed by Kinect) as well as 3D. In addition, the dataset comes with the manual landmarks of 6 positions in the face: left eye, right eye, the tip of nose, left side of mouth, right side of mouth and the chin [Figure 2]. Other information of the person such as gender, year of birth, glasses (this person wears the glasses or not), capture time of each session are also available.\r\n\r\nSource: [KinectFaceDB](http://rgb-d.eurecom.fr/)", "variants": ["KinectFaceDB"]}
{"id": "KINNEWS and KIRNEWS", "title": "KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for Kinyarwanda and Kirundi", "contents": "Two news datasets (KINNEWS and KIRNEWS) for multi-class classification of news articles in Kinyarwanda and Kirundi, two low-resource African languages. The two languages are mutually intelligible.\r\n\r\nSource: [KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for Kinyarwanda and Kirundi](/paper/kinnews-and-kirnews-benchmarking-cross)", "variants": ["KINNEWS and KIRNEWS"]}
{"id": "KINS", "title": "", "contents": "Augments the KITTI with more instance pixel-level annotation for 8 categories.\r\n\r\nSource: [Amodal Instance Segmentation with KINS Dataset](http://jiaya.me/papers/amodel_cvpr19.pdf)", "variants": ["KINS"]}
{"id": "Korean HateSpeech Dataset", "title": "BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection", "contents": "Presents 9.4K manually labeled entertainment news comments for identifying Korean toxic speech, collected from a widely used online news platform in Korea.\r\n\r\nSource: [BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection](/paper/beep-korean-corpus-of-online-news-comments)", "variants": ["Korean HateSpeech Dataset"]}
{"id": "KorNLI", "title": "", "contents": "**KorNLI** is a Korean Natural Language Inference (NLI) dataset. The dataset is constructed by automatically translating the training sets of the SNLI, XNLI and MNLI datasets. To ensure translation quality, two professional translators with at least seven years of experience who specialize in academic papers/books as well as business contracts post-edited a half of the dataset each and cross-checked each other’s translation afterward.\nIt contains 942,854 training examples translated automatically and 7,500 evaluation (development and test) examples translated manually\n\nSource: [https://github.com/kakaobrain/KorNLUDatasets](https://github.com/kakaobrain/KorNLUDatasets)", "variants": ["KorNLI"]}
{"id": "KQA Pro", "title": "KQA Pro: A Large-Scale Dataset with Interpretable Programs and Accurate SPARQLs for Complex Question Answering over Knowledge Base", "contents": "A large-scale dataset for Complex KBQA.\r\n\r\nSource: [KQA Pro: A Large-Scale Dataset with Interpretable Programs and Accurate SPARQLs for Complex Question Answering over Knowledge Base](/paper/kqa-pro-a-large-diagnostic-dataset-for)", "variants": ["KQA Pro"]}
{"id": "Kuzushiji-49", "title": "", "contents": "Kuzushiji-49 is an MNIST-like dataset that has 49 classes (28x28 grayscale, 270,912 images) from 48 Hiragana characters and one Hiragana iteration mark.\r\n\r\nSource: [Deep Learning for Classical Japanese Literature](/paper/deep-learning-for-classical-japanese)\r\nImage Source: [Clanuwat et al](https://arxiv.org/pdf/1812.01718v1.pdf)", "variants": ["Kuzushiji-49"]}
{"id": "Kvasir-Instrument", "title": "Kvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy", "contents": "Consists of  annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists.\r\n\r\nSource: [Kvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy](/paper/kvasir-instrument-diagnostic-and-therapeutic)", "variants": ["Kvasir-Instrument"]}
{"id": "LaPa", "title": "", "contents": "A large-scale Landmark guided face Parsing dataset (LaPa) for face parsing. It consists of more than 22,000 facial images with abundant variations in expression, pose and occlusion, and each image of LaPa is provided with a 11-category pixel-level label map and 106-point landmarks.\r\n\r\nSource: [LaPa](https://github.com/JDAI-CV/lapa-dataset)", "variants": ["LaPa"]}
{"id": "LCCC", "title": "A Large-Scale Chinese Short-Text Conversation Dataset", "contents": "Contains a base version (6.8million dialogues) and a large version (12.0 million dialogues). \r\n\r\nSource: [A Large-Scale Chinese Short-Text Conversation Dataset](/paper/a-large-scale-chinese-short-text-conversation)", "variants": ["LCCC"]}
{"id": "LC-QuAD 2.0", "title": "", "contents": "LC-QuAD 2.0 is a Large Question Answering dataset with 30,000 pairs of question and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version.\r\n\r\nSource: [LC-QuAD 2.0](http://lc-quad.sda.tech/)", "variants": ["LC-QuAD 2.0"]}
{"id": "Leaf counting dataset", "title": "", "contents": "Dataset containing  9372 RGB images of weeds with the number of leaves counted. The images are collected in fields across Denmark using Nokia and Samsung cell phone cameras; Samsung, Nikon, Canon and Sony consumer cameras; and a Point Grey industrial camera.\r\n\r\nSource: [Leaf counting dataset](https://vision.eng.au.dk/leaf-counting-dataset/)", "variants": ["Leaf counting dataset"]}
{"id": "Legal Documents Entity Recognition", "title": "", "contents": "Court decisions from 2017 and 2018 were selected for the dataset, published online by the Federal Ministry of Justice and Consumer Protection. The documents originate from seven federal courts: Federal Labour Court (BAG), Federal Fiscal Court (BFH), Federal Court of Justice (BGH), Federal Patent Court (BPatG), Federal Social Court (BSG), Federal Constitutional Court (BVerfG) and Federal Administrative Court (BVerwG).\r\n\r\nSource: [Legal Documents Entity Recognition](https://github.com/elenanereiss/Legal-Entity-Recognition)", "variants": ["Legal Documents Entity Recognition"]}
{"id": "LEMMA", "title": "", "contents": "The **LEMMA** dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.\r\n\r\nBoth the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.\r\n\r\nSource: [LEMMA](https://sites.google.com/view/lemma-activity/home)", "variants": ["LEMMA"]}
{"id": "Lenta Short Sentences", "title": "Russian Natural Language Generation: Creation of a Language Modelling Dataset and Evaluation with Modern Neural Architectures", "contents": "The **Lenta Short Sentences** dataset is a text dataset for language modelling for the Russian language. It consists of 236K sentences sampled from the Lenta News dataset.\n\nSource: [https://arxiv.org/pdf/2005.02470.pdf](https://arxiv.org/pdf/2005.02470.pdf)", "variants": ["Lenta Short Sentences"]}
{"id": "Libri-Adapt", "title": "Libri-Adapt: A New Speech Dataset for Unsupervised Domain Adaptation", "contents": "Libri-Adapt aims to support unsupervised domain adaptation research on speech recognition models.\r\n\r\nSource: [Libri-Adapt: A New Speech Dataset for Unsupervised Domain Adaptation](/paper/libri-adapt-a-new-speech-dataset-for)", "variants": ["Libri-Adapt"]}
{"id": "LibriMix", "title": "LibriMix: An Open-Source Dataset for Generalizable Speech Separation", "contents": "LibriMix is an open-source alternative to wsj0-2mix. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. \r\n\r\nSource: [LibriMix: An Open-Source Dataset for Generalizable Speech Separation](/paper/librimix-an-open-source-dataset-for)", "variants": ["LibriMix"]}
{"id": "LiMiT", "title": "", "contents": "The limit dataset of ~24K sentences that describe literal motion (~14K sentences), and sentences not describing motion or other type of motion (e.g. fictive motion). Senteces were extracted from electronic books categorized as fiction or novels, and a portion from the NetActivity Captions Dataset.\r\n\r\nSource: [LiMiT](https://github.com/ilmgut/limit_dataset)", "variants": ["LiMiT"]}
{"id": "LinCE", "title": "LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation", "contents": "A centralized benchmark for Linguistic Code-switching Evaluation (LinCE) that combines ten corpora covering four different code-switched language pairs (i.e., Spanish-English, Nepali-English, Hindi-English, and Modern Standard Arabic-Egyptian Arabic) and four tasks (i.e., language identification, named entity recognition, part-of-speech tagging, and sentiment analysis). \r\n\r\nSource: [LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation](/paper/lince-a-centralized-benchmark-for-linguistic)", "variants": ["LinCE"]}
{"id": "Liputan6", "title": "Liputan6: A Large-scale Indonesian Dataset for Text Summarization", "contents": "A large-scale Indonesian summarization dataset consisting of harvested articles from Liputan6.com, an online news portal, resulting in 215,827 document-summary pairs.\r\n\r\nSource: [Liputan6: A Large-scale Indonesian Dataset for Text Summarization](/paper/liputan6-a-large-scale-indonesian-dataset-for)", "variants": ["Liputan6"]}
{"id": "LiveQA", "title": "LiveQA: A Question Answering Dataset over Sports Live", "contents": "A new question answering dataset constructed from play-by-play live broadcast. It contains 117k multiple-choice questions written by human commentators for over 1,670 NBA games, which are collected from the Chinese Hupu (https://nba.hupu.com/games) website.\r\n\r\nSource: [LiveQA: A Question Answering Dataset over Sports Live](/paper/liveqa-a-question-answering-dataset-over)", "variants": ["LiveQA"]}
{"id": "LIVE-YT-HFR", "title": "Subjective and Objective Quality Assessment of High Frame Rate Videos", "contents": "LIVE-YT-HFR comprises of 480 videos having 6 different frame rates, obtained from 16 diverse contents.\r\n\r\nSource: [Subjective and Objective Quality Assessment of High Frame Rate Videos](/paper/subjective-and-objective-quality-assessment-1)", "variants": ["LIVE-YT-HFR"]}
{"id": "WI-LOCNESS", "title": "The BEA-2019 Shared Task on Grammatical Error Correction", "contents": "WI-LOCNESS is part of the [Building Educational Applications 2019 Shared Task for Grammatical Error Correction](https://www.cl.cam.ac.uk/research/nl/bea2019st/). It consists of two datasets:\r\n\r\n- **LOCNESS**: is a corpus consisting of essays written by native English students. \r\n- **Cambridge English Write & Improve** (**W&I**): Write & Improve (Yannakoudakis et al., 2018) is an online web platform that assists non-native English students with their writing. Specifically, students from around the world submit letters, stories, articles and essays in response to various prompts, and the W&I system provides instant feedback. Since W&I went live in 2014, W&I annotators have manually annotated some of these submissions and assigned them a CEFR level.\r\n\r\nSource: [The BEA-2019 Shared Task on Grammatical Error Correction](/paper/the-bea-2019-shared-task-on-grammatical-error)\r\n\r\nImage source: [WI-LOCNESS](https://www.cl.cam.ac.uk/research/nl/bea2019st/#data)", "variants": ["BEA-2019 (test)", "WI-LOCNESS"]}
{"id": "Logic2Text", "title": "Logic2Text: High-Fidelity Natural Language Generation from Logical Forms", "contents": "**Logic2Text** is a large-scale dataset with 10,753 descriptions involving common logic types paired with the underlying logical forms. The logical forms show diversified graph structure of free schema, which poses great challenges on the model's ability to understand the semantics.\n\nSource: [https://github.com/czyssrs/Logic2Text](https://github.com/czyssrs/Logic2Text)", "variants": ["Logic2Text"]}
{"id": "LogiQA", "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning", "contents": "LogiQA consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. The dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting.\r\n\r\nSource: [LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning](/paper/logiqa-a-challenge-dataset-for-machine)\r\nImage Source: [https://arxiv.org/pdf/2007.08124v1.pdf](https://arxiv.org/pdf/2007.08124v1.pdf)", "variants": ["LogiQA"]}
{"id": "LogoDet-3K", "title": "LogoDet-3K: A Large-Scale Image Dataset for Logo Detection", "contents": "A logo detection dataset with full annotation, which has 3,000 logo categories, about 200,000 manually annotated logo objects and 158,652 images. LogoDet-3K creates a more challenging benchmark for logo detection, for its higher comprehensive coverage and wider variety in both logo categories and annotated objects compared with existing datasets. \r\n\r\nSource: [LogoDet-3K: A Large-Scale Image Dataset for Logo Detection](/paper/logodet-3k-a-large-scale-image-dataset-for)", "variants": ["LogoDet-3K"]}
{"id": "Long-Term Crowd Flow", "title": "Laying the Foundations of Deep Long-Term Crowd Flow Prediction", "contents": "A synthetic dataset of procedurally generated environments, dynamically simulated crowd flows, and statically derived “proxy” crowd flows (which have more error but are more efficient to compute), for model training and evaluation.\r\n\r\nSource: [Laying the Foundations of Deep Long-Term Crowd Flow Prediction](/paper/laying-the-foundations-of-deep-long-term)", "variants": ["Long-Term Crowd Flow"]}
{"id": "LSMDC-Context", "title": "Enriching Video Captions With Contextual Text", "contents": "The Large Scale Movie Description Challenge (LSMDC) - Context is an augmented version of the original LSMDC dataset with movie scripts as contextual text.\n\nSource: [https://github.com/primle/LSMDC-Context](https://github.com/primle/LSMDC-Context)", "variants": ["LSMDC-Context"]}
{"id": "Lyft Level 5 Prediction", "title": "One Thousand and One Hours: Self-driving Motion Prediction Dataset", "contents": "A self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. \r\n\r\nSource: [One Thousand and One Hours: Self-driving Motion Prediction Dataset](/paper/one-thousand-and-one-hours-self-driving)", "variants": ["Lyft Level 5 Prediction"]}
{"id": "m2caiSeg", "title": "m2caiSeg: Semantic Segmentation of Laparoscopic Images using Convolutional Neural Networks", "contents": "Created from endoscopic video feeds of real-world surgical procedures. Overall, the data consists of 307 images, each of which is annotated for the organs and different surgical instruments present in the scene.\r\n\r\nSource: [m2caiSeg: Semantic Segmentation of Laparoscopic Images using Convolutional Neural Networks](/paper/m2caiseg-semantic-segmentation-of)", "variants": ["m2caiSeg"]}
{"id": "M2E2", "title": "Cross-media Structured Common Space for Multimedia Event Extraction", "contents": "Aims to extract events and their arguments from multimedia documents. Develops the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments.\r\n\r\nSource: [Cross-media Structured Common Space for Multimedia Event Extraction](/paper/cross-media-structured-common-space-for)", "variants": ["M2E2"]}
{"id": "Makeup Datasets", "title": "", "contents": "A dataset of female face images assembled for studying the impact of makeup on face recognition.\r\n\r\nSource: [Makeup Datasets](http://www.antitza.com/makeup-datasets.html)", "variants": ["Makeup Datasets"]}
{"id": "Malaria Dataset", "title": "", "contents": "The dataset contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells.\r\n\r\nSource: [Malaria Dataset](https://lhncbc.nlm.nih.gov/publication/pub9932)", "variants": ["Malaria Dataset"]}
{"id": "MalayalamMixSentiment", "title": "A Sentiment Analysis Dataset for Code-Mixed Malayalam-English", "contents": "MalayalamMixSentiment is a Sentiment Analysis Dataset for Code-Mixed Malayalam-English.\r\n\r\nSource: [https://github.com/bharathichezhiyan/MalayalamMixSentiment](https://github.com/bharathichezhiyan/MalayalamMixSentiment)", "variants": ["MalayalamMixSentiment"]}
{"id": "MAMe", "title": "A Closer Look at Art Mediums: The MAMe Image Classification Dataset", "contents": "The **MAMe** dataset contains images of high-resolution and variable shape of artworks from 3 different museums:\r\n- The Metropolitan Museum of Art of New York\r\n- The Los Angeles County Museum of Art\r\n- The Cleveland Museum of Art\r\n\r\nSource: [https://github.com/HPAI-BSC/MAMe-baselines](https://github.com/HPAI-BSC/MAMe-baselines)\r\nImage Source: [Pares´ et al](https://arxiv.org/pdf/2007.13693.pdf)", "variants": ["MAMe"]}
{"id": "MANTRA", "title": "MANTRA: A Machine Learning reference lightcurve dataset for astronomical transient event recognition", "contents": "An annotated dataset of 4869 transient and 71207 non-transient object lightcurves built from the Catalina Real Time Transient Survey. \r\n\r\nSource: [MANTRA: A Machine Learning reference lightcurve dataset for astronomical transient event recognition](/paper/mantra-a-machine-learning-reference)", "variants": ["MANTRA"]}
{"id": "Market1203-Reid-Dataset", "title": "", "contents": "This dataset contains 1203 individuals captured from two disjoint camera views. To each person, one to twelve images are captured from one to six different orientations under one camera view and are normalized to 128x64 pixels. This dataset is constructed based on the Market-1501 benchmark data and the orientation label for each image has been manually annotated.\n\nSource: [https://github.com/charliememory/Market1203-Reid-Dataset](https://github.com/charliememory/Market1203-Reid-Dataset)", "variants": ["Market1203-Reid-Dataset"]}
{"id": "MASATI", "title": "", "contents": "The MASATI dataset contains color images in dynamic marine environments, and it can be used to evaluate ship detection methods. Each image may contain one or multiple targets in different weather and illumination conditions. The datasets is composed of 7,389 satellite images labeled according to the following seven classes: land, coast, sea, ship, multi, coast-ship, and detail. In addition, labeling with the bounding box for the location of the vessels is also included.\r\n\r\nSource: [MASATI](http://www.iuii.ua.es/datasets/masati/index.html)", "variants": ["MASATI"]}
{"id": "MaskedFace-Net", "title": "MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images in the Context of COVID-19", "contents": "Proposes three types of masked face detection dataset; namely, the Correctly Masked Face Dataset (CMFD), the Incorrectly Masked Face Dataset (IMFD) and their combination for the global masked face detection (MaskedFace-Net).\r\n\r\nSource: [MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images in the Context of COVID-19](/paper/maskedface-net-a-dataset-of-correctly)", "variants": ["MaskedFace-Net"]}
{"id": "MASRI-HEADSET", "title": "MASRI-HEADSET: A Maltese Corpus for Speech Recognition", "contents": "MASRI-HEADSET is a corpus that was developed by the MASRI project at the University of Malta. It consists of 8 hours of speech paired with text, recorded by using short text snippets in a laboratory environment. The speakers were recruited from different geographical locations all over the Maltese islands, and were roughly evenly distributed by gender. \r\n\r\nSource: [MASRI-HEADSET: A Maltese Corpus for Speech Recognition](/paper/masri-headset-a-maltese-corpus-for-speech-1)", "variants": ["MASRI-HEADSET"]}
{"id": "MAVEN", "title": "MAVEN: A Massive General Domain Event Detection Dataset", "contents": "Contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. \r\n\r\nSource: [MAVEN: A Massive General Domain Event Detection Dataset](/paper/maven-a-massive-general-domain-event)", "variants": ["MAVEN"]}
{"id": "MC-TACO", "title": "", "contents": "MC-TACO is a dataset of 13k question-answer pairs that require temporal commonsense comprehension. The dataset contains five temporal properties, (1) duration (how long an event takes), (2) temporal ordering (typical order of events), (3) typical time (when an event occurs), (4) frequency (how often an event occurs), and (5) stationarity (whether a state is maintained for a very long time or indefinitely). \r\n\r\nSource: [MC-TACO](https://leaderboard.allenai.org/mctaco/submissions/public)", "variants": ["MC-TACO"]}
{"id": "MD4K", "title": "A Deeper Look at Salient Object Detection: Bi-stream Network with a Small Training Dataset", "contents": "A small-scale training set, which only contains 4K images.\r\n\r\nSource: [A Deeper Look at Salient Object Detection: Bi-stream Network with a Small Training Dataset](/paper/a-deeper-look-at-salient-object-detection-bi)", "variants": ["MD4K"]}
{"id": "MD Gender", "title": "Multi-Dimensional Gender Bias Classification", "contents": "Provides eight automatically annotated large scale datasets with gender information. \r\n\r\nSource: [Multi-Dimensional Gender Bias Classification](/paper/multi-dimensional-gender-bias-classification)", "variants": ["MD Gender"]}
{"id": "mEBAL", "title": "mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level Estimation", "contents": "A multimodal database for eye blink detection and attention level estimation. \r\n\r\nSource: [mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level Estimation](/paper/mebal-a-multimodal-database-for-eye-blink)", "variants": ["mEBAL"]}
{"id": "MeDAL", "title": "MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining", "contents": "The  Medical Dataset for Abbreviation Disambiguation for Natural Language Understanding (**MeDAL**) is a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. It was published at the ClinicalNLP workshop at EMNLP.\n\nSource: [https://github.com/BruceWen120/medal](https://github.com/BruceWen120/medal)\nImage Source: [https://github.com/BruceWen120/medal](https://github.com/BruceWen120/medal)", "variants": ["MeDAL"]}
{"id": "MedDG", "title": "MedDG: A Large-scale Medical Consultation Dataset for Building Medical Dialogue System", "contents": "**MedDG** is a large-scale high-quality Medical Dialogue dataset related to 12 types of common Gastrointestinal diseases. It contains more than 17K conversations collected from the online health consultation community. Five different categories of entities, including diseases, symptoms, attributes, tests, and medicines, are annotated in each conversation of MedDG as additional labels.\nTwo kinds of medical dialogue tasks are proposed for this dataset:\n* Next entity prediction\n* Doctor response generation\n\nSource: [https://github.com/lwgkzl/MedDG](https://github.com/lwgkzl/MedDG)", "variants": ["MedDG"]}
{"id": "MedICaT", "title": "MedICaT: A Dataset of Medical Images, Captions, and Textual References", "contents": "**MedICaT** is a dataset of medical images, captions, subfigure-subcaption annotations, and inline textual references.\nFigures and captions are extracted from open access articles in PubMed Central and corresponding reference text is derived from S2ORC.\nThe dataset consists of:\n217,060 figures from 131,410 open access papers\n7507 subcaption and subfigure annotations for 2069 compound figures\nInline references for ~25K figures in the ROCO dataset\n\nSource: [https://github.com/allenai/medicat](https://github.com/allenai/medicat)", "variants": ["MedICaT"]}
{"id": "MEDIQA-AnS", "title": "Question-Driven Summarization of Answers to Consumer Health Questions", "contents": "The first summarization collection containing question-driven summaries of answers to consumer health questions. This dataset can be used to evaluate single or multi-document summaries generated by algorithms using extractive or abstractive approaches. \r\n\r\nSource: [Question-Driven Summarization of Answers to Consumer Health Questions](/paper/question-driven-summarization-of-answers-to)", "variants": ["MEDIQA-AnS"]}
{"id": "Medley2K", "title": "Medley2K: A Dataset of Medley Transitions", "contents": "A dataset called Medley2K that consists of 2,000 medleys and 7,712 labeled transitions. \r\n\r\nSource: [Medley2K: A Dataset of Medley Transitions](/paper/medley2k-a-dataset-of-medley-transitions)", "variants": ["Medley2K"]}
{"id": "MedMNIST", "title": "MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis", "contents": "A collection of 10 pre-processed medical open datasets. MedMNIST is standardized to perform classification tasks on lightweight 28x28 images, which requires no background knowledge. \r\n\r\nSource: [MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis](/paper/medmnist-classification-decathlon-a)", "variants": ["MedMNIST"]}
{"id": "Mega-COV", "title": "Mega-COV: A Billion-Scale Dataset of 100+ Languages for COVID-19", "contents": "**Mega-COV** is a billion-scale dataset from Twitter for studying COVID-19. The dataset is diverse (covers 234 countries), longitudinal (goes as back as 2007), multilingual (comes in 65 languages), and has a significant number of location-tagged tweets (~32M tweets).\n\nSource: [https://github.com/UBC-NLP/megacov](https://github.com/UBC-NLP/megacov)\nImage Source: [https://github.com/UBC-NLP/megacov](https://github.com/UBC-NLP/megacov)", "variants": ["Mega-COV"]}
{"id": "Melinda", "title": "MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification", "contents": "Introduces a new dataset, MELINDA, for Multimodal biomEdicaL experImeNt methoD clAssification. The dataset is collected in a fully automated distant supervision manner, where the labels are obtained from an existing curated database, and the actual contents are extracted from papers associated with each of the records in the database. \r\n\r\nSource: [MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification](/paper/melinda-a-multimodal-dataset-for-biomedical)", "variants": ["Melinda"]}
{"id": "Metaphorics", "title": "", "contents": "Metaphorics is a newly introduced non-contextual skeleton action dataset. All the datasets introduced so far in the skeleton human action recognition have categories based only on verb-based actions.\r\n\r\nSource: [Metaphorics](https://github.com/skelemoa/quovadis/tree/master/metaphorics)", "variants": ["Metaphorics"]}
{"id": "methods2test", "title": "Unit Test Case Generation with Transformers", "contents": "**methods2test** is a supervised dataset consisting of Test Cases and their corresponding Focal Methods from a set of Java software repositories.\nMethods2test was constructed by parsing the Java projects to obtain classes and methods with their associated metadata. Next each Test Class was matched to its corresponding Focal Class. Finally, each Test Case within a Test Class was mapped to the related Focal Method to obtain a set of Mapped Test Cases.\n\nSource: [https://github.com/microsoft/methods2test](https://github.com/microsoft/methods2test)", "variants": ["methods2test"]}
{"id": "#MeTooMA", "title": "", "contents": "The dataset consists of tweets belonging to #MeToo movement on Twitter, labelled into different categories. \r\n\r\nSource: [#MeTooMA](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JN4EYU)", "variants": ["#MeTooMA"]}
{"id": "AraMeter", "title": "", "contents": "A dataset to identify the meters of Arabic poems.\r\n\r\nSource: [https://github.com/zaidalyafeai/MetRec](https://github.com/zaidalyafeai/MetRec)", "variants": ["AraMeter"]}
{"id": "MEVA", "title": "MEVA: A Large-Scale Multiview, Multimodal Video Dataset for Activity Detection", "contents": "Large-scale dataset for human activity recognition. Existing security datasets either focus on activity counts by aggregating public video disseminated due to its content, which typically excludes same-scene background video, or they achieve persistence by observing public areas and thus cannot control for activity content. The dataset is over 9300 hours of untrimmed, continuous video, scripted to include diverse, simultaneous activities, along with spontaneous background activity.\r\n\r\nSource: [MEVA: A Large-Scale Multiview, Multimodal Video Dataset for Activity Detection](/paper/meva-a-large-scale-multiview-multimodal-video)", "variants": ["MEVA"]}
{"id": "Mewsli-9", "title": "Entity Linking in 100 Languages", "contents": "A large new multilingual dataset for multilingual entity linking.\r\n\r\nSource: [Entity Linking in 100 Languages](/paper/entity-linking-in-100-languages)", "variants": ["Mewsli-9"]}
{"id": "Mid-Air Dataset", "title": "", "contents": "Mid-Air, The Montefiore Institute Dataset of Aerial Images and Records, is a multi-purpose synthetic dataset for low altitude drone flights. It provides a large amount of synchronized data corresponding to flight records for multi-modal vision sensors and navigation sensors mounted on board of a flying quadcopter. Multi-modal vision sensors capture RGB pictures, relative surface normal orientation, depth, object semantics and stereo disparity.\r\n\r\nSource: [Mid-Air Dataset](https://midair.ulg.ac.be)", "variants": ["Mid-Air Dataset"]}
{"id": "MitoEM", "title": "", "contents": "Contains mitochondria instances.\r\n\r\nSource: [MitoEM](https://mitoem.grand-challenge.org/)", "variants": ["MitoEM"]}
{"id": "MITOS_WSI_CMC", "title": "A completely annotated whole slide image dataset of canine breast cancer to aid human breast cancer research", "contents": "A dataset of 21 WSIs of CMC completely annotated for MF. For this, a pathologist screened all WSIs for potential MF and structures with a similar appearance.\r\n\r\nSource: [A completely annotated whole slide image dataset of canine breast cancer to aid human breast cancer research](/paper/dogs-as-model-for-human-breast-cancer-a)", "variants": ["MITOS_WSI_CMC"]}
{"id": "MJU-Waste", "title": "A Multi-Level Approach to Waste Object Segmentation", "contents": "**MJU-Waste** is an RGBD waste object segmentation dataset that is made public to facilitate future research in this area.\r\n\r\nSource: [https://github.com/realwecan/mju-waste](https://github.com/realwecan/mju-waste)\r\nImage Source: [Wang et al](https://arxiv.org/pdf/2007.04259v1.pdf)", "variants": ["MJU-Waste"]}
{"id": "MKQA", "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering", "contents": "Multilingual Knowledge Questions and Answers (MKQA) is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). The goal of this dataset is to provide a challenging benchmark for question answering quality across a wide set of languages. Answers are based on a language-independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering.", "variants": ["MKQA"]}
{"id": "MK-SQuIT", "title": "MK-SQuIT: Synthesizing Questions using Iterative Template-filling", "contents": "An example dataset of 110,000 question/query pairs across four WikiData domains.\r\n\r\nSource: [MK-SQuIT: Synthesizing Questions using Iterative Template-filling](/paper/mk-squit-synthesizing-questions-using)", "variants": ["MK-SQuIT"]}
{"id": "MLM", "title": "MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities", "contents": "A new resource to train and evaluate multitask systems on samples in multiple modalities and three languages. \r\n\r\nSource: [MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities](/paper/mlm-a-benchmark-dataset-for-multitask)", "variants": ["MLM"]}
{"id": "MLQE", "title": "Unsupervised Quality Estimation for Neural Machine Translation", "contents": "The **MLQE** dataset is a dataset for sentence-level Machine Translation Quality Estimation. It consists of 6 language pairs representing NMT training in high, medium, and low-resource scenarios. The corpus is extracted from Wikipedia, and 10K segments per language pair are annotated.\n\nSource: [https://github.com/facebookresearch/mlqe](https://github.com/facebookresearch/mlqe)", "variants": ["MLQE"]}
{"id": "MLQE-PE", "title": "MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset", "contents": "The Multilingual Quality Estimation and Automatic Post-editing (**MLQE-PE**) Dataset is a dataset for Machine Translation (MT) Quality Estimation (QE) and Automatic Post-Editing (APE). The dataset contains seven language pairs, with human labels for 9,000 translations per language pair in the following formats: sentence-level direct assessments and post-editing effort, and word-level good/bad labels. It also contains the post-edited sentences, as well as titles of the articles where the sentences were extracted from, and the neural MT models used to translate the text.\n\nSource: [https://github.com/sheffieldnlp/mlqe-pe](https://github.com/sheffieldnlp/mlqe-pe)", "variants": ["MLQE-PE"]}
{"id": "MLRSNet", "title": "MLRSNet: A Multi-label High Spatial Resolution Remote Sensing Dataset for Semantic Scene Understanding", "contents": "**MLRSNet** is a a multi-label high spatial resolution remote sensing dataset for semantic scene understanding. It provides different perspectives of the world captured from satellites. That is, it is composed of high spatial resolution optical satellite images. MLRSNet contains 109,161 remote sensing images that are annotated into 46 categories, and the number of sample images in a category varies from 1,500 to 3,000. The images have a fixed size of 256×256 pixels with various pixel resolutions (~10m to 0.1m). Moreover, each image in the dataset is tagged with several of 60 predefined class labels, and the number of labels associated with each image varies from 1 to 13. The dataset can be used for multi-label based image classification, multi-label based image retrieval, and image segmentation.\r\n\r\nSource: [https://github.com/cugbrs/MLRSNet](https://github.com/cugbrs/MLRSNet)\r\nImage Source: [Qi et al](https://arxiv.org/pdf/2010.00243v1.pdf)", "variants": ["MLRSNet"]}
{"id": "MLSUM", "title": "MLSUM: The Multilingual Summarization Corpus", "contents": "A large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community.\r\n\r\nSource: [MLSUM: The Multilingual Summarization Corpus](/paper/mlsum-the-multilingual-summarization-corpus)", "variants": ["MLSUM", "MLSUM de", "MLSUM es"]}
{"id": "MNIST-1D", "title": "Scaling down Deep Learning", "contents": "A minimalist, low-memory, and low-compute alternative to classic deep learning benchmarks. The training examples are 20 times smaller than MNIST examples yet they differentiate more clearly between linear, nonlinear, and convolutional models which attain 32, 68, and 94% accuracy respectively (these models obtain 94, 99+, and 99+% on MNIST).\r\n\r\nSource: [Scaling down Deep Learning](/paper/scaling-down-deep-learning)", "variants": ["MNIST-1D"]}
{"id": "Mo2Cap2", "title": "", "contents": "A large ground truth training corpus of top-down fisheye images.\r\n\r\nSource: [Mo2Cap2](http://gvv.mpi-inf.mpg.de/projects/wxu/Mo2Cap2/)", "variants": ["Mo2Cap2"]}
{"id": "MOCHA", "title": "MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics", "contents": "Contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. \r\n\r\nSource: [MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics](/paper/mocha-a-dataset-for-training-and-evaluating)", "variants": ["MOCHA"]}
{"id": "MOD++", "title": "0-MMS: Zero-Shot Multi-Motion Segmentation With A Monocular Event Camera", "contents": "Includes challenging sequences and extensive data stratification in-terms of camera and object motion, velocity magnitudes, direction, and rotational speeds.\r\n\r\nSource: [0-MMS: Zero-Shot Multi-Motion Segmentation With A Monocular Event Camera](/paper/moms-with-events-multi-object-motion)", "variants": ["MOD++"]}
{"id": "Molweni", "title": "Molweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure", "contents": "A machine reading comprehension (MRC) dataset with discourse structure built over multiparty dialog. Molweni's source samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising 88,303 utterances. \r\n\r\nSource: [Molweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure](/paper/molweni-a-challenge-multiparty-dialogues)", "variants": ["Molweni"]}
{"id": "Moral Stories", "title": "Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences", "contents": "A crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning.\r\n\r\nSource: [Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences](/paper/moral-stories-situated-reasoning-about-norms)", "variants": ["Moral Stories"]}
{"id": "MOR-UAV", "title": "MOR-UAV: A Benchmark Dataset and Baselines for Moving Object Recognition in UAV Videos", "contents": "A large-scale video dataset for MOR in aerial videos.\r\n\r\nSource: [MOR-UAV: A Benchmark Dataset and Baselines for Moving Object Recognition in UAV Videos](/paper/mor-uav-a-benchmark-dataset-and-baselines-for)", "variants": ["MOR-UAV"]}
{"id": "MosMedData", "title": "MosMedData: Chest CT Scans With COVID-19 Related Findings Dataset", "contents": "MosMedData contains anonymised human lung computed tomography (CT) scans with COVID-19 related findings, as well as without such findings. A small subset of studies has been annotated with binary pixel masks depicting regions of interests (ground-glass opacifications and consolidations). CT scans were obtained between 1st of March, 2020 and 25th of April, 2020, and provided by municipal hospitals in Moscow, Russia.\r\n\r\nSource: [MosMedData: Chest CT Scans With COVID-19 Related Findings Dataset](/paper/mosmeddata-chest-ct-scans-with-covid-19)", "variants": ["MosMedData"]}
{"id": "Mouse Embryo Tracking Database", "title": "", "contents": "The **Mouse Embryo Tracking Database** is a dataset for tracking mouse embryos. The dataset contains, for each of the 100 examples: (1) the uncompressed frames, up to the 10th frame after the appearance of the 8th cell; (2) a text file with the trajectories of all the cells, from appearance to division (for cells of generations 1 to 3), where a trajectory is a sequence of pairs (center, radius); (3) a movie file showing the trajectories of the cells.", "variants": ["Mouse Embryo Tracking Database"]}
{"id": "MPI FAUST Dataset", "title": "FAUST: Dataset and Evaluation for 3D Mesh Registration", "contents": "Contains 300 scans of 10 people in a wide range of poses together with an evaluation methodology.\r\n\r\nSource: [FAUST: Dataset and Evaluation for 3D Mesh Registration](/paper/faust-dataset-and-evaluation-for-3d-mesh)", "variants": ["MPI FAUST Dataset"]}
{"id": "MRNet", "title": "A Comparative Study of Existing and New Deep Learning Methods for Detecting Knee Injuries using the MRNet Dataset", "contents": "The MRNet dataset consists of 1,370 knee MRI exams performed at Stanford University Medical Center. The dataset contains 1,104 (80.6%) abnormal exams, with 319 (23.3%) ACL tears and 508 (37.1%) meniscal tears; labels were obtained through manual extraction from clinical reports. \r\n\r\nSource: [MRNet](https://stanfordmlgroup.github.io/competitions/mrnet/)", "variants": ["MRNet"]}
{"id": "MultiReQA", "title": "MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering Models", "contents": "**MultiReQA** is a cross-domain evaluation for retrieval question answering models. Retrieval question answering (ReQA) is the task of retrieving a sentence-level answer to a question from an open corpus. MultiReQA is a new multi-domain ReQA evaluation suite composed of eight retrieval QA tasks drawn from publicly available QA datasets from the MRQA shared task.\nMultiReQA contains the sentence boundary annotation from eight publicly available QA datasets including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, BioASQ, RelationExtraction, and TextbookQA. Five of these datasets, including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, contain both training and test data, and three, in cluding BioASQ, RelationExtraction, TextbookQA, contain only the test data.\n\nSource: [https://github.com/google-research-datasets/MultiReQA](https://github.com/google-research-datasets/MultiReQA)", "variants": ["MultiReQA"]}
{"id": "MultiviewX", "title": "Multiview Detection with Feature Perspective Transformation", "contents": "**MultiviewX** is a synthetic Multiview pedestrian detection dataset. It is build using pedestrian models from PersonX, in Unity.\nThe MultiviewX dataset covers a square of 16 meters by 25 meters. The ground plane is quantized into a 640x1000 grid. There are 6 cameras with overlapping field-of-view in the MultiviewX dataset, each of which outputs a 1080x1920 resolution image. On average, 4.41 cameras are covering the same location.\n\nSource: [https://github.com/hou-yz/MVDet](https://github.com/hou-yz/MVDet)\nImage Source: [https://github.com/hou-yz/MVDet](https://github.com/hou-yz/MVDet)", "variants": ["MultiviewX"]}
{"id": "MultiWOZ-coref", "title": "MultiWOZ 2.3: A multi-domain task-oriented dataset enhanced with annotation corrections and co-reference annotation", "contents": "**MultiWOZ-coref**, (or MultiWOZ 2.3) is an extension of the MultiWOZ dataset that adds co-reference annotations in addition to corrections of dialogue acts and dialogue states.\n\nSource: [https://github.com/lexmen318/MultiWOZ_2.3](https://github.com/lexmen318/MultiWOZ_2.3)", "variants": ["MultiWOZ-coref"]}
{"id": "Multi-XScience", "title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles", "contents": "**Multi-XScience** is a large-scale dataset for multi-document summarization of scientific articles. It has 30,369, 5,066 and 5,093 samples for the train, validation and test split respectively. The average document length is 778.08 words and the average summary length is 116.44 words.\n\nSource: [https://github.com/yaolu/Multi-XScience](https://github.com/yaolu/Multi-XScience)", "variants": ["Multi-XScience"]}
{"id": "MUTLA", "title": "MUTLA: A Large-Scale Dataset for Multimodal Teaching and Learning Analytics", "contents": "This dataset includes time-synchronized multimodal data records of students (learning logs, videos, EEG brainwaves) as they work in various subjects from Squirrel AI Learning System (SAIL) to solve problems of varying difficulty levels. The dataset resources include user records from the learner records store of SAIL, brainwave data collected by EEG headset devices, and video data captured by web cameras while students worked in the SAIL products. \r\n\r\nSource: [MUTLA: A Large-Scale Dataset for Multimodal Teaching and Learning Analytics](/paper/mutla-a-large-scale-dataset-for-multimodal)", "variants": ["MUTLA"]}
{"id": "TAPOS", "title": "Intra- and Inter-Action Understanding via Temporal Action Parsing", "contents": "**TAPOS** is a new dataset developed on sport videos with manual annotations of sub-actions, and conduct a study on temporal action parsing on top. A sport activity usually consists of multiple sub-actions and that the awareness of such temporal structures is beneficial to action recognition.\r\n\r\nTAPOS contains 16,294 valid instances in total, across 21 action classes. These instances have a duration of 9.4\r\nseconds on average. The number of instances within each class is different, where the largest class high jump has over\r\n1,600 instances, and the smallest class beam has 200 instances. The average number of sub-actions also varies\r\nfrom class to class, where parallel bars has 9 sub-actions on average, and long jump has 3 sub-actions on average. All instances are split into train, validation and test sets, of sizes 13094, 1790, and 1763, respectively.", "variants": ["TAPOS"]}
{"id": "MWE-CWI", "title": "Detecting Multiword Expression Type Helps Lexical Complexity Assessment", "contents": "Multiword expressions (MWEs) represent lexemes that should be treated as single lexical units due to their idiosyncratic nature. **MWE-CWI** is a dataset for MWE detection based on the Complex Word Identification Shared Task 2018 dataset.\n\nSource: [https://github.com/ekochmar/MWE-CWI](https://github.com/ekochmar/MWE-CWI)", "variants": ["MWE-CWI"]}
{"id": "Nagoya University Extremely Low-resolution FIR Image Action Dataset", "title": "", "contents": "A pedestrian dataset for Person Re-identification.\r\n\r\nSource: [Nagoya University Extremely Low-resolution FIR Image Action Dataset](https://www.murase.m.is.nagoya-u.ac.jp/~kawanishiy/en/datasets.html)", "variants": ["Nagoya University Extremely Low-resolution FIR Image Action Dataset"]}
{"id": "NAIST COVID", "title": "NAIST COVID: Multilingual COVID-19 Twitter and Weibo Dataset", "contents": "NAIST COVID is a multilingual dataset of social media posts related to COVID-19, consisting of microblogs in English and Japanese from Twitter and those in Chinese from Weibo. The data cover microblogs from January 20, 2020, to March 24, 2020.\r\n\r\nSource: [NAIST COVID: Multilingual COVID-19 Twitter and Weibo Dataset](/paper/naist-covid-multilingual-covid-19-twitter-and)", "variants": ["NAIST COVID"]}
{"id": "NatCat", "title": "Natcat: Weakly Supervised Text Classification with Naturally Annotated Datasets", "contents": "A general purpose text categorization dataset (NatCat) from three online resources: Wikipedia, Reddit, and Stack Exchange. These datasets consist of document-category pairs derived from manual curation that occurs naturally by their communities.\r\n\r\nSource: [Natcat: Weakly Supervised Text Classification with Naturally Annotated Datasets](/paper/natcat-weakly-supervised-text-classification)", "variants": ["NatCat"]}
{"id": "NATS-Bench", "title": "NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size", "contents": "A unified benchmark on searching for both topology and size, for (almost) any up-to-date NAS algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates for architecture topology and 32,768 for architecture size on three datasets. \r\n\r\nSource: [NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size](/paper/nats-bench-benchmarking-nas-algorithms-for)", "variants": ["NATS-Bench"]}
{"id": "NCD", "title": "Image Colorization: A Survey and Dataset", "contents": "The **Natural-Color Dataset** (**NCD**) is an image colorization dataset where images are true to their colors. For example, a carrot will have an orange color in most images. Bananas will be either greenish or yellowish. It contains 723 images from the internet distributed in 20 categories. Each image has an object and a white background.\n\nSource: [https://github.com/saeed-anwar/ColorSurvey#dataset](https://github.com/saeed-anwar/ColorSurvey#dataset)\nImage Source: [https://github.com/saeed-anwar/ColorSurvey](https://github.com/saeed-anwar/ColorSurvey)", "variants": ["NCD"]}
{"id": "NDD20", "title": "NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation", "contents": "Northumberland Dolphin Dataset 2020 (NDD20) is a challenging image dataset annotated for both coarse and fine-grained instance segmentation and categorisation. This dataset, the first release of the NDD, was created in response to the rapid expansion of computer vision into conservation research and the production of field-deployable systems suited to extreme environmental conditions -- an area with few open source datasets. NDD20 contains a large collection of above and below water images of two different dolphin species for traditional coarse and fine-grained segmentation.\r\n\r\nSource: [NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation](/paper/ndd20-a-large-scale-few-shot-dolphin-dataset)\r\nImage Source: [Trotter et al](https://arxiv.org/pdf/2005.13359v1.pdf)", "variants": ["NDD20"]}
{"id": "NERGRIT Corpus", "title": "", "contents": "NERGRIT involves machine learning based NLP Tools and a corpus used for Indonesian Named Entity Recognition, Statement Extraction, and Sentiment Analysis. \r\n\r\nSource: [NERGRIT Corpus](https://github.com/grit-id/nergrit-corpus)", "variants": ["NERGRIT Corpus"]}
{"id": "NetiLook", "title": "", "contents": "A large-scale clothing dataset named NetiLook to discover netizen-style comments.\r\n\r\nSource: [NetiLook](https://mashyu.github.io/NSC/)", "variants": ["NetiLook"]}
{"id": "Neural Code Search Evaluation Dataset", "title": "", "contents": "Neural-Code-Search-Evaluation-Dataset presents an evaluation dataset consisting of natural language query and code snippet pairs, with the hope that future work in this area can use this dataset as a common benchmark. \r\n\r\nSource: [Neural Code Search Evaluation Dataset](https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset/)", "variants": ["Neural Code Search Evaluation Dataset"]}
{"id": "Neural Conversational QA", "title": "Neural Conversational QA: Learning to Reason vs Exploiting Patterns", "contents": "A modified data-set that has fewer spurious patterns than the original data-set, consequently allowing models to learn better.\r\n\r\nSource: [Neural Conversational QA: Learning to Reason vs Exploiting Patterns](/paper/neural-conversational-qa-learning-to-reason-1)", "variants": ["Neural Conversational QA"]}
{"id": "PAF Benchmark", "title": "", "contents": "Introduce three new neuromorphic vision datasets recorded by a novel neuromorphic vision sensor named Dynamic Vision Sensors (DVS).\r\n\r\nSource: [Neuromorphic Vision Datasets for Pedestrian Detection, Action Recognition, and Fall Detection](https://www.frontiersin.org/articles/10.3389/fnbot.2019.00038/full)", "variants": ["PAF Benchmark"]}
{"id": "NewB", "title": "NewB: 200,000+ Sentences for Political Bias Detection", "contents": "A text corpus of more than 200,000 sentences from eleven news sources regarding Donald Trump.\r\n\r\nSource: [NewB: 200,000+ Sentences for Political Bias Detection](/paper/newb-200000-sentences-for-political-bias)", "variants": ["NewB"]}
{"id": "New Brown Corpus", "title": "A Visuospatial Dataset for Naturalistic Verb Learning", "contents": "A new dataset for training and evaluating grounded language models. \r\n\r\nSource: [A Visuospatial Dataset for Naturalistic Verb Learning](/paper/a-visuospatial-dataset-for-naturalistic-verb)", "variants": ["New Brown Corpus"]}
{"id": "Newspaper Navigator", "title": "The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America", "contents": "The largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model.\r\n\r\nSource: [The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America](/paper/the-newspaper-navigator-dataset-extracting)", "variants": ["Newspaper Navigator"]}
{"id": "NewsPH-NLI", "title": "Investigating the True Performance of Transformers in Low-Resource Languages: A Case Study in Automatic Corpus Creation", "contents": "NewsPH-NLI is a sentence entailment benchmark dataset in the low-resource Filipino language. \r\n\r\nSource: [Investigating the True Performance of Transformers in Low-Resource Languages: A Case Study in Automatic Corpus Creation](/paper/investigating-the-true-performance-of)", "variants": ["NewsPH-NLI"]}
{"id": "NLI-TR", "title": "Data and Representation for Turkish Natural Language Inference", "contents": "Natural Language Inference in Turkish (NLI-TR) provides translations of two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. \r\n\r\nSource: [Data and Representation for Turkish Natural Language Inference](/paper/use-of-machine-translation-to-obtain-labeled)\r\nImage Source: [https://arxiv.org/pdf/2004.14963.pdf](https://arxiv.org/pdf/2004.14963.pdf)", "variants": ["NLI-TR"]}
{"id": "NSMC", "title": "", "contents": "This is a movie review dataset in the Korean language. Reviews were scraped from Naver Movies.\r\n\r\nSource: [NSMC](https://github.com/e9t/nsmc/)", "variants": ["NSMC"]}
{"id": "NTPairs", "title": "Understanding Effects of Editing Tweets for News Sharing by Media Accounts through a Causal Inference Framework", "contents": "The **NTPairs** dataset consists of the pairs of news articles and their corresponding tweets that were published by eight media outlets in 2018. The eight outlets were selected to consider diverse outlets, which employ a different editing style for news sharing, in terms of publishing channels and political leaning.\n\nSource: [https://github.com/bywords/NTPairs](https://github.com/bywords/NTPairs)\nImage Source: [https://github.com/bywords/NTPairs](https://github.com/bywords/NTPairs)", "variants": ["NTPairs"]}
{"id": "NumerSense", "title": "Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models", "contents": "Contains 13.6k masked-word-prediction probes, 10.5k for fine-tuning and 3.1k for testing.\r\n\r\nSource: [Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models](/paper/birds-have-four-legs-numersense-probing)", "variants": ["NumerSense"]}
{"id": "NYC3DCars", "title": "", "contents": "A vehicle detection database for vision tasks set in the real world.\r\n\r\nSource: [NYC3DCars](http://nyc3d.cs.cornell.edu/)", "variants": ["NYC3DCars"]}
{"id": "O4B", "title": "Open4Business(O4B): An Open Access Dataset for Summarizing Business Documents", "contents": "O4B is a dataset of 17,458 open access business articles and their reference summaries. The dataset introduces a new challenge for summarization in the business domain, requiring highly abstractive and more concise summaries as compared to other existing datasets. \r\n\r\nSource: [Open4Business(O4B): An Open Access Dataset for Summarizing Business Documents](/paper/open4business-o4b-an-open-access-dataset-for)", "variants": ["O4B"]}
{"id": "OASIS", "title": "OASIS: A Large-Scale Dataset for Single Image 3D in the Wild", "contents": "A dataset for single-image 3D in the wild consisting of annotations of detailed 3D geometry for 140,000 images.\r\n\r\nSource: [OASIS: A Large-Scale Dataset for Single Image 3D in the Wild](/paper/oasis-a-large-scale-dataset-for-single-image-1)", "variants": ["OASIS"]}
{"id": "Objectron", "title": "Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations", "contents": "The **Objectron** dataset is a collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment. In each video, the camera moves around the object, capturing it from different angles. The data also contain manually annotated 3D bounding boxes for each object, which describe the object’s position, orientation, and dimensions. The dataset consists of 15K annotated video clips supplemented with over 4M annotated images in the following categories: bikes, books, bottles, cameras, cereal boxes, chairs, cups, laptops, and shoes. To ensure geo-diversity, the dataset is collected from 10 countries across five continents.\n\nSource: [https://github.com/google-research-datasets/Objectron](https://github.com/google-research-datasets/Objectron)\nImage Source: [https://github.com/google-research-datasets/Objectron](https://github.com/google-research-datasets/Objectron)", "variants": ["Objectron"]}
{"id": "OBP", "title": "Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation", "contents": "Open Bandit Dataset is a public real-world logged bandit feedback data. The dataset is provided by ZOZO, Inc., the largest Japanese fashion e-commerce company with over 5 billion USD market capitalization (as of May 2020). The company uses multi-armed bandit algorithms to recommend fashion items to users in a large-scale fashion e-commerce platform called ZOZOTOWN.\r\n\r\nSource: [OBP](https://github.com/st-tech/zr-obp)", "variants": ["OBP"]}
{"id": "OCR-VQA", "title": "", "contents": "OCR-VQA dataset contains 207572 images and associated question-answer pairs.\r\n\r\nSource: [OCR-VQA](https://ocr-vqa.github.io/)", "variants": ["OCR-VQA"]}
{"id": "ODMS", "title": "Learning Object Depth from Camera Motion and Video Object Segmentation", "contents": "**ODMS** is a dataset for learning Object Depth via Motion and Segmentation. ODMS training data are configurable and extensible, with each training example consisting of a series of object segmentation masks, camera movement distances, and ground truth object depth. As a benchmark evaluation, the dataset provides four ODMS validation and test sets with 15,650 examples in multiple domains, including robotics and driving.\n\nSource: [https://github.com/griffbr/ODMS](https://github.com/griffbr/ODMS)\nImage Source: [https://github.com/griffbr/ODMS](https://github.com/griffbr/ODMS)", "variants": ["ODMS"]}
{"id": "OffComBR", "title": "", "contents": "Offensive comments obtained from Brazilian website.\r\n\r\nSource: [OffComBR](http://www.inf.ufrgs.br/~rppelle/hatedetector/)", "variants": ["OffComBR"]}
{"id": "OGB", "title": "Open Graph Benchmark: Datasets for Machine Learning on Graphs", "contents": "The **Open Graph Benchmark** (**OGB**) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. OGB datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can be evaluated using the OGB Evaluator in a unified manner.\r\nOGB is a community-driven initiative in active development.\r\n\r\nSource: [https://ogb.stanford.edu/](https://ogb.stanford.edu/)\r\nImage Source: [https://ogb.stanford.edu/](https://ogb.stanford.edu/)", "variants": ["OGB", "ogbn-arxiv", "ogbn-products", "ogbn-proteins", "ogbg-molhiv"]}
{"id": "Omni-MOT", "title": "Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking", "contents": "The Omni-MOT is realistic CARLA based large-scale dataset with over 14M frames for multiple vehicle tracking . The dataset comprises 14M+ frames, 250K tracks, 110 million bounding boxes, three weather conditions, three crowd levels and three camera views in five simulated towns.\r\n\r\nSource: [Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking](/paper/simultaneous-detection-and-tracking-with)", "variants": ["Omni-MOT"]}
{"id": "One Million Posts Corpus", "title": "", "contents": "An annotated data set consisting of user comments posted to an Austrian newspaper website (in German language).\r\n\r\nDER STANDARD is an Austrian daily broadsheet newspaper. On the newspaper’s website, there is a discussion section below each news article where readers engage in online discussions. The data set contains a selection of user posts from the 12 month time span from 2015-06-01 to 2016-05-31. There are 11,773 labeled and 1,000,000 unlabeled posts in the data set. The labeled posts were annotated by professional forum moderators employed by the newspaper.\r\n\r\nSource: [One Million Posts Corpus](https://ofai.github.io/million-post-corpus/)", "variants": ["One Million Posts Corpus"]}
{"id": "OneStopQA", "title": "STARC: Structured Annotations for Reading Comprehension", "contents": "OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.\r\n\r\nSource: [STARC: Structured Annotations for Reading Comprehension](/paper/starc-structured-annotations-for-reading)", "variants": ["OneStopQA"]}
{"id": "OOVD", "title": "", "contents": "This data set was created to understand the potential for machine learning, computer vision, and HPC to improve the energy efficiency aspects of traffic control by leveraging GRIDSMART traffic cameras as sensors for adaptive traffic control, with a sensitivity to the fuel consumption characteristics of the traffic in the camera’s visual field. GRIDSMART cameras—an existing, fielded commercial product—sense the presence of vehicles at intersections and replace more conventional sensors (such as inductive loops) to issue calls to traffic control. These cameras, which have horizon-to-horizon view, offer the potential for an improved view of the traffic environment which can be used to generate better control algorithms.\r\n\r\nSource: [OOVD](https://www.ornl.gov/project/ornl-overhead-vehicle-dataset-oovd)", "variants": ["OOVD"]}
{"id": "openDD", "title": "openDD: A Large-Scale Roundabout Drone Dataset", "contents": "Annotated using images taken by a drone in 501 separate flights, totalling in over 62 hours of trajectory data. As of today, openDD is by far the largest publicly available trajectory dataset recorded from a drone perspective, while comparable datasets span 17 hours at most.\r\n\r\nSource: [openDD: A Large-Scale Roundabout Drone Dataset](/paper/opendd-a-large-scale-roundabout-drone-dataset)", "variants": ["openDD"]}
{"id": "OpenEDS2020", "title": "OpenEDS2020: Open Eyes Dataset", "contents": "OpenEDS2020 is a dataset of eye-image sequences captured at a frame rate of 100 Hz under controlled illumination, using a virtual-reality head-mounted display mounted with two synchronized eye-facing cameras. The dataset, which is anonymized to remove any personally identifiable information on participants, consists of 80 participants of varied appearance performing several gaze-elicited tasks, and is divided in two subsets: 1) Gaze Prediction Dataset, with up to 66,560 sequences containing 550,400 eye-images and respective gaze vectors, created to foster research in spatio-temporal gaze estimation and prediction approaches; and 2) Eye Segmentation Dataset, consisting of 200 sequences sampled at 5 Hz, with up to 29,500 images, of which 5% contain a semantic segmentation label, devised to encourage the use of temporal information to propagate labels to contiguous frames. \r\n\r\nSource: [OpenEDS2020: Open Eyes Dataset](/paper/openeds2020-open-eyes-dataset)\r\nImage Source: [Palmero et al](https://arxiv.org/pdf/2005.03876v1.pdf)", "variants": ["OpenEDS2020"]}
{"id": "OpenLORIS-object", "title": "", "contents": "(L)ifel(O)ng (R)obotic V(IS)ion (OpenLORIS) - Object Recognition Dataset (OpenLORIS-Object) is designed for accelerating the lifelong/continual/incremental learning research and application，currently focusing on improving the continuous learning capability of the common objects in the home scenario.\r\n\r\nSource: [OpenLORIS-object](https://lifelong-robotic-vision.github.io/dataset/Data_Object-Recognition.html)", "variants": ["OpenLORIS-object"]}
{"id": "OpenSurfaces", "title": "", "contents": "**OpenSurfaces** is a large database of annotated surfaces created from real-world consumer photographs. The  framework used for the annotation process draws on crowdsourcing to segment surfaces from photos, and then annotate them with rich surface properties, including material, texture and contextual information.", "variants": ["OpenSurfaces"]}
{"id": "OpenViDial", "title": "OpenViDial: A Large-Scale, Open-Domain Dialogue Dataset with Visual Contexts", "contents": "**OpenViDial** is a large-scale open-domain dialogue dataset with visual contexts. The dialogue turns and visual contexts are extracted from movies and TV series, where each dialogue turn is paired with the corresponding visual context in which it takes place. OpenViDial contains a total number of 1.1 million dialogue turns, and thus 1.1 million visual contexts stored in images.\n\nSource: [https://github.com/ShannonAI/OpenViDial](https://github.com/ShannonAI/OpenViDial)\nImage Source: [https://github.com/ShannonAI/OpenViDial](https://github.com/ShannonAI/OpenViDial)", "variants": ["OpenViDial"]}
{"id": "Opinosis", "title": "", "contents": "This dataset contains sentences extracted from user reviews on a given topic. Example topics are “performance of Toyota Camry” and “sound quality of ipod nano”, etc. In total there are 51 such topics  with each topic having approximately 100 sentences (on average). The reviews were obtained from various sources – Tripadvisor (hotels), Edmunds.com (cars) and Amazon.com (various electronics).  This dataset was used for the following automatic text summarization project .\r\n\r\nSource: [Opinosis](http://kavita-ganesan.com/opinosis-opinion-dataset/)", "variants": ["Opinosis"]}
{"id": "OPUS-100", "title": "Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation", "contents": "A novel multilingual dataset with 100 languages.\r\n\r\nSource: [Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation](/paper/improving-massively-multilingual-neural)", "variants": ["OPUS-100"]}
{"id": "OrangeSum", "title": "BARThez: a Skilled Pretrained French Sequence-to-Sequence Model", "contents": "Source: [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](/paper/barthez-a-skilled-pretrained-french-sequence)\r\n\r\n**OrangeSum** is a single-document extreme summarization dataset with two tasks: title and abstract. Ground truth summaries are respectively 11.42 and 32.12 words in length on average, for the title and abstract tasks respectively, while document sizes are 315 and 350 words.\r\n\r\nThe motivation for OrangeSum was to put together a French equivalent of the XSum dataset.\r\n\r\nUnlike the historical CNN, DailyMail, and NY Times datasets, OrangeSum requires the models to display a high degree of abstractivity to perform well.\r\nOrangeSum was created by scraping articles and their titles and abstracts from the Orange Actu website.\r\n\r\nScraped pages cover almost a decade from Feb 2011 to Sep 2020, and belong to five main categories: France, world, politics, automotive, and society.\r\nThe society category is itself divided into 8 subcategories: health, environment, people, culture, media, high-tech, unsual (\"insolite\" in French), and miscellaneous.\r\n\r\nThe dataset is publicly available at: https://github.com/Tixierae/OrangeSum.", "variants": ["OrangeSum"]}
{"id": "ORCAS", "title": "ORCAS: 18 Million Clicked Query-Document Pairs for Analyzing Search", "contents": "ORCAS is a click-based dataset. It covers 1.4 million of the TREC DL documents, providing 18 million connections to 10 million distinct queries.", "variants": ["ORCAS"]}
{"id": "ORConvQA", "title": "Open-Retrieval Conversational Question Answering", "contents": "Enhances QuAC by adapting it to an open-retrieval setting. It is an aggregation of three existing datasets: (1) the QuAC dataset that offers information-seeking conversations, (2) the CANARD dataset that consists of context-independent rewrites of QuAC questions, and (3) the Wikipedia corpus that serves as the knowledge source of answering questions.\r\n\r\nSource: [ORConvQA](https://github.com/prdwb/orconvqa-release)", "variants": ["ORConvQA"]}
{"id": "ORKG-QA", "title": "Question Answering on Scholarly Knowledge Graphs", "contents": "A preliminary dataset of related tables and a corresponding set of natural language questions.\r\n\r\nSource: [Question Answering on Scholarly Knowledge Graphs](/paper/question-answering-on-scholarly-knowledge)", "variants": ["ORKG-QA"]}
{"id": "OTT-QA", "title": "Open Question Answering over Tables and Text", "contents": "The Open Table-and-Text Question Answering (**OTT-QA**) dataset contains open questions which require retrieving tables and text from the web to answer. This dataset is re-annotated from the previous HybridQA dataset. The dataset is collected by UCSB NLP group and issued under MIT license.\n\nSource: [https://github.com/wenhuchen/OTT-QA](https://github.com/wenhuchen/OTT-QA)\nImage Source: [https://github.com/wenhuchen/OTT-QA](https://github.com/wenhuchen/OTT-QA)", "variants": ["OTT-QA"]}
{"id": "Spherical-Navi", "title": "", "contents": "A novel 360◦ fisheye panoramas dataset, i.e., the Spherical-Navi image dataset is collected, with a unique labeling strategy enabling automatic generation of an arbitrary number of negative samples (wrong heading direction).\r\n\r\nSource: [Convolutional Neural Network-Based Robot Navigation Using Uncalibrated Spherical Images](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5492478/pdf/sensors-17-01341.pdf)", "variants": ["Spherical-Navi"]}
{"id": "PARADE", "title": "PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge", "contents": "PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge.\r\n\r\nSource: [PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge](/paper/parade-a-new-dataset-for-paraphrase)", "variants": ["PARADE"]}
{"id": "Parallel Meaning Bank", "title": "The Parallel Meaning Bank: A Framework for Semantically Annotating Multiple Languages", "contents": "The **Parallel Meaning Bank** (PMB), developed at the University of Groningen and building upon the Groningen Meaning Bank, comprises sentences and texts in raw and tokenised format, syntactic analysis, word senses, thematic roles, reference resolution, and formal meaning representations. The main objective of the PMB is to provide fine-grained meaning representations for words, sentences and texts. Sentences are, in isolation, often ambiguous. The aim is to provide the most likely interpretation for a sentence, with a minimal use of underspecification.\r\n\r\nThe PMB annotations include gold standard data, which is fully manually corrected, as well as silver (partially manually corrected) and bronze (with no manual corrections) data. The releases so far contain documents for English, German, Italian and Dutch, but for future releases it is planned to include Chinese and Japanese.\r\n\r\nSource: [The Parallel Meaning Bank: A Framework for Semantically Annotating Multiple Languages](/paper/the-parallel-meaning-bank-a-framework-for)", "variants": ["Parallel Meaning Bank"]}
{"id": "Bilingual Corpus of Arabic-English Parallel Tweets", "title": "Constructing a Bilingual Corpus of Parallel Tweets", "contents": "A bilingual corpus of English-Arabic parallel tweets and a list of Twitter accounts who post English-Arabic tweets regularly.\r\n\r\nSource: [Constructing a Bilingual Corpus of Parallel Tweets](/paper/constructing-a-bilingual-corpus-of-parallel)", "variants": ["Bilingual Corpus of Arabic-English Parallel Tweets"]}
{"id": "ParaPat", "title": "ParaPat: The Multi-Million Sentences Parallel Corpus of Patents Abstracts", "contents": "A parallel corpus from the open access Google Patents dataset in 74 language pairs, comprising more than 68 million sentences and 800 million tokens. Sentences were automatically aligned using the Hunalign algorithm for the largest 22 language pairs, while the others were abstract (i.e. paragraph) aligned.\r\n\r\nSource: [ParaPat: The Multi-Million Sentences Parallel Corpus of Patents Abstracts](/paper/parapat-the-multi-million-sentences-parallel)", "variants": ["ParaPat"]}
{"id": "ParCorFull", "title": "ParCorFull: a Parallel Corpus Annotated with Full Coreference", "contents": "ParCorFull is a parallel corpus annotated with full coreference chains that has been created to address an important problem that machine translation and other multilingual natural language processing (NLP) technologies face -- translation of coreference across languages. This corpus contains parallel texts for the language pair English-German, two major European languages. Despite being typologically very close, these languages still have systemic differences in the realisation of coreference, and thus pose problems for multilingual coreference resolution and machine translation. This parallel corpus covers the genres of planned speech (public lectures) and newswire. It is richly annotated for coreference in both languages, including annotation of both nominal coreference and reference to antecedents expressed as clauses, sentences and verb phrases.\r\n\r\nSource: [ParCorFull: a Parallel Corpus Annotated with Full Coreference](/paper/parcorfull-a-parallel-corpus-annotated-with)", "variants": ["ParCorFull"]}
{"id": "Paris Art Deco Facades", "title": "", "contents": "Anew dataset of facade images from Paris following the Art-deco style.\r\n\r\nSource: [Learning grammars for architecture-specific facade parsing](https://hal.inria.fr/hal-01069379/file/paper2.pdf)", "variants": ["Paris Art Deco Facades"]}
{"id": "PEC", "title": "Towards Persona-Based Empathetic Conversational Models", "contents": "A novel large-scale multi-domain dataset for persona-based empathetic conversations. \r\n\r\nSource: [Towards Persona-Based Empathetic Conversational Models](/paper/endowing-empathetic-conversational-models)", "variants": ["PEC"]}
{"id": "Perlex", "title": "PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction", "contents": "Persian dataset for relation extraction, which is an expert-translated version of the \"Semeval-2010-Task-8\" dataset. \r\n\r\nSource: [PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction](/paper/perlex-a-bilingual-persian-english-gold)", "variants": ["Perlex"]}
{"id": "PerSenT", "title": "Author's Sentiment Prediction", "contents": "PerSenT is a dataset of crowd-sourced annotations of the sentiment expressed by the authors towards the main entities in news articles. The dataset also includes paragraph-level sentiment annotations to provide more fine-grained supervision for the task. \r\n\r\nSource: [Author's Sentiment Prediction](/paper/author-s-sentiment-prediction)", "variants": ["PerSenT"]}
{"id": "PEYMA", "title": "PEYMA: A Tagged Corpus for Persian Named Entities", "contents": "Peyma is a Persian NER dataset to train and test NER systems. It is constructed by collecting documents from ten news websites.", "variants": ["PEYMA"]}
{"id": "PheMT", "title": "PheMT: A Phenomenon-wise Dataset for Machine Translation Robustness on User-Generated Contents", "contents": "**PheMT** is a phenomenon-wise dataset designed for evaluating the robustness of Japanese-English machine translation systems. The dataset is based on the MTNT dataset, with additional annotations of four linguistic phenomena common in UGC; Proper Noun, Abbreviated Noun, Colloquial Expression, and Variant\r\n\r\nSource: [https://github.com/cl-tohoku/PheMT](https://github.com/cl-tohoku/PheMT)\r\nImage Source: [Fujii et al](https://arxiv.org/pdf/2011.02121v1.pdf)", "variants": ["PheMT"]}
{"id": "PHINC", "title": "PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation", "contents": "PHINC is a parallel corpus of the 13,738 code-mixed English-Hindi sentences and their corresponding translation in English. The translations of sentences are done manually by the annotators. \r\n\r\nSource: [PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation](/paper/phinc-a-parallel-hinglish-social-media-code)", "variants": ["PHINC"]}
{"id": "Photoswitch", "title": "The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the Advancement of Synthetic Chemistry", "contents": "A benchmark for molecular machine learning where improvements in model performance can be immediately observed in the throughput of promising molecules synthesized in the lab. Photoswitches are a versatile class of molecule for medical and renewable energy applications where a molecule's efficacy is governed by its electronic transition wavelengths.\r\n\r\nSource: [The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the Advancement of Synthetic Chemistry](/paper/the-photoswitch-dataset-a-molecular-machine)", "variants": ["Photoswitch"]}
{"id": "PhraseCut", "title": "PhraseCut: Language-based Image Segmentation in the Wild", "contents": "**PhraseCut** is a dataset consisting of 77,262 images and 345,486 phrase-region pairs. The dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated.\r\n\r\nSource: [PhraseCut: Language-based Image Segmentation in the Wild](/paper/phrasecut-language-based-image-segmentation-1)\r\nImage Source: [https://people.cs.umass.edu/~chenyun/publication/phrasecut/](https://people.cs.umass.edu/~chenyun/publication/phrasecut/)", "variants": ["PhraseCut"]}
{"id": "pic2kcal", "title": "Multi-Task Learning for Calorie Prediction on a Novel Large-Scale Recipe Dataset Enriched with Nutritional Information", "contents": "The pic2kal benchmark for calorie prediction contains 308,000 images from over 70,000 recipes including photographs, ingredients and instructions, matched with nutritional information.\n\nSource: [https://arxiv.org/abs/2011.01082](https://arxiv.org/abs/2011.01082)\nImage Source: [https://github.com/phiresky/pic2kcal](https://github.com/phiresky/pic2kcal)", "variants": ["pic2kcal"]}
{"id": "PicTropes", "title": "Overview of PicTropes, a film trope dataset", "contents": "PicTropes is a dataset of films and the tropes that they use created from the database DBTropes.org.\r\n\r\nSource: [Overview of PicTropes, a film trope dataset](/paper/overview-of-pictropes-a-film-trope-dataset)", "variants": ["PicTropes"]}
{"id": "Pinterest Complete The Look", "title": "Bootstrapping Complete The Look at Pinterest", "contents": "The Pinterest Complete the Look dataset consists of over 1 million outfits and 4 million objects. It can be used to predict style compatibility between fashion items in order to recommend complementary items that complete an outfit.\n\nSource: [https://arxiv.org/abs/2006.10792](https://arxiv.org/abs/2006.10792)\nImage Source: [https://github.com/eileenforwhat/complete-the-look-dataset](https://github.com/eileenforwhat/complete-the-look-dataset)", "variants": ["Pinterest Complete The Look"]}
{"id": "Plaintext Jokes", "title": "", "contents": "There are about 208 000 jokes in this database scraped from three sources.\r\n\r\nSource: [Plaintext Jokes](https://github.com/taivop/joke-dataset)", "variants": ["Plaintext Jokes"]}
{"id": "Planar Manipulator Dataset", "title": "", "contents": "The dataset consists of 90 000 color videos that show a planar robot manipulator executing articulated manipulation tasks. More precisely, the manipulator grasps a circular object of random color and size and places it on top of a square object/platform of again random color and size. The initial conﬁgurations (location, size and color) of the objects were randomly sampled during generation. Different from other datasets such as the moving MNIST dataset, the samples comprise a goal-oriented task as described, making it more suitable for testing prediction capabilities of an ML model. For instance, one can use it as a toy dataset to investigate the capacity and output behavior of a deep neural network before testing it on real-world data.\n\nSource: [https://github.com/ferreirafabio/PlanarManipulatorDataset](https://github.com/ferreirafabio/PlanarManipulatorDataset)\nImage Source: [https://github.com/ferreirafabio/PlanarManipulatorDataset](https://github.com/ferreirafabio/PlanarManipulatorDataset)", "variants": ["Planar Manipulator Dataset"]}
{"id": "pn-summary", "title": "Leveraging ParsBERT and Pretrained mT5 for Persian Abstractive Text Summarization", "contents": "Pn-summary is a dataset for Persian abstractive text summarization.\n\nSource: [https://arxiv.org/abs/2012.11204](https://arxiv.org/abs/2012.11204)\nImage Source: [https://github.com/hooshvare/pn-summary](https://github.com/hooshvare/pn-summary)", "variants": ["pn-summary"]}
{"id": "PoC", "title": "Understanding Points of Correspondence between Sentences for Abstractive Summarization", "contents": "A dataset containing the documents, source and fusion sentences, and human annotations of points of correspondence between sentences. The dataset bridges the gap between coreference resolution and summarization.\r\n\r\nSource: [Understanding Points of Correspondence between Sentences for Abstractive Summarization](/paper/understanding-points-of-correspondence)", "variants": ["PoC"]}
{"id": "PoKi", "title": "PoKi: A Large Dataset of Poems by Children", "contents": "**PoKi** is a corpus of 61,330 poems written by children from grades 1 to 12. PoKi is especially useful in studying child language because it comes with information about the age of the child authors (their grade).\n\nSource: [https://github.com/whipson/PoKi-Poems-by-Kids](https://github.com/whipson/PoKi-Poems-by-Kids)", "variants": ["PoKi"]}
{"id": "PolEmo 2.0", "title": "", "contents": "PolEmo 2.0: Corpus of Multi-Domain Consumer Reviews, evaluation data for article presented at CoNLL.\r\n\r\nSource: [PolEmo 2.0 Sentiment Analysis Dataset for CoNLL](https://clarin-pl.eu/dspace/handle/11321/710)", "variants": ["PolEmo 2.0"]}
{"id": "PolicyQA", "title": "PolicyQA: A Reading Comprehension Dataset for Privacy Policies", "contents": "A dataset that contains 25,017 reading comprehension style examples curated from an existing corpus of 115 website privacy policies. PolicyQA provides 714 human-annotated questions written for a wide range of privacy practices.\r\n\r\nSource: [PolicyQA: A Reading Comprehension Dataset for Privacy Policies](/paper/policyqa-a-reading-comprehension-dataset-for)", "variants": ["PolicyQA"]}
{"id": "Polish Political Advertising Dataset", "title": "Political Advertising Dataset: the use case of the Polish 2020 Presidential Elections", "contents": "A dataset for detecting specific text chunks and categories of political advertising in the Polish language. It contains 1,705 human-annotated tweets tagged with nine categories, which constitute campaigning under Polish electoral law.\r\n\r\nSource: [Political Advertising Dataset: the use case of the Polish 2020 Presidential Elections](/paper/political-advertising-dataset-the-use-case-of)", "variants": ["Polish Political Advertising Dataset"]}
{"id": "PolitiFact", "title": "", "contents": "Fact-checking (FC) articles which contains pairs (multimodal tweet and a FC-article) from politifact.com.\r\n\r\nSource: [Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News](/paper/where-are-the-facts-searching-for-fact)", "variants": ["PolitiFact"]}
{"id": "POLUSA", "title": "The POLUSA Dataset: 0.9M Political News Articles Balanced by Time and Outlet Popularity", "contents": "A dataset that represents the online media landscape as perceived by an average US news consumer. The dataset contains 0.9M articles covering policy topics published between Jan. 2017 and Aug. 2019 by 18 news outlets representing the political spectrum. Each outlet is labeled by its political leaning derived using a systematic aggregation of eight data sources. The news dataset is balanced with respect to publication date and outlet popularity. POLUSA enables studying a variety of subjects, e.g., media effects and political partisanship.\r\n\r\nSource: [The POLUSA Dataset: 0.9M Political News Articles Balanced by Time and Outlet Popularity](/paper/the-polusa-dataset-0-9m-political-news)", "variants": ["POLUSA"]}
{"id": "Pow-Wow", "title": "Pow-Wow: A Dataset and Study on Collaborative Communication in Pommerman", "contents": "A dataset for studying situated goal-directed human communication.\r\n\r\nSource: [Pow-Wow: A Dataset and Study on Collaborative Communication in Pommerman](/paper/pow-wow-a-dataset-and-study-on-collaborative)", "variants": ["Pow-Wow"]}
{"id": "prachathai-67k", "title": "", "contents": "The prachathai-67k dataset was scraped from the news site Prachathai excluding articles with less than 500 characters of body text (mostly images and cartoons). It contains 67,889 articles with 51,797 tags from August 24, 2004 to November 15, 2018.\r\n\r\nSource: [prachathai-67k](https://github.com/PyThaiNLP/prachathai-67k/)", "variants": ["prachathai-67k"]}
{"id": "PRECOG", "title": "", "contents": "The **PREdiction of Clinical Outcomes from Genomic profiles** (or PRECOG) encompasses 166 cancer expression data sets, including overall survival data for ~18,000 patients diagnosed with 39 distinct malignancies.", "variants": ["PRECOG"]}
{"id": "Procon20", "title": "Stance Prediction for Contemporary Issues: Data and Experiments", "contents": "A novel stance detection dataset covering 419 different controversial issues and their related pros and cons collected by procon.org in nonpartisan format. \r\n\r\nSource: [Stance Prediction for Contemporary Issues: Data and Experiments](/paper/stance-prediction-for-contemporary-issues)", "variants": ["Procon20"]}
{"id": "Products-10K", "title": "Products-10K: A Large-scale Product Recognition Dataset", "contents": "Contains 10,000 fine-grained SKU-level products frequently bought by online customers in JD.com.\r\n\r\nSource: [Products-10K: A Large-scale Product Recognition Dataset](/paper/products-10k-a-large-scale-product)", "variants": ["Products-10K"]}
{"id": "Prostate MRI Segmentation Dataset", "title": "Shape-aware Meta-learning for Generalizing Prostate MRI Segmentation to Unseen Domains", "contents": "This prostate MRI segmentation dataset is collected from six different data sources.\n\nSource: [https://github.com/liuquande/SAML](https://github.com/liuquande/SAML)", "variants": ["Prostate MRI Segmentation Dataset"]}
{"id": "ProtoQA", "title": "ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning", "contents": "**ProtoQA** is a question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international game show FAMILY- FEUD. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers.\n\nSource: [https://github.com/iesl/protoqa-data](https://github.com/iesl/protoqa-data)", "variants": ["ProtoQA"]}
{"id": "PROX", "title": "", "contents": "A dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. \r\n\r\nSource: [PROX](https://prox.is.tue.mpg.de)", "variants": ["PROX"]}
{"id": "PubFig", "title": "", "contents": "The PubFig database is a large, real-world face dataset consisting of 58,797 images of 200 people collected from the internet. Unlike most other existing face datasets, these images are taken in completely uncontrolled situations with non-cooperative subjects. Thus, there is large variation in pose, lighting, expression, scene, camera, imaging conditions and parameters, etc. The PubFig dataset is similar in spirit to the Labeled Faces in the Wild (LFW) dataset.\r\n\r\nSource: [PubFig: Public Figures Face Database](https://www1.cs.columbia.edu/CAVE/databases/pubfig/)", "variants": ["PubFig"]}
{"id": "PUBHEALTH", "title": "Explainable Automated Fact-Checking for Public Health Claims", "contents": "**PUBHEALTH** is a comprehensive dataset for explainable automated fact-checking of public health claims. Each instance in the PUBHEALTH dataset has an associated veracity label (true, false, unproven, mixture). Furthermore each instance in the dataset has an explanation text field. The explanation is a justification for which the claim has been assigned a particular veracity label.\n\nSource: [https://github.com/neemakot/Health-Fact-Checking](https://github.com/neemakot/Health-Fact-Checking)", "variants": ["PUBHEALTH"]}
{"id": "public_meetings", "title": "Align then Summarize: Automatic Alignment Methods for Summarization Corpus Creation", "contents": "The **public_meetings** corpus contains meetings, made of pairs of automatic transcriptions from audio recordings and meeting reports written by a professional. 22 aligned meetings are provided in total.\r\n\r\nSource: [public_meetings](https://github.com/pltrdy/public_meetings)", "variants": ["public_meetings"]}
{"id": "Pump and dump dataset", "title": "Pump and Dumps in the Bitcoin Era: Real Time Detection of Cryptocurrency Market Manipulations", "contents": "The **Pump and dump dataset** is an annotated set of messages to detect cryptocurrency market manipulations. It consists of a list of a list of pump and dumps arranged by groups on Telegram. All the pump and dumps in the dataset are on the trading pair SYM/BTC.\n\nSource: [https://github.com/SystemsLab-Sapienza/pump-and-dump-dataset](https://github.com/SystemsLab-Sapienza/pump-and-dump-dataset)", "variants": ["Pump and dump dataset"]}
{"id": "QBSUM", "title": "QBSUM: a Large-Scale Query-Based Document Summarization Dataset from Real-world Applications", "contents": "A high-quality large-scale dataset consisting of 49,000+ data samples for the task of Chinese query-based document summarization. \r\n\r\nSource: [QBSUM: a Large-Scale Query-Based Document Summarization Dataset from Real-world Applications](/paper/qbsum-a-large-scale-query-based-document)", "variants": ["QBSUM"]}
{"id": "QuAIL", "title": "", "contents": "A new kind of question-answering dataset that combines commonsense, text-based, and unanswerable questions, balanced for different genres and reasoning types. Reasoning type annotation for 9 types of reasoning: temporal, causality, factoid, coreference, character properties, their belief states, subsequent entity states, event durations, and unanswerable. Genres: CC license fiction, Voice of America news, blogs, user stories from Quora 800 texts, 18 questions for each (~14K questions).\r\n\r\nSource: [QuAIL - Question Answering for Artificial Intelligence](http://text-machine.cs.uml.edu/lab2/projects/quail/)", "variants": ["QuAIL"]}
{"id": "Quda", "title": "Quda: Natural Language Queries for Visual Data Analytics", "contents": "Aims to help V-NLIs recognize analytic tasks from free-form natural language by training and evaluating cutting-edge multi-label classification models. The dataset contains  diverse user queries, and each is annotated with one or multiple analytic tasks. \r\n\r\nSource: [Quda: Natural Language Queries for Visual Data Analytics](/paper/quda-natural-language-queries-for-visual-data)", "variants": ["Quda"]}
{"id": "QuerYD", "title": "QuerYD: A video dataset with high-quality text and audio narrations", "contents": "A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content.\r\n\r\nSource: [QuerYD: A video dataset with high-quality textual and audio narrations](/paper/queryd-a-video-dataset-with-high-quality)", "variants": ["QuerYD"]}
{"id": "Quizbowl", "title": "Quizbowl: The Case for Incremental Question Answering", "contents": "Consists of multiple sentences whose clues are arranged by difficulty (from obscure to obvious) and uniquely identify a well-known entity such as those found on Wikipedia.\r\n\r\nSource: [Quizbowl: The Case for Incremental Question Answering](/paper/quizbowl-the-case-for-incremental-question)", "variants": ["Quizbowl"]}
{"id": "RADIATE", "title": "RADIATE: A Radar Dataset for Automotive Perception", "contents": "**RADIATE** (**RAdar Dataset In Adverse weaThEr**) is new automotive dataset created by Heriot-Watt University which includes Radar, Lidar, Stereo Camera and GPS/IMU.\nThe data is collected in different weather scenarios (sunny, overcast, night, fog, rain and snow) to help the research community to develop new methods of vehicle perception.\nThe radar images are annotated in 7 different scenarios: Sunny (Parked), Sunny/Overcast (Urban), Overcast (Motorway), Night (Motorway), Rain (Suburban), Fog (Suburban) and Snow (Suburban). The dataset contains 8 different types of objects (car, van, truck, bus, motorbike, bicycle, pedestrian and group of pedestrians).\n\nSource: [https://github.com/marcelsheeny/radiate_sdk](https://github.com/marcelsheeny/radiate_sdk)\nImage Source: [https://github.com/marcelsheeny/radiate_sdk](https://github.com/marcelsheeny/radiate_sdk)", "variants": ["RADIATE"]}
{"id": "RAF-ML", "title": "", "contents": "Real-world Affective Faces Multi Label (RAF-ML) is a multi-label facial expression dataset with around 5K great-diverse facial images downloaded from the Internet with blended emotions and variability in subjects' identity, head poses, lighting conditions and occlusions. During annotation, 315 well-trained annotators are employed to ensure each image can be annotated enough independent times. And images with multi-peak label distribution are selected out to constitute the RAF-ML.\r\n\r\nRAF-ML provides 4908 number of real-world images with blended emotions, 6-dimensional expression distribution vector for each image, 5 accurate landmark locations and 37 automatic landmark locations, and baseline classifier outputs for multi-label emotion recognition.\r\n\r\nSource: [Real-world Affective Faces Multi Label](http://whdeng.cn/RAF/model2.html)", "variants": ["RAF-ML"]}
{"id": "RainNet", "title": "RainNet: A Large-Scale Dataset for Spatial Precipitation Downscaling", "contents": "**RainNet** is a real (non-simuated) large-scale spatial precipitation downscaling dataset that contains 62,424 pairs of low-resolution and high-resolution precipitation maps for 17 years. Contrary to simulated data, this real dataset covers various types of real meteorological phenomena (e.g., Hurricane, Squall, etc.), and shows the physical characters - Temporal Misalignment, Temporal Sparse and Fluid Properties - that challenge the downscaling algorithms.\n\nSource: [https://github.com/neuralchen/RainNet](https://github.com/neuralchen/RainNet)\nImage Source: [https://github.com/neuralchen/RainNet](https://github.com/neuralchen/RainNet)", "variants": ["RainNet"]}
{"id": "RareAct", "title": "RareAct: A video dataset of unusual interactions", "contents": "**RareAct** is a video dataset of unusual actions, including actions like “blend phone”, “cut keyboard” and “microwave shoes”. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.\n\nSource: [https://github.com/antoine77340/RareAct](https://github.com/antoine77340/RareAct)\nImage Source: [https://github.com/antoine77340/RareAct](https://github.com/antoine77340/RareAct)", "variants": ["RareAct"]}
{"id": "RarePlanes Dataset", "title": "RarePlanes: Synthetic Data Takes Flight", "contents": "The dataset specifically focuses on the value of synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very-high resolution dataset built to test the value of synthetic data from an overhead perspective. Previous research has shown that synthetic data can reduce the amount of real training data needed and potentially improve performance for many tasks in the computer vision domain. The real portion of the dataset consists of 253 Maxar WorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700 hand-annotated aircraft. \r\n\r\nSource: [RarePlanes: Synthetic Data Takes Flight](/paper/rareplanes-synthetic-data-takes-flight)", "variants": ["RarePlanes Dataset"]}
{"id": "RAVEN-FAIR", "title": "Scale-Localized Abstract Reasoning", "contents": "**RAVEN-FAIR** is a modified version of the RAVEN dataset.\n\nSource: [https://github.com/yanivbenny/RAVEN_FAIR](https://github.com/yanivbenny/RAVEN_FAIR)", "variants": ["RAVEN-FAIR"]}
{"id": "ReCO", "title": "ReCO: A Large Scale Chinese Reading Comprehension Dataset on Opinion", "contents": "A human-curated ChineseReading Comprehension dataset on Opinion. The questions in ReCO are opinion based queries issued to the commercial search engine. The passages are provided by the crowdworkers who extract the support snippet from the retrieved documents. \r\n\r\nSource: [ReCO: A Large Scale Chinese Reading Comprehension Dataset on Opinion](/paper/reco-a-large-scale-chinese-reading)", "variants": ["ReCO"]}
{"id": "RED", "title": "Transferable Active Grasping and Real Embodied Dataset", "contents": "The **Real Embodied Dataset** (**RED**) is a computer vision large-scale dataset for grasping in cluttered scenes. It contains complete segmentation masks for partially occluded objects, with their order of occlusion.\n\nSource: [https://arxiv.org/pdf/2004.13358.pdf](https://arxiv.org/pdf/2004.13358.pdf)\nImage Source: [https://arxiv.org/pdf/2004.13358.pdf](https://arxiv.org/pdf/2004.13358.pdf)", "variants": ["Red Wine", "RED"]}
{"id": "ReDWeb-S", "title": "Learning Selective Mutual Attention and Contrast for RGB-D Saliency Detection", "contents": "**ReDWeb-S** is a large-scale challenging dataset for Salient Object Detection. It has totally 3179 images with various real-world scenes and high-quality depth maps. The dataset is split into a training set with 2179 RGB-D image pairs and a testing set with the remaining 1000 image pairs.\n\nSource: [https://github.com/nnizhang/SMAC](https://github.com/nnizhang/SMAC)\nImage Source: [https://github.com/nnizhang/SMAC](https://github.com/nnizhang/SMAC)", "variants": ["ReDWeb-S"]}
{"id": "redwood-3dscan", "title": "", "contents": "A dataset of more than ten thousand 3D scans of real objects. \r\n\r\nSource: [A Large Dataset of Object Scans](/paper/a-large-dataset-of-object-scans)", "variants": ["redwood-3dscan"]}
{"id": "REFreSD", "title": "Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank", "contents": "Consists of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.\r\n\r\nSource: [Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank](/paper/detecting-fine-grained-cross-lingual-semantic)", "variants": ["REFreSD"]}
{"id": "ReINTEL", "title": "ReINTEL: A Multimodal Data Challenge for Responsible Information Identification on Social Network Sites", "contents": "10,000 news collected from a social network in Vietnam.\r\n\r\nSource: [ReINTEL: A Multimodal Data Challenge for Responsible Information Identification on Social Network Sites](/paper/reintel-a-multimodal-data-challenge-for)", "variants": ["ReINTEL"]}
{"id": "RELLIS-3D", "title": "RELLIS-3D Dataset: Data, Benchmarks and Analysis", "contents": "**RELLIS-3D** is a multi-modal dataset for off-road robotics. It was collected in an off-road environment containing annotations for 13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis Campus of Texas A&M University and presents challenges to existing algorithms related to class imbalance and environmental topography. The dataset also provides full-stack sensor data in ROS bag format, including RGB camera images, LiDAR point clouds, a pair of stereo images, high-precision GPS measurement, and IMU data.\n\nSource: [https://github.com/unmannedlab/RELLIS-3D](https://github.com/unmannedlab/RELLIS-3D)\nImage Source: [https://github.com/unmannedlab/RELLIS-3D](https://github.com/unmannedlab/RELLIS-3D)", "variants": ["RELLIS-3D Dataset", "RELLIS-3D"]}
{"id": "RELX", "title": "The RELX Dataset and Matching the Multilingual Blanks for Cross-Lingual Relation Classification", "contents": "**RELX** is a benchmark dataset for cross-lingual relation classification in English, French, German, Spanish and Turkish.\n\nSource: [https://github.com/boun-tabi/RELX](https://github.com/boun-tabi/RELX)", "variants": ["RELX"]}
{"id": "Rendered WB dataset", "title": "When Color Constancy Goes Wrong: Correcting Improperly White-Balanced Images", "contents": "A dataset of over 65,000 pairs of incorrectly white-balanced images and their corresponding correctly white-balanced images.\r\n\r\nSource: [When Color Constancy Goes Wrong: Correcting Improperly White-Balanced Images](/paper/when-color-constancy-goes-wrong-correcting)", "variants": ["Rendered WB dataset"]}
{"id": "RESIDE", "title": "", "contents": "A new large-scale benchmark consisting of both synthetic and real-world hazy images, called REalistic Single Image DEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, and is divided into five subsets, each serving different training or evaluation purposes. \r\n\r\nSource: [RESIDE](https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=3D0)", "variants": ["RESIDE"]}
{"id": "Retail50K", "title": "PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments", "contents": "A dataset to encourage the community to adapt oriented bounding box (OBB) detectors for more complex environments.\r\n\r\nSource: [PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments](/paper/piou-loss-towards-accurate-oriented-object)", "variants": ["Retail50K"]}
{"id": "RF signal", "title": "", "contents": "This dataset is used for **RF signal** recognition, used to recognize different RF devices based on the signals they transmitted.\n\nSource: [https://arxiv.org/pdf/1908.09931.pdf](https://arxiv.org/pdf/1908.09931.pdf)", "variants": ["RF signal"]}
{"id": "DIML/CVl RGB-D Dataset", "title": "", "contents": "This dataset contains synchronized RGB-D frames from both Kinect v2 and Zed stereo camera. For the outdoor scene, the authors first generate disparity maps using an accurate stereo matching method and convert them using calibration parameters. A per-pixel confidence map of disparity is also provided. The scenes are captured at various places, e.g., offices, rooms, dormitory, exhibition center, street, road etc., from Yonsei University and Ewha University.\r\n\r\nSource: [DIML/CVl RGB-D Dataset](https://dimlrgbd.github.io/)", "variants": ["DIML/CVl RGB-D Dataset"]}
{"id": "RGB-DAVIS Dataset", "title": "Joint Filtering of Intensity Images and Neuromorphic Events for High-Resolution Noise-Robust Imaging", "contents": "Used to show systematic performance improvement in applications such as high frame-rate video synthesis, feature/corner detection and tracking, as well as high dynamic range image reconstruction.\r\n\r\nSource: [Joint Filtering of Intensity Images and Neuromorphic Events for High-Resolution Noise-Robust Imaging](/paper/joint-filtering-of-intensity-images-and)", "variants": ["RGB-DAVIS Dataset"]}
{"id": "RGB-D Object dataset", "title": "", "contents": "The dataset contains 300 objects organized into 51 categories and has been made publicly available to the research community so as to enable rapid progress based on this promising technology.\r\n\r\nSource: [RGB-D Object dataset](http://rgbd-dataset.cs.washington.edu/dataset.html)", "variants": ["RGB-D Object dataset"]}
{"id": "Rijksmuseum Challenge 2014", "title": "", "contents": "Dataset used for the challenge to apply computer vision techniques on art objects (paintings, sculptures, drawings etc) from the Rijksmuseum (in Amsterdam, the Netherlands).\r\n\r\nSource: [Rijksmuseum Challenge 2014](https://figshare.com/articles/Rijksmuseum_Challenge_2014/5660617)", "variants": ["Rijksmuseum Challenge 2014"]}
{"id": "RiSAWOZ", "title": "RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling", "contents": "**RiSAWOZ** is a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labelled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, it also includes linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks.\n\nSource: [https://github.com/terryqj0107/RiSAWOZ](https://github.com/terryqj0107/RiSAWOZ)", "variants": ["RiSAWOZ"]}
{"id": "RISE", "title": "Project RISE: Recognizing Industrial Smoke Emissions", "contents": "**RISE** is a large-scale video dataset for Recognizing Industrial Smoke Emissions. A citizen science approach was adopted to collaborate with local community members to annotate whether a video clip has smoke emissions. The dataset contains 12,567 clips from 19 distinct views from cameras that monitored three industrial facilities. These daytime clips span 30 days over two years, including all four seasons.\n\nSource: [https://arxiv.org/abs/2005.06111](https://arxiv.org/abs/2005.06111)\nImage Source: [https://github.com/CMU-CREATE-Lab/deep-smoke-machine](https://github.com/CMU-CREATE-Lab/deep-smoke-machine)", "variants": ["RISE"]}
{"id": "Road Scene Graph", "title": "Road Scene Graph: A Semantic Graph-Based Scene Representation Dataset for Intelligent Vehicles", "contents": "A special scene-graph for intelligent vehicles. Different to classical data representation, this graph provides not only object proposals but also their pair-wise relationships. By organizing them in a topological graph, these data are explainable, fully-connected, and could be easily processed by GCNs (Graph Convolutional Networks).\r\n\r\nSource: [Road Scene Graph: A Semantic Graph-Based Scene Representation Dataset for Intelligent Vehicles](/paper/road-scene-graph-a-semantic-graph-based-scene)", "variants": ["Road Scene Graph"]}
{"id": "RoadText-1K", "title": "RoadText-1K: Text Detection & Recognition Dataset for Driving Videos", "contents": "A dataset for text in driving videos. The dataset is 20 times larger than the existing largest dataset for text in videos. The dataset comprises 1000 video clips of driving without any bias towards text and with annotations for text bounding boxes and transcriptions in every frame. \r\n\r\nSource: [RoadText-1K: Text Detection & Recognition Dataset for Driving Videos](/paper/roadtext-1k-text-detection-recognition)", "variants": ["RoadText-1K"]}
{"id": "Robot@Home dataset", "title": "", "contents": "The Robot-at-Home dataset (Robot@Home) is a collection of raw and processed data from five domestic settings compiled by a mobile robot equipped with 4 RGB-D cameras and a 2D laser scanner. Its main purpose is to serve as a testbed for semantic mapping algorithms through the categorization of objects and/or rooms.\r\n\r\nSource: [Robot@Home dataset](http://mapir.isa.uma.es/mapirwebsite/index.php/mapir-downloads/203-robot-at-home-dataset.html)", "variants": ["Robot@Home dataset"]}
{"id": "Robotic Instruments", "title": "", "contents": "Provides 8x 225-frame robotic surgical videos, captured at 2 Hz, where a trained team at Intuitive Surgical has manually labelled the different parts and types. The users are invited to test their algorithms on 8x 75-frame videos and 2x 300-frame videos which act as a test set.\r\n\r\nSource: [Robotic Instruments](https://endovissub2017-roboticinstrumentsegmentation.grand-challenge.org)", "variants": ["Robotic Instruments"]}
{"id": "RobustPointSet", "title": "RobustPointSet: A Dataset for Benchmarking Robustness of Point Cloud Classifiers", "contents": "A dataset for robustness analysis of point cloud classification models (independent of data augmentation) to input transformations.\r\n\r\nSource: [RobustPointSet: A Dataset for Benchmarking Robustness of Point Cloud Classifiers](/paper/robustpointset-a-dataset-for-benchmarking)", "variants": ["RobustPointSet"]}
{"id": "Roman Urdu Data Set", "title": "", "contents": "Tagged for Sentiment (Positive, Negative, Neutral).\r\n\r\nSource: [Roman Urdu Data Set](https://archive.ics.uci.edu/ml/datasets/Roman+Urdu+Data+Set)", "variants": ["Roman Urdu Data Set"]}
{"id": "RP2K", "title": "RP2K: A Large-Scale Retail Product Dataset for Fine-Grained Image Classification", "contents": "A new large-scale retail product dataset for fine-grained image classification. Unlike previous datasets focusing on relatively few products, more than 500,000 images of retail products on shelves were collected, belonging to 2000 different products. The dataset aims to advance the research in retail object recognition, which has massive applications such as automatic shelf auditing and image-based product information retrieval.\r\n\r\nSource: [RP2K: A Large-Scale Retail Product Dataset for Fine-Grained Image Classification](/paper/rp2k-a-large-scale-retail-product-dataset)", "variants": ["RP2K"]}
{"id": "RTN", "title": "Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types", "contents": "A corpus of real-world spoken personal narratives comprising 10,296 narrative clauses from 594 video transcripts. \r\n\r\nSource: [Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types](/paper/exploring-aspects-of-similarity-between)", "variants": ["RTN"]}
{"id": "RuBQ", "title": "RuBQ: A Russian Dataset for Question Answering over Wikidata", "contents": "The first Russian knowledge base question answering (KBQA) dataset. The high-quality dataset consists of 1,500 Russian questions of varying complexity, their English machine translations, SPARQL queries to Wikidata, reference answers, as well as a Wikidata sample of triples containing entities with Russian labels. The dataset creation started with a large collection of question-answer pairs from online quizzes. The data underwent automatic filtering, crowd-assisted entity linking, automatic generation of SPARQL queries, and their subsequent in-house verification.\r\n\r\nSource: [RuBQ: A Russian Dataset for Question Answering over Wikidata](/paper/rubq-a-russian-dataset-for-question-answering)", "variants": ["RuBQ"]}
{"id": "Runway", "title": "", "contents": "A runway dataset, designing features suitable for capturing outfit appearance, collecting human judgments of outfit similarity, and learning similarity functions on the features to mimic those judgments.\r\n\r\nSource: [Runway](http://tamaraberg.com/runway2realway/)", "variants": ["Runway"]}
{"id": "Ruralscapes", "title": "Semantics through Time: Semi-supervised Segmentation of Aerial Videos with Iterative Label Propagation", "contents": "A dataset with high resolution (4K) images and manually-annotated dense labels every 50 frames.\r\n\r\nSource: [Semantics through Time: Semi-supervised Segmentation of Aerial Videos with Iterative Label Propagation](/paper/semantics-through-time-semi-supervised)", "variants": ["Ruralscapes"]}
{"id": "RxR", "title": "Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding", "contents": "Room-Across-Room (RxR) is a multilingual dataset for Vision-and-Language Navigation (VLN) for Matterport3D environments. In contrast to related datasets such as Room-to-Room (R2R), RxR is 10x larger, multilingual (English, Hindi and Telugu), with longer and more variable paths, and it includes and fine-grained visual groundings that relate each word to pixels/surfaces in the environment.\r\n\r\nSource: [Room-Across-Room (RxR) Dataset](https://github.com/google-research-datasets/RxR)", "variants": ["RxR"]}
{"id": "S2TLD", "title": "SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing", "contents": "**S2TLD** is a traffic light dataset, which contains 5,786 images of approximately 1,080 * 1,920 pixels and 720 * 1,280 pixels. It also contains 5 categories (include red, yellow, green, off and wait on) of 1,4130 instances. The scenes cover a decent variety of road scenes and typical:\n* Busy street scenes inner-city,\n* Dense stop-and-go traffic\n* Strong changes in illumination/exposure\n* Flickering/Fluctuating traffic lights\n* Multiple visible traffic lights\n* Image parts that can be confused with traffic lights (e.g. large round tail lights)\n\nSource: [https://github.com/Thinklab-SJTU/S2TLD](https://github.com/Thinklab-SJTU/S2TLD)\nImage Source: [https://github.com/Thinklab-SJTU/S2TLD](https://github.com/Thinklab-SJTU/S2TLD)", "variants": ["S2TLD"]}
{"id": "S3O4D", "title": "Disentangling by Subspace Diffusion", "contents": "The data consists of 100,000 renderings each of the Bunny and Dragon objects from the Stanford 3D Scanning Repository. More objects may be added in the future, but only the Bunny and Dragon are used in the paper. Each object is rendered with a uniformly sampled illumination from a point on the 2-sphere, and a uniformly sampled 3D rotation. The true latent states are provided as NumPy arrays along with the images. The lighting is given as a 3-vector with unit norm, while the rotation is provided both as a quaternion and a 3x3 orthogonal matrix.\r\n\r\nSource: [S3O4D](https://github.com/deepmind/deepmind-research/tree/master/geomancer#stanford-3d-objects-for-disentangling-s3o4d)", "variants": ["S3O4D"]}
{"id": "Salient-KITTI", "title": "Salient Bundle Adjustment for Visual SLAM", "contents": "**Salient-KITTI** is a saliency map prediction dataset based on KITTI.\n\nSource: [https://arxiv.org/pdf/2012.11863.pdf](https://arxiv.org/pdf/2012.11863.pdf)\nImage Source: [https://github.com/Saixiaoma/SBA-SLAM](https://github.com/Saixiaoma/SBA-SLAM)", "variants": ["Salient-KITTI"]}
{"id": "San Francisco Landmark Dataset", "title": "", "contents": "The San Francisco Landmark Dataset contains a database of 1.7 million images of buildings in San Francisco with ground truth labels, geotags, and calibration data, as well as a difficult query set of 803 cell phone images taken with a variety of different camera phones. The data is originally acquired by vehicle-mounted cameras with wide-angle lenses capturing spherical panoramic images. For all visible buildings in each panorama, a set of overlapping perspective images is generated.\r\n\r\nPaper: [D. Chen, G. Baatz, K. Koeser, S. Tsai, R. Vedantham, T. Pylvanainen, K. Roimela, X. Chen, J. Bach, M. Pollefeys, B. Girod, and R. Grzeszczuk, \"City-scale landmark identification on mobile devices\", IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June 2011.](https://ieeexplore.ieee.org/document/5995610)", "variants": ["San Francisco Landmark Dataset"]}
{"id": "SARA", "title": "A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering", "contents": "A dataset for statutory reasoning in tax law entailment and question answering.\r\n\r\nSource: [A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering](/paper/a-dataset-for-statutory-reasoning-in-tax-law)", "variants": ["SARA"]}
{"id": "SBWCE", "title": "", "contents": "This resource consists of an unannotated corpus of the Spanish language of nearly 1.5 billion words, compiled from different corpora and resources from the web; and a set of word vectors (or embeddings), created from this corpus using the word2vec algorithm, provided by the gensim package. These embeddings were evaluated by translating to Spanish word2vec’s word relation test set.\r\n\r\nSource: [SBWCE](https://crscardellino.github.io/SBWCE/)", "variants": ["SBWCE"]}
{"id": "scb-mt-en-th-2020", "title": "scb-mt-en-th-2020: A Large English-Thai Parallel Corpus", "contents": "scb-mt-en-th-2020 is an English-Thai machine translation dataset with over 1 million segment pairs, curated from various sources, namely news, Wikipedia articles, SMS messages, task-based dialogs, web-crawled data and government documents.\r\n\r\nSource: [scb-mt-en-th-2020: A Large English-Thai Parallel Corpus](/paper/scb-mt-en-th-2020-a-large-english-thai)\r\nImage Source: [Lowphansirikul et al](https://arxiv.org/pdf/2007.03541v1.pdf)", "variants": ["scb-mt-en-th-2020"]}
{"id": "SCDB", "title": "Explaining AI-based Decision Support Systems using Concept Localization Maps", "contents": "Includes annotations for 10 distinguishable concepts.\r\n\r\nSource: [Explaining AI-based Decision Support Systems using Concept Localization Maps](/paper/explaining-ai-based-decision-support-systems)", "variants": ["SCDB"]}
{"id": "ScienceIE", "title": "", "contents": "The shared task ScienceIE at SemEval 2017 deals with automatic extraction of keyphrases from Computer Science, Material Sciences and Physics publications, as well as extracting types of keyphrases and relations between keyphrases.\r\nPROCESS, TASK and MATERIAL form the fundamental objects in scientific works. Scientific research and practice is founded upon gaining, maintaining and understanding the body of existing scientific work in specific areas related to such fundamental objects. \r\n\r\nSource: [ScienceIE](https://scienceie.github.io/)", "variants": ["ScienceIE"]}
{"id": "SciTLDR", "title": "TLDR: Extreme Summarization of Scientific Documents", "contents": "A new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. \r\n\r\nSource: [TLDR: Extreme Summarization of Scientific Documents](/paper/tldr-extreme-summarization-of-scientific)", "variants": ["SciTLDR"]}
{"id": "Scruples", "title": "Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes", "contents": "Dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members.\r\n\r\nSource: [Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes](/paper/scruples-a-corpus-of-community-ethical)", "variants": ["Scruples"]}
{"id": "Search4Code", "title": "Code Search Intent Classification Using Weak Supervision", "contents": "**Search4Code** is a large-scale web query based dataset of code search queries for C# and Java. The Search4Code data is mined from Microsoft Bing's anonymized search query logs using weak supervision technique.\n\nSource: [https://github.com/microsoft/Search4Code](https://github.com/microsoft/Search4Code)", "variants": ["Search4Code"]}
{"id": "SeasonDepth", "title": "SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments", "contents": "Aa new cross-season scaleless monocular depth prediction dataset from CMU Visual Localization dataset through structure from motion.\r\n\r\nSource: [SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments](/paper/seasondepth-cross-season-monocular-depth)", "variants": ["SeasonDepth"]}
{"id": "SensatUrban", "title": "Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges", "contents": "The SensatUrbat dataset is an urban-scale photogrammetric point cloud dataset with nearly three billion richly annotated points, which is five times the number of labeled points than the existing largest point cloud dataset. The dataset consists of large areas from two UK cities, covering about 6 km^2 of the city landscape. In the dataset, each 3D point is labeled as one of 13 semantic classes, such as ground, vegetation, car, etc..\r\n\r\nSource: [https://github.com/QingyongHu/SensatUrban](https://github.com/QingyongHu/SensatUrban)\nImage Source: [https://github.com/QingyongHu/SensatUrban](https://github.com/QingyongHu/SensatUrban)", "variants": ["SensatUrban"]}
{"id": "Sentiment140", "title": "", "contents": "Sentiment140 is a dataset that allows you to discover the sentiment of a brand, product, or topic on Twitter.\r\n\r\nSource: [Sentiment140](http://help.sentiment140.com/home)", "variants": ["Sentiment140"]}
{"id": "Sentimental LIAR", "title": "Sentimental LIAR: Extended Corpus and Deep Learning Models for Fake Claim Classification", "contents": "The **Sentimental LIAR** dataset is a modified and further extended version of the LIAR extension introduced by Kirilin et al. In this dataset, the multi-class labeling of LIAR is converted to a binary annotation by changing half-true, false, barely-true and pants-fire labels to False, and the remaining labels to True. Furthermore, the speaker names are converted to numerical IDs in order to avoid bias with regards to the textual representation of names. The binary-label dataset is then extended by adding sentiments derived using the Google NLP API. Sentiment analysis determines the overall attitude of the text (i.e., whether it is positive or negative), and is quantified by a numerical score. If the sentiment score is positive, then the sample is tagged as Positive for the sentiment attribute, otherwise Negative is assigned.\nA further extension is introduced by adding emotion scores extracted using the IBM NLP API for each claim, which determine the detected level of 6 emotional states namely anger, sadness, disgust, fear and joy. The score for each emotion is between the range of 0 and 1.\n\nSource: [https://github.com/UNHSAILLab/SentimentalLIAR](https://github.com/UNHSAILLab/SentimentalLIAR)", "variants": ["Sentimental LIAR"]}
{"id": "SFU-Store-Nav", "title": "SFU-Store-Nav: A Multimodal Dataset for Indoor Human Navigation", "contents": "A dataset collected in a set of experiments that involves human participants and a robot.\r\n\r\nSource: [SFU-Store-Nav: A Multimodal Dataset for Indoor Human Navigation](/paper/sfu-store-nav-a-multimodal-dataset-for-indoor)", "variants": ["SFU-Store-Nav"]}
{"id": "SG-NLG", "title": "Schema-Guided Natural Language Generation", "contents": "The SG-NLG dataset is a pre-processed version of the [DSTC8 Schema-Guided Dialogue SGD dataset](https://paperswithcode.com/dataset/sgd), designed specifically for data-to-text Natural Language Generation (NLG). The original DSTC8 SGD contains ~20,000 dialogues spanning across ~20 domains.\r\n\r\nThis SG-NLG dataset is designed to make it easier to conduct NLG experiments on the SGD data. It consists of pre-processed SGD data by pairing the schema for each system turn with the corresponding set of natural language strings that realize it. It also “delexicalizes” the prompts (replace related values with fixed names) to convert them into templates that make them more generic for use within a dialog system.\r\n\r\nThe final SG-NLG dataset is composed of nearly 4K MRs and over 140K templates. \r\n\r\nSource: [The Schema-Guided Natural Language Generation (SG-NLG) Dataset](https://github.com/alexa/schema-guided-nlg)", "variants": ["SG-NLG"]}
{"id": "ShapeNet-Skeleton", "title": "SkeletonNet: A Topology-Preserving Solution for Learning Mesh Reconstruction of Object Surfaces from RGB Images", "contents": "The **ShapeNet-Skeleton** dataset has ground-truth skeleton point sets and skeletal volumes for object instances in the ShapeNet dataset.\n\nSource: [https://arxiv.org/pdf/2008.05742.pdf](https://arxiv.org/pdf/2008.05742.pdf)", "variants": ["ShapeNet-Skeleton"]}
{"id": "3D Shapes Dataset", "title": "", "contents": "3dshapes is a dataset of 3D shapes procedurally generated from 6 ground truth independent latent factors. These factors are floor colour, wall colour, object colour, scale, shape and orientation.\r\n\r\nSource: [3D Shapes Dataset ](https://github.com/deepmind/3d-shapes)", "variants": ["3D Shapes Dataset"]}
{"id": "SHIDC-BC-Ki-67", "title": "PathoNet: Deep learning assisted evaluation of Ki-67 and tumor infiltrating lymphocytes (TILs) as prognostic factors in breast cancer; A large dataset and baseline", "contents": "Benchmark for BC Ki-67 stained cell detection and further annotated classification of cells.\r\n\r\nSource: [PathoNet: Deep learning assisted evaluation of Ki-67 and tumor infiltrating lymphocytes (TILs) as prognostic factors in breast cancer; A large dataset and baseline](/paper/pathonet-deep-learning-assisted-evaluation-of)", "variants": ["SHIDC-BC-Ki-67"]}
{"id": "SidechainNet", "title": "SidechainNet: An All-Atom Protein Structure Dataset for Machine Learning", "contents": "**SidechainNet** is a protein structure prediction dataset that directly extends ProteinNet. Specifically, SidechainNet adds measurements for protein angles and coordinates that describe the complete, all-atom protein structure (backbone and sidechain, excluding hydrogens) instead of the protein backbone alone.\n\nSource: [https://github.com/jonathanking/sidechainnet](https://github.com/jonathanking/sidechainnet)", "variants": ["SidechainNet"]}
{"id": "SIMMC", "title": "Situated and Interactive Multimodal Conversations", "contents": "Situated Interactive MultiModal Conversations (**SIMMC**) is the task of taking multimodal actions grounded in a co-evolving multimodal input content in addition to the dialog history. This dataset contains two SIMMC datasets totalling ~13K human-human dialogs (~169K utterances) using a multimodal Wizard-of-Oz (WoZ) setup, on two shopping domains: (a) furniture (grounded in a shared virtual environment) and (b) fashion (grounded in an evolving set of images).\n\nSource: [https://github.com/facebookresearch/simmc](https://github.com/facebookresearch/simmc)\nImage Source: [https://github.com/facebookresearch/simmc](https://github.com/facebookresearch/simmc)", "variants": ["SIMMC"]}
{"id": "SIS", "title": "Inferring symmetry in natural language", "contents": "Comprises of 400 naturalistic usages of literature-informed verbs spanning the spectrum of symmetry-asymmetry.\r\n\r\nSource: [Inferring symmetry in natural language](/paper/inferring-symmetry-in-natural-language)", "variants": ["SIS"]}
{"id": "SI-SCORE", "title": "On Robustness and Transferability of Convolutional Neural Networks", "contents": "A synthetic dataset uses for a systematic analysis across common factors of variation.\r\n\r\nSource: [On Robustness and Transferability of Convolutional Neural Networks](/paper/on-robustness-and-transferability-of)", "variants": ["SI-SCORE"]}
{"id": "SIZER", "title": "SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing", "contents": "Dataset of clothing size variation which includes  different subjects wearing casual clothing items in various sizes, totaling to approximately 2000 scans. This dataset includes the scans, registrations to the SMPL model, scans segmented in clothing parts, garment category and size labels. \r\n\r\nSource: [SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing](/paper/sizer-a-dataset-and-model-for-parsing-3d)", "variants": ["SIZER"]}
{"id": "SketchGraphs", "title": "SketchGraphs: A Large-Scale Dataset for Modeling Relational Geometry in Computer-Aided Design", "contents": "**SketchGraphs** is a dataset of 15 million sketches extracted from real-world CAD models intended to facilitate research in both ML-aided design and geometric program induction.\nEach sketch is represented as a geometric constraint graph where edges denote designer-imposed geometric relationships between primitives, the nodes of the graph.\n\nSource: [https://github.com/PrincetonLIPS/SketchGraphs](https://github.com/PrincetonLIPS/SketchGraphs)\nImage Source: [https://github.com/PrincetonLIPS/SketchGraphs](https://github.com/PrincetonLIPS/SketchGraphs)", "variants": ["SketchGraphs"]}
{"id": "Skill2vec", "title": "Skill2vec: Machine Learning Approach for Determining the Relevant Skills from Job Description", "contents": "Collects a huge number of job descriptions from Dice.com - one of the most popular career website about Tech jobs in USA. From these job descriptions, skills are extracted for each one by using skills dictionary. Now, the dataset is presented by a list of collections of skills based on job descriptions. After crawling, there are a total of 5GB with more than 1,400,000 job descriptions. From these data, skills are extracted and performed as a list of skills in the same context, the context here includes skills in the same job description.\r\n\r\nSource: [Skill2vec: Machine Learning Approach for Determining the Relevant Skills from Job Description](/paper/skill2vec-machine-learning-approach-for)", "variants": ["Skill2vec"]}
{"id": "SKU110K-R", "title": "Dynamic Refinement Network for Oriented and Densely Packed Object Detection", "contents": "**SKU110K-R** is a dataset relabeled with oriented bounding boxes based on SKU110K. It is focused on evaluating oriented and densely packed object detection.\n\nSource: [https://github.com/Anymake/DRN_CVPR2020](https://github.com/Anymake/DRN_CVPR2020)\nImage Source: [https://github.com/Anymake/DRN_CVPR2020](https://github.com/Anymake/DRN_CVPR2020)", "variants": ["SKU110K-R"]}
{"id": "SLURP", "title": "SLURP: A Spoken Language Understanding Resource Package", "contents": "A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets.\r\n\r\nSource: [SLURP: A Spoken Language Understanding Resource Package](/paper/slurp-a-spoken-language-understanding-1)", "variants": ["SLURP"]}
{"id": "SMS Spam Collection Data Set", "title": "", "contents": "This corpus has been collected from free or free for research sources at the Internet:\r\n\r\n- A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages.\r\n- A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available.\r\n- A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis.\r\n- the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages.\r\n\r\nSource: [SMS Spam Collection Data Set](http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)", "variants": ["SMS Spam Collection Data Set"]}
{"id": "SoccerNet-v2", "title": "SoccerNet-v2: A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos", "contents": "A novel large-scale corpus of manual annotations for the SoccerNet video dataset, along with open challenges to encourage more research in soccer understanding and broadcast production.\r\n\r\nSource: [SoccerNet-v2 : A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos](/paper/soccernet-v2-a-dataset-and-benchmarks-for)", "variants": ["SoccerNet", "SoccerNet-v2"]}
{"id": "SoloDance", "title": "C2F-FWN: Coarse-to-Fine Flow Warping Network for Spatial-Temporal Consistent Motion Transfer", "contents": "A large-scale HVMT dataset named SoloDance.\r\n\r\nSource: [C2F-FWN: Coarse-to-Fine Flow Warping Network for Spatial-Temporal Consistent Motion Transfer](/paper/c2f-fwn-coarse-to-fine-flow-warping-network)", "variants": ["SoloDance"]}
{"id": "SONYC-UST-V2", "title": "SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context", "contents": "A dataset for urban sound tagging with spatiotemporal information. This dataset is aimed for the development and evaluation of machine listening systems for real-world urban noise monitoring. While datasets of urban recordings are available, this dataset provides the opportunity to investigate how spatiotemporal metadata can aid in the prediction of urban sound tags. SONYC-UST-V2 consists of 18510 audio recordings from the \"Sounds of New York City\" (SONYC) acoustic sensor network, including the timestamp of audio acquisition and location of the sensor. \r\n\r\nSource: [SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context](/paper/sonyc-ust-v2-an-urban-sound-tagging-dataset)", "variants": ["SONYC-UST-V2"]}
{"id": "SoyCultivarVein", "title": "", "contents": "The SoyCultivarVein dataset is a publicly available dataset, which comprises 100 categories (cultivars) with 6 samples (leaf images) in each cultivar and thus has a total number of 100×6 = 600 images (Yu et al. 2019). The leaves in the SoyCultivarVein dataset are highly similar due to the fact that they all belong to the same species, making it a new and challenging dataset for the artificial intelligence and pattern analysis research community.\r\n\r\nSource: [Patchy Image Structure Classification Using Multi-Orientation Region Transform](https://arxiv.org/pdf/1912.00622v1.pdf)", "variants": ["SoyCultivarVein"]}
{"id": "SpeakingFaces", "title": "SpeakingFaces: A Large-Scale Multimodal Dataset of Voice Commands with Visual and Thermal Video Streams", "contents": "SpeakingFaces is a publicly-available large-scale dataset developed to support multimodal machine learning research in contexts that utilize a combination of thermal, visual, and audio data streams; examples include human-computer interaction (HCI), biometric authentication, recognition systems, domain transfer, and speech recognition. SpeakingFaces is comprised of well-aligned high-resolution thermal and visual spectra image streams of fully-framed faces synchronized with audio recordings of each subject speaking approximately 100 imperative phrases.\r\n\r\nSource: [SpeakingFaces: A Large-Scale Multimodal Dataset of Voice Commands with Visual and Thermal Video Streams](/paper/speakingfaces-a-large-scale-multimodal)", "variants": ["SpeakingFaces"]}
{"id": "SPIRS", "title": "Reactive Supervision: A New Method for Collecting Sarcasm Data", "contents": "Releases a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features. The dataset is expected to advance sarcasm detection research. \r\n\r\nSource: [Reactive Supervision: A New Method for Collecting Sarcasm Data](/paper/reactive-supervision-a-new-method-for)", "variants": ["SPIRS"]}
{"id": "SPLASH", "title": "Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback", "contents": "A dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback.\r\n\r\nSource: [Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback](/paper/speak-to-your-parser-interactive-text-to-sql)", "variants": ["SPLASH"]}
{"id": "SQuAD-es", "title": "", "contents": "Stanford Question Answering Dataset (SQuAD) into Spanish.\r\n\r\nSource: [SQuAD-es](https://github.com/ccasimiro88/TranslateAlignRetrieve)", "variants": ["SQuAD-es"]}
{"id": "SQuAD-it", "title": "", "contents": "SQuAD-it is derived from the SQuAD dataset and it is obtained through semi-automatic translation of the SQuAD dataset into Italian. It represents a large-scale dataset for open question answering processes on factoid questions in Italian. The dataset contains more than 60,000 question/answer pairs derived from the original English dataset.\r\n\r\nSource: [SQuAD-it](https://github.com/crux82/squad-it)", "variants": ["SQuAD-it"]}
{"id": "SQuAD-shifts", "title": "The Effect of Natural Distribution Shift on Question Answering Models", "contents": "Provides four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. \r\n\r\nSource: [The Effect of Natural Distribution Shift on Question Answering Models](/paper/the-effect-of-natural-distribution-shift-on)", "variants": ["SQuAD-shifts"]}
{"id": "StanfordExtra", "title": "Who Left the Dogs Out? 3D Animal Reconstruction with Expectation Maximization in the Loop", "contents": "An 'in the wild' dataset of 20,580 dog images for which 2D joint and silhouette annotations were collected.\r\n\r\nSource: [Who Left the Dogs Out? 3D Animal Reconstruction with Expectation Maximization in the Loop](/paper/who-left-the-dogs-out-3d-animal)", "variants": ["StanfordExtra"]}
{"id": "STAR", "title": "STAR: A Schema-Guided Dialog Dataset for Transfer Learning", "contents": "A schema-guided task-oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog.\r\n\r\nSource: [STAR: A Schema-Guided Dialog Dataset for Transfer Learning](/paper/star-a-schema-guided-dialog-dataset-for)", "variants": ["STAR"]}
{"id": "Stream-51", "title": "Stream-51: Streaming Classification and Novelty Detection from Videos", "contents": "A new dataset for streaming classification consisting of temporally correlated images from 51 distinct object categories and additional evaluation classes outside of the training distribution to test novelty recognition. \r\n\r\nSource: [Stream-51: Streaming Classification and Novelty Detection from Videos](/paper/stream-51-streaming-classification-and)", "variants": ["Stream-51"]}
{"id": "Exact Street2Shop", "title": "Where to Buy It: Matching Street Clothing Photos in Online Shops", "contents": "A dataset containing 404,683 shop photos collected from 25 different online retailers and 20,357 street photos, providing a total of 39,479 clothing item matches between street and shop photos.\r\n\r\nSource: [Where to Buy It: Matching Street Clothing Photos in Online Shops](/paper/where-to-buy-it-matching-street-clothing)", "variants": ["Exact Street2Shop"]}
{"id": "SubEdits", "title": "Can Automatic Post-Editing Improve NMT?", "contents": "**SubEdits** is a human-annnoated post-editing dataset of neural machine translation outputs, compiled from in-house NMT outputs and human post-edits of subtitles form Rakuten Viki. It is collected from English-German annotations and contains 160k triplets.\r\n\r\nSource: [https://github.com/shamilcm/pedra](https://github.com/shamilcm/pedra)\r\nImage Source: [Chollampatt et al](https://arxiv.org/pdf/2009.14395v1.pdf)", "variants": ["SubEdits"]}
{"id": "SubjQA", "title": "SubjQA: A Dataset for Subjectivity and Review Comprehension", "contents": "**SubjQA** is a question answering dataset that focuses on subjective (as opposed to factual) questions and answers. The dataset consists of roughly 10,000 questions over reviews from 6 different domains: books, movies, grocery, electronics, TripAdvisor (i.e. hotels), and restaurants. Each question is paired with a review and a span is highlighted as the answer to the question (with some questions having no answer). Moreover, both questions and answer spans are assigned a subjectivity label by annotators. Questions such as \"How much does this product weigh?\" is a factual question (i.e., low subjectivity), while \"Is this easy to use?\" is a subjective question (i.e., high subjectivity).\n\nSource: [https://github.com/megagonlabs/SubjQA](https://github.com/megagonlabs/SubjQA)", "variants": ["SubjQA"]}
{"id": "SuspectGuilt Corpus", "title": "Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives", "contents": "A corpus of annotated crime stories from English-language newspapers in the U.S. For SuspectGuilt, annotators read short crime articles and provided text-level ratings concerning the guilt of the main suspect as well as span-level annotations indicating which parts of the story they felt most influenced their ratings. SuspectGuilt thus provides a rich picture of how linguistic choices affect subjective guilt judgments.\r\n\r\nSource: [Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives](/paper/modeling-subjective-assessments-of-guilt-in)", "variants": ["SuspectGuilt Corpus"]}
{"id": "Swiss3DCities", "title": "Semantic Segmentation on Swiss3DCities: A Benchmark Study on Aerial Photogrammetric 3D Pointcloud Dataset", "contents": "Swiss3DCities is a dataset that is manually annotated for semantic segmentation with per-point labels, and is built using photogrammetry from images acquired by multirotors equipped with high-resolution cameras.\r\n\r\nSource: [Semantic Segmentation on Swiss3DCities: A Benchmark Study on Aerial Photogrammetric 3D Pointcloud Dataset](/paper/semantic-segmentation-on-swiss3dcities-a)\r\nImage Source: [Can et al](https://arxiv.org/pdf/2012.12996.pdf)", "variants": ["Swiss3DCities"]}
{"id": "Synthetic Keystroke", "title": "Revisiting the Threat Space for Vision-based Keystroke Inference Attacks", "contents": "This dataset is a large-scale synthetic dataset to simulate the attack scenario for a keystroke inference attack.\n\nSource: [https://arxiv.org/abs/2009.05796](https://arxiv.org/abs/2009.05796)", "variants": ["Synthetic Keystroke"]}
{"id": "TAO", "title": "TAO: A Large-Scale Benchmark for Tracking Any Object", "contents": "TAO is a federated dataset for Tracking Any Object, containing 2,907 high resolution videos, captured in diverse environments, which are half a minute long on average. A bottom-up approach was used for discovering a large vocabulary of 833 categories, an order of magnitude more than prior tracking benchmarks. \r\n\r\nThe dataset was annotated by labelling tracks for objects that move at any point in the video, and giving names to them post factum. \r\n\r\nSource: [TAO: A Large-Scale Benchmark for Tracking Any Object](/paper/tao-a-large-scale-benchmark-for-tracking-any)", "variants": ["TAO"]}
{"id": "TaPaCo", "title": "TaPaCo: A Corpus of Sentential Paraphrases for 73 Languages", "contents": "TaPaCo is a freely available paraphrase corpus for 73 languages extracted from the Tatoeba database.\r\n\r\nSource: [TaPaCo: A Corpus of Sentential Paraphrases for 73 Languages](/paper/tapaco-a-corpus-of-sentential-paraphrases-for)", "variants": ["TaPaCo"]}
{"id": "Taskmaster-2", "title": "", "contents": "The **Taskmaster-2** dataset consists of 17,289 dialogs in seven domains: restaurants (3276), food ordering (1050), movies (3047), hotels (2355), flights (2481), music (1602), and sports (3478).", "variants": ["Taskmaster-2"]}
{"id": "TB-Places", "title": "", "contents": "TB-Places is a data set of garden images for testing algorithms for visual place recognition. It contains images with ground truth camera pose recorded in two real gardens at different times, with a total of four different recording sessions, with varying light conditions.\r\n\r\nSource: [TB-Places](https://github.com/marialeyva/TB_Places)", "variants": ["TB-Places"]}
{"id": "TCG", "title": "Traffic Control Gesture Recognition for Autonomous Vehicles", "contents": "The **TCG** dataset is used to evaluate **Traffic Control Gesture** recognition for autonomous driving. The dataset is based on 3D body skeleton input to perform traffic control gesture classification on every time step. The dataset consists of 250 sequences from several actors, ranging from 16 to 90 seconds per sequence.\n\nSource: [https://arxiv.org/pdf/2007.16072.pdf](https://arxiv.org/pdf/2007.16072.pdf)\nImage Source: [https://github.com/againerju/tcg_recognition](https://github.com/againerju/tcg_recognition)", "variants": ["TCG-dataset", "TCG"]}
{"id": "TextSeg", "title": "Rethinking Text Segmentation: A Novel Dataset and A Text-Specific Refinement Approach", "contents": "**TextSeg** is a large-scale fine-annotated and multi-purpose text detection and segmentation dataset, collecting scene and design text with six types of annotations: word- and character-wise bounding polygons, masks and transcriptions.\n\nSource: [https://github.com/SHI-Labs/Rethinking-Text-Segmentation](https://github.com/SHI-Labs/Rethinking-Text-Segmentation)\nImage Source: [https://github.com/SHI-Labs/Rethinking-Text-Segmentation](https://github.com/SHI-Labs/Rethinking-Text-Segmentation)", "variants": ["TextSeg"]}
{"id": "Textual Visual Semantic Dataset", "title": "Textual Visual Semantic Dataset for Text Spotting", "contents": "Extends the COCO-text [Veit et al. 2016] with information about the scene (such as objects and places appearing in the image) to enable researchers to include semantic relations between texts and scene in their Text Spotting systems, and to offer a common framework for such approaches.\r\n\r\nSource: [Textual Visual Semantic Dataset for Text Spotting](/paper/textual-visual-semantic-dataset-for-text)", "variants": ["Textual Visual Semantic Dataset"]}
{"id": "TextZoom", "title": "Scene Text Image Super-Resolution in the Wild", "contents": "**TextZoom** is a super-resolution dataset that consists of paired Low Resolution – High Resolution scene text images. The images are captured by cameras with different focal length in the wild.\n\nSource: [https://github.com/JasonBoy1/TextZoom](https://github.com/JasonBoy1/TextZoom)\nImage Source: [https://github.com/JasonBoy1/TextZoom](https://github.com/JasonBoy1/TextZoom)", "variants": ["TextZoom"]}
{"id": "TG-ReDial", "title": "Towards Topic-Guided Conversational Recommender System", "contents": "**TG-ReDial** is a a topic-guided conversational recommendation dataset for research on conversational/interactive recommender systems.\n\nSource: [https://github.com/RUCAIBox/TG-ReDial](https://github.com/RUCAIBox/TG-ReDial)\nImage Source: [https://github.com/RUCAIBox/TG-ReDial](https://github.com/RUCAIBox/TG-ReDial)", "variants": ["TG-ReDial"]}
{"id": "TicketTalk", "title": "TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems", "contents": "A movie ticketing dialog dataset with 23,789 annotated conversations. The movie ticketing conversations range from completely open-ended and unrestricted to more structured, both in terms of their knowledge base, discourse features, and number of turns. In qualitative human evaluations, model-generated responses trained on just 10,000 TicketTalk dialogs were rated to \"make sense\" 86.5 percent of the time, almost the same as human responses in the same contexts.\r\n\r\nSource: [TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems](/paper/tickettalk-toward-human-level-performance)", "variants": ["TicketTalk"]}
{"id": "TikTok Comments", "title": "Building domain specific lexicon based on TikTok comment dataset", "contents": "TikTok Comments is a domain specific lexicon based on TikTok comments dataset.\r\n\r\n<data>", "variants": ["TikTok Comments"]}
{"id": "Tilt-RGBD", "title": "Surface Normal Estimation of Tilted Images via Spatial Rectifier", "contents": "Includes considerable roll and pitch camera motion.\r\n\r\nSource: [Surface Normal Estimation of Tilted Images via Spatial Rectifier](/paper/surface-normal-estimation-of-tilted-images)", "variants": ["Tilt-RGBD"]}
{"id": "Time-Lapse Hyperspectral Radiance Images", "title": "", "contents": "These sequences of hyperspectral radiance images have been taken from scenes undergoing natural illumination changes. In each scene, hyperspectral images were acquired at about 1-hour intervals. \r\n\r\nSource: [Time-Lapse Hyperspectral Radiance Images of Natural Scenes 2015](https://personalpages.manchester.ac.uk/staff/d.h.foster/Time-Lapse_HSIs/Time-Lapse_HSIs_2015.html)", "variants": ["Time-Lapse Hyperspectral Radiance Images"]}
{"id": "TinySocial", "title": "Characterizing Datasets for Social Visual Question Answering, and the New TinySocial Dataset", "contents": "TinySocial is a dataset to enable research on Social Visual Question Answering.\r\n\r\nSource: [Characterizing Datasets for Social Visual Question Answering, and the New TinySocial Dataset](/paper/characterizing-datasets-for-social-visual)", "variants": ["TinySocial"]}
{"id": "TinyVIRAT", "title": "TinyVIRAT: Low-resolution Video Action Recognition", "contents": "TinyVIRAT contains natural low-resolution activities. The actions in TinyVIRAT videos have multiple labels and they are extracted from surveillance videos which makes them realistic and more challenging.\r\n\r\nSource: [TinyVIRAT: Low-resolution Video Action Recognition](/paper/tinyvirat-low-resolution-video-action)", "variants": ["TinyVIRAT"]}
{"id": "TJU-DHD", "title": "TJU-DHD: A Diverse High-Resolution Dataset for Object Detection", "contents": "**TJU-DHD** is a high-resolution dataset for object detection and pedestrian detection. The dataset contains 115,354 high-resolution images (52% images have a resolution of 1624×1200 pixels and 48% images have a resolution of at least 2,560×1,440 pixels) and 709,330 labelled objects in total with a large variance in scale and appearance.\n\nSource: [https://github.com/tjubiit/TJU-DHD](https://github.com/tjubiit/TJU-DHD)\nImage Source: [https://github.com/tjubiit/TJU-DHD](https://github.com/tjubiit/TJU-DHD)", "variants": ["TJU-DHD"]}
{"id": "TME Motorway Dataset", "title": "", "contents": "The “Toyota Motor Europe (TME) Motorway Dataset” is composed by 28 clips for a total of approximately 27 minutes (30000+ frames) with vehicle annotation. Annotation was semi-automatically generated using laser-scanner data. Image sequences were selected from acquisition made in North Italian motorways in December 2011. This selection includes variable traffic situations, number of lanes, road curvature, and lighting, covering most of the conditions present in the complete acquisition.\r\n\r\nThe dataset comprises:\r\n\r\n- Image acquisition: stereo, 20 Hz frequency , 1024x768 grayscale losslessly compressed images, 32° horizontal field of view, bayer coded color information (in OpenCV use CV_BayerGB2GRAY and CV_BayerGB2BGR color conversion codes; please note that left camera was rotated upside down, convert to color/grayscale BEFORE flipping the image). A checkboard calibration sequence is made available.\r\n- Laser-scanner generated vehicle annotation and classification (car/truck).\r\n- A software evaluation toolkit (C++ source code).\r\n\r\nSource: [TME Motorway Dataset](http://cmp.felk.cvut.cz/data/motorway/)", "variants": ["TME Motorway Dataset"]}
{"id": "Torque", "title": "TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions", "contents": "Torque is an English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships.\r\n\r\nSource: [TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions](/paper/torque-a-reading-comprehension-dataset-of)", "variants": ["Torque"]}
{"id": "Toulouse Vanishing Points Dataset", "title": "", "contents": "Toulouse Vanishing Points Dataset is a public photographs database of Manhattan scenes taken with an iPad Air 1. The purpose of this dataset is the evaluation of vanishing points estimation algorithms. Its originality is the addition of Inertial Measurement Unit (IMU) data synchronized with the camera under the form of rotation matrices. Moreover, contrary to existing works which provide vanishing points of reference in the form of single points, there are computed uncertainty regions.\r\n\r\nSource: [Toulouse Vanishing Points Dataset](http://ubee.enseeiht.fr/dokuwiki/doku.php?id=public:toulousevpdataset)", "variants": ["Toulouse Vanishing Points Dataset"]}
{"id": "Toyota Smarthome", "title": "Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection", "contents": "A large scale dataset with daily-living activities performed in a natural manner.\r\n\r\nSource: [Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection](/paper/toyota-smarthome-untrimmed-real-world)", "variants": ["Toyota Smarthome"]}
{"id": "TRACT", "title": "", "contents": "TRACT is a small scale manually annotated corpus for abuse classification problem.\r\n\r\nSource: [TRACT](https://github.com/Saichethan/TRACT)", "variants": ["TRACT"]}
{"id": "Traditional Chinese Landscape Painting Dataset", "title": "End-to-End Chinese Landscape Painting Creation Using Generative Adversarial Networks", "contents": "This dataset consists of 2,192 high-quality traditional Chinese landscape paintings (中国山水画). All paintings are sized 512x512, from the following sources:\n* Princeton University Art Museum, 362 paintings\n* Harvard University Art Museum, 101 paintings\n* Metropolitan Museum of Art, 428 paintings\n* Smithsonian's Freer Gallery of Art, 1,301 paintings\n\nSource: [https://github.com/alicex2020/Chinese-Landscape-Painting-Dataset](https://github.com/alicex2020/Chinese-Landscape-Painting-Dataset)\nImage Source: [https://github.com/alicex2020/Chinese-Landscape-Painting-Dataset](https://github.com/alicex2020/Chinese-Landscape-Painting-Dataset)", "variants": ["Traditional Chinese Landscape Painting Dataset"]}
{"id": "Transient Biometrics Nails Dataset", "title": "", "contents": "An extended version of an experimental dataset, called **Transient Biometrics Nails Dataset** (TBND), was created. TBND is composed of images of the right index finger. During acquisition the subject was instructed to lay her finger over a flat white surface and a simple point-and-shoot camera was used to acquire an image without the the use of a flash. No explicit instructions with respect to force applied were given and thus the results incorporate arbitrary force differences between users and capture sessions. Acquisition was thus done in a semi-controlled environment; apart from the white background and indirect lighting, the images present variation with respect to scale, focal plane and illumination. The dataset consists of three subsets, each one compromising the same 93 subjects, but varying on acquisition date. The first subset D01 consists of images acquired on the first acquisition day. The second subset D02 is composed of images acquired one day later. The third subset D30 was acquired 1 month after the first acquisition date. Given acquisition restrictions, the acquisitions of D30 have up to two days’ tolerance. This represents a massive expansion of the originally collected dataset TBND V01\r\n\r\nSource: [Transient Biometrics Nails Dataset](http://www.idi.ntnu.no/grupper/vis/tbnd/)", "variants": ["Transient Biometrics Nails Dataset"]}
{"id": "TREK-100", "title": "Is First Person Vision Challenging for Object Tracking? The TREK-100 Benchmark Dataset", "contents": "The dataset is composed of 100 video sequences densely annotated with 60K bounding boxes, 17 sequence attributes, 13 action verb attributes and 29 target object attributes. \r\n\r\nSource: [Is First Person Vision Challenging for Object Tracking? The TREK-100 Benchmark Dataset](/paper/is-first-person-vision-challenging-for-object)", "variants": ["TREK-100"]}
{"id": "TriageSQL", "title": "Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL", "contents": "**TriageSQL** is a cross-domain text-to-SQL question intention classification benchmark that requires models to distinguish four types of unanswerable questions from answerable questions.\n\nSource: [https://github.com/chatc/TriageSQL](https://github.com/chatc/TriageSQL)", "variants": ["TriageSQL"]}
{"id": "TTPLA", "title": "TTPLA: An Aerial-Image Dataset for Detection and Segmentation of Transmission Towers and Power Lines", "contents": "**TTPLA** is a public dataset which is a collection of aerial images on Transmission Towers (TTs) and Power Lines (PLs). It can be used for detection and segmentation of transmission towers and power lines. It consists of 1,100 images with the resolution of 3,840×2,160 pixels, as well as manually labelled 8,987 instances of TTs and PLs.\n\nSource: [https://github.com/r3ab/ttpla_dataset](https://github.com/r3ab/ttpla_dataset)\nImage Source: [https://github.com/r3ab/ttpla_dataset](https://github.com/r3ab/ttpla_dataset)", "variants": ["TTPLA"]}
{"id": "TTS-Portuguese Corpus", "title": "End-To-End Speech Synthesis Applied to Brazilian Portuguese", "contents": "The dataset has 10.5 hours from a single speaker.\r\n\r\nSource: [End-To-End Speech Synthesis Applied to Brazilian Portuguese](/paper/end-to-end-speech-synthesis-applied-to)", "variants": ["TTS-Portuguese Corpus"]}
{"id": "TUNIZI", "title": "TUNIZI: a Tunisian Arabizi sentiment analysis Dataset", "contents": "A sentiment analysis Tunisian Arabizi Dataset, collected from social networks, preprocessed for analytical studies and annotated manually by Tunisian native speakers.\r\n\r\nSource: [TUNIZI: a Tunisian Arabizi sentiment analysis Dataset](/paper/tunizi-a-tunisian-arabizi-sentiment-analysis)", "variants": ["TUNIZI"]}
{"id": "TURBID Dataset", "title": "", "contents": "The TURBID, is an open image dataset that has been generated to contribute with the underwater research area. TURBID consists in a collection of five different subsets of degraded images with its respective ground-truth.\r\n\r\nSource: [TURBID Dataset](http://amandaduarte.com.br/turbid/)", "variants": ["TURBID Dataset"]}
{"id": "TVPR", "title": "", "contents": "The TVPR (Top View Person Re-identification) dataset stores depth frames (640x480) collected using Asus Xtion Pro Live in top-view configuration. This setup choice is primarily due to the reduction of occlusions and it has also the advantage of being privacy preserving, because faces are not recorded by the camera. The use of an RGB-D camera allows to extract anthropometric features for the recognition of people passing under the camera.\r\n\r\nSource: [TVPR](http://vrai.dii.univpm.it/re-id-dataset)", "variants": ["TVPR"]}
{"id": "TweetEval", "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification", "contents": "TweetEval introduces an evaluation framework consisting of seven heterogeneous Twitter-specific classification tasks.\r\n\r\nSource: [TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification](/paper/tweeteval-unified-benchmark-and-comparative)\r\nImage Source: [https://arxiv.org/pdf/2010.12421v2.pdf](https://arxiv.org/pdf/2010.12421v2.pdf)", "variants": ["TweetEval"]}
{"id": "Twitter Conversations Dataset", "title": "Conversational Document Prediction to Assist Customer Care Agents", "contents": "This dataset is used for the task of conversational document prediction. The dataset includes conversations that occurred between users and customer care agents in 25 organizations on the Twitter platform. Each conversation ends with a customer care agent providing a URL to a document to resolve the issue the user is facing. The task is to predict the document given a dialog context.\r\nThe train, dev and test datasets include 10000, 525 and 500 conversations respectively.\r\n\r\nSource: [https://github.com/IBM/twitter-customer-care-document-prediction](https://github.com/IBM/twitter-customer-care-document-prediction)\r\nImage Source: [https://arxiv.org/pdf/2010.02305v1.pdf](https://arxiv.org/pdf/2010.02305v1.pdf)", "variants": ["Twitter Conversations Dataset"]}
{"id": "Twitter Flood", "title": "Finding Relevant Flood Images on Twitter using Content-based Filters", "contents": "This dataset contains two subsets of flood images from Twitter: The Harz17 dataset comprises images from tweets containing flood-related keywords during the occurrence of a flood in the Harz region in Germany in July 2017. Similarly, the Rhine18 dataset comprises images related to a flood of the river Rhine in January 2018.\n\nSource: [https://github.com/cvjena/twitter-flood-dataset](https://github.com/cvjena/twitter-flood-dataset)", "variants": ["Twitter Flood"]}
{"id": "UCFRep", "title": "Context-aware and Scale-insensitive Temporal Repetition Counting", "contents": "The **UCFRep** dataset contains 526 annotated repetitive action videos. This dataset is built from the action recognition dataset UCF101.\n\nSource: [https://github.com/Xiaodomgdomg/Deep-Temporal-Repetition-Counting](https://github.com/Xiaodomgdomg/Deep-Temporal-Repetition-Counting)", "variants": ["UCFRep"]}
{"id": "UC Merced Land Use Dataset", "title": "", "contents": "This is a 21 class land use image dataset meant for research purposes.\r\n\r\nThere are 100 images for each of the following classes:\r\n\r\n- agricultural\r\n- airplane\r\n- baseballdiamond\r\n- beach\r\n- buildings\r\n- chaparral\r\n- denseresidential\r\n- forest\r\n- freeway\r\n- golfcourse\r\n- harbor\r\n- intersection\r\n- mediumresidential\r\n- mobilehomepark\r\n- overpass\r\n- parkinglot\r\n- river\r\n- runway\r\n- sparseresidential\r\n- storagetanks\r\n- tenniscourt\r\n- Each image measures 256x256 pixels.\r\n\r\nThe images were manually extracted from large images from the USGS National Map Urban Area Imagery collection for various urban areas around the country. The pixel resolution of this public domain imagery is 1 foot.\r\n\r\nSource: [UC Merced Land Use Dataset](http://weegee.vision.ucmerced.edu/datasets/landuse.html)", "variants": ["UC Merced Land Use Dataset"]}
{"id": "UIT-ViNames", "title": "Gender Prediction Based on Vietnamese Names with Machine Learning Techniques", "contents": "This dataset comprises over 26,000 full names annotated with genders. \r\n\r\nSource: [Gender Prediction Based on Vietnamese Names with Machine Learning Techniques](/paper/gender-prediction-based-on-vietnamese-names)", "variants": ["UIT-ViNames"]}
{"id": "UIT-ViNewsQA", "title": "New Vietnamese Corpus for Machine Reading Comprehension of Health News Articles", "contents": "A new corpus for the low-resource Vietnamese language to evaluate models of machine reading comprehension. The corpus comprises 10,138 human-generated question-answer pairs. Crowdworkers created the questions and answers based on a set of over 2,030 online Vietnamese news articles from the VnExpress news website, where the answers comprised spans extracted from the corresponding articles. \r\n\r\nSource: [New Vietnamese Corpus for Machine ReadingComprehension of Health News Articles](/paper/new-vietnamese-corpus-for-machine)", "variants": ["UIT-ViNewsQA"]}
{"id": "UIT-ViQuAD", "title": "A Vietnamese Dataset for Evaluating Machine Reading Comprehension", "contents": "A new dataset for the low-resource language as Vietnamese to evaluate MRC models. This dataset comprises over 23,000 human-generated question-answer pairs based on 5,109 passages of 174 Vietnamese articles from Wikipedia. \r\n\r\nSource: [A Vietnamese Dataset for Evaluating Machine Reading Comprehension](/paper/a-vietnamese-dataset-for-evaluating-machine)", "variants": ["UIT-ViQuAD"]}
{"id": "UMC005 English-Urdu", "title": "", "contents": "UMC005 English-Urdu is a parallel corpus of texts in English and Urdu language with sentence alignments. The corpus can be used for experiments with statistical machine translation.\r\n\r\nThe texts come from four different sources:\r\n\r\n- Quran\r\n- Bible\r\n- Penn Treebank (Wall Street Journal)\r\n- Emille corpus\r\n\r\nSource: [UMC005 English-Urdu](http://ufal.ms.mff.cuni.cz/umc/005-en-ur/)", "variants": ["UMC005 English-Urdu"]}
{"id": "United Nations Parallel Corpus", "title": "The United Nations Parallel Corpus v1.0", "contents": "The first parallel corpus composed from United Nations documents published by the original data creator. The parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish.\r\n\r\nSource: [The United Nations Parallel Corpus v1.0](https://www.aclweb.org/anthology/L16-1561)", "variants": ["United Nations Parallel Corpus"]}
{"id": "MultiUN", "title": "", "contents": "The MultiUN parallel corpus is extracted from the United Nations Website , and then cleaned and converted to XML at Language Technology Lab in DFKI GmbH (LT-DFKI), Germany. The documents were published by UN from 2000 to 2009.\r\n\r\nSource: [MultiUN](http://www.euromatrixplus.net/multi-un/)", "variants": ["MultiUN"]}
{"id": "UPIQ", "title": "Consolidated Dataset and Metrics for High-Dynamic-Range Image Quality", "contents": "Contains over 4,000 images created by realigning and merging existing HDR and standard-dynamic-range (SDR) datasets.\r\n\r\nSource: [Consolidated Dataset and Metrics for High-Dynamic-Range Image Quality](/paper/consolidated-dataset-and-metrics-for-high)", "variants": ["UPIQ"]}
{"id": "Bend the Truth", "title": "", "contents": "\"Bend the Truth\" dataset contains news in six different domains: technology, education, business, sports, politics, and entertainment. The real news included in the dataset were collected from a variety of mainstream news websites predominantly in Pakistan, India, UK, and the USA. These news channels are BBC Urdu News, CNN Urdu, Express-News, Jung News, Noway Waqat, and many other reliable news websites. The fake news included in this dataset consist of fake versions of the real news in the dataset, written by professional journalists. \r\n\r\nSource: [Bend the Truth](https://github.com/MaazAmjad/Datasets-for-Urdu-news)", "variants": ["Bend the Truth"]}
{"id": "Urdu Sentiment Corpus", "title": "", "contents": "Consists of Urdu tweets for the sentiment analysis and polarity detection. The dataset is consisting of tweets, such that it casts a political shadow and presents a competitive environment between two separate political parties versus the government of Pakistan. Overall, the dataset is comprising over 17, 185 tokens with 52% records as positive, and 48% records as negative. \r\n\r\nSource: [Urdu Sentiment Corpus (v1.0): Linguistic Exploration and Visualization of Labeled Dataset for Urdu Sentiment Analysis](https://ieeexplore.ieee.org/abstract/document/9080043)", "variants": ["Urdu Sentiment Corpus"]}
{"id": "US-4", "title": "Effective Sample Pair Generation for Ultrasound Video Contrastive Representation Learning", "contents": "The **US-4** is a dataset of Ultrasound (US) images. It is a video-based image dataset that contains over 23,000 high-resolution images from four US video sub-datasets, where two sub-datasets are newly collected by experienced doctors for this dataset.\n\nSource: [https://github.com/983632847/USCL](https://github.com/983632847/USCL)\nImage Source: [https://github.com/983632847/USCL](https://github.com/983632847/USCL)", "variants": ["US-4"]}
{"id": "VeRi Dataset", "title": "", "contents": "To facilitate the research of vehicle re-identification (Re-Id), a large-scale benchmark dateset is built for vehicle Re-Id in the real-world urban surveillance scenario, named “VeRi”. The featured properties of VeRi include:\r\n\r\n- It contains over 50,000 images of 776 vehicles captured by 20 cameras covering an 1.0 km^2 area in 24 hours, which makes the dataset scalable enough for vehicle Re-Id and other related research.\r\n- The images are captured in a real-world unconstrained surveillance scene and labeled with varied attributes, e.g. BBoxes, types, colors, and brands. So complicated models can be learnt and evaluated for vehicle Re-Id.\r\n- Each vehicle is captured by 2 ∼ 18 cameras in different viewpoints, illuminations, resolutions, and occlusions, which provides high recurrence rate for vehicle Re-Id in practical surveillance environment.\r\n- It is also labeled with sufficient license plates and spatiotemporal information, such as the BBoxes of plates, plate strings, the timestamps of vehicles, and the distances between neighbouring cameras.\r\n\r\nSource: [VeRi Dataset](https://vehiclereid.github.io/VeRi/)", "variants": ["VeRi Dataset"]}
{"id": "VGG-Sound", "title": "VGGSound: A Large-scale Audio-Visual Dataset", "contents": "Consists of more than 210k videos for 310 audio classes.\r\n\r\nSource: [VGGSound: A Large-scale Audio-Visual Dataset](/paper/vggsound-a-large-scale-audio-visual-dataset)", "variants": ["VGG-Sound"]}
{"id": "VIA", "title": "Visually Impaired Aid using Convolutional Neural Networks, Transfer Learning, and Particle Competition and Cooperation", "contents": "The **VIA** dataset is a dataset for aiding the visually impaired. The proposed datase1 consists of 342 images divided into two classes: 175 of them are “clear-path” and 167 are “nonclear” path. They were taken using a smartphone camera and resized to 750 × 1000 pixels. The smartphone was placed in the user chest height and inclined approximately 30 to 60 from the ground, so it could capture a few meters of the path ahead, and beyond the reach of a regular white cane\n\nSource: [https://arxiv.org/abs/2005.04473](https://arxiv.org/abs/2005.04473)", "variants": ["VIA"]}
{"id": "VIDIT", "title": "VIDIT: Virtual Image Dataset for Illumination Transfer", "contents": "A reference evaluation benchmark and to push forward the development of illumination manipulation methods.\r\n\r\nSource: [VIDIT: Virtual Image Dataset for Illumination Transfer](/paper/vidit-virtual-image-dataset-for-illumination)", "variants": ["VIDIT"]}
{"id": "ViMMRC", "title": "", "contents": "A challenging machine comprehension corpus with multiple-choice questions, intended for research on the machine comprehension of Vietnamese text. This corpus includes 2,783 multiple-choice questions and answers based on a set of 417 Vietnamese texts used for teaching reading comprehension for 1st to 5th graders. Answers may be extracted from the contents of single or multiple sentences in the corresponding reading text.\r\n\r\nSource: [ViMMRC](https://sites.google.com/uit.edu.vn/uit-nlp/datasets-projects)", "variants": ["ViMMRC"]}
{"id": "Vistas-NP", "title": "Dense open-set recognition with synthetic outliers generated by Real NVP", "contents": "The **Vistas-NP** dataset is an out-of-distribution detection dataset based on the Mapillary Vistas dataset. The original Vistas dataset consists of 18,000 training images and 2,000 validation images with 66 classes. In Vistas-NP the human classes are used as outliers due to their dispersion across scenes and visual diversity from other objects. The dataset is created by excluding all images with class person and the three rider classes to the test subset. Consequently, the dataset has 8,003 train images and 830 validation images. The test set contains 11,167.\n\nSource: [https://github.com/matejgrcic/Vistas-NP](https://github.com/matejgrcic/Vistas-NP)", "variants": ["Vistas-NP"]}
{"id": "Visual Question Answering", "title": "", "contents": "**Visual Question Answering (VQA)** is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. The first version of the dataset was released in October 2015. [VQA v2.0](/dataset/visual-question-answering-v2-0) was released in April 2017.", "variants": ["VQA v1 test-dev", "VQA v1 test-std", "Visual Question Answering"]}
{"id": "ViText2SQL", "title": "A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese", "contents": "**ViText2SQL** is a dataset for the Vietnamese Text-to-SQL semantic parsing task, consisting of about 10K question and SQL query pairs.\n\nSource: [https://github.com/VinAIResearch/ViText2SQL](https://github.com/VinAIResearch/ViText2SQL)", "variants": ["ViText2SQL"]}
{"id": "ViTT", "title": "Multimodal Pretraining for Dense Video Captioning", "contents": "The ViTT dataset consists of human produced segment-level annotations for 8,169 videos. Of these, 5,840 videos have been annotated once, and the rest of the videos have been annotated twice or more. A total of 12,461 sets of annotations are released. The videos in the dataset are from the [Youtube-8M dataset](https://paperswithcode.com/dataset/youtube-8m).\r\n\r\nAn annotation has the following format:\r\n```\r\n{\r\n  \"id\": \"FmTp\",\r\n  \"annotations\": [\r\n    {\r\n      \"timestamp\": 260,\r\n      \"tag\": \"Opening\"\r\n    },\r\n    {\r\n      \"timestamp\": 16000,\r\n      \"tag\": \"Displaying technique\"\r\n    },\r\n    {\r\n      \"timestamp\": 23990,\r\n      \"tag\": \"Showing foot positioning\"\r\n    },\r\n    {\r\n      \"timestamp\": 55530,\r\n      \"tag\": \"Demonstrating crossover\"\r\n    },\r\n    {\r\n      \"timestamp\": 114100,\r\n      \"tag\": \"Closing\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nSource: [Video Timeline Tags (ViTT)](https://github.com/google-research-datasets/Video-Timeline-Tags-ViTT)", "variants": ["ViTT"]}
{"id": "VLEngagement", "title": "VLEngagement: A Dataset of Scientific Video Lectures for Evaluating Population-based Engagement", "contents": "A novel dataset that consists of content-based and video-specific features extracted from publicly available scientific video lectures and several metrics related to user engagement. \r\n\r\nSource: [VLEngagement: A Dataset of Scientific Video Lectures for Evaluating Population-based Engagement](/paper/vlengagement-a-dataset-of-scientific-video)", "variants": ["VLEngagement"]}
{"id": "VMSMO", "title": "VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles", "contents": "The Video-based Multimodal Summarization with Multimodal Output (**VMSMO**) corpus consists of 184,920 document-summary pairs, with 180,000 training pairs, 2,460 validation and test pairs. The task for this dataset is generating and appropriate textual summary of an article and choosing a proper cover frame from a video accompanying the article.\n\nSource: [https://github.com/yingtaomj/VMSMO](https://github.com/yingtaomj/VMSMO)", "variants": ["VMSMO"]}
{"id": "VoxClamantis", "title": "A Corpus for Large-Scale Phonetic Typology", "contents": "A large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. \r\n\r\nSource: [A Corpus for Large-Scale Phonetic Typology](/paper/a-corpus-for-large-scale-phonetic-typology)", "variants": ["VoxClamantis"]}
{"id": "VoxPopuli", "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation", "contents": "VoxPopuli is a large-scale multilingual corpus providing 100K hours of unlabelled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16 languages and their aligned oral interpretations into 5 other languages totaling 5.1K hours.\r\n\r\nSource: [VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation](/paper/voxpopuli-a-large-scale-multilingual-speech)", "variants": ["VoxPopuli"]}
{"id": "VT5000", "title": "RGBT Salient Object Detection: A Large-scale Dataset and Benchmark", "contents": "Includes 5000 spatially aligned RGBT image pairs with ground truth annotations. VT5000 has 11 challenges collected in different scenes and environments for exploring the robustness of algorithms.\r\n\r\nSource: [RGBT Salient Object Detection: A Large-scale Dataset and Benchmark](/paper/rgbt-salient-object-detection-a-large-scale)", "variants": ["VT5000"]}
{"id": "WebChild", "title": "WebChild 2.0 : Fine-Grained Commonsense Knowledge Distillation", "contents": "One of the largest commonsense knowledge bases available, describing over 2 million disambiguated concepts and activities, connected by over 18 million assertions.\r\n\r\nSource: [WebChild 2.0 : Fine-Grained Commonsense Knowledge Distillation](/paper/webchild-20-fine-grained-commonsense)", "variants": ["WebChild"]}
{"id": "WIDER", "title": "Recognize Complex Events From Static Images by Fusing Deep Channels", "contents": "**WIDER** is a dataset for complex event recognition from static images. As of v0.1, it contains 61 event categories and around 50574 images annotated with event class labels.", "variants": ["WIDER"]}
{"id": "WIDER Attribute Dataset", "title": "", "contents": "The **WIDER Attribute** dataset is a human attribute recognition dataset with human attribute and image event annotations. Images are selected from the WIDER dataset. There are a total of 13,789 images. A bounding box is annotated for each person in these images, with no more than 20 people (with top resolutions) in a crowd image, resulting in 57,524 boxes in total and 4+ boxes per image on average. For each bounding box, 14 distinct human attributes are labelled. There are 805,336 labels in total.", "variants": ["WIDER Attribute Dataset"]}
{"id": "WikiAsp", "title": "WikiAsp: A Dataset for Multi-domain Aspect-based Summarization", "contents": "A large-scale dataset for multi-domain aspect-based summarization that attempts to spur research in the direction of open-domain aspect-based summarization. \r\n\r\nSource: [WikiAsp: A Dataset for Multi-domain Aspect-based Summarization](/paper/wikiasp-a-dataset-for-multi-domain-aspect)", "variants": ["WikiAsp"]}
{"id": "WikiCoref", "title": "WikiCoref: An English Coreference-annotated Corpus of Wikipedia Articles", "contents": "WikiCoref is an English corpus annotated for anaphoric relations, where all documents are from the English version of Wikipedia. \r\n\r\nSource: [WikiCoref: An English Coreference-annotated Corpus of Wikipedia Articles](https://www.aclweb.org/anthology/L16-1021)", "variants": ["WikiCoref"]}
{"id": "WikiDocEdits", "title": "Text Editing by Command", "contents": "A dataset of single-sentence edits crawled from Wikipedia. \r\n\r\nSource: [Text Editing by Command](/paper/text-editing-by-command)", "variants": ["WikiDocEdits"]}
{"id": "Wiki-en", "title": "", "contents": "**Wiki-en** is an annotated English dataset for domain detection extracted from Wikipedia. It includes texts from 7 different domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW), “Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN).\n\nSource: [https://arxiv.org/pdf/1907.11499.pdf](https://arxiv.org/pdf/1907.11499.pdf)", "variants": ["Wiki-en"]}
{"id": "WikiLingua", "title": "WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization", "contents": "WikiLingua includes ~770k article and summary pairs in 18 languages from WikiHow. Gold-standard article-summary alignments across languages are extracted by aligning the images that are used to describe each how-to step in an article.\r\n\r\nSource: [https://github.com/esdurmus/Wikilingua](https://github.com/esdurmus/Wikilingua)\r\nImage Source: [Ladhak et al](https://arxiv.org/pdf/2010.03093v1.pdf)", "variants": ["WikiLingua", "WikiLingua (es->en)", "WikiLingua (ru->en)", "WikiLingua (tr->en)", "WikiLingua (vi->en)"]}
{"id": "WikiLinks", "title": "", "contents": "A method for automatically gathering massive amounts of naturally-occurring cross-document reference data is used to create the Wikilinks dataset comprising of 40 million mentions over 3 million entities. \r\n\r\nSource: [WikiLinks](http://www.iesl.cs.umass.edu/data/data-wiki-links)", "variants": ["WikiLinks"]}
{"id": "WikiReading Recycled", "title": "From Dataset Recycling to Multi-Property Extraction and Beyond", "contents": "A newly developed public dataset and the task of multiple property extraction. It uses the same data as WikiReading but does not inherit its predecessor's identified disadvantages.\r\n\r\nSource: [From Dataset Recycling to Multi-Property Extraction and Beyond](/paper/from-dataset-recycling-to-multi-property)", "variants": ["WikiReading Recycled"]}
{"id": "Wilds", "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts", "contents": "Builds on top of recent data collection efforts by domain experts in these applications and provides a unified collection of datasets with evaluation metrics and train/test splits that are representative of real-world distribution shifts.\r\n\r\nSource: [WILDS: A Benchmark of in-the-Wild Distribution Shifts](/paper/wilds-a-benchmark-of-in-the-wild-distribution)", "variants": ["Wilds"]}
{"id": "WildDeepfake", "title": "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection", "contents": "**WildDeepfake** is a dataset for real-world deepfakes detection which consists of 7,314 face sequences extracted from 707 deepfake videos that are collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop more effective detectors against real-world deepfakes.\n\nSource: [https://github.com/deepfakeinthewild/deepfake-in-the-wild](https://github.com/deepfakeinthewild/deepfake-in-the-wild)\nImage Source: [https://github.com/deepfakeinthewild/deepfake-in-the-wild](https://github.com/deepfakeinthewild/deepfake-in-the-wild)", "variants": ["WildDeepfake"]}
{"id": "Wisesight Sentiment Corpus", "title": "", "contents": "Social media message with sentiment label (positive, neutral, negative, question).\r\n\r\nSource: [Wisesight Sentiment Corpus](https://github.com/PyThaiNLP/wisesight-sentiment/tree/v1.0)", "variants": ["Wisesight Sentiment Corpus"]}
{"id": "XED", "title": "XED: A Multilingual Dataset for Sentiment Analysis and Emotion Detection", "contents": "XED is a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages. \r\n\r\nSource: [XED: A Multilingual Dataset for Sentiment Analysis and Emotion Detection](/paper/xed-a-multilingual-dataset-for-sentiment)", "variants": ["XED"]}
{"id": "XL-WiC", "title": "XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization", "contents": "A large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer.\r\n\r\nSource: [XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization](/paper/xl-wic-a-multilingual-benchmark-for)", "variants": ["XL-WiC"]}
{"id": "X-MARS", "title": "Pose-Driven Deep Models for Person Re-Identification", "contents": "The **X-MARS** dataset proposes new splits for the MARS dataset, to allow for cross-evaluation with the Market-1501 dataset without training and test overlap between the two datasets.\n\nSource: [https://github.com/andreas-eberle/x-mars](https://github.com/andreas-eberle/x-mars)", "variants": ["X-MARS"]}
{"id": "XOR-TYDI QA", "title": "XOR QA: Cross-lingual Open-Retrieval Question Answering", "contents": "A large-scale dataset built on questions from TyDi QA lacking same-language answers.\r\n\r\nSource: [XOR QA: Cross-lingual Open-Retrieval Question Answering](/paper/xor-qa-cross-lingual-open-retrieval-question)", "variants": ["XOR-TYDI QA"]}
{"id": "LAReQA", "title": "LAReQA: Language-agnostic answer retrieval from a multilingual pool", "contents": "A challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. \r\n\r\nSource: [LAReQA: Language-agnostic answer retrieval from a multilingual pool](/paper/lareqa-language-agnostic-answer-retrieval)", "variants": ["LAReQA"]}
{"id": "X-SRL", "title": "X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset", "contents": "SRL is the task of extracting semantic predicate-argument structures from sentences. **X-SRL** is a multilingual parallel Semantic Role Labelling (SRL) corpus for English (EN), German (DE), French (FR) and Spanish (ES) that is based on English gold annotations and shares the same labelling scheme across languages.\r\n\r\nSource: [https://github.com/Heidelberg-NLP/xsrl_mbert_aligner](https://github.com/Heidelberg-NLP/xsrl_mbert_aligner)\r\nImage Source: [Daza et al](https://arxiv.org/pdf/2010.01998v1.pdf)", "variants": ["X-SRL"]}
{"id": "YASO", "title": "YASO: A New Benchmark for Targeted Sentiment Analysis", "contents": "YASO is a crowd-sourced TSA evaluation dataset, collected using a new annotation scheme for labeling targets and their sentiments. The dataset contains 2,215 English sentences from movie, business and product reviews, and 7,415 terms and their corresponding sentiments annotated within these sentences. \r\n\r\nSource: [YASO: A New Benchmark for Targeted Sentiment Analysis](/paper/yaso-a-new-benchmark-for-targeted-sentiment)", "variants": ["YASO"]}
{"id": "YCBInEOAT Dataset", "title": "se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains", "contents": "A new dataset with significant occlusions related to object manipulation.\r\n\r\nSource: [se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains](/paper/se-3-tracknet-data-driven-6d-pose-tracking-by)", "variants": ["YCBInEOAT Dataset"]}
{"id": "Yoga-82", "title": "Yoga-82: A New Dataset for Fine-grained Classification of Human Poses", "contents": "Dataset for large-scale yoga pose recognition with 82 classes.\r\n\r\nSource: [Yoga-82: A New Dataset for Fine-grained Classification of Human Poses](/paper/yoga-82-a-new-dataset-for-fine-grained)", "variants": ["Yoga-82"]}
{"id": "ZEST", "title": "Learning from Task Descriptions", "contents": "A new English language dataset structured for task-oriented evaluation on unseen tasks. \r\n\r\nSource: [Learning from Task Descriptions](/paper/learning-from-task-descriptions)", "variants": ["ZEST"]}
{"id": "CoNSeP", "title": "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images", "contents": "The colorectal nuclear segmentation and phenotypes (CoNSeP) dataset consists of 41 H&E stained image tiles, each of size 1,000×1,000 pixels at 40× objective magnification. The images were extracted from 16 colorectal adenocarcinoma (CRA) WSIs, each belonging to an individual patient, and scanned with an Omnyx VL120 scanner within the department of pathology at University Hospitals Coventry and Warwickshire, UK.\r\n\r\nSource: [HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images](https://arxiv.org/pdf/1812.06499.pdf)\r\nImage Source: [Graham et al](https://arxiv.org/pdf/1812.06499.pdf)", "variants": ["CoNSeP"]}
{"id": "CUHK Image Cropping", "title": "Learning the Change for Automatic Image Cropping", "contents": "**CUHK Image Cropping**  is a dataset for image cropping. The photos are of varying aesthetic quality and span a variety of image categories, including animal, architecture, human, landscape, night, plant and man-made objects. Each image is manually cropped by three expert photographers (graduate students in art whose primary medium is photography) to form three training sets. There are 1,000 photos in the dataset.", "variants": ["CUHK Image Cropping"]}
{"id": "PCN", "title": "", "contents": "**Pedestrian Color Naming (PCN)** is a dataset for pedestrian color naming, which contains 14,213 images, each of which hand-labeled with color label for each pixel. All images in the PCN dataset are obtained from the Market- 1501 dataset.", "variants": ["PCN"]}
{"id": "WWW Crowd", "title": "Deeply Learned Attributes for Crowded Scene Understanding", "contents": "**WWW Crowd** provides 10,000 videos with over 8 million frames from 8,257 diverse scenes, therefore offering a comprehensive dataset for the area of crowd understanding.", "variants": ["WWW Crowd"]}
{"id": "Gutenberg Poem Dataset", "title": "Investigating Societal Biases in a Poetry Composition System", "contents": "Gutenberg Poem Dataset is used for the next verse prediction component.\r\n\r\nSource: [Gutenberg Poem Dataset](https://arxiv.org/pdf/2011.02686.pdf)", "variants": ["Gutenberg Poem Dataset"]}
{"id": "MovieShots", "title": "A Unified Framework for Shot Type Classification Based on Subject Centric Lens", "contents": "MovieShots is a dataset to facilitate the shot type analysis in videos. It is a large-scale shot type annotation set that contains 46K shots from 7,858 movies covering a wide\r\nvariety of movie genres to ensure the inclusion of all scale and movement types of shot. Each shot has two attributes, shot scale and shot movement.\r\n\r\nShot scale has five categories: 1) long shot (LS) is taken from a long distance, sometimes as far as a quarter of a mile away; 2) full shot (FS) barely includes the human body in full; 3) medium shot (MS) contains a figure from the knees or waist up; 4) close-up shot (CS) concentrates on a relatively small object, showing the face of the hand of a person; (5) extreme close-up shot (ECS) shows even smaller parts such as the image of an eye or a mouth.\r\n\r\nShot movement has four categories: 1) in static shot, the camera is fixed but the subject is flexible to move; 2) for motion shot, the camera moves or rotates; 3) the camera zooms in for push shot, and 4) zooms out for pull shot. While all the four movement types are widely used in movies, the use of push and pull shots only takes a very small portion. The usage of different shots usually depends on the movie genres and the preferences of the filmmakers.\r\n\r\nSource: [A Unified Framework for Shot Type Classification Based on Subject Centric Lens](https://anyirao.com/projects/ShotType.html)", "variants": ["MovieShots"]}
{"id": "RDD-2020", "title": "Transfer Learning-based Road Damage Detection for Multiple Countries", "contents": "The Road Damage Dataset 2020 (RDD-2020) Secondly is a large-scale heterogeneous dataset comprising 26620 images collected from multiple countries using smartphones. The images are collected from roads in India, Japan and the Czech Republic.\r\n\r\nSource: [Transfer Learning-based Road Damage Detection for Multiple Countries](https://arxiv.org/abs/2008.13101)", "variants": ["RDD-2020"]}
{"id": "3D-FUTURE", "title": "3D-FUTURE: 3D Furniture shape with TextURE", "contents": "3D-FUTURE (3D FUrniture shape with TextURE) is a 3D dataset that contains 20,240 photo-realistic synthetic images captured in 5,000 diverse scenes, and 9,992 involved unique industrial 3D CAD shapes of furniture with high-resolution informative textures developed by professional designers.\r\n\r\nSource: [3D-FUTURE](https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future)\r\nImage Source: [https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future](https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future)", "variants": ["3D-FUTURE"]}
{"id": "BrixIA", "title": "End-to-end learning for semiquantitative rating of COVID-19 severity on Chest X-rays", "contents": "**BrixIA Covid-19** is a large dataset of CXR images corresponding to the entire amount of images taken for both triage and patient monitoring in sub-intensive and intensive care units during one month (between March 4th and April 4th 2020) of pandemic peak at the ASST Spedali Civili di Brescia, and contains all the variability originating from a real clinical scenario. It includes 4,707 CXR images of COVID-19 subjects, acquired with both CR and DX modalities, in AP or PA projection, and retrieved from the facility RIS-PACS system.", "variants": ["BrixIA"]}
{"id": "MaleX", "title": "", "contents": "**MaleX** is a curated dataset of malware and benign Windows executable samples for malware researchers. The dataset contains 1,044,394 Windows executable binaries with 864,669 labelled as malware and 179,725 as benign. This dataset has reasonable number of samples and is sufficient to test data-driven machine learning classification methods and also to measure the performance of the designed models in terms of scalability and adaptability.", "variants": ["MaleX"]}
{"id": "AIST++", "title": "", "contents": "**AIST++** is a 3D dance dataset which contains 3D motion reconstructed from real dancers paired with music. The AIST++ Dance Motion Dataset is constructed from the AIST Dance Video DB. With multi-view videos, an elaborate pipeline is designed to estimate the camera parameters, 3D human keypoints and 3D human dance motion sequences:\r\n\r\n- It provides 3D human keypoint annotations and camera parameters for 10.1M images, covering 30 different subjects in 9 views. These attributes makes it the largest and richest existing dataset with 3D human keypoint annotations.\r\n- It also contains 1,408 sequences of 3D human dance motion, represented as joint rotations along with root trajectories. The dance motions are equally distributed among 10 dance genres with hundreds of choreographies. Motion durations vary from 7.4 sec. to 48.0 sec. All the dance motions have corresponding music.", "variants": ["AIST++"]}
{"id": "TiMoS", "title": "", "contents": "**Tropes in Movie Synopses (TiMoS)** is a dataset of movie tropes collected from a Wikipedia-style website, TVTropes3 with 5623 movie synopses associated with 95 most occurred tropes. The movies are diverse in genre, filming year, length, and style, making the task challenging and unable to rely on patterns from a specific domain. The tropes involve character trait, role interaction, situation, and storyline, which could be sensed by a non-expert human but remains challenging for machines that have more than 100 million parameters and pre-trained with 11,000 books and the whole\r\nWikipedia (23.97 F1 score while a human could reach 64.87).", "variants": ["TiMoS"]}
{"id": "COCO Earthquake", "title": "End-to-end Deep Learning Methods for Automated Damage Detection in Extreme Events at Various Scales", "contents": "**COCO Earthquake** is a dataset similar to Common Objects in Context (COCO) used for cracking segmentation. The images selected in the dataset are at various scales, and the tool referred to as the COCO Annotator is used to label cracks for training. In these labeled images, cracks are in yellow and background is in purple. Size of the training and labeling images is varied from 168×300 to 4600×3070. By excluding steel structures, 2,021 images are labeled when surface cracks appeared on structural or nonstructural materials at various scales.", "variants": ["COCO Earthquake"]}
{"id": "MM-WHS 2017", "title": "", "contents": "The **MM-WHS 2017** dataset is a dataset for multi-modality whole heart segmentation. It provides 20 labeled and 40 unlabeled CT volumes, as well as 20 labeled and 40 unlabeled MR volumes. In total there are 120 multi-modality cardiac images acquired in a real clinical environment.", "variants": ["Multi-Modality Whole Heart Segmentation Challenge 2017", "MM-WHS 2017"]}
{"id": "EMIDEC", "title": "", "contents": "The **MICCAI 2020 EMIDEC** dataset is a dataset for classifying normal and pathological cases from the clinical information with or without DE-MRI, and secondly to automatically detect the different relevant areas (the myocardial contours, the infarcted area and the permanent microvascular obstruction area (no-reflow area)) from a series of short-axis DE-MRI covering the left ventricle. The segmentation allows one to make a quantification of the MI, in absolute value (mm3) or percentage of the myocardium.\r\n\r\nThe database consists of 150 exams (all from different patients) divided into 50 cases with normal MRI after injection of a contrast agent and 100 cases with myocardial infarction (and then with a hyperenhanced area on DE-MRI), whatever their inclusion in the cardiac emergency department. Along with MRI, clinical characteristics are provided to distinguish normal and pathological cases. The training set has 100 cases.\r\n\r\nLalande, A.; Chen, Z.; Decourselle, T.; Qayyum, A.; Pommier, T.; Lorgis, L.; de la Rosa, E.; Cochet, A.; Cottin, Y.; Ginhac, D.; Salomon, M.; Couturier, R.; Meriaudeau, F. Emidec: A Database Usable for the Automatic Evaluation of Myocardial Infarction from Delayed-Enhancement Cardiac MRI. Data 2020, 5, 89.\r\ndoi: https://doi.org/10.3390/data5040089", "variants": ["EMIDEC"]}
{"id": "WHU-Hi", "title": "WHU-Hi: UAV-borne hyperspectral with high spatial resolution (H2) benchmark datasets for hyperspectral image classification", "contents": "WHU-Hi dataset (Wuhan UAV-borne hyperspectral image) is collected and shared by the RSIDEA research group of Wuhan University, and it could serve as a benchmark dataset for precise crop classification and hyperspectral image classification studies. The WHU-Hi dataset contains three individual UAV-borne hyperspectral datasets: WHU-Hi-LongKou, WHU-Hi-HanChuan, and WHU-Hi-HongHu. All the datasets were acquired in farming areas with various crop types in Hubei province, China, via a Headwall Nano-Hyperspec sensor mounted on a UAV platform. Compared with spaceborne and airborne hyperspectral platforms, unmanned aerial vehicle (UAV)-borne hyperspectral systems can acquire hyperspectral imagery with a high spatial resolution (which we refer to here as H2 imagery). The research was published in Remote Sensing of Environment.\r\n\r\nSource: [http://rsidea.whu.edu.cn/resource_WHUHi_sharing.htm](http://rsidea.whu.edu.cn/resource_WHUHi_sharing.htm)\r\nImage Source: [http://rsidea.whu.edu.cn/resource_WHUHi_sharing.htm](http://rsidea.whu.edu.cn/resource_WHUHi_sharing.htm)", "variants": ["WHU-Hi"]}
{"id": "Botswana", "title": "", "contents": "**Botswana** is a hyperspectral image classification dataset. The NASA EO-1 satellite acquired a sequence of data over the Okavango Delta, Botswana in 2001-2004. The Hyperion sensor on EO-1 acquires data at 30 m pixel resolution over a 7.7 km strip in 242 bands covering the 400-2500 nm portion of the spectrum in 10 nm windows. Preprocessing of the data was performed by the UT Center for Space Research to mitigate the effects of bad detectors, inter-detector miscalibration, and intermittent anomalies. Uncalibrated and noisy bands that cover water absorption features were removed, and the remaining 145 bands were included as candidate features: [10-55, 82-97, 102-119, 134-164, 187-220]. The data analyzed in this study, acquired May 31, 2001, consist of observations from 14 identified classes representing the land cover types in seasonal swamps, occasional swamps, and drier woodlands located in the distal portion of the Delta.\r\n\r\nSource: M Graña, MA Veganzons, B Ayerdi", "variants": ["Botswana"]}
{"id": "Houston", "title": "", "contents": "**Houston** is a hyperspectral image classification dataset. The hyperspectral imagery consists of 144 spectral bands in the 380 nm to 1050 nm region and has been calibrated to at-sensor spectral radiance units, SRU =$latex \\mu \\text{W} /( \\text{cm}^2 \\text{ sr nm})$. The corresponding co-registered DSM consists of elevation in meters above sea level (per the Geoid 2012A model).", "variants": ["Houston"]}
{"id": "Pavia Centre", "title": "", "contents": "**Pavia Centre** is a hyperspectral dataset acquired by the ROSIS sensor during a flight campaign over Pavia, northern Italy. The number of spectral bands is 102 for Pavia Centre. Pavia Centre is a 1096*1096 pixels image. The geometric resolution is 1.3 meters. Image groundtruths differentiate 9 classes each. Pavia scenes were provided by Prof. Paolo Gamba from the Telecommunications and Remote Sensing Laboratory, Pavia university (Italy).", "variants": ["Pavia Centre"]}
{"id": "SEVIR", "title": "SEVIR : A Storm Event Imagery Dataset for Deep Learning Applications in Radar and Satellite Meteorology", "contents": "**SEVIR** is an annotated, curated and spatio-temporally aligned dataset containing over 10,000 weather events that each consist of 384 km x 384 km image sequences spanning 4 hours of time. Images in SEVIR were sampled and aligned across five different data types: three channels (C02, C09, C13) from the GOES-16 advanced baseline imager, NEXRAD vertically integrated liquid mosaics, and GOES-16 Geostationary Lightning Mapper (GLM) flashes. Many events in SEVIR were selected and matched to the NOAA Storm Events database so that additional descriptive information such as storm impacts and storm descriptions can be linked to the rich imagery provided by the sensors.\n\nSource: [https://proceedings.neurips.cc//paper/2020/file/fa78a16157fed00d7a80515818432169-Paper.pdf](https://proceedings.neurips.cc//paper/2020/file/fa78a16157fed00d7a80515818432169-Paper.pdf)", "variants": ["SEVIR"]}
{"id": "ISI-PPT", "title": "", "contents": "This is a Dataset for Arabic/English text detection and optical character recognition. All image data are text-slides extracted from PowerPoint files downloaded from Internet through the Google API.\nAll annotations are automatically generated mainly through the WinCom32 Python API. Postprocess is also applied to place a more accurate text bounding box or to suppress false-alarms, e.g. a text box only containing spaces. Finally, all annotation results are briefly reviewed by human to reject extreme bad samples, e.g. a slide with a large portion of copied table as image. In summary, this dataset contains 10,692 images, and roughly 100K line samples.\n\nSource: [https://gitlab.com/rex-yue-wu/ISI-PPT-Dataset](https://gitlab.com/rex-yue-wu/ISI-PPT-Dataset)\nImage Source: [https://gitlab.com/rex-yue-wu/ISI-PPT-Dataset](https://gitlab.com/rex-yue-wu/ISI-PPT-Dataset)", "variants": ["ISI-PPT"]}
{"id": "QTuna", "title": "QTUNA: A Corpus for Understanding How Speakers Use Quantification", "contents": "The QTUNA dataset is the result of a series of elicitation experiments in which human speakers were asked to perform a linguistic task that invites the use of quantified expressions in order to inform possible Natural Language Generation algorithms that mimic humans' use of quantified expressions.\n\nSource: [https://github.com/a-quei/qtuna](https://github.com/a-quei/qtuna)", "variants": ["QTuna"]}
{"id": "LDDRS", "title": "Full-Time Monocular Road Detection Using Zero-Distribution Prior of Angle of Polarization", "contents": "The **LWIR DoFP Dataset of Road Scene** (**LDDRS**) is a road detection dataset with 2,113 annotated images. It contains both day and night scenes, with multiple cars and pedestrians per image.\n\nSource: [https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700460.pdf](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700460.pdf)", "variants": ["LDDRS"]}
{"id": "Para-Quality", "title": "A Study of Incorrect Paraphrases in Crowdsourced User Utterances", "contents": "Used to investigate common crowdsourced paraphrasing issues and for detecting the quality issues.\r\n\r\nSource: [A Study of Incorrect Paraphrases in Crowdsourced User Utterances](/paper/a-study-of-incorrect-paraphrases-in)", "variants": ["Para-Quality"]}
{"id": "German affixoid dataset", "title": "", "contents": "German affixoids are a type of morpheme in between affixes and free stems.\r\n\r\nSource: [Distinguishing affixoid formations from compounds](https://www.aclweb.org/anthology/C18-1325.pdf)", "variants": ["German affixoid dataset"]}
{"id": "STREETS", "title": "STREETS: A Novel Camera Network Dataset for Traffic Flow", "contents": "A novel traffic flow dataset from publicly available web cameras in the suburbs of Chicago, IL.\r\n\r\nSource: [STREETS: A Novel Camera Network Dataset for Traffic Flow](/paper/streets-a-novel-camera-network-dataset-for)", "variants": ["STREETS"]}
{"id": "CC-DBP", "title": "", "contents": "**CC-DBP** is a dataset for knowledge base population research using Common Crawl and DBpedia.\n\nSource: [https://github.com/IBM/cc-dbp](https://github.com/IBM/cc-dbp)", "variants": ["CC-DBP"]}
{"id": "Earnings Call", "title": "What You Say and How You Say It Matters: Predicting Stock Volatility Using Verbal and Vocal Cues", "contents": "The Earning Calls dataset consists of processed earning conference calls data (text and audio). It can be used to predict financial risk from both textual and vocal features from conference calls.\n\nSource: [https://www.aclweb.org/anthology/P19-1038/](https://www.aclweb.org/anthology/P19-1038/)", "variants": ["Earnings Call"]}
{"id": "Pascal Panoptic Parts", "title": "Cityscapes-Panoptic-Parts and PASCAL-Panoptic-Parts datasets for Scene Understanding", "contents": "The Cityscapes-Panoptic-Parts dataset is an extension of the Cityscapes dataset with panoptic segmentation annotations and par-level labels for selected semantic classes.\n\nSource: [https://arxiv.org/pdf/2004.07944.pdf](https://arxiv.org/pdf/2004.07944.pdf)\nImage Source: [https://github.com/pmeletis/panoptic_parts](https://github.com/pmeletis/panoptic_parts)", "variants": ["Pascal Panoptic Parts"]}
{"id": "needadvice", "title": "Help! Need Advice on Identifying Advice", "contents": "**needadvice** is a dataset for advice classification extracted from Reddit. In this dataset, posts are annotated for whether they contain advice or not. It contains 6,148 samples for training, 816 for validation and 898 for testing.\n\nSource: [https://github.com/venkatasg/Advice-EMNLP2020](https://github.com/venkatasg/Advice-EMNLP2020)", "variants": ["needadvice"]}
{"id": "AuxAI", "title": "Primer AI's Systems for Acronym Identification and Disambiguation", "contents": "**AuxAI** is a distantly supervised dataset for acronym identification.\n\nSource: [https://github.com/PrimerAI/sdu-data](https://github.com/PrimerAI/sdu-data)", "variants": ["AuxAI"]}
{"id": "Snopes", "title": "Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News", "contents": "Fact-checking (FC) articles which contains pairs (multimodal tweet and a FC-article) from snopes.com.\r\n\r\nSource: [Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News](/paper/where-are-the-facts-searching-for-fact)", "variants": ["Snopes"]}
{"id": "DailyDialog++", "title": "Improving Dialog Evaluation with a Multi-reference Adversarial Dataset and Large Scale Pretraining", "contents": "Consists of (i) five relevant responses for each context and (ii) five adversarially crafted irrelevant responses for each context.\r\n\r\nSource: [Improving Dialog Evaluation with a Multi-reference Adversarial Dataset and Large Scale Pretraining](/paper/improving-dialog-evaluation-with-a-multi)", "variants": ["DailyDialog++"]}
{"id": "Multilingual LibriSpeech", "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research", "contents": "Multilingual LibriSpeech is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. \r\n\r\nSource: [MLS: A Large-Scale Multilingual Dataset for Speech Research](/paper/mls-a-large-scale-multilingual-dataset-for)", "variants": ["Multilingual LibriSpeech"]}
{"id": "Action Recognition in the Dark", "title": "ARID: A New Dataset for Recognizing Action in the Dark", "contents": "ARID is a dataset for action recognition in dark videos. It consists of over 3,780 video clips with 11 action categories.\r\n\r\nSource: [ARID: A New Dataset for Recognizing Action in the Dark](/paper/arid-a-new-dataset-for-recognizing-action-in)", "variants": ["Action Recognition in the Dark"]}
{"id": "Skeletics 152", "title": "Quo Vadis, Skeleton Action Recognition ?", "contents": "A curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset.\r\n\r\nSource: [Quo Vadis, Skeleton Action Recognition ?](/paper/quo-vadis-skeleton-action-recognition)", "variants": ["Skeletics-152", "Skeletics 152"]}
{"id": "TVQA+", "title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering", "contents": "TVQA+ contains 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers.\r\n\r\nSource: [TVQA+: Spatio-Temporal Grounding for Video Question Answering](https://arxiv.org/pdf/1904.11574v2.pdf)\r\nImage Source: [https://github.com/jayleicn/TVQAplus](https://github.com/jayleicn/TVQAplus)", "variants": ["TVQA+"]}
{"id": "T2 Guiding", "title": "Understanding Guided Image Captioning Performance across Domains", "contents": "T2 Guiding is a dataset of 1000 images, each with six image labels. The images are from the Open Images Dataset (OID) and the dataset includes 2 sets of machine-generated labels for these images.\r\n\r\n* Object labels: Three random object labels generated by a FRCNN model trained on Visual Genome.\r\n* Image labels: Three random image labels obtained from Google Cloud Vision API.\r\n\r\nSource: [T2-Guiding](https://github.com/google-research-datasets/T2-Guiding)", "variants": ["T2 Guiding"]}
{"id": "FarsBase-KBP", "title": "FarsBase-KBP: A Knowledge Base Population System for the Persian Knowledge Graph", "contents": "FarsBase-KBP contains 22015 sentences, in which the entities and relation types are linked to the FarsBase ontology. This gold dataset can be reused for benchmarking KBP systems in the Persian language.\r\n\r\nSource: [](https://arxiv.org/pdf/2005.01879.pdf)", "variants": ["FarsBase-KBP"]}
{"id": "ROSE", "title": "ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model", "contents": "Retinal OCTA SEgmentation dataset (ROSE) consists of 229 OCTA images with vessel annotations at either centerline-level or pixel level.\r\n\r\nSource: [ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model](https://arxiv.org/pdf/2007.05201.pdf)\r\nImage Source: [https://imed.nimte.ac.cn/dataofrose.html](https://imed.nimte.ac.cn/dataofrose.html)", "variants": ["ROSE"]}
{"id": "UDIVA", "title": "Context-Aware Personality Inference in Dyadic Scenarios: Introducing the UDIVA Dataset", "contents": "UDIVA is a new non-acted dataset of face-to-face dyadic interactions, where interlocutors perform competitive and collaborative tasks with different behavior elicitation and cognitive workload. The dataset consists of 90.5 hours of dyadic interactions among 147 participants distributed in 188 sessions, recorded using multiple audiovisual and physiological sensors. Currently, it includes sociodemographic, self and peer-reported personality, internal state, and relationship profiling from participants. \r\n\r\nSource: [Context-Aware Personality Inference in Dyadic Scenarios: Introducing the UDIVA Dataset](https://arxiv.org/pdf/2012.14259.pdf)", "variants": ["UDIVA"]}
{"id": "LIV360SV", "title": "A deep learning approach to identify unhealthy advertisements in street view images", "contents": "The dataset contains 26,645, 360 degree, street-level images collected via cycling with a GoPro Fusion camera, recorded Jan 14th -- 18th 2020. 10,106 advertisements were identified and classified as food (1335), alcohol (217), gambling (149) and other (8405) (e.g., cars and broadband).\r\n\r\nSource: [A deep learning approach to identify unhealthy advertisements in street view images](https://arxiv.org/pdf/2007.04611)", "variants": ["LIV360SV"]}
{"id": "i3-video", "title": "Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube", "contents": "The i3-video dataset contains \"is-it-instructional\" annotations for 6.4k videos from [Youtube-8M](https://paperswithcode.com/dataset/youtube-8m). The videos are considered to be instructional if they focus on real-world human actions accompanied by procedural language that explains what’s happening on screen in reasonable details.\r\n\r\nSource: [i3-video](https://github.com/google-research-datasets/i3-video)", "variants": ["i3-video"]}
{"id": "ImagiFilter", "title": "ImagiFilter: A resource to enable the semi-automatic mining of images at scale", "contents": "ImagiFilter focusses on photographic and/or natural images, a very common use-case in computer vision research. Annotations for coarse prediction are provided, i.e. photographic vs. non-photographic, and smaller fine-grained prediction tasks where the non-photographic class is broken down into five classes: maps, drawings, graphs, icons, and sketches.\r\n\r\nSource: [ImagiFilter: A resource to enable the semi-automatic mining of images at scale](https://arxiv.org/pdf/2008.09152)", "variants": ["ImagiFilter"]}
{"id": "EPIC30M", "title": "EPIC30M: An Epidemics Corpus Of Over 30 Million Relevant Tweets", "contents": "EPIC30M contains a subset of 26.2 millions tweets related to three general diseases, namely Ebola, Cholera and Swine Flu, and another subset of 4.7 millions tweets of six global epidemic outbreaks, including 2009 H1N1 Swine Flu, 2010 Haiti Cholera, 2012 Middle-East Respiratory Syndrome (MERS), 2013 West African Ebola, 2016 Yemen Cholera and 2018 Kivu Ebola.\r\n\r\nSource: [EPIC30M: An Epidemics Corpus Of Over 30 Million Relevant Tweets](https://arxiv.org/pdf/2006.08369)", "variants": ["EPIC30M"]}
{"id": "The Spoken Wikipedia Corpora", "title": "", "contents": "The SWC is a corpus of aligned Spoken Wikipedia articles from the English, German, and Dutch Wikipedia. This corpus has several outstanding characteristics:\r\n\r\n- hundreds of hours of aligned audio\r\n- from a diverse set of readers\r\n- about a diverse set of topics\r\n- in a well-researched textual genre\r\n- licensed under a free license (CC BY-SA 4.0)\r\n- Annotations can be mapped back to the original html\r\n- phoneme-level alignments", "variants": ["The Spoken Wikipedia Corpora"]}
{"id": "Flickr Audio Caption Corpus", "title": "", "contents": "The Flickr 8k Audio Caption Corpus contains 40,000 spoken captions of 8,000 natural images. It was collected in 2015 to investigate multimodal learning schemes for unsupervised speech pattern discovery. For a description of the corpus, see:\r\n\r\nD. Harwath and J. Glass, \"Deep Multimodal Semantic Embeddings for Speech and Images,\" 2015 IEEE Automatic Speech Recognition and Understanding Workshop, pp. 237-244, Scottsdale, Arizona, USA, December 2015", "variants": ["Flickr Audio Caption Corpus"]}
{"id": "PCVC", "title": "", "contents": "The **Persian Consonant Vowel Combination (PCVC)** dataset is a phoneme based speech dataset, and also the first free Persian speech dataset to help Persian speech researchers. This dataset contains of 23 Persian consonants and 6 vowels. The sound samples are all possible combinations of vowels and consonants (138 samples for each speaker) with a length of 30000 data samples. The sample rate of all speech samples is 48000 which means there are 48000 sound samples in every 1 second. In each sample, sound starts with consonant and then there is a vowel sound and at last there is silent. length of silence is dependent on length of combination of consonant and vowel. For example if combination ends in 20000th data sample, so the rest of 10000 sample (until 30000, the length of each sound sample) are silence.", "variants": ["PCVC"]}
{"id": "2000 HUB5 English", "title": "", "contents": "**2000 HUB5 English Evaluation Transcripts** was developed by the Linguistic Data Consortium (LDC)  and consists of transcripts of 40 English telephone conversations used in the 2000 HUB5 evaluation sponsored by NIST (National Institute of Standards and Technology). \r\n\r\nThe Hub5 evaluation series focused on conversational speech over the telephone with the particular task of transcribing conversational speech into text. Its goals were to explore promising new areas in the recognition of conversational speech, to develop advanced technology incorporating those ideas and to measure the performance of new technology.", "variants": ["Hub5'00 SwitchBoard", "2000 HUB5 English", "SWBD Eval2000"]}
{"id": "Parkinson Speech Dataset", "title": "", "contents": "**Parkinson Speech Dataset** is an audio dataset consisting of recordings of 20 Parkinson's Disease (PD) patients and 20 healthy subjects. From all subjects, multiple types of sound recordings (26) are taken. The goal is to classify which patients have Parkinson's.", "variants": ["Parkinson Speech Dataset"]}
{"id": "Arabic Speech Corpus", "title": "", "contents": "The **Arabic Speech Corpus** (1.5 GB) is a Modern Standard Arabic (MSA) speech corpus for speech synthesis. The corpus contains phonetic and orthographic transcriptions of more than 3.7 hours of MSA speech aligned with recorded speech on the phoneme level. The annotations include word stress marks on the individual phonemes The Speech corpus has been developed as part of PhD work carried out by Nawar Halabi at the University of Southampton. The corpus was recorded in south Levantine Arabic (Damascian accent) using a professional studio. Synthesized speech as an output using this corpus has produced a high quality, natural voice.", "variants": ["Arabic Speech Corpus"]}
{"id": "Mivia Audio Events Dataset", "title": "", "contents": "The **MIVIA audio events** data set is composed of a total of 6000 events for surveillance applications, namely glass breaking, gun shots and screams. The 6000 events are divided into a training set (composed of 4200 events) and a test set (composed of 1800 events).\r\n\r\nIn audio surveillance applications, the events of interest (for instance a scream) can occur at different distances from the microphone that correspond to different levels of the signal-to-noise ratio. Moreover, in these applications the events are generally mixed with a complex background, usually composed of several types of different sounds depending on the specific environments both indoor and outdoor (household appliances, cheering of crowds, talking people, traffic jam, passing cars or motorbikes etc.).\r\n\r\nThe data set is designed to provide each audio event at 6 different values of signal-to-noise ratio (namely 5dB, 10dB, 15dB, 20dB, 25dB and 30dB) and overimposed to different combinations of environmental sounds in order to simulate their occurrence in different ambiences.", "variants": ["Mivia Audio Events Dataset"]}
{"id": "RWCP Sound Scene Database", "title": "", "contents": "The **RWCP Sound Scene Database** includes non-speech sounds recorded in an anechoic room, reconstructed signals in various rooms, impulse responses for a microphone array, speech data recorded with the same array, and recordings of background noises. It is intended for use when simulating sound scenes. It was developed by the Real Acoustic Environments Working Group of the Real World Computing Partnership (RWCP). The data was recorded from 1998 to 2000.", "variants": ["RWCP Sound Scene Database"]}
{"id": "NAR", "title": "", "contents": "**NAR** is a dataset  of audio recordings made with the humanoid robot Nao in real world conditions for sound recognition benchmarking. All the recordings were collected using the robot’s microphone and thus have the following characteristics:\r\n- recorded with low-quality sensors (300 Hz – 18 kHz bandpass)\r\n- suffering from typical fan noise from the robot’s internal hardware\r\n- recorded in mutiple real domestic environments (no special acoustic charateristics, reverberations, presence of multiple sound sources and unknown locations)", "variants": ["NAR"]}
{"id": "IISc VINE", "title": "A Naturalness Evaluation Database for Video Prediction Models", "contents": "Indian Institute of Science VIdeo Naturalness Evaluation (IISc VINE) is a database consisting of 300 videos, obtained by applying different prediction models on different datasets, and accompanying human opinion scores. \r\n\r\nSource: [A Naturalness Evaluation Database for Video Prediction Models](https://arxiv.org/abs/2005.00356)", "variants": ["IISc VINE"]}
{"id": "MineNav", "title": "MineNav: An Expandable Synthetic Dataset Based on Minecraft for Aircraft Visual Navigation", "contents": "MinNav is a synthetic dataset based on the sandbox game Minecraft. The dataset uses several plug-in program to generate rendered image sequences with time-aligned depth maps, surface normal maps and camera poses. Thanks for the large game's community, there is an extremely large number of 3D open-world environment, users can find suitable scenes for shooting and build data sets through it and they can also build scenes in-game. \r\n\r\nSource: [MineNav: An Expandable Synthetic Dataset Based on Minecraft for Aircraft Visual Navigation](https://arxiv.org/pdf/2008.08454)", "variants": ["MineNav"]}
{"id": "Minecraft House", "title": "", "contents": "**Minecraft House** is a crowd sourced dataset that collects examples of humans building houses in Minecraft.  Each user is asked to build a CraftAssist: A Framework for Dialogue-enabled Interactive Agents house on a fixed time budget (30 minutes), without any additional guidance or instructions. Every action of the user is recorded using the Cuberite server.\r\n\r\nThe data collection was performed in Minecraft’s creative mode, where the user is given unlimited resources, has access to all material block types and can freely move in\r\nthe game world. The action space of the environment is straight-forward: moving in x-y-z dimensions, choosing a block type, and placing or breaking a block.\r\n\r\nThere are 2586 houses in total.", "variants": ["Minecraft House"]}
{"id": "CryoNuSeg", "title": "CryoNuSeg: A Dataset for Nuclei Instance Segmentation of Cryosectioned H&E-Stained Histological Images", "contents": "CryoNuSeg is a fully annotated FS-derived cryosectioned and H&E-stained nuclei instance segmentation dataset. The dataset contains images from 10 human organs that were not exploited in other publicly available datasets, and is provided with three manual mark-ups to allow measuring intra-observer and inter-observer variability.\r\n\r\nSource: [CryoNuSeg: A Dataset for Nuclei Instance Segmentation of Cryosectioned H&E-Stained Histological Images](https://arxiv.org/pdf/2101.00442.pdf)\r\nImage Source: [https://github.com/masih4/CryoNuSeg](https://github.com/masih4/CryoNuSeg)", "variants": ["CryoNuSeg"]}
{"id": "Princeton Shape", "title": "", "contents": "The **Princeton Shape** dataset provides a repository of 3D models and software tools for evaluating shape-based retrieval and analysis algorithms.  The motivation is to promote the use of standardized data sets and evaluation methods for research in matching, classification, clustering, and recognition of 3D models.  Researchers are encouraged to use these resources to produce comparisons of competing algorithms in future publications. There are 1,814 models in total.", "variants": ["Princeton Shape"]}
{"id": "IKEA 3D", "title": "", "contents": "**IKEA 3D** is a dataset of IKEA 3D models and aligned images, which is suitable for pose estimation. There are 759 images and 219 models including Sketchup (skp) and Wavefront (obj) files.", "variants": ["IKEA 3D"]}
{"id": "RSOC", "title": "Counting from Sky: A Large-scale Dataset for Remote Sensing Object Counting and A Benchmark Method", "contents": "RSOC is a large-scale object counting dataset with remote sensing images, which contains four important geographic objects: buildings, crowded ships in harbors, large-vehicles and small-vehicles in parking lots.\r\n\r\nSource :[Counting from Sky: A Large-scale Dataset for Remote Sensing Object Counting and A Benchmark Method](https://arxiv.org/pdf/2008.12470)", "variants": ["RSOC"]}
{"id": "RGRS", "title": "Presenting a Dataset for Collaborator Recommending Systems in Academic Social Network: a Case Study on ReseachGate", "contents": "RGRS is a dataset for collaboratior recommendation on the ResearchGate academic social network. The data has been collected from Jan. 2019 to April 2019 and\r\nincludes raw data of 3980 RG users. \r\n\r\nSource: [Presenting a Dataset for Collaborator Recommending Systems in Academic Social Network: a Case Study on ReseatchGate](https://arxiv.org/abs/2101.01141)", "variants": ["RGRS"]}
{"id": "ObjectNet3D", "title": "", "contents": "**ObjectNet3D** is a large scale database for 3D object recognition, named, that consists of 100 categories, 90,127 images, 201,888 objects in these images and 44,147 3D shapes. Objects in the images in the database are aligned with the 3D shapes, and the alignment provides both accurate 3D pose annotation and the closest 3D shape annotation for each 2D object. Consequently, the database is useful for recognizing the 3D pose and 3D shape of objects from 2D images. Authors also provide baseline experiments on four tasks: region proposal generation, 2D object detection, joint 2D detection and 3D object pose estimation, and image-based 3D shape retrieval, which can serve as baselines for future research.", "variants": ["ObjectNet3D"]}
{"id": "Event-Stream Dataset", "title": "Event-based Robotic Grasping Detection with Neuromorphic Vision Sensor and Event-Stream Dataset", "contents": "Event-Stream Dataset is a robotic grasping dataset with 91 objects.\r\n\r\nSource: [Event-based Robotic Grasping Detection with Neuromorphic Vision Sensor and Event-Stream Dataset](https://arxiv.org/pdf/2004.13652)", "variants": ["Event-Stream Dataset"]}
{"id": "Thingi10K", "title": "", "contents": "**Thingi10K** is a dataset of 3D-Printing Models. Specifically there are 10,000 models from featured “things” on thingiverse.com, suitable for testing 3D printing techniques such as structural analysis , shape optimization, or solid geometry operations.", "variants": ["Thingi10K"]}
{"id": "MSAW", "title": "SpaceNet 6: Multi-Sensor All Weather Mapping Dataset", "contents": "Multi-Sensor All Weather Mapping (MSAW) is a dataset and challenge, which features two collection modalities (both SAR and optical). The dataset and challenge focus on mapping and building footprint extraction using a combination of these data sources. MSAW covers 120 km^2 over multiple overlapping collects and is annotated with over 48,000 unique building footprints labels, enabling the creation and evaluation of mapping algorithms for multi-modal data. \r\n\r\nSource: [SpaceNet 6: Multi-Sensor All Weather Mapping Dataset](https://arxiv.org/pdf/2004.06500)", "variants": ["MSAW"]}
{"id": "3D-FRONT", "title": "", "contents": "**3D-FRONT** (3D Furnished Rooms with layOuts and semaNTics) is large-scale, and comprehensive repository of synthetic indoor scenes highlighted by professionally designed layouts and a large number of rooms populated by high-quality textured 3D models with style compatibility. From layout semantics down to texture details of individual objects, the dataset is freely available to the academic community and beyond. \r\n\r\n3D-FRONT contains 18,797 rooms diversely furnished by 3D objects. In addition, the 7,302 furniture objects all come with high-quality textures. While the floorplans and layout designs are directly sourced from professional creations, the interior designs in terms of furniture styles, color, and textures have been carefully curated based on a recommender system to attain consistent styles as expert designs.", "variants": ["3D-FRONT"]}
{"id": "3ThreeDWorld", "title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation", "contents": "**TDW** is a 3D virtual world simulation platform, utilizing state-of-the-art video game engine technology. A TDW simulation consists of two components: a) the Build, a compiled executable running on the Unity3D Engine, which is responsible for image rendering, audio synthesis and physics simulations; and b) the Controller, an external Python interface to communicate with the build.", "variants": ["3ThreeDWorld"]}
{"id": "RealEstate10K", "title": "", "contents": "**RealEstate10K** is a large dataset of camera poses corresponding to 10 million frames derived from about 80,000 video clips, gathered from about 10,000 YouTube videos. For each clip, the poses form a trajectory where each pose specifies the camera position and orientation along the trajectory. These poses are derived by running SLAM and bundle adjustment algorithms on a large set of videos.", "variants": ["RealEstate10K"]}
{"id": "WildestFaces", "title": "Red Carpet to Fight Club: Partially-supervised Domain Transfer for Face Recognition in Violent Videos", "contents": "WildestFaces is tailored to study cross-domain recognition under a variety of adverse conditions. \r\n\r\nSource: [Red Carpet to Fight Club: Partially-supervised Domain Transfer for Face Recognition in Violent Videos](https://arxiv.org/pdf/2009.07576)", "variants": ["WildestFaces"]}
{"id": "FAD", "title": "Predicting Personal Traits from Facial Images using Convolutional Neural Networks Augmented with Facial Landmark Information", "contents": "FAD is a dataset that have roughly 200,000 attribute labels for the above traits, for over 10,000 facial images.\r\n\r\nSource: [Predicting Personal Traits from Facial Images using Convolutional Neural Networks Augmented with Facial Landmark Information](https://arxiv.org/pdf/1605.09062)", "variants": ["FAD"]}
{"id": "PHSPD", "title": "Polarization Human Shape and Pose Dataset", "contents": "PHSPD is a home-grown polarization image dataset of various human shapes and poses.\r\n\r\nSource: [Polarization Human Shape and Pose Dataset](https://arxiv.org/pdf/2004.14899)", "variants": ["PHSPD"]}
{"id": "OC20", "title": "", "contents": "**Open Catalyst 2020** is a dataset for catalysis in chemical engineering. Focusing on molecules that are important in renewable energy applications, the OC20 data set comprises over 1.3 million relaxations of molecular adsorptions onto surfaces, the largest data set of electrocatalyst structures to date.", "variants": ["OC20"]}
{"id": "DUS", "title": "", "contents": "The **Daimler Urban Segmentation Dataset** is a dataset for semantic segmentation. It consists of video sequences recorded in urban traffic. The dataset consists of 5000 rectified stereo image pairs with a resolution of 1024x440. 500 frames (every 10th frame of the sequence) come with pixel-level semantic class annotations into 5 classes: ground, building, vehicle, pedestrian, sky. Dense disparity maps are provided as a reference, however these are not manually annotated but computed using semi-global matching (sgm).", "variants": ["DUS"]}
{"id": "HumanAct12", "title": "Action2Motion: Conditioned Generation of 3D Human Motions", "contents": "**HumanAct12** is a new 3D human motion dataset adopted from the polar image and 3D pose dataset PHSPD, with proper temporal cropping and action annotating. Statistically, there are 1191 3D motion clips(and 90,099 poses in total) which are categorized into 12 action classes, and 34 fine-grained sub-classes. The action types includes daily actions such as walk, run, sit down, jump up, warm up, etc. Fine-grained action types contain more specific information like Warm up by bowing left side, Warm up by pressing left leg, etc. \r\n\r\nSource: [Action2Motion: Conditioned Generation of 3D Human Motions](https://ericguo5513.github.io/action-to-motion/)", "variants": ["HumanAct12"]}
{"id": "Interestingness", "title": "", "contents": "The **Interestingness** dataset contains movie excerpts and key-frames and corresponding ground truth files based on classification into interesting and non-interesting samples. It is used for multimedia content interestingness classification. The dataset is composed of:\r\n\r\n- Shots and key-frames from a set of 78 Hollywood-like movie trailers of different genres\r\n- The corresponding ground truth\r\n- Additional low-level and mid-level features", "variants": ["Interestingness"]}
{"id": "LASIESTA", "title": "", "contents": "**LASIESTA (Labeled and Annotated Sequences for Integral Evaluation of SegmenTation Algorithms)** is a segmentation and detection dataset composed by many real indoor and outdoor sequences organized into categories, each of one covering a specific challenge in moving object detection strategies.", "variants": ["LASIESTA"]}
{"id": "FIRE", "title": "", "contents": "**Fundus Image Registration Dataset (FIRE)** is a dataset consisting of 129 retinal images forming 134 image pairs. These image pairs are split into 3 different categories depending on their characteristics. The images were acquired with a Nidek AFC-210 fundus camera, which acquires images with a resolution of 2912x2912 pixels and a FOV of 45° both in the x and y dimensions. Images were acquired at the Papageorgiou Hospital, Aristotle University of Thessaloniki, Thessaloniki from 39 patients.", "variants": ["FIRE"]}
{"id": "300-VW", "title": "", "contents": "**300 Videos in the Wild (300-VW)** is a dataset for evaluating facial landmark tracking algorithms in the wild. The dataset authors collected a large number of long facial videos recorded in the wild. Each video has duration of ~1 minute (at 25-30 fps). All frames have been annotated with regards to the same mark-up (i.e. set of facial landmarks) used in the 300 W competition as well (a total of 68 landmarks). The dataset includes 114 videos (circa 1 min each).", "variants": ["300-VW (C)", "300-VW"]}
{"id": "Imp1k", "title": "Predicting Visual Importance Across Graphic Design Types", "contents": "Imp1k is a new dataset of designs annotated with importance information.\r\n\r\nSource: [Predicting Visual Importance Across Graphic Design Types](https://arxiv.org/pdf/2008.02912)", "variants": ["Imp1k"]}
{"id": "PIROPO", "title": "", "contents": "The **PIROPO database** (People in Indoor ROoms with Perspective and Omnidirectional cameras) comprises multiple sequences recorded in two different indoor rooms, using both omnidirectional and perspective cameras. The sequences contain people in a variety of situations, including people walking, standing, and sitting. Both annotated and non-annotated sequences are provided, where ground truth is point-based (each person in the scene is represented by the point located in the center of its head). In total, more than 100,000 annotated frames are available.", "variants": ["PIROPO"]}
{"id": "Million-AID", "title": "DiRS: On Creating Benchmark Datasets for Remote Sensing Image Interpretation", "contents": "Million-AID is a large-scale benchmark dataset containing million instances for RS scene classification.\r\n\r\nSource: [DiRS: On Creating Benchmark Datasets for Remote Sensing Image Interpretation](https://arxiv.org/pdf/2006.12485)\r\nImage Source: [https://captain-whu.github.io/DiRS/](https://captain-whu.github.io/DiRS/)", "variants": ["Million-AID"]}
{"id": "Edge Milling Heads", "title": "", "contents": "The **Edge Milling Heads** data set comprises 144 images of an edge profile cutting head of a milling machine. The head tool contains a total of 30 cutting inserts. The cutting head is formed by 6 diagonals of inserts in radial direction along the tool perimeter, encompassing 5 inserts per diagonal in axial direction. Positions of the last and first inserts of consecutive diagonals are aligned in the same vertical. Therefore, even though there are 30 inserts in total, there are 24 equally spaced positions of inserts along the tool perimeter. Additionally, inserts are squared shape with four 90º indexable cutting edges. Inserts are fastened with a screw. Rake angle is 0.\r\n\r\nImages were taken with a monochrome camera Genie M1280 1/3’’ with active resolution of 1280 × 960 pixels. AZURE-2514MM fixed lens with 25 mm focal length were used.\r\n\r\nSource: [Computer Vision Online](https://computervisiononline.com/dataset/1105138755)", "variants": ["Edge Milling Heads"]}
{"id": "VxC TSG", "title": "", "contents": "The **VXC TSG** is based on samples taken from the ceramic tile industry and is comprised of 14 ceramic tile models, 42 surface grades and 960 pieces. It has been built in the VxC laboratory, at the Polytechnic University of Valencia, in collaboration with Keraben S.A., a large ceramic tile company located at Nules province of Castellón (Spain).\r\n\r\nSource: [Computer Vision Online](https://computervisiononline.com/dataset/1105138707)", "variants": ["VxC TSG"]}
{"id": "Panoramic Image Database", "title": "", "contents": "The **Panoramic Image Database** is a panoramic image dataset. The databases were collected by Andrew Vardy while visiting with the Computer Engineering group in February and March of 2004. Images were captured by a robot-mounted camera, pointed upwards at a hyperbolic mirror. The camera was an ImagingSource DFK 4303. The robot was an ActivMedia Pioneer 3-DX. The mirror was a large wide-view hyperbolic mirror from Accowle Ltd. The hyperbolic mirror expands the camera's field of view to allow the capture of panoramic images.", "variants": ["Panoramic Image Database"]}
{"id": "OTCBVS", "title": "", "contents": "**OCTCBVS** is a benchmark dataset for testing and evaluating novel and state-of-the-art computer vision algorithms. The benchmark contains videos and images recorded in and beyond the visible spectrum and is available for free to all researchers in the international computer vision communities.\r\n\r\nSource: [Computer Vision Online](https://computervisiononline.com/dataset/1105138704)", "variants": ["OTCBVS"]}
{"id": "DogCentric Activity", "title": "", "contents": "The **DogCentric Activity** dataset is composed of dog activity videos taken from a first-person animal viewpoint. The dataset contains 10 different types of activities, including activities performed by the dog himself/herself, interactions between people and the dog, and activities performed by people or cars. \r\n\r\nThe authors attached a GoPro camera to the back of each of the four dogs, and their owners took them on a walk to their familiar walking routes. The walking routes are in various environments, such as residential area, a park along a river, a sand beach, a field, streets with traffic, etc. Thus even though different dogs do the same activity, their background varies.\r\n\r\nThe video contains various activities with 10 activities of interest chosen as target activities: 'playing with a ball', 'waiting for a car to passed by', 'drinking water', 'feeding', 'turning dog's head to the left', 'turning dog's head to the right', 'petting', 'shaking dog's body by himself', 'sniffing', and 'walking'. The videos are in 320*240 image resolution, 48 frames per second.", "variants": ["DogCentric Activity"]}
{"id": "Biwi Kinect Head Pose", "title": "", "contents": "Biwi Kinect Head Pose is a challenging dataset mainly inspired by the automotive setup.  It is acquired with the Microsoft Kinect sensor, a structured IR light device. It contains about 15k frame, with RGB. (640 × 480) and depth maps (640 × 480). Twenty subjects have been involved in the recordings: four of them were recorded twice, for a total of 24 sequences. The ground truth of yaw, pitch and roll angles is reported together with the head center and the calibration matrix. \r\n\r\nSource: [POSEidon: Face-from-Depth for Driver Pose Estimation](/paper/poseidon-face-from-depth-for-driver-pose)", "variants": ["Biwi Kinect Head Pose"]}
{"id": "SVLD", "title": "A Dataset and Benchmarks for Multimedia Social Analysis", "contents": "The social vision and language dataset is a large-scale multimodal dataset designed for research into social contextual learning.\r\n\r\nSource: [A Dataset and Benchmarks for Multimedia Social Analysis](/paper/a-dataset-and-benchmarks-for-multimedia)\r\nImage Source: [https://cannylab.github.io/svld/](https://cannylab.github.io/svld/)", "variants": ["SVLD"]}
{"id": "CMU Wilderness Multilingual Speech Dataset", "title": "", "contents": "The CMU Wilderness Multilingual Speech Dataset is a dataset of over 700 different languages providing audio, aligned text and word pronunciations. On average each language provides around 20 hours of sentence-lengthed transcriptions. \r\n\r\nSource: [Alan W Black \"CMU Wilderness Multilingual Speech Dataset\" ICASSP 2019, Brighton, UK.](https://ieeexplore.ieee.org/document/8683536)", "variants": ["CMU Wilderness Multilingual Speech Dataset"]}
{"id": "Aesthetic Visual Analysis", "title": "", "contents": "**Aesthetic Visual Analysis** is a dataset for aesthetic image assessment that contains over 250,000 images along with a rich variety of meta-data including a\r\nlarge number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style.", "variants": ["AVA", "Aesthetic Visual Analysis"]}
{"id": "BigBIRD", "title": "", "contents": "BigBIRD is a 3D dataset of 125 objects, with the following data for each object:\r\n\r\n* 600 12 megapixel images, sampling the viewing hemisphere\r\n* 600 registered RGB-D point clouds from a Carmine 1.09 sensor\r\n* Pose information for each of the above images and point clouds\r\n* Segmentation masks for each of the above images (and segmented point clouds)\r\n* Merged point clouds consisting of data from all 600 viewpoints\r\n* Reconstructed meshes from the merged point clouds\r\n\r\nPaper: [ICRA 2014 \"A Large-Scale 3D Database of Object Instances.\"](https://people.eecs.berkeley.edu/~pabbeel/papers/2014-ICRA-BigBIRD.pdf)", "variants": ["BigBIRD"]}
{"id": "CUHK Square Dataset", "title": "", "contents": "CUHK Square data set is for transfer learning research on adapting generic pedestrian detectors. It includes a traffic video sequence of 60 minutes long. It is recorded by a stationary camera. The size of the scene is 720 by 576. \r\n\r\nIn order to evaluate the performance of human detection on this data set, ground truth of pedestrians of some sampled frames are manually labeled. \r\n\r\nPaper Source: [Transferring a generic pedestrian detector towards specific scenes](https://doi.org/10.1109/CVPR.2012.6248064)\r\n\r\nSource: [CUHK Square Dataset](http://mmlab.ie.cuhk.edu.hk/datasets/cuhk_square/index.html)\r\n\r\nImage Source: [CUHK Square Dataset](http://mmlab.ie.cuhk.edu.hk/datasets/cuhk_square/index.html)", "variants": ["CUHK Square Dataset"]}
{"id": "CUHK Occlusion Dataset", "title": "", "contents": "CUHK occlusion dataset includes 1,063 images with occluded pedestrians. It is used for Human Detection with occlusion handling in crowded scenes.\r\n\r\nPaper: [A discriminative deep model for pedestrian detection with occlusion handling](https://doi.org/10.1109/CVPR.2012.6248062)\r\n\r\nSource: [CUHK Occlusion Dataset](http://mmlab.ie.cuhk.edu.hk/datasets/cuhk_occlusion/index.html)\r\n\r\nImage Source: [CUHK Occlusion Dataset](http://mmlab.ie.cuhk.edu.hk/datasets/cuhk_occlusion/index.html)", "variants": ["CUHK Occlusion Dataset"]}
{"id": "Grand Central Station Dataset", "title": "", "contents": "The Grand central station dataset includes a video with 50,010 frames which is used for Scene Understanding and Crowd Analysis.\r\n\r\nPaper: [Understanding collective crowd behaviors: Learning a Mixture model of Dynamic pedestrian-Agents](https://doi.org/10.1109/CVPR.2012.6248013)\r\n\r\nSource: [Train Station Dataset](http://www.ee.cuhk.edu.hk/~xgwang/grandcentral.html)\r\n\r\nImage Source: [Train Station Dataset](http://www.ee.cuhk.edu.hk/~xgwang/grandcentral.html)", "variants": ["Grand Central Station Dataset"]}
{"id": "ArtEmis", "title": "ArtEmis: Affective Language for Visual Art", "contents": "ArtEmis is a large-scale dataset aimed at providing a detailed understanding of the interplay between visual content, its emotional effect, and explanations for the latter in language. In contrast to most existing annotation datasets in computer vision, this dataset focuses on the affective experience triggered by visual artworks an the annotators were asked to indicate the dominant emotion they feel for a given image and, crucially, to also provide a grounded verbal explanation for their emotion choice. This leads to a rich set of signals for both the objective content and the affective impact of an image, creating associations with abstract concepts (e.g., “freedom” or “love”), or references that go beyond what is directly visible, including visual similes and metaphors, or subjective references to personal experiences. \r\n\r\nThis dataset focuses on visual art (e.g., paintings, artistic photographs) as it is a prime example of imagery created to elicit emotional responses from its viewers. ArtEmis contains 439K emotion attributions and explanations from humans, on 81K artworks from WikiArt.\r\n\r\nPaper: [ArtEmis: Affective Language for Visual Art](https://arxiv.org/abs/2101.07396)\r\n\r\nSource: [ArtEmis Dataest](https://www.artemisdataset.org/)\r\n\r\nImage Source: [ArtEmis Dataest](https://www.artemisdataset.org/)", "variants": ["ArtEmis"]}
{"id": "BreakHis", "title": "", "contents": "The Breast Cancer Histopathological Image Classification (BreakHis) is  composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients using different magnifying factors (40X, 100X, 200X, and 400X).  It contains 2,480  benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format). This database has been built in collaboration with the P&D Laboratory - Pathological Anatomy and Cytopathology, Parana, Brazil.\r\n\r\nPaper: [F. A. Spanhol, L. S. Oliveira, C. Petitjean and L. Heutte, \"A Dataset for Breast Cancer Histopathological Image Classification,\" in IEEE Transactions on Biomedical Engineering, vol. 63, no. 7, pp. 1455-1462, July 2016, doi: 10.1109/TBME.2015.2496264](https://doi.org/10.1109/TBME.2015.2496264)\r\n\r\nSource: [https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/](https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/)\r\n\r\nImage Source: [https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/](https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/)", "variants": ["BreakHis"]}
{"id": "2D Hela", "title": "", "contents": "2D HeLa is a dataset of fluorescence microscopy images of HeLa cells stained with various organelle-specific fluorescent dyes. The images include 10 organelles, which are DNA (Nuclei), ER (Endoplasmic reticulum), Giantin, (cis/medial Golgi), GPP130 (cis Golgi), Lamp2 (Lysosomes), Mitochondria, Nucleolin (Nucleoli), Actin, TfR (Endosomes), Tubulin.\r\nThe purpose of the dataset is to train a computer program to automatically identify sub-cellular organelles.\r\n\r\n\r\nPaper: [M. V. Boland and R. F. Murphy (2001). A Neural Network Classifier Capable of Recognizing the Patterns of all Major Subcellular Structures in Fluorescence Microscope Images of HeLa Cells. Bioinformatics 17:1213-1223](https://doi.org/10.1093/bioinformatics/17.12.1213)\r\n\r\nSource: [Identifying Sub-cellular Organelles](https://ome.grc.nia.nih.gov/iicbu2008/hela/index.html)\r\n\r\nImage Source: [Identifying Sub-cellular Organelles](https://ome.grc.nia.nih.gov/iicbu2008/hela/index.html)", "variants": ["2D Hela"]}
{"id": "PointPattern", "title": "Path Integral Based Convolution and Pooling for Graph Neural Networks", "contents": "PointPattern is a graph classification dataset constructed by simple point patterns from statistical mechanics. The authors simulated three point patterns in 2D: hard disks in equilibrium (HD), Poisson point process, and random sequential adsorption (RSA) of disks. The HD and Poisson distributions can be seen as simple models that describe the microstructures of liquids and gases while the RSA is a nonequilibrium stochastic process that introduces new particles one by one subject to nonoverlapping conditions. \r\n\r\nThese systems are well known to be structurally different, while being easy to simulate, thus provides a solid and controllable classification task. For each point pattern, the\r\nparticles are treated as nodes, and edges are subsequently drawn according to whether two particles are within a threshold distance.\r\n\r\nSource: [Path Integral Based Convolution and Pooling for Graph Neural Networks](https://arxiv.org/pdf/2006.16811.pdf)\r\nImage Source: [Path Integral Based Convolution and Pooling for Graph Neural Networks](https://arxiv.org/pdf/2006.16811.pdf)", "variants": ["PointPattern"]}
{"id": "Humans in 3D", "title": "", "contents": "H3D (Humans in 3D) is a dataset of annotated people. The annotations include:\r\n\r\n* The joints and other keypoints (eyes, ears, nose, shoulders, elbows, wrists, hips, knees and ankles)\r\n* The 3D pose inferred from the keypoints.\r\n* Visibility boolean for each keypoint\r\n* Region annotations (upper clothes, lower clothes, dress, socks, shoes, hands, gloves, neck, face, hair, hat, sunglasses, bag, occluder)\r\n* Body type (male, female or child)\r\n\r\nPaper: [Poselets: Body part detectors trained using 3D human pose annotations](https://doi.org/10.1109/ICCV.2009.5459303)\r\n\r\nSource: [H3D](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/h3d/)\r\n\r\nImage Source: [H3D](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/h3d/)", "variants": ["H3D", "Humans in 3D"]}
{"id": "BelgaLogos", "title": "", "contents": "BelgaLogos is a dataset for logo detection and recognition. The images of BelgaLogos dataset have been provided and are copyrighted by BELGA press agency. They are freely available for research purpose only. The dataset is composed of 10,000 images covering all aspects of life and current affairs: politics and economics, finance and social affairs, sports, culture and personalities. All images are in JPEG format and have been re-sized with a maximum value of height and width equal to 800 pixels, preserving aspect ratio. \r\n\r\nPaper: [Alexis Joly and Olivier Buisson, Logo retrieval with a contrario visual query expansion, In Proceedings of the Seventeen ACM international Conference on Multimedia, 2009.](https://doi.org/10.1145/1631272.1631361)\r\n\r\nSource: [BelgaLogos dataset](http://www-sop.inria.fr/members/Alexis.Joly/BelgaLogos/BelgaLogos.html)\r\n\r\nImage Source: [BelgaLogos dataset](http://www-sop.inria.fr/members/Alexis.Joly/BelgaLogos/BelgaLogos.html)", "variants": ["BelgaLogos"]}
{"id": "Aspects dataset", "title": "", "contents": "This dataset contains video shots for two different classes: tigers and cars. The shots were collected from 188 car ads (~1–2 min each) and 14 nature documentaries about tigers (~40 min), amounting to roughly 14 h of video. The videos were partitioned into shorter shots, and only those showing at least one instance of the class were kept. This produced 806 shots for the car and 1880 for the tiger class, typically 1–100 sec in length.\r\n\r\nPaper: [Discovering object aspects from video](https://doi.org/10.1016/j.imavis.2016.04.014)\r\n\r\nSource: [Aspects dataset](http://calvin-vision.net/aspects-dataset/)\r\n\r\nImage Source: [Aspects dataset](http://calvin-vision.net/aspects-dataset/)", "variants": ["Aspects dataset"]}
{"id": "POET", "title": "", "contents": "The POET (Pascal Objects Eye Tracking) is a dataset that consists of eye tracking data for the complete trainval set of ten objects classes (cat, dog, bicycle, motorbike, boat, aeroplane, horse, cow, sofa, dining table) from [Pascal VOC 2012](pascal-voc) (6,270 images in total). Each image is annotated with the eye movement record of five participants, whose task was to identify which object class was present in the image.\r\n\r\nPaper: [Training object class detectors from eye tracking data](https://doi.org/10.1007/978-3-319-10602-1_24)\r\n\r\nSource: [Pascal Objects Eye Tracking (POET) v1.1](http://calvin-vision.net/datasets/poet-dataset/)\r\n\r\nImage Source: [Pascal Objects Eye Tracking (POET) v1.1](http://calvin-vision.net/datasets/poet-dataset/)", "variants": ["POET"]}
{"id": "AMUSE", "title": "", "contents": "The automotive multi-sensor (AMUSE) dataset consists of inertial and other complementary sensor data combined with monocular, omnidirectional, high frame rate visual data taken in real traffic scenes during multiple test drives.\r\n\r\nPaper: [A Multi-sensor Traffic Scene Dataset with Omnidirectional Video](https://doi.org/10.1109/CVPRW.2013.110)\r\n\r\nSource: [AMUSE](http://www.cvl.isy.liu.se/en/research/datasets/amuse/)\r\n\r\nImage Source: [AMUSE](http://www.cvl.isy.liu.se/en/research/datasets/amuse/)", "variants": ["AMUSE"]}
{"id": "IMO", "title": "", "contents": "Dataset of annotated independently moving objects (IMO). This dataset contains left and right images, stereo images, stereo disparity from SGM, and vehicle labels as well as a ground truth annotations. \r\n\r\nPaper: [Independently Moving Object Trajectories from Sequential Hierarchical Ransac](https://users.isy.liu.se/cvl/perfo/abstracts/persson21.html)\r\n\r\nSource: [IMO Dataset](http://www.cvl.isy.liu.se/en/research/datasets/imo-dataset/)\r\n\r\nImage Source: [IMO Dataset](http://www.cvl.isy.liu.se/en/research/datasets/imo-dataset/)", "variants": ["IMO"]}
{"id": "LTIR", "title": "", "contents": "The LTIR dataset is a thermal infrared dataset for evaluation of Short-Term Single-Object (STSO) tracking.\r\n\r\nThe dataset contains\r\n\r\n* 20 thermal infrared sequences, one .png per frame. Some sequences are available in both 8- and 16-bits.\r\n* Bounding box annotations of one object per sequence.\r\n* Local per-frame annotations.\r\n\r\nPaper: [A thermal Object Tracking benchmark](https://doi.org/10.1109/AVSS.2015.7301772)\r\n\r\nSource: [The LTIR dataset v1.0](http://www.cvl.isy.liu.se/en/research/datasets/ltir/version1.0/)\r\n\r\nImage Source: [The LTIR dataset v1.0](http://www.cvl.isy.liu.se/en/research/datasets/ltir/version1.0/)", "variants": ["LTIR"]}
{"id": "Family101", "title": "", "contents": "The Family101 dataset is the a large-scale dataset of families across several generations. It contains 101 different families with distinct family names, including 206 nuclear families, 607 individuals, with 14,816 images. The dataset are composed of renowned public families.\r\n\r\nPaper: [Kinship Classification by Modeling Facial Feature Heredity People](https://doi.org/10.1109/ICIP.2013.6738614)\r\n\r\nSource: [Kinship Classification by Modeling Facial Feature Heredity People](http://chenlab.ece.cornell.edu/projects/KinshipClassification/index.html)\r\n\r\nImage Source: [Kinship Classification by Modeling Facial Feature Heredity People](http://chenlab.ece.cornell.edu/projects/KinshipClassification/index.html)", "variants": ["Family101"]}
{"id": "KinFaceW", "title": "", "contents": "KinFaceW consists of two kinship datasets: KinFaceW-I and KinFaceW-II. Face images were collected from the internet, including some public figure face images as well as their parents' or children's face images. Face images are captured under uncontrolled environments in two datasets with no restriction in terms of pose, lighting, background, expression, age, ethnicity, and partial occlusion. The difference of KinFaceW-I and KinFaceW-II is that face images with a kin relation were acquired from different photos in KinFaceW-I and the same photo in KinFaceW-II in most cases.\r\n\r\nPaper: [Neighborhood Repulsed Metric Learning for Kinship Verification](https://doi.org/10.1109/CVPR.2012.6247978)\r\n\r\nSource: [KinFaceW Database](https://www.kinfacew.com/datasets.html)\r\n\r\nImage Source: [KinFaceW Database](https://www.kinfacew.com/datasets.html)", "variants": ["KinFaceW"]}
{"id": "Boxy", "title": "", "contents": "A large vehicle detection dataset with almost two million annotated vehicles for training and evaluating object detection methods for self-driving cars on freeways.\r\n\r\nThe dataset consists of:\r\n\r\n* 200,000 images\r\n* 1,990,000 annotated vehicles\r\n* 5 Megapixel resolution\r\n* Sunshine, rain, dusk, night\r\n* Clear freeways, heavy traffic, traffic jams\r\n\r\nPaper: [Boxy Vehicle Detection in Large Images](https://doi.org/10.1109/ICCVW.2019.00112)\r\n\r\nSource: [Boxy](https://boxy-dataset.com/boxy/)\r\n\r\nImage Source: [Boxy](https://boxy-dataset.com/boxy/)", "variants": ["Boxy"]}
{"id": "FRIDA", "title": "", "contents": "FRIDA and FRIDA2 are databases of numerical images easily usable to evaluate in a systematic way the performance of visibility and contrast restoration algorithms. FRIDA comprises 90 synthetic images of 18 urban road scenes. FRIDA2 comprises 330 synthetic images of 66 diverse road scenes. The view point is closed to the one of the vehicle's driver. To each image without fog is associated 4 foggy images and a depthmap. Different kind of fog are added on each of the 4 associated images: uniform fog, heterogeneous fog, cloudy fog, and cloudy heterogeneous fog. These scenes can be used to test visibility and contrast restoration algorithms intensively and in an objective way, as well as \"shape from fog\" algorithms. The calibration parameters of the camera are given. \r\n\r\nPaper: [Improved Visibility of Road Scene Images under Heterogeneous Fog](https://doi.org/10.1109/IVS.2010.5548128)\r\n\r\nSource: [FRIDA (Foggy Road Image DAtabase) image database](http://perso.lcpc.fr/tarel.jean-philippe/bdd/frida.html)\r\n\r\nImage Source: [FRIDA (Foggy Road Image DAtabase) image database](http://perso.lcpc.fr/tarel.jean-philippe/bdd/frida.html)", "variants": ["FRIDA"]}
{"id": "Ford Campus Vision and Lidar Data Set", "title": "", "contents": "Ford Campus Vision and Lidar Data Set is a dataset collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. The vehicle is outfitted with a professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system. \r\n\r\nThis dataset consists of the time-registered data from these sensors mounted on the vehicle, collected while driving the vehicle around the Ford Research campus and downtown Dearborn, Michigan during November-December 2009. The vehicle path trajectory in these datasets contain several large and small-scale loop closures, which should be useful for testing various state of the art computer vision and SLAM (Simultaneous Localization and Mapping) algorithms.\r\n\r\nPaper: [Ford Campus vision and lidar data set](https://doi.org/10.1177%2F0278364911400640)\r\n\r\nSource: [Ford Campus vision and lidar data set](http://robots.engin.umich.edu/SoftwareData/Ford)\r\n\r\nImage Source: [Ford Campus vision and lidar data set](https://doi.org/10.1177%2F0278364911400640)", "variants": ["Ford Campus Vision and Lidar Data Set"]}
{"id": "JAAD", "title": "", "contents": "JAAD is a dataset for studying joint attention in the context of autonomous driving. The focus is on pedestrian and driver behaviors at the point of crossing and factors that influence them. To this end, JAAD dataset provides a richly annotated collection of 346 short video clips (5-10 sec long) extracted from over 240 hours of driving footage. These videos filmed in several locations in North America and Eastern Europe represent scenes typical for everyday urban driving in various weather conditions.\r\n\r\nBounding boxes with occlusion tags are provided for all pedestrians making this dataset suitable for pedestrian detection.\r\n\r\nBehavior annotations specify behaviors for pedestrians that interact with or require attention of the driver. For each video there are several tags (weather, locations, etc.) and timestamped behavior labels from a fixed list (e.g. stopped, walking, looking, etc.). In addition, a list of demographic attributes is provided for each pedestrian (e.g. age, gender, direction of motion, etc.) as well as a list of visible traffic scene elements (e.g. stop sign, traffic signal, etc.) for each frame.\r\n\r\nPaper: [Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior](https://doi.org/10.1109/ICCVW.2017.33)\r\n\r\nSource: [JAAD](http://data.nvision2.eecs.yorku.ca/JAAD_dataset/)\r\n\r\nImage Source: [Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior](https://doi.org/10.1109/ICCVW.2017.33)", "variants": ["JAAD"]}
{"id": "LISA Vehicle Detection", "title": "", "contents": "This is a dataset for vehicle detection. It consists of:\r\n\r\n* Three color video sequences captured at different times of the day and illumination settings: morning, evening, sunny, cloudy, etc.\r\n* Different driving environments: highway and urban.\r\n* Varying traffic conditions: light to dense traffic\r\n\r\nPaper: [A General Active-Learning Framework for On-Road Vehicle Recognition and Tracking](https://doi.org/10.1109/TITS.2010.2040177)\r\n\r\nSource: [Vehicle Detection Dataset](http://cvrr.ucsd.edu/LISA/vehicledetection.html)\r\n\r\nImage Source: [Vehicle Detection Dataset](http://cvrr.ucsd.edu/LISA/vehicledetection.html)", "variants": ["LISA Vehicle Detection"]}
{"id": "LLAMAS", "title": "", "contents": "The unsupervised Labeled Lane MArkerS dataset (LLAMAS) is a dataset for lane detection and segmentation. It contains over 100,000 annotated images, with annotations of over 100 meters at a resolution of 1276 x 717 pixels. The Unsupervised Llamas dataset was annotated by creating high definition maps for automated driving including lane markers based on Lidar. \r\n\r\nPaper: [Unsupervised Labeled Lane Markers Using Maps](https://doi.org/10.1109/ICCVW.2019.00111)\r\n\r\nSource: [Unsupervised Llamas Lane Marker Dataset](https://unsupervised-llamas.com/llamas/)\r\n\r\nImage Source: [Unsupervised Llamas Lane Marker Dataset](https://unsupervised-llamas.com/llamas/)", "variants": ["LLAMAS"]}
{"id": "REC-COCO", "title": "Inferring spatial relations from textual descriptions of images", "contents": "Relations in Captions (REC-COCO) is a new dataset that contains associations between caption tokens and bounding boxes in images. REC-COCO is based on the MS-COCO and V-COCO datasets. For each image in V-COCO, we collect their corresponding captions from MS-COCO and automatically align the concept triplet in V-COCO to the tokens in the caption. This requires finding the token for concepts such as PERSON. As a result, REC-COCO contains the captions and the tokens which correspond to each subject and object, as well as the bounding boxes for the subject and object.\r\n\r\nSource: [Inferring spatial relations from textual descriptions of images](https://arxiv.org/pdf/2102.00997v1.pdf)\r\nImage Source: [https://arxiv.org/pdf/2102.00997v1.pdf](https://arxiv.org/pdf/2102.00997v1.pdf)", "variants": ["REC-COCO"]}
{"id": "CSI Screenplay Summarization Corpus", "title": "Screenplay Summarization Using Latent Narrative Structure", "contents": "The dataset contains gold-standard summary labels for 39 \"CSI: Crime Scene Investigation\" episodes from seasons 1-5. Each episode contains the full-length screenplay and human annotations for its summary. The annotations include:\r\n\r\n1. scene-level binary labels denoting whether the scene belongs to the summary of the episode\r\n2. aspect-based labels for the scenes that belong to the summary, i.e., which aspect of the summary the scene addresses (e.g., information about the victim, the crime scene, the perpetrator etc.)\r\n3. sentence-level binary labels denoting the sentences of the screenplay that belong to the summary for 10 episodes of the dataset", "variants": ["CSI Screenplay Summarization Corpus"]}
{"id": "FPV-O", "title": "", "contents": "FPV-O is a multi-subject first-person vision dataset of office activities. Office activities include person-to-person interactions, such as chatting and handshaking, person-to-object interactions, such as using a computer or a whiteboard, as well as generic activities such as walking. The videos in the dataset present a number of challenges that, in addition to intra-class differences and inter-class similarities, include frames with illumination changes, motion blur, and lack of texture. \r\n\r\nPaper: [A First-Person Vision Dataset of Office Activities](https://doi.org/10.1007/978-3-030-20984-1_3)\r\n\r\nSource: [A first-person vision dataset of office activities](http://www.eecs.qmul.ac.uk/~andrea/fpvo.html)\r\n\r\nImage Source: [A First-Person Vision Dataset of Office Activities](https://doi.org/10.1007/978-3-030-20984-1_3)", "variants": ["FPV-O"]}
{"id": "l2d", "title": "Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio", "contents": "This dataset is composed of paired videos of people dancing 3 different music styles: Ballet, Michael Jackson and Salsa.\r\nIt contains multimodal data (visual data, temporal-graphs and audio) careful-selected from publicly available videos of dancers performing representative movements of the music style and audio data from the respective styles.\r\n\r\nThis dataset was used to train and evaluate methodologies for motion generation from audio. \r\n\r\nWe split the samples into training and evaluation sets. \r\n\r\nThe training set has 2352 samples of movements sequences with length 64. Which 525 are from Ballet style, 966 from Michael Jackson (MJ) and 861 from Salsa.\r\n\r\nThe evaluation set has 471 samples. 134 from Ballet, 102 from MJ and 235 from Salsa.", "variants": ["l2d"]}
{"id": "OccludedPASCAL3D+", "title": "Robust Object Detection under Occlusion with Context-Aware CompositionalNets", "contents": "The **OccludedPASCAL3D+** is a dataset is designed to evaluate the robustness to occlusion for a number of computer vision tasks, such as object detection, keypoint detection and pose estimation. In the OccludedPASCAL3D+ dataset, we simulate partial occlusion by superimposing objects cropped from the MS-COCO dataset on top of objects from the PASCAL3D+ dataset. We only use ImageNet subset in PASCAL3D+, which has 10812 testing images.\r\n\r\nSource: [OccludedPASCAL3D+](https://github.com/Angtian/OccludedPASCAL3D)\r\n\r\nImage source: [https://github.com/Angtian/OccludedPASCAL3D](https://github.com/Angtian/OccludedPASCAL3D)", "variants": ["OccludedPASCAL3D+"]}
{"id": "MHRI dataset", "title": "", "contents": "The dataset includes recordings from 10 different users teaching the robot different common kitchen objects, that consists of synchronized recordings from three cameras and a microphone mounted on the robot:\r\n\r\n    An RGB-d camera covers the user manipulation and interaction with the robot\r\n    An RGB-d camera mounted on the top of the robot provides a top view of the whole scenario\r\n    A HD-RGB camera points to the user head to capture face and expressions", "variants": ["MHRI dataset"]}
{"id": "highD Dataseth", "title": "", "contents": "The highD dataset is a new dataset of naturalistic vehicle trajectories recorded on German highways. Using a drone, typical limitations of established traffic data collection methods such as occlusions are overcome by the aerial perspective. Traffic was recorded at six different locations and includes more than 110 500 vehicles. Each vehicle's trajectory, including vehicle type, size and manoeuvres, is automatically extracted. Using state-of-the-art computer vision algorithms, the positioning error is typically less than ten centimeters. Although the dataset was created for the safety validation of highly automated vehicles, it is also suitable for many other tasks such as the analysis of traffic patterns or the parameterization of driver models.", "variants": ["highD Dataseth"]}
{"id": "rounD Dataset", "title": "", "contents": "The rounD dataset is a new dataset of naturalistic road user trajectories recorded at German roundabouts. Using a drone, typical limitations of established traffic data collection methods like occlusions are overcome. Traffic was recorded at three different locations. The trajectory for each road user and its type is extracted. Using state-of-the-art computer vision algorithms, the positional error is typically less than 10 centimetres. The dataset is applicable on many tasks such as road user prediction, driver modeling, scenario-based safety validation of automated driving systems or data-driven development of HAD system components.", "variants": ["rounD Dataset"]}
{"id": "CE4", "title": "Cycle-consistent Generative Adversarial Networks for Neural Style Transfer using data from Chang'E-4", "contents": "Given the difficulty to handle planetary data we provide downloadable files in PNG format from the missions Chang'E-3 and Chang'E-4. In addition to a set of scripts to do the conversion given a different PDS4 Dataset. \r\n\r\nThis set of images constitute one of the first available datasets to tackle problems of Computer Vision and Learning in the context of space exploration.", "variants": ["CE4"]}
{"id": "KITTI-trajectory-prediction", "title": "MANTRA: Memory Augmented Networks for Multiple Trajectory Prediction", "contents": "KITTI is a well established dataset in the computer vision community. It has often been used for trajectory prediction despite not having a well defined split, generating non comparable baselines in different works. This dataset aims at bridging this gap and proposes a well defined split of the KITTI data.\r\nSamples are collected as 6 seconds chunks (2seconds for past and 4 for future) in a sliding window fashion from all trajectories in the dataset, including the egovehicle. There are a total of 8613 top-view trajectories for training and 2907 for testing.\r\nSince top-view maps are not provided by KITTI, semantic labels of static categories obtained with DeepLab-v3+ from all frames are projected in a common top-view map using the Velodyne 3D point cloud and IMU. The resulting maps have a spatial resolution of 0.5 meters and are provided along with the trajectories.", "variants": ["KITTI-trajectory-prediction"]}
{"id": "EmoContext", "title": "SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text", "contents": "EmoContext consists of three-turn English Tweets. The emotion labels include happiness, sadness, anger and other.", "variants": ["EC", "EmoContext"]}
{"id": "Glint360K", "title": "Partial FC: Training 10 Million Identities on a Single Machine", "contents": "The largest and cleanest face recognition dataset Glint360K, \r\nwhich contains **`17,091,657`** images of **`360,232`** individuals, baseline models trained on Glint360K can easily achieve state-of-the-art performance.", "variants": ["Glint360K"]}
{"id": "IndicCorp", "title": "IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages", "contents": "IndicCorp is a large monolingual corpora with around 9 billion tokens covering 12 of the major Indian languages. It has been developed by discovering and scraping thousands of web sources - primarily news, magazines and books, over a duration of several months.\r\n\r\n**Languages covered**: Assamese, Bengali, English, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu\r\n\r\n**Corpus Format**: The corpus is a single large text file containing one sentence per line. The publicly released version is randomly shuffled, untokenized and deduplicated. \r\n\r\n**Downloads**\r\n\r\n| Language | \\# News Articles* | Sentences     | Tokens        | Link     |\r\n| -------- | ----------------- | ------------- | ------------- | -------- |\r\n| as       | 0.60M             | 1.39M   |  32.6M  | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/as.tar.xz) |\r\n| bn       | 3.83M             | 39.9M | 836M  | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/bn.tar.xz) |\r\n| en       | 3.49M             | 54.3M | 1.22B | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/en.tar.xz) |\r\n| gu       | 2.63M             | 41.1M | 719M  | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/gu.tar.xz) |\r\n| hi       | 4.95M             | 63.1M |  1.86B | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/hi.tar.xz) |\r\n| kn       | 3.76M             | 53.3M | 713M  | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/bn.tar.xz) |\r\n| ml       | 4.75M             | 50.2M |  721M  | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/ml.tar.xz) |\r\n| mr       | 2.31M             | 34.0M | 551M  | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/mr.tar.xz) |\r\n| or       | 0.69M             | 6.94M   | 107M   | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/or.tar.xz) |\r\n| pa       | 2.64M             | 29.2M |  773M  | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/pa.tar.xz) |\r\n| ta       | 4.41M             |  31.5M   |  582M  | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/ta.tar.xz) |\r\n| te       | 3.98M             | 47.9M   |  674M  | [link](https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/te.tar.xz) |\r\n\r\n\\* Excluding articles obtained from the OSCAR corpus", "variants": ["IndicCorp"]}
{"id": "RuFa", "title": "", "contents": "RuFa (Ruqaa-Farsi) dataset contains images of text written in one of two Arabic fonts: Ruqaa and Nastaliq (Farsi). The dataset contains 40,000 synthesized image and 516 real one, 40,516 in total. Images are in RGB JPG format at 100×100px. Text in the images has varying number of words, position, size, and opacity.\r\n\r\nReal images were extracted from:\r\n\r\n1. “The Rules of Arabic Calligraphy” by Hashem Al-Khatat - 1986.\r\n\r\n2. “Ottman Fonts” by Muhammad Amin Osmanli Ketbkhana.\r\n\r\nThe synthetization process is described in detail [in this post](https://mhmoodlan.github.io/blog/arabic-font-classification).\r\n\r\nDataset folder structure:\r\n\r\n**/rufa (40,516 images)**\r\n\r\n* /real (516 images)\r\n\r\n        * /ruqaa (260 images)\r\n\r\n        * /farsi   (256 images)\r\n\r\n* /synth (40,000 images)\r\n\r\n        * /ruqaa (20,000 images)\r\n\r\n        * /farsi   (20,000 images)", "variants": ["RuFa"]}
{"id": "Synbols", "title": "Synbols: Probing Learning Algorithms with Synthetic Datasets", "contents": "Synbols is a dataset generator designed for probing the behavior of learning algorithms. By defining the distribution over latent factors one can craft a dataset specifically tailored to answer specific questions about a given algorithm.\r\n\r\nDefault versions of these datasets are also materialized and can serve as benchmarks.", "variants": ["Synbols"]}
{"id": "GEM", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics", "contents": "Generation, Evaluation, and Metrics (GEM) is a benchmark environment for Natural Language Generation with a focus on its Evaluation, both through human annotations and automated Metrics.\r\n\r\nGEM aims to:\r\n\r\n- measure NLG progress across 13 datasets spanning many NLG tasks and languages.\r\n- provide an in-depth analysis of data and models presented via data statements and challenge sets.\r\n- develop standards for evaluation of generated text using both automated and human metrics.\r\n\r\nIt is our goal to regularly update GEM and to encourage toward more inclusive practices in dataset development by extending existing data or developing datasets for additional languages.\r\n\r\nSource: [https://gem-benchmark.com/](https://gem-benchmark.com/)\r\nImage Source: [Gehrmann et al](https://arxiv.org/pdf/2102.01672.pdf)", "variants": ["GEM"]}
{"id": "ALFWorld", "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning", "contents": "ALFWorld contains interactive TextWorld environments (Côté et. al) that parallel embodied worlds in the ALFRED dataset (Shridhar et. al). The aligned environments allow agents to reason and learn high-level policies in an abstract space before solving embodied tasks through low-level actuation.   \r\n\r\nSource: [ALFWorld](https://github.com/alfworld/alfworld)", "variants": ["ALFWorld"]}
{"id": "HQ-WMCA", "title": "Deep Models and Shortwave Infrared Information to Detect Face Presentation Attacks", "contents": "The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database consists of 2904 short multi-modal video recordings of both bona-fide and presentation attacks. There are 555 bonafide presentations from 51 participants and the remaining 2349 are presentation attacks. The data is recorded from several channels including color, depth, thermal, infrared (spectra), and short-wave infrared (spectra).", "variants": ["HQ-WMCA"]}
{"id": "The Best Sarcasm Annotated Dataset in Spanish", "title": "Reconocimiento automático del sarcasmo: ¡Esto va a funcionar bien!", "contents": "### Content\r\n\r\nThis dataset contains all utterances of two episodes of South Park (Latin American voices) and two episodes of Archer (Spanish voices). The order of the utterances is shuffled. Each utterance has been annotated based on whether it is sarcastic or not. Sarcastic expressions also contain further annotation based on different theories on sarcasm.\r\n\r\nThis corpus is unique because it is annotated from primarily audiovisual media. It also contains a lot of negative examples of sentences that are meant to be humorous or outrageous, but not sarcastic. This data provides thus a closer to real life benchmark for any sarcasm detection system.\r\n\r\n### Cite\r\n\r\nI annotated this data for my MA thesis, so please cite it if you use this data.\r\n\r\nHämäläinen, Mika (2016). [Reconocimiento automático del sarcasmo: ¡Esto va a funcionar bien!](https://www.researchgate.net/publication/339182029_Reconocimiento_automatico_del_sarcasmo_Esto_va_a_funcionar_bien). Helsinki: University of Helsinki, Department of Modern Languages.\r\n\r\n\r\n### Inspiration\r\n\r\n- Sarcasm detection\r\n- Prediction of the theoretical categories of sarcasm", "variants": ["The Best Sarcasm Annotated Dataset in Spanish"]}
{"id": "MIRACL-VC1", "title": "", "contents": "MIRACL-VC1 is a lip-reading dataset including both depth and color images. It can be used for diverse research fields like visual speech recognition, face detection, and biometrics. Fifteen speakers (five men and ten women) positioned in the frustum of an MS Kinect sensor and utter ten times a set of ten words and ten phrases (see the table below). Each instance of the dataset consists of a synchronized sequence of color and depth images (both of 640x480 pixels).  The MIRACL-VC1 dataset contains a total number of 3000 instances.", "variants": ["MIRACL-VC1"]}
{"id": "XD-Violence", "title": "Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision", "contents": "XD-Violence is a large-scale audio-visual dataset for violence detection in videos.", "variants": ["XD-Violence"]}
{"id": "PatentMatch", "title": "PatentMatch: A Dataset for Matching Patent Claims & Prior Art", "contents": "We address the computer-assisted search for prior art by creating a training dataset for supervised machine learning called PatentMatch. It contains pairs of claims from patent applications and semantically corresponding text passages of different degrees from cited patent documents. Each pair has been labeled by technically-skilled patent examiners from the European Patent Office. Accordingly, the label indicates the degree of semantic correspondence (matching), i.e., whether the text passage is prejudicial to the novelty of the claimed invention or not.", "variants": ["PatentMatch"]}
{"id": "A Dataset of Journalists' Interactions with Their Readership", "title": "A Dataset of Journalists' Interactions with Their Readership: When Should Article Authors Reply to Reader Comments?", "contents": "We present a dataset of dialogs in which journalists of The Guardian replied to reader comments and identify the reasons why. Based on this data, we formulate the novel task of recommending reader comments to journalists that are worth reading or replying to, i.e., ranking comments in such a way that the top comments are most likely to require the journalists' reaction.", "variants": ["A Dataset of Journalists' Interactions with Their Readership"]}
{"id": "HeartSeg", "title": "", "contents": "The medaka (Oryzias latipes) and the zebrafish (Danio rerio) are used as a model organism for a variety of subjects in biomedical research. The presented work aims to study the potential of automated ventricular dimension estimation through heart segmentation in medaka. For more on this, it's time for a closer look on our paper and the supplementary materials.\r\n\r\nSee our paper here: https://www.liebertpub.com/doi/10.1089/zeb.2019.1754\r\n\r\nSee demonstration of our algorithm and framework on the test set data:\r\nhttps://youtu.be/i5bX_XbwXq0\r\n\r\nThe raw data was provided by:\r\nDr. Jakob Gierten\r\n\r\nAffiliated with:\r\nDepartment of Pediatric Cardiology, University Hospital Heidelberg, Im Neuenheimer Feld 430, 69120 Heidelberg, Germany\r\nCentre for Organismal Studies, Heidelberg University, Im Neuenheimer Feld 230, 69120 Heidelberg, Germany\r\n\r\nContributing\r\n\r\nWe hope this work sparks additional research in this direction. Either by contributing to this framework, deploying the framework, or reusing the annotated ground truth data.\r\nIn any case feel free to reach out and make sure to reference this work.\r\n\r\nSchutera, M., Just, S., Gierten, J., Mikut, R., Reischl, M., & Pylatiuk, C. (2019). Machine learning methods for automated quantification of ventricular dimensions. Zebrafish, 16(6), 542-545.\r\nContact: mark.schutera@kit.edu and pylatiuk@kit.edu", "variants": ["HeartSeg"]}
{"id": "Interspeech 2021 Deep Noise Suppression Challenge", "title": "Interspeech 2021 Deep Noise Suppression Challenge", "contents": "The Deep Noise Suppression (DNS) challenge is designed to foster innovation in the area of noise suppression to achieve superior perceptual speech quality.\r\n\r\nThis challenge has two two tracks:\r\n\r\n**Track 1: Real-Time Denoising track for wide band scenario**\r\n\r\nThe noise suppressor must take less than the stride time Ts (in ms) to process a frame of size T (in ms) on an Intel Core i5 quad-core machine clocked at 2.4 GHz or equivalent processor. For example, Ts = T/2 for 50% overlap between frames. The total algorithmic latency allowed including the frame size T, stride time Ts, and any look ahead must be less than or equal to 40ms. For example, for a real-time system that receives 20ms audio chunks, if you use a frame length of 20ms with a stride of 10ms resulting in an algorithmic latency of 30ms, then you satisfy the latency requirements. If you use a frame of size 32ms with a stride of 16ms resulting in an algorithmic latency of 48ms, then your method does not satisfy the latency requirements as the total algorithmic latency exceeds 40ms. If your frame size plus stride T1=T+Ts is less than 40ms, then you can use up to (40-T1) ms future information.\r\n\r\n**Track 2: Real-Time Denoising track for full band scenario**\r\n\r\nSatisfy Track 1 requirements but at 48 kHz.", "variants": ["Interspeech 2021 Deep Noise Suppression Challenge"]}
{"id": "WEB-FORUM-52", "title": "Harvest -- An Open Source Toolkit for Extracting Posts and Post Metadata from Web Forums", "contents": "The WEB-FORUM-52 gold standard comprises (i) 13 web forums from the health domain, (ii) 15 forums obtained from a Wikipedia list of popular forums (https://en.wikipedia.org/wiki/List_of_Internet_forums), (iii) 13 forums mentioned on a list of popular German Web forums (https://www.beliebte-foren.de), (iv) nine forums obtained from WPressBlog (https://www.wpressblog.com/free-forum-posting-sites-list/) and (v) two additional forums. For most forums two web pages (from different threads) were used and stored together with gold standard annotations that have been manually created by domain experts and describe the post text, post date, post user and direct URL to the post.", "variants": ["WEB-FORUM-52"]}
{"id": "MOBIO", "title": "", "contents": "The MOBIO database consists of bi-modal (audio and video) data taken from 152 people. The database has a female-male ratio or nearly 1:2 (100 males and 52 females) and was collected from August 2008 until July 2010 in six different sites from five different countries. This led to a diverse bi-modal database with both native and non-native English speakers.\r\n\r\nIn total 12 sessions were captured for each client: 6 sessions for Phase I and 6 sessions for Phase II. The Phase I data consists of 21 questions with the question types ranging from: Short Response Questions, Short Response Free Speech, Set Speech, and Free Speech. The Phase II data consists of 11 questions with the question types ranging from:  Short Response Questions, Set Speech, and Free Speech. A more detailed description of the questions asked of the clients is provided below.\r\n\r\nThe database was recorded using two mobile devices: a mobile phone and a laptop computer. The mobile phone used to capture the database was a NOKIA N93i mobile while the laptop computer was a standard 2008 MacBook. The laptop was only used to capture part of the first session, this first session consists of data captured on both the laptop and the mobile phone.", "variants": ["MOBIO"]}
{"id": "FRLL-Morphs", "title": "", "contents": "FRLL-Morphs is a dataset of morphed faces based on images selected from the publicly available Face Research London Lab dataset [1].\r\n \r\nWe created the database by selecting similar looking pairs of people, and made 4 types of morphs for each pair using the following morphing tools: OpenCV [2], FaceMorpher [3], StyleGAN 2 [3], WebMorpher [4].\r\n\r\n* [1] https://figshare.com/articles/dataset/Face_Research_Lab_London_Set/5047666\r\n* [2] https://www.learnopencv.com/face-morph-using-opencv-cpp-python\r\n* [3] https://github.com/yaopang/FaceMorpher/tree/master/facemorpher\r\n* [4] https://github.com/NVlabs/stylegan2", "variants": ["FRLL-Morphs"]}
{"id": "VisualMRC", "title": "VisualMRC: Machine Reading Comprehension on Document Images", "contents": "VisualMRC is a visual machine reading comprehension dataset that proposes a task: given a question and a document image, a model produces an abstractive answer.\r\n\r\nYou can find more details, analyses, and baseline results in the paper, \r\nVisualMRC: Machine Reading Comprehension on Document Images, AAAI 2021.\r\n\r\n\r\nStatistics:\r\n10,197 images\r\n30,562 QA pairs\r\n10.53 average question tokens (tokenizing with NLTK tokenizer)\r\n9.53 average answer tokens (tokenizing wit NLTK tokenizer)\r\n151.46 average OCR tokens (tokenizing with NLTK tokenizer)", "variants": ["VisualMRC"]}
{"id": "FERET-Morphs", "title": "", "contents": "FERET-Morphs is a dataset of morphed faces selected from the publicly available FERET dataset [1].\r\n\r\nWe created the database by selecting similar looking pairs of people, and made 3 types of morphs for each pair using the following morphing tools: OpenCV [2], FaceMorpher [3], StyleGAN 2 [3].\r\n\r\n* [1] https://www.nist.gov/itl/products-and-services/color-feret-database\r\n* [2] https://www.learnopencv.com/face-morph-using-opencv-cpp-python\r\n* [3] https://github.com/yaopang/FaceMorpher/tree/master/facemorpher\r\n* [4] https://github.com/NVlabs/stylegan2", "variants": ["FERET-Morphs"]}
{"id": "FRGC-Morphs", "title": "Vulnerability Analysis of Face Morphing Attacks from Landmarks and Generative Adversarial Networks", "contents": "FRGC-Morphs is a dataset of morphed faces selected from the publicly available FRGC dataset [1].\r\n\r\nWe created the database by selecting similar looking pairs of people, and made 3 types of morphs for each pair using the following morphing tools: OpenCV [2], FaceMorpher [3], StyleGAN 2 [3].\r\n\r\n* [1] https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc\r\n* [2] https://www.learnopencv.com/face-morph-using-opencv-cpp-python\r\n* [3] https://github.com/yaopang/FaceMorpher/tree/master/facemorpher\r\n* [4] https://github.com/NVlabs/stylegan2", "variants": ["FRGC-Morphs"]}
{"id": "NISP- A Multi-lingual Multi-accent Dataset for Speaker Profiling", "title": "NISP: A Multi-lingual Multi-accent Dataset for Speaker Profiling", "contents": "We announce the release of a new multilingual speaker dataset called NITK-IISc Multilingual Multi-accent Speaker Profiling(NISP) dataset. The dataset contains speech in six different languages -- five Indian languages along with Indian English. The dataset contains speech data from 345 bilingual speakers in India. Each speaker has contributed about 4-5 minutes of data that includes recordings in both English and their mother tongue. The transcript for the text is provided in UTF-8 format. For every speaker, the dataset contains speaker meta-data such as L1, native place, medium of instruction, current residing place etc. In addition the dataset also contains physical parameter information of the speakers such as age, height, shoulder size and weight. We hope that the dataset is useful for a diverse set of research activities including multilingual speaker recognition, language and accent recognition, automatic speech recognition etc.", "variants": ["NISP- A Multi-lingual Multi-accent Dataset for Speaker Profiling"]}
{"id": "NinaPro DB2", "title": "", "contents": "The second Ninapro database includes 40 intact subjects and it is thoroughly described in the paper: \"Manfredo Atzori, Arjan Gijsberts, Claudio Castellini, Barbara Caputo, Anne-Gabrielle Mittaz Hager, Simone Elsig, Giorgio Giatsidis, Franco Bassetto & Henning Müller. Electromyography data for non-invasive naturally-controlled robotic hand prostheses. Scientific Data, 2014\" (http://www.nature.com/articles/sdata201453).\r\nPlease, cite this paper for any work related to the Ninapro database.\r\nPlease, use also the paper by Gijsberts et al., 2014 (http://publications.hevs.ch/index.php/publications/show/1629) for more information about the database.", "variants": ["NinaPro DB2"]}
{"id": "POLIT-FALSE-n-LEGIT NEWS DB 2016-2017", "title": "", "contents": "The LiT.RL POLIT-FALSE-n-LEGIT NEWS DB 2016-2017 contains a total of 274 news articles about U.S. Politics, content-matched in pairs of legitimate and falsified news. The database is free and released under an open license for educational and research purposes.", "variants": ["POLIT-FALSE-n-LEGIT NEWS DB 2016-2017"]}
{"id": "GQN rooms-ring-camera", "title": "", "contents": "GQN rooms-ring-camera consist of scenes of a variable number of random objects captured in a square room of size 7x7 units. Wall textures, floor textures as well as the shapes of the objects are randomly chosen within a fixed pool of discrete options. There are 5 possible wall textures (red, green, cerise, orange, yellow), 3 possible floor textures (yellow, white, blue) and 7 possible object shapes (box, sphere, cylinder, capsule, cone, icosahedron and triangle). Each scene contains 1, 2 or 3 objects. In this simplified version of the dataset, the camera only moves on a fixed ring and always faces the center of the room.", "variants": ["GQN rooms-ring-camera"]}
{"id": "ISOT Fake News Dataset", "title": "", "contents": "The ISOT Fake News dataset is a compilation of several thousands fake news and truthful articles, obtained from different legitimate news sites and sites flagged as unreliable by Politifact.com.", "variants": ["ISOT Fake News Dataset"]}
{"id": "ObjectsRoom", "title": "", "contents": "The **ObjectsRoom** dataset is based on the MuJoCo environment used by the Generative Query Network [4] and is a multi-object extension of the 3d-shapes dataset. The training set contains 1M scenes with up to three objects. We also provide ~1K test examples for the following variants:\r\n\r\n2.1 Empty room: scenes consist of the sky, walls, and floor only.\r\n\r\n2.2 Six objects: exactly 6 objects are visible in each image.\r\n\r\n2.3 Identical color: 4-6 objects are placed in the room and have an identical, randomly sampled color.\r\n\r\nDatapoints consist of an image and fixed number of masks. The first four masks correspond to the sky, floor, and two halves of the wall respectively. The remaining masks correspond to the foreground objects.\r\n\r\nSource: [Objects Room](https://github.com/deepmind/multi_object_datasets)", "variants": ["ObjectsRoom"]}
{"id": "SVDC Fake News Dataset", "title": "", "contents": "A labeled dataset that presents fake news surrounding the conflict in Syria. The dataset consists of a set of articles/news labeled by 0 (fake) or 1 (credible). Credibility of articles are computed with respect to a ground truth information obtained from the Syrian Violations Documentation Center  (VDC). In particular, for each article, we crowdsource the information extraction (e.g., date, location, Number of casualties) job using the crowdsourcing platform Figure Eight (formally CrowdFlower). Then, we match those articles against the VDC database to be able to deduce whether an article is fake or not. The dataset can be used to train machine learning models to detect fake news.", "variants": ["SVDC Fake News Dataset"]}
{"id": "DND", "title": "", "contents": "Benchmarking Denoising Algorithms with Real Photographs\r\n\r\nThis dataset consists of 50 pairs of noisy and (nearly) noise-free images captured with four consumer cameras. Since the images are of very high-resolution, the providers extract 20 crops of size 512 × 512 from each image, thus yielding a total of 1000 patches.", "variants": ["DND"]}
{"id": "Cityscapes-VPS", "title": "Video Panoptic Segmentation", "contents": "Cityscapes-VPS is a video extension of the Cityscapes validation split. It provides 2500-frame panoptic labels that temporally extend the 500 Cityscapes image-panoptic labels. There are total 3000-frame panoptic labels which correspond to 5, 10, 15, 20, 25, and 30th frames of each 500 videos, where all instance ids are associated over time. It not only supports video panoptic segmentation (VPS) task, but also provides super-set annotations for video semantic segmentation (VSS) and video instance segmentation (VIS) tasks.", "variants": ["Cityscapes-VPS"]}
{"id": "PS-Plant dataset", "title": "", "contents": "Automated leaf segmentation is a challenging area in computer vision. Recent advances in machine learning approaches allowed to achieve better results than traditional image processing techniques; however, training such systems often require large annotated data sets. To contribute with annotated data sets and help to overcome this bottleneck in plant phenotyping research, here we provide a novel photometric stereo (PS) data set with annotated leaf masks. This data set forms part of the work done in the BBSRC Tools and Resources Development project BB/N02334X/1.\r\n\r\nSource: [A photometric stereo-based 3D imaging system using computer vision and deep learning for tracking plant growth](https://academic.oup.com/gigascience/article/8/5/giz056/5498634)\r\nImage Source: [PS-Plant training dataset description](https://datashare.ed.ac.uk/bitstream/handle/10283/3280/PS-Plant%20training%20data%20set%20description.pdf?sequence=3&isAllowed=y)", "variants": ["PS-Plant dataset"]}
{"id": "Real SVBRDF", "title": "Deep SVBRDF Estimation on Real Materials", "contents": "A total of 80 real material samples were captured in a dark room. For each material, multiple captures were collected at different distances from the camera (between 250 and 650 mm) to observe both macro- and micro-level details. The dataset is mostly comprised of planar specimens but also includes non-planar objects such as mugs, globes, crumpled paper, etc. As shown above, it contains a rich diversity of materials, including diffuse or specular wrapping papers, fabrics, anisotropic metals, plastics, rugs, ceramic and wood flooring samples, etc. Each capture set includes 12 LDR (8 bpp) RGB-D images at 4K pixel resolution. Each set is captured at 50% and 100% of maximum light intensity. In total, we captured 462 such image sets (combinations of light intensities, distances to the camera, and material sample).", "variants": ["Real SVBRDF"]}
{"id": "MLPF", "title": "MLPF: Efficient machine-learned particle-flow reconstruction using graph neural networks", "contents": "Dataset of 50,000 top quark-antiquark (ttbar) events produced in proton-proton collisions at 14 TeV, overlaid with minimum bias events corresponding to a pileup of 200 on average. The dataset consists of detector hits as the input, generator particles as the ground truth and reconstructed particles from DELPHES for additional validation.\r\nThe DELPHES model corresponds to a CMS-like detector with a multi-layered charged particle tracker, an electromagnetic and hadron calorimeter. Pythia8 and Delphes3 were used for the simulation.\r\n\r\nEach file contains a bzip2-compressed python pickle with the following contents:\r\n```\r\n> data = pickle.load(bz2.BZ2File(\"out/pythia8_ttbar/tev14_pythia8_ttbar_0_0.pkl.bz2\", \"rb\"))\r\n\r\n# Each file contains lists of arrays X (detector elements), ygen (generator particles) and ycand (rule-based PF particles from Delphes) for 100 events\r\n> len(data[\"ycand\"]), len(data[\"ygen\"]), len(data[\"X\"])\r\n100, 100, 100\r\n\r\n#Each element in the list corresponds to an event. The first event in the file contains 5992 detector elements, ygen and ycand are 0-padded to the same length as X\r\n> data[\"X\"][0].shape, data[\"ygen\"][0].shape, data[\"ycand\"][0].shape, \r\n((5992, 12), (5992, 7), (5992, 7))\r\n\r\n# The X rows are detector elements: calorimeter towers and tracks with the following 12-features (0-padded)\r\n# tower: [type==1, Et (GeV), eta, sin phi, cos phi, E (GeV), Eem (GeV), Ehad (GeV), 0, 0, 0, 0]\r\n# track: [type==2, pt (GeV), eta, sin phi, cos phi, P (GeV), eta_outer, sin phi_outer, cos phi_outer, charge, is_gen_muon, is_gen_electron]\r\n\r\n# The ygen (ycand) rows are generator-level truth particles (rule-based PF particles from Delphes) with the following features:\r\n# [pid, charge, pt (GeV), eta, sin phi, cos phi, E (GeV)]\r\n# pid==0: placeholder/padding entry\r\n# pid==1: charged hadrons\r\n# pid==2: neutral hadrons\r\n# pid==3: photons\r\n# pid==4: electrons\r\n# pid==5: muons\r\n```", "variants": ["MLPF"]}
{"id": "University of Washington/Northwestern University (UW/NU) Corpus", "title": "", "contents": "The University of Washington/Northwestern University (UW/NU) Corpus contains recordings and textgrids of Pacific Northwest and Northern Cities speakers reading a subset of the IEEE \"Harvard\" sentences. The UW/NU Corpus Version 1.0 has been used to study the effects of dialectal variation on speech intelligibility, while version 2.0 is being used in ongoing research in speech intelligibility and gender interaction. Development is supported by the National Institutes of Health, National Institute on Deafness and Other Communication Disorders grant R01-DC006014. The PN/NC Corpus is well suited for both clinical and research studies where high-fidelity recordings and regional accent control are desirable.", "variants": ["University of Washington/Northwestern University (UW/NU) Corpus"]}
{"id": "UNITOPATHO", "title": "UniToPatho, a labeled histopathological dataset for colorectal polyps classification and adenoma dysplasia grading", "contents": "Histopathological characterization of colorectal polyps allows to tailor patients' management and follow up with the ultimate aim of avoiding or promptly detecting an invasive carcinoma. Colorectal polyps characterization relies on the histological analysis of tissue samples to determine the polyps malignancy and dysplasia grade. Deep neural networks achieve outstanding accuracy in medical patterns recognition, however they require large sets of annotated training images. We introduce UniToPatho, an annotated dataset of 9536 hematoxylin and eosin stained patches extracted from 292 whole-slide images, meant for training deep neural networks for colorectal polyps classification and adenomas grading. The slides are acquired through a Hamamatsu Nanozoomer S210 scanner at 20× magnification (0.4415 μm/px)", "variants": ["UNITOPATHO"]}
{"id": "WiC-TSV", "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context", "contents": "WiC-TSV is a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, it is a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as a binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model.", "variants": ["WiC-TSV"]}
{"id": "Clinical Admission Notes from MIMIC-III", "title": "Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration", "contents": "This dataset is created from **MIMIC-III** ([Medical Information Mart for Intensive Care III](https://paperswithcode.com/dataset/mimic-iii)) and contains simulated patient admission notes. The clinical notes contain information about a patient at **admission time** to the ICU and are labelled for four outcome prediction tasks: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay.\r\n\r\nTo obtain the data one first has to [gain access](https://mimic.physionet.org/gettingstarted/access/) to the MIMIC-III dataset and then run the scripts introduced in the linked repository.", "variants": ["Clinical Admission Notes from MIMIC-III"]}
{"id": "IBC", "title": "An empirical evaluation of functional alignment using inter-subject decoding", "contents": "The Individual Brain Charting (IBC) project aims at providing a new generation of functional-brain atlases. To map cognitive mechanisms in a fine scale, task-fMRI data at high-spatial-resolution are being acquired on a fixed cohort of 12 participants, while performing many different tasks. These data—free from both inter-subject and inter-site variability—are publicly available as means to support the investigation of functional segregation and connectivity as well as individual variability with a view to establishing a better link between brain systems and behavior.\r\n\r\n** What’s special about the IBC dataset?**\r\n\r\n* Taskwise dataset: spanning the cognitive spectrum within subject\r\n* Fixed cohort over a 10-year span to minimize inter-subject variability\r\n* Fixed experimental setting to minimize inter-site variability\r\n* Multimodal: fMRI (task-based and resting state), DWI, structural\r\n\r\n** Main characteristics **\r\n\r\n* 12 healthy participants (aged 27-40 at the time of recruitment)\r\n* Spatial resolution: 1.5mm (isotropic); Temporal resolution: 2s\r\n* Scanner: Siemens 3T Magnetom Prisma; Coil: 64-channel\r\n* 50 acquisitions per participant upon completion of the dataset in 2022", "variants": ["IBC"]}
{"id": "MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge", "title": "", "contents": "Under Institutional Review Board (IRB) supervision, 50 abdomen CT scans of were randomly selected from a combination of an ongoing colorectal cancer chemotherapy trial, and a retrospective ventral hernia study. The 50 scans were captured during portal venous contrast phase with variable volume sizes (512 x 512 x 85 - 512 x 512 x 198) and field of views (approx. 280 x 280 x 280 mm3 - 500 x 500 x 650 mm3). The in-plane resolution varies from 0.54 x 0.54 mm2 to 0.98 x 0.98 mm2, while the slice thickness ranges from 2.5 mm to 5.0 mm. The standard registration data was generated by NiftyReg.\r\n\r\nSource: [MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge](https://www.synapse.org/#!Synapse:syn3193805/wiki/217789)\r\n\r\nImage source: [MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge](https://www.synapse.org/#!Synapse:syn3193805/wiki/217789)", "variants": ["MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge", "Synapse multi-organ CT"]}
{"id": "Alchemy", "title": "Alchemy: A structured task distribution for meta-reinforcement learning", "contents": "The DeepMind Alchemy environment is a meta-reinforcement learning benchmark that presents tasks sampled from a task distribution with deep underlying structure. It was created to test for the ability of agents to reason and plan via latent state inference, as well as useful exploration and experimentation. \r\n\r\nAlchemy is a single-player video game, implemented in Unity. The player sees a first-person view of a table with a number of objects on it, including a set of colored stones, a set of dishes containing colored potions, and a central cauldron. Stones have different point values, and points are collected when stones are added to the cauldron. By dipping stones into the potions, the player can transform the stones’ appearance, and thus their value, increasing the number of points that can be won.\r\n\r\nSource: [dm_alchemy: DeepMind Alchemy environment](https://github.com/deepmind/dm_alchemy)\r\n\r\nImage Source: [dm_alchemy: DeepMind Alchemy environment](https://github.com/deepmind/dm_alchemy)", "variants": ["Alchemy"]}
{"id": "Biase et al", "title": "", "contents": "Source: [Cell fate inclination within 2-cell and 4-cell mouse embryos revealed by single-cell RNA sequencing](https://pubmed.ncbi.nlm.nih.gov/25096407/)", "variants": ["Biase et al"]}
{"id": "Goolam et al", "title": "", "contents": "Source: [Heterogeneity in Oct4 and Sox2 Targets Biases Cell Fate in 4-Cell Mouse Embryos](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4819611/)", "variants": ["Goolam et al"]}
{"id": "Yan et al", "title": "", "contents": "Source: [Single-cell RNA-Seq profiling of human preimplantation embryos and embryonic stem cells](https://pubmed.ncbi.nlm.nih.gov/23934149/)", "variants": ["Yan et al"]}
{"id": "Deng et al", "title": "", "contents": "Source: [Single-cell RNA-seq reveals dynamic, random monoallelic gene expression in mammalian cells](https://pubmed.ncbi.nlm.nih.gov/24408435/)", "variants": ["Deng et al"]}
{"id": "Pollen et al", "title": "", "contents": "TPM values together with cell type annotations that were obtained from Alex Pollen on 15/10/15\r\n\r\nSource: [Low-coverage single-cell mRNA sequencing reveals cellular heterogeneity and activated signaling pathways in developing cerebral cortex](https://www.nature.com/articles/nbt.2967)", "variants": ["Pollen et al"]}
{"id": "Treutlein et al", "title": "", "contents": "Source: [Reconstructing lineage hierarchies of the distal lung epithelium using single-cell RNA-seq](https://pubmed.ncbi.nlm.nih.gov/24739965/)", "variants": ["Treutlein et al"]}
{"id": "EPIC-KITCHENS-100", "title": "Rescaling Egocentric Vision", "contents": "This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the \"test of time\" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit \"two years on\".\r\nThe dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.", "variants": ["EPIC-KITCHENS-100"]}
{"id": "twitter politicians data", "title": "SAGE: Sequential Attribute Generator for Analyzing Glioblastomas using Limited Dataset", "contents": "Dataset based on Twitter usernames of American politicians. Data extracted from Wikidata.\r\n\r\nThe same politician can appear several times: if he has different pseudonyms on Twitter or Instagram, if he has been in several parties, or if several Twitter account IDs are associated with him. But the data is sorted in ascending order by name, so it is visible", "variants": ["twitter politicians data"]}
{"id": "3D Platelet EM", "title": "Dense cellular segmentation for EM using 2D–3D neural network ensembles", "contents": "The platelet-em dataset contains two 3D scanning electron microscope (EM) images of human platelets, as well as instance and semantic segmentations of those two image volumes.\r\nThis data has been reviewed by NIBIB, contains no PII or PHI, and is cleared for public release. All files use a multipage uint16 TIF format. A 3D image with size [Z, X, Y] is saved as Z pages of size [X, Y]. Image voxels are approximately 40x10x10 nm", "variants": ["3D Platelet EM"]}
{"id": "Sintel 4D LFV", "title": "Depth estimation from 4D light field videos", "contents": "A medium-scale synthetic 4D Light Field video dataset for depth (disparity) estimation. From the open-source movie Sintel. The dataset consists of 24 synthetic 4D LFVs with 1,204x436 pixels, 9x9 views, and 20–50 frames, and has ground-truth disparity values, so that can be used for training deep learning-based methods. Each scene was rendered with a clean pass after modifying the production file of Sintel with reference to the MPI Sintel dataset.", "variants": ["Sintel 4D LFV"]}
{"id": "Bee4Exp Honeybee Detection", "title": "A Method for Detection of Small Moving Objects in UAV Videos", "contents": "A dataset for flying honeybee detection introduced in [\"A Method for Detection of Small Moving Objects in UAV Videos\"](https://www.mdpi.com/2072-4292/13/4/653). \r\n\r\nThis dataset consists of three videos with flying honeybees in a natural environment.", "variants": ["Bee4Exp Honeybee Detection"]}
{"id": "MHSMA", "title": "", "contents": "The MHSMA dataset is a collection of human sperm images from 235 patients with male factor infertility. Each image is labeled by experts for normal or abnormal sperm acrosome, head, vacuole, and tail.\r\n\r\nThe training, validation, and test sets contain 1000, 240, and 300 images, respectively.\r\n\r\nImages are available in two different crop sizes: 128x128- and 64x64-pixel.\r\n\r\nPaper: [A novel deep learning method for automatic assessment of human sperm images](https://doi.org/10.1016/j.compbiomed.2019.04.030)", "variants": ["MHSMA"]}
{"id": "Metric-Type of Numerical Tables", "title": "Metric-Type Identification for Multi-Level Header Numerical Tables in Scientific Papers", "contents": "**Metric-Type of Numerical Tables** is a dataset extracted from scientific papers (ACL anthology website) consisting of header tables, captions, and metric-types.\r\n\r\nImage source: [Suadaa et al.](https://arxiv.org/pdf/2102.00819v1.pdf)", "variants": ["Metric-Type of Numerical Tables"]}
{"id": "Deeply vocal characterizer", "title": "", "contents": "Deeply vocal characterizer is a human nonverbal vocalization dataset. This sample dataset consists of about 0.6 hours(56.7 hours in the full set) of audio(16 kHz, 16-bit, mono) across 16 human nonverbal vocalization classes, including throat-clearing, coughing, laughing, panting, and etc. The audio contents are crowdsourced by the general public of South Korea.\r\n\r\nThe dataset is a subset(approximately 1%) of a much bigger dataset which were recorded under the same circumstances as these open-source datasets. Please contact us(contact@deeplyinc.com) for the full set with the research/commercial license.", "variants": ["Deeply vocal characterizer"]}
{"id": "Deeply Korean read speech", "title": "", "contents": "Deeply Korean read speech corpus contains pairs of Korean speakers reading a script with *__3 distinct text sentiments (negative, neutral, positive)__*, with *__3 distinct voice sentiments (negative, neutral, positive)__*, are recorded. The recordings took place in *__3 different types of places__*, which are *an anechoic chamber, studio apartment, and dance studio*, of which the level of reverberation differs. And in order to examine the effect of the distance of mic from the source and device, every experiment is recorded at *__3 distinct distances__* with *__2 types of smartphone__*, *iPhone X, and Galaxy S7*.\r\n\r\nThis sample dataset consists of about 3 hours(290 hours in the full set) of audio(16 kHz, 16-bit, mono), and one pair of speakers. The dataset is a subset(approximately 1%) of a much bigger dataset which were recorded under the same circumstances as these open-source datasets. Please contact us(contact@deeplyinc.com) for the full set with the research/commercial license.", "variants": ["Deeply Korean read speech"]}
{"id": "Deeply Parent-Child vocal interaction", "title": "", "contents": "Deeply Parent-Child Vocal Interaction contains the interaction of 24 pairs of parent and child(total 48 speakers), such as *__reading fairy tales, singing children’s songs, conversing, and others__*, is recorded. The recordings took place in *__3 different types of places__*, which are *an anechoic chamber, studio apartment, and dance studio*, of which the level of reverberation differs. And in order to examine the effect of the distance of mic from the source and device, every experiment is recorded at *__3 distinct distances)__* with *__2 types of smartphone__*, *iPhone X, and Galaxy S7*.\r\n\r\nThis sample dataset consists of about 3 hours(282 hours in the full set) of audio(16 kHz, 16-bit, mono), and one pair of speakers. The dataset is a subset(approximately 1%) of a much bigger dataset which were recorded under the same circumstances as these open-source datasets. Please contact us(contact@deeplyinc.com) for the full set with the research/commercial license.", "variants": ["Deeply Parent-Child vocal interaction"]}
{"id": "MIT-BIH AFDB", "title": "", "contents": "This database includes 25 long-term ECG recordings of human subjects with atrial fibrillation (mostly paroxysmal).\r\n\r\nOf these, 23 records include the two ECG signals (in the .dat files); records 00735 and 03665 are represented only by the rhythm (.atr) and unaudited beat (.qrs annotation files.\r\n\r\nThe individual recordings are each 10 hours in duration, and contain two ECG signals each sampled at 250 samples per second with 12-bit resolution over a range of ±10 millivolts. The original analog recordings were made at Boston's Beth Israel Hospital (now the Beth Israel Deaconess Medical Center) using ambulatory ECG recorders with a typical recording bandwidth of approximately 0.1 Hz to 40 Hz. The rhythm annotation files (with the suffix .atr) were prepared manually; these contain rhythm annotations of types (AFIB (atrial fibrillation), (AFL (atrial flutter), (J (AV junctional rhythm), and (N (used to indicate all other rhythms). (The original rhythm annotation files, still available in the old directory, used AF, AFL, J, and N to mark these rhythms; the atr annotations in this directory have been revised for consistency with those used for the MIT-BIH Arrhythmia Database.) Beat annotation files (with the suffix .qrs) were prepared using an automated detector and have not been corrected manually. For some records, manually corrected beat annotation files (with the suffix .qrsc) are available. (The .qrs annotations may be useful for studies of methods for automated AF detection, where such methods must be robust with respect to typical QRS detection errors. The .qrsc annotations may be preferred for basic studies of AF itself, where QRS detection errors would be confounding.) Note that in both .qrs and .qrsc files, no distinction is made among beat types (all beats are labelled as if normal).\r\n\r\nSource: [MIT-BIH Atrial Fibrillation Database](https://physionet.org/content/afdb/1.0.0/)", "variants": ["MIT-BIH AFDB"]}
{"id": "ARC-DA", "title": "Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the Direct-Answer AI2 Reasoning Challenge", "contents": "**ARC Direct Answer Questions** (**ARC-DA**) dataset consists of 2,985 grade-school level, direct-answer (\"open response\", \"free form\") science questions derived from the ARC multiple-choice question set released as part of the AI2 Reasoning Challenge in 2018.\r\n\r\n### How the dataset was built\r\nThese questions were derived from the ARC multiple-choice question set released as part of the AI2 Reasoning Challenge in 2018. The ARC Easy and ARC Challenge set questions in the original dataset were combined and then filtered/modified by the following process:\r\n\r\n- Turking: Each of the multiple-choice questions was presented as a direct answer question to five crowdsourced workers to gather additional answers.\r\n\r\n- Heuristic filtering: The questions were filtered based on the following heuristic filters:\r\n    - Questions having a threshold number of turker answers, as a proxy for concreteness of the question.\r\n    - Questions having at least two turker-provided answers with word overlap, as a measure of confidence in the correctness of the answers, and also straightforwardness of the question.\r\n    - Other heuristics to identify questions that only make sense as multiple-choice questions, such as, questions starting with the phrase “Which of the following”.\r\n\r\n- Further manual vetting: We had volunteers in house do another pass of vetting where they:\r\n     - Marked highly open-ended questions with too many answer choices, such as “Name an insect”, or otherwise invalid questions, for removal. These are filtered out.\r\n     - Removed some of the bad answers gathered from turking.\r\n     - Reworded questions to make them more suited to direct answer question format, for e.g., a question such as “What element is contained in table salt?” which would make sense as a multiple-choice question, needs be reworded to something like “Name an element present in table salt”.\r\n     - Added any additional answers to the questions they could think of that were not present in the turker provided answers.\r\n\r\nImage source: [ARC-DA dataset](https://allenai.org/data/arc-da)", "variants": ["ARC-DA"]}
{"id": "Switchboard-1 Corpus", "title": "", "contents": "The Switchboard-1 Telephone Speech Corpus (LDC97S62) consists of approximately 260 hours of speech and was originally collected by Texas Instruments in 1990-1, under DARPA sponsorship. The first release of the corpus was published by NIST and distributed by the LDC in 1992-3.\r\n\r\nSwitchboard is a collection of about 2,400 two-sided telephone conversations among 543 speakers (302 male, 241 female) from all areas of the United States. A computer-driven robot operator system handled the calls, giving the caller appropriate recorded prompts, selecting and dialing another person (the callee) to take part in a conversation, introducing a topic for discussion and recording the speech from the two subjects into separate channels until the conversation was finished. About 70 topics were provided, of which about 50 were used frequently. Selection of topics and callees was constrained so that: (1) no two speakers would converse together more than once and (2) no one spoke more than once on a given topic.\r\n\r\nSource: [https://catalog.ldc.upenn.edu/LDC97S62](https://catalog.ldc.upenn.edu/LDC97S62)", "variants": ["Switchboard   Hub500", "262 - hour SWB - 1 training data", "Switchboard corpus", "Switchboard-1 Corpus"]}
{"id": "MRDA", "title": "", "contents": "The **MRDA** corpus consists of about 75 hours of speech from 75 naturally-occurring meetings among 53 speakers. The tagset used for labeling is a modified version of the SWBD-DAMSL tagset. It is annotated with three types of information: marking of the dialogue act segment boundaries, marking of the dialogue acts and marking of correspondences between dialogue acts.\r\n\r\nDescription from [NLP Progress](http://nlpprogress.com/english/dialogue.html)", "variants": ["ICSI Meeting Recorder Dialog Act (MRDA) corpus", "MRDA"]}
{"id": "Synthetic and Real Apache Log Records", "title": "On Automatic Parsing of Log Records", "contents": "Each file contains a specific dataset described in the [paper](https://arxiv.org/abs/2102.06320) \"On Automatic Parsing of Log Records\". For example, `T_E.txt` contains the data for the dataset $T_E$. \r\n\r\nIn a file, each log string resides on a separate line and contains a 2-tuple separated by tab (`\\t`). The first element of the tuple is the actual log string that has to be parsed. The second element is the corresponding “translation” specifying the field name for each of the characters of the first element.", "variants": ["Synthetic and Real Apache Log Records"]}
{"id": "COVID-19 Fake News Dataset", "title": "Fighting an Infodemic: COVID-19 Fake News Dataset", "contents": "Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news and rumors are rampant on social media. Believing in rumors can cause significant harm. This is further exacerbated at the time of a pandemic. To tackle this, we curate and release a manually annotated dataset of 10,700 social media posts and articles of real and fake news on COVID-19. We benchmark the annotated dataset with four machine learning baselines - Decision Tree, Logistic Regression , Gradient Boost , and Support Vector Machine (SVM). We obtain the best performance of 93.46\\% F1-score with SVM.", "variants": ["COVID-19 Fake News Dataset"]}
{"id": "Real Blur Dataset", "title": "Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms", "contents": "The dataset consists of 4,738 pairs of images of 232 different scenes including reference pairs. All images were captured both in the camera raw and JPEG formats, hence generating two datasets: RealBlur-R from the raw images, and RealBlur-J from the JPEG images. Each training set consists of 3,758 image pairs, while each test set consists of 980 image pairs.\r\n\r\nThe deblurring result is first aligned to its ground truth sharp image using a homography estimated by the enhanced correlation coefficients method, and PSNR or SSIM is computed in sRGB color space.", "variants": ["Real Blur Dataset", "RealBlur-R", "RealBlur-J", "RealBlur-R (trained on GoPro)", "RealBlur-J (trained on GoPro)"]}
{"id": "DAGM2007", "title": "", "contents": "This is a synthetic dataset for defect detection on textured surfaces. It was originally created for a competition at the 2007 symposium of the DAGM (Deutsche Arbeitsgemeinschaft für Mustererkennung e.V., the German chapter of the International Association for Pattern Recognition). The competition was hosted together with the GNSS (German Chapter of the European Neural Network Society).\r\n\r\nAfter the competition, the dataset has been used as a test dataset in multiple projects and research papers. It is publicly available from the University of Heidelberg website (Heidelberg Collaboratory for Image Processing).\r\n\r\nThe data is artificially generated, but similar to real world problems. The first six out of ten datasets, denoted as development datasets, are supposed to be used for algorithm development. The remaining four datasets, which are referred to as competition datasets, can be used to evaluate the performance. Researchers should consider not using or analyzing the competition datasets before the development is completed as a code of honour.", "variants": ["DAGM2007"]}
{"id": "DSTC 8 Track 2", "title": "", "contents": "Dialog System Technology Challenges 8 (DSTC) Track 2 builds on the success of DSTC 7 Track 1 (NOESIS: Noetic End-to-End Response Selection Challenge). It proposes an extension of the task, incorporating new elements that are vital for the creation of a deployed task-oriented dialogue system. Specifically, three new dimensions are added to the challenge:\r\n\r\n- Conversations with more than 2 participants\r\n- Predicting whether a dialogue has solved the problem yet,\r\n- Handling multiple simultaneous conversations. Each of these adds an exciting new dimension and brings the task closer to the creation of systems able to handle the complexity of real-world conversation.\r\n- This challenge is offered with two goal oriented dialog datasets, used in 4 subtasks. \r\n\r\nSource: [Dialog System Technology Challenges 8 (DSTC 8) - Track 2](https://github.com/dstc8-track2/NOESIS-II/)\r\n\r\nImage source: [DSTC 8 Track 2](https://github.com/dstc8-track2/NOESIS-II/)", "variants": ["DSTC 8 Track 2"]}
{"id": "Ubuntu Chat Corpus", "title": "", "contents": "The **Ubuntu Chat Corpus** (**UCC**) is composed of archived chat logs from Ubuntu's Internet Relay Chat technical support channels. Ubuntu uses IRC as one of many modes of technical support -- it offers real-time problem solving. The authors have taken some of the archived messages (which are in the public domain), reorganized the file structure, removed some unnecessary system messages, and compressed them to make it easier to obtain.\r\n\r\nSource: [Ubuntu Chat Corpus](https://daviduthus.org/UCC/)\r\n\r\nImage Source: [Ubuntu Chat Corpus](https://daviduthus.org/UCC/)", "variants": ["Ubuntu Chat Corpus"]}
{"id": "OSCAR", "title": "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages", "contents": "**OSCAR** or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture. The dataset used for training multilingual models such as BART incorporates 138 GB of text.", "variants": ["OSCAR"]}
{"id": "S-SOD", "title": "Densely Deformable Efficient Salient Object Detection Network", "contents": "To validate the generalization abilities of SOD models, we create a small-scale dataset by collecting the most challenging images with varying brightness and contrast, background and foreground colors overlap, among many others. We conclude that the current models, including ours, are not trust-worthy for real-world practice, demanding extensive future research for more efficient and generalized SOD models.", "variants": ["S-SOD"]}
{"id": "CommonCrawl", "title": "", "contents": "The Common Crawl corpus contains petabytes of data collected over 12 years of web crawling. The corpus contains raw web page data, metadata extracts and text extracts. Common Crawl data is stored on Amazon Web Services’ Public Data Sets and on multiple academic cloud platforms across the world.", "variants": ["CommonCrawl"]}
{"id": "CEDAR Signature", "title": "Machine learning for signature verification", "contents": "CEDAR Signature is a database of off-line signatures for signature verification. Each of 55 individuals contributed 24 signatures thereby creating 1,320 genuine signatures. Some were asked to forge three other writers’ signatures, eight times\r\nper subject, thus creating 1,320 forgeries. Each signature was scanned at 300 dpi gray-scale and binarized using a gray-scale histogram. Salt pepper noise removal and slant normalization were two steps involved in image preprocessing. The database has 24 genuines and 24 forgeries available for each writer.", "variants": ["CEDAR Signature"]}
{"id": "PAQ", "title": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them", "contents": "**Probably Asked Questions** (**PAQ**) is a very large resource of 65M automatically-generated QA-pairs. PAQ is a semi-structured Knowledge Base (KB) of 65M natural language QA-pairs, which models can memorise and/or learn to retrieve from. PAQ differs from traditional KBs in that questions and answers are stored in natural language, and that questions are generated such that they are likely to appear in ODQA datasets. PAQ is automatically constructed using a question generation model and Wikipedia.\r\n\r\nSource: [Lewis et al.](https://arxiv.org/pdf/2102.07033.pdf)\r\n\r\nImage source: [Lewis et al.](https://arxiv.org/pdf/2102.07033.pdf)", "variants": ["PAQ"]}
{"id": "BABEL", "title": "", "contents": "**BABEL** is a multilingual corpus of conversational telephone speech from IARPA, which includes Asian and African languages.", "variants": []}
{"id": "BTFDBB", "title": "", "contents": "Reflectance measurements of Bidirectional Texture Functions (BTFs)\r\n\r\nDatabase contains both flat samples:\r\n\r\n![](https://cg.cs.uni-bonn.de/typo3temp/pics/T_d3f3eb3fec.png)\r\n![](https://cg.cs.uni-bonn.de/typo3temp/pics/L_0446825446.png)\r\n![](https://cg.cs.uni-bonn.de/typo3temp/pics/S_1eb36192f6.png)\r\n\r\nas well as 3D geometry with texture mapped BTFs:\r\n\r\n![](https://cg.cs.uni-bonn.de/typo3temp/pics/g_6638002797.jpg)\r\n![](https://cg.cs.uni-bonn.de/typo3temp/pics/O_0f9dce16bf.jpg)\r\n\r\nfurthermore, there are some multispectral BTFs:\r\n\r\n![](https://cg.cs.uni-bonn.de/typo3temp/pics/p_cc0a283544.jpg)", "variants": ["BTFDBB"]}
{"id": "CoNLL-2014 Shared Task: Grammatical Error Correction", "title": "The CoNLL-2014 Shared Task on Grammatical Error Correction", "contents": "CoNLL-2014 will continue the CoNLL tradition of having a high profile shared task in natural language processing. This year's shared task will be grammatical error correction, a continuation of the CoNLL shared task in 2013. A participating system in this shared task is given short English texts written by non-native speakers of English. The system detects the grammatical errors present in the input texts, and returns the corrected essays. The shared task in 2014 will require a participating system to correct all errors present in an essay (i.e., not restricted to just five error types in 2013). Also, the evaluation metric will be changed to F0.5, weighting precision twice as much as recall.\r\n\r\nThe grammatical error correction task is impactful since it is estimated that hundreds of millions of people in the world are learning English and they benefit directly from an automated grammar checker. However, for many error types, current grammatical error correction methods do not achieve a high performance and thus more research is needed.\r\n\r\nSource: [CoNLL-2014 Shared Task: Grammatical Error Correction](https://www.comp.nus.edu.sg/~nlp/conll14st.html)\r\n\r\nImage source: [Tou Ng et al.](https://www.aclweb.org/anthology/W14-1701.pdf)", "variants": ["CoNLL-2014 Shared Task", "CoNLL-2014 Shared Task: Grammatical Error Correction", "CoNLL-2014 Shared Task (10 annotations)"]}
{"id": "UBOFAB19", "title": "Learned Fitting of Spatially Varying BRDFs", "contents": "A database of several hundred high quality fabric material measurements, provided as carefully calibrated rectified HDR images, together with SVBRDF fits.\r\n\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/UBOFAB19/train/img/mat0061pv.png)\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/UBOFAB19/train/img/mat0025pv.png)\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/UBOFAB19/val/img/mat0047pv.png)\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/UBOFAB19/val/img/mat0056pv.png)\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/UBOFAB19/val/img/mat0058pv.png)\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/UBOFAB19/val/img/mat0059pv.png)\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/UBOFAB19/train/img/mat0054pv.png)\r\n\r\nMeasurement HDR images are provided in OpenEXR format\r\n\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/UBOFAB19/val/html/img/mat0047_inputs.png)\r\n\r\nSVBRDF fits are provided in X-Rite AxF format\r\n\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/UBOFAB19/val/html/img/mat0047_labels.png)\r\n\r\nFurther geometric and radiometric calibration data is available as well.", "variants": ["UBOFAB19"]}
{"id": "APPBENCH", "title": "Fabric Appearance Benchmark", "contents": "A database of 56 high quality fabric material measurements, provided as carefully calibrated rectified HDR images, together with SVBRDF fits. Used in the [Fabric Appearance Challange](https://competitions.codalab.org/competitions/24979).\r\n\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/APPBENCH/html/videos/mat0386.webp)\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/APPBENCH/html/videos/mat0396.webp)\r\n\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/APPBENCH/html/videos/mat0413.webp)\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/APPBENCH/html/videos/mat0410.webp)\r\n\r\nMeasurement HDR images are provided in OpenEXR format\r\n\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/APPBENCH/html/img/mat0386_inputs.png)\r\n\r\nSVBRDF fits are provided in X-Rite AxF format\r\n\r\n![](https://cg.cs.uni-bonn.de/uploads/svbrdfs/APPBENCH/html/img/mat0386_labels.png)\r\n\r\nFurther geometric and radiometric calibration data is available as well.", "variants": ["APPBENCH"]}
{"id": "Chickenpox Cases in Hungary", "title": "Chickenpox Cases in Hungary: a Benchmark Dataset for Spatiotemporal Signal Processing with Graph Neural Networks", "contents": "**Chickenpox Cases in Hungary** is a spatio-temporal dataset of weekly chickenpox (childhood disease) cases from Hungary. It can be used as a longitudinal dataset for benchmarking the predictive performance of spatiotemporal graph neural network architectures. The dataset consists of a county-level adjacency matrix and time series of the county-level reported cases between 2005 and 2015. There are 2 specific related tasks:\r\n\r\n- County level case count prediction.\r\n- National level case count prediction.", "variants": ["Chickenpox Cases in Hungary"]}
{"id": "WNUT 2017 Emerging and Rare entity recognition", "title": "", "contents": "This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarisation), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet “so.. kktny in 30 mins?” - even human experts find entity kktny hard to detect and resolve. This task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text.\r\n\r\nThe goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities.", "variants": ["Long-tail emerging entities", "WNUT 2017 Emerging and Rare entity recognition"]}
{"id": "RUSHOLD", "title": "", "contents": "RUHSOLD is hate speech and offensive language dataset in Roman Urdu. The dataset contains over 10 thousand tweets that are hand labelled into the following categories:\r\n1) Abusive/Offensive\r\n2) Untargeted\r\n3) Sexism\r\n4) Religious\r\n5) Neutral", "variants": ["RUSHOLD"]}
{"id": "PNT", "title": "", "contents": "**The Parsing Time Normalizations** (**PNT**) corpus in SCATE format allows the representation of a wider variety of time expressions than previous approaches. This corpus was release with SemEval 2018 Task 6.\r\n\r\nSource: [NLP Progress](http://nlpprogress.com/english/temporal_processing.html)", "variants": ["PNT"]}
{"id": "WHU-Specular dataset", "title": "Learning to Detect Specular Highlights from Real-world Images", "contents": "WHU-Specular is a large dataset of annotated specular highlight regions created from real-world images. It can be used for specular highlight detection task. It contains 4310 image pairs (specular images and corresponding highlight masks). We randomly selected 3,017 images as the training set, and other 1293 images as the testing set.\r\n\r\nSource: [SHDNet](https://github.com/fu123456/SHDNet)", "variants": ["WHU-Specular dataset"]}
{"id": "B-T4SA", "title": "", "contents": "", "variants": ["B-T4SA"]}
{"id": "AVSpeech", "title": "Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation", "contents": "**AVSpeech** is a large-scale audio-visual dataset comprising\r\nspeech clips with no interfering background signals. The segments\r\nare of varying length, between 3 and 10 seconds long, and in each clip\r\nthe only visible face in the video and audible sound in the soundtrack\r\nbelong to a single speaking person. In total, the dataset contains\r\nroughly 4700 hours of video segments with approximately 150,000\r\ndistinct speakers, spanning a wide variety of people, languages\r\nand face poses.", "variants": ["AVSpeech"]}
{"id": "AM-2k", "title": "", "contents": "AM-2k contains 2,000 high-resolution natural animal images from 20 categories along with manually labeled alpha mattes.", "variants": ["AM-2K", "AM-2k"]}
{"id": "BG-20k", "title": "End-to-end Animal Image Matting", "contents": "BG-20k contains 20,000 high-resolution background images excluded salient objects, which can be used to help generate high quality synthetic data.", "variants": ["BG-20k"]}
{"id": "OntoNotes 5.0", "title": "", "contents": "**OntoNotes 5.0** is a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).\r\n\r\nOntoNotes Release 5.0 contains the content of earlier releases - and adds source data from and/or additional annotations for, newswire, broadcast news, broadcast conversation, telephone conversation and web data in English and Chinese and newswire data in Arabic.\r\n\r\nSource: [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)", "variants": ["Ontonotes v5 (English)", "OntoNotes", "OntoNotes 5.0"]}
{"id": "MECCANO", "title": "The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain", "contents": "The MECCANO dataset is the first dataset of egocentric videos to study human-object interactions in industrial-like settings.\r\nThe MECCANO dataset has been acquired in an industrial-like scenario in which subjects built a toy model of a motorbike. We considered 20 object classes which include the 16 classes categorizing the 49 components, the two tools (screwdriver and wrench), the instructions booklet and a partial_model class.\r\n\r\nAdditional details related to the MECCANO:\r\n\r\n20 different subjects in 2 countries (IT, U.K.)\r\nVideo Acquisition: 1920x1080 at 12.00 fps\r\n11 training videos and 9 validation/test videos\r\n8857 video segments temporally annotated indicating the verbs which describe the actions performed\r\n64349 active objects annotated with bounding boxes\r\n12 verb classes, 20 objects classes and 61 action classes", "variants": ["MECCANO"]}
{"id": "ecoset", "title": "An ecologically motivated image dataset for deep learning yields better models of human vision", "contents": "**Ecoset**, an ecologically motivated image dataset, is a large-scale image dataset designed for human visual neuroscience, which consists of over 1.5 million images from 565 basic-level categories. Category selection was based on English nouns that most frequently occur in spoken language (estimated on a set of 51 million words obtained from American television and film subtitles) and concreteness ratings from human observers. Ecoset consists of basic-level categories (including human categories man, woman, and child) that describe physical things in the world (rather than abstract concepts) that are important to humans.", "variants": ["ecoset"]}
{"id": "MSU Deinterlacer Benchmark", "title": "", "contents": "This is a dataset for video deinterlacing problem. The dataset contains 40 video sequences. Each sequence's length is 1 second. Resolution of all video sequences is 1920x1080. FPS varies from 24 to 60. TFF interlacing was used to get interlaced data from GT.", "variants": ["MSU Deinterlacer Benchmark"]}
{"id": "StrategyQA", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies", "contents": "**StrategyQA** is a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy.\r\nIt includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs.\r\nQuestions in StrategyQA are short, topic-diverse, and cover a wide range of strategies.", "variants": ["StrategyQA"]}
{"id": "ICB", "title": "", "contents": "A carefully chosen set of high-resolution high-precision natural images suited for compression algorithm evaluation.\r\n\r\nThe images historically used for compression research (lena, barbra, pepper etc...) have outlived their useful life and its about time they become a part of history only. They are too small, come from data sources too old and are available in only 8-bit precision.\r\n\r\nThese high-resolution high-precision images have been carefully selected to aid in image compression research and algorithm evaluation. These are photographic images chosen to come from a wide variety of sources and each one picked to stress different aspects of algorithms. Images are available in 8-bit, 16-bit and 16-bit linear variations, RGB and gray.\r\n\r\nThese Images are available without any prohibitive copyright restrictions.\r\n\r\nThese images are (c) there respective owners. You are granted full redistribution and publication rights on these images provided:\r\n\r\n1. The origin of the pictures must not be misrepresented; you must not claim that you took the original pictures. If you use, publish or redistribute them, an acknowledgment would be appreciated but is not required.\r\n2. Altered versions must be plainly marked as such, and must not be misinterpreted as being the originals.\r\n3. No payment is required for distribution of this material, it must be available freely under the conditions stated here. That is, it is prohibited to sell the material.\r\n4. This notice may not be removed or altered from any distribution.\r\n\r\n*For grayscale evaluation, use the Grayscale 8 bit dataset, for color evaluation, use the Color 8 bit dataset.*\r\n\r\n```\r\n@online{icb,\r\n  author = {Rawzor},\r\n  title  = {Image Compression Benchmark},\r\n  url    = {http://imagecompression.info/}\r\n}\r\n```", "variants": ["ICB", "ICB (Quality 10 Color)", "ICB (Quality 10 Grayscale)", "ICB (Quality 20 Color)", "ICB (Quality 30 Color)", "ICB (Quality 20 Grayscale)", "ICB (Quality 30 Grayscale)"]}
{"id": "LIVE1", "title": "", "contents": "Quality Assessment research strongly depends upon subjective experiments to provide calibration data as well as a testing mechanism. After all, the goal of all QA research is to make quality predictions that are in agreement with subjective opinion of human observers. In order to calibrate QA algorithms and test their performance, a data set of images and videos whose quality has been ranked by human subjects is required. The QA algorithm may be trained on part of this data set, and tested on the rest. \r\n\r\n```\r\n\r\n@article{sheikh2006statistical,\r\n  title={A statistical evaluation of recent full reference image quality assessment algorithms},\r\n  author={Sheikh, Hamid R and Sabir, Muhammad F and Bovik, Alan C},\r\n  journal={IEEE Transactions on image processing},\r\n  volume={15},\r\n  number={11},\r\n  pages={3440--3451},\r\n  year={2006},\r\n  publisher={IEEE}\r\n}\r\n\r\n@online{sheikh2006live,\r\n  title={LIVE image quality assessment database},\r\n  author={Sheikh, HR and Wang, Z and Cormack, L and Bovik, AC},\r\n  url={http://live.ece.utexas.edu/research/quality}\r\n}\r\n```", "variants": ["LIVE1", "LIVE1 (Quality 10 Color)", "LIVE1 (Quality 20 Color)", "LIVE1 (Quality 30 Color)", "Live1 (Quality 10 Grayscale)", "LIVE1 (Quality 20 Grayscale)", "LIVE1 (Quality 30 Grayscale)", "LIVE1 (Quality 40 Grayscale)"]}
{"id": "Classic5", "title": "Pointwise shape-adaptive DCT for high-quality deblocking of compressed color images", "contents": "Five classic grayscale images commonly used for image quality assessment tasks.", "variants": ["Classic5", "Classic5 (Quality 10 Grayscale)", "Classic5 (Quality 20 Grayscale)", "Classic5 (Quality 30 Grayscale)", "Classic5 (Quality 40 Grayscale)"]}
{"id": "RailEye3D Dataset", "title": "RGB-D Railway Platform Monitoring and Scene Understanding for Enhanced Passenger Safety", "contents": "This repository provides annotations for the RailEye3D dataset, a collection of train-platform scenarios for applications targeting passenger safety and automation of train dispatching. It consists of 10 image sequences captured at 6 railway stations in Austria. The corresponding image data can be requested by email.\r\n\r\nWe provide annotations for multi-object tracking in both our own unified format as well as the ground-truth format used in the MOTChallenge.\r\n\r\n    anno/unified/bboxes contains csv-files for each frame with object instances defined as: TLx, TLy, BRx, BRy, labelName, objectId\r\n    anno/unified/params provides the additional parameters degree_of_occlusion, beyond_safety_line\r\n    anno/mot contains MOT-compatible gt-files for each sequence\r\n\r\nEach sequence name contains a prefix corresponding to one of the following railway stations:\r\n\r\n    red-00: Stockerau\r\n    red-01: Hausleiten\r\n    red-02: Absdorf-Hippersdorf\r\n    red-03: Kirchberg am Wagram\r\n    red-04: Fels\r\n    red-06: Etsdorf-Straß", "variants": ["RailEye3D Dataset"]}
{"id": "GraspNet-1Billion", "title": "GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping", "contents": "**GraspNet-1Billion** provides large-scale training data and a standard evaluation platform for the task of general robotic grasping. The dataset contains 97,280 RGB-D image with over one billion grasp poses.", "variants": ["GraspNet-1Billion"]}
{"id": "MAEC", "title": "", "contents": "**MAEC** is a new, large-scale multi-modal, text-audio paired, earnings-call dataset named MAEC, based on S&P 1500 companies. \r\n\r\nSource: [MAEC: A Multimodal Aligned Earnings Conference Call Dataset for Financial Risk Prediction](https://dl.acm.org/doi/10.1145/3340531.3412879)", "variants": ["MAEC"]}
{"id": "NuCLS", "title": "NuCLS: A scalable crowdsourcing, deep learning approach and dataset for nucleus classification, localization and segmentation", "contents": "The NuCLS dataset contains over 220,000 labeled nuclei from breast cancer images from TCGA. These nuclei were annotated through the collaborative effort of pathologists, pathology residents, and medical students using the Digital Slide Archive. These data can be used in several ways to develop and validate algorithms for nuclear detection, classification, and segmentation, or as a resource to develop and evaluate methods for interrater analysis.\r\n\r\nData from both single-rater and multi-rater studies are provided. For single-rater data we provide both pathologist-reviewed and uncorrected annotations. For multi-rater datasets we provide annotations generated with and without suggestions from weak segmentation and classification algorithms.\r\n\r\nSource: [Amgad et al.](https://arxiv.org/pdf/2102.09099v1.pdf)\r\n\r\nImage source:  [Amgad et al.](https://arxiv.org/pdf/2102.09099v1.pdf)", "variants": ["NuCLS"]}
{"id": "K-Hairstyle", "title": "K-Hairstyle: A Large-scale Korean hairstyle dataset for virtual hair editing and hairstyle classification", "contents": "K-hairstyle is a novel large-scale Korean hairstyle dataset with 256,679 high-resolution images. In addition, K-hairstyle contains various hair attributes annotated by Korean expert hair stylists and hair segmentation masks.\r\n\r\nSource: [K-Hairstyle: A Large-scale Korean hairstyle dataset for virtual hair editing and hairstyle classification](https://paperswithcode.com/paper/k-hairstyle-a-large-scale-korean-hairstyle)\r\n\r\nImage source: [Kim et al.](https://arxiv.org/pdf/2102.06288v1.pdf)", "variants": ["K-Hairstyle"]}
{"id": "CC12M", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts", "contents": "Conceptual 12M (CC12M) is a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training.\r\n\r\nSource: [Changpinyo et al.](https://arxiv.org/pdf/2102.08981v1.pdf)\r\n\r\nImage source: [Changpinyo et al.](https://arxiv.org/pdf/2102.08981v1.pdf)", "variants": ["CC12M"]}
{"id": "ACDC", "title": "", "contents": "The goal of the challenge is to:\r\n\r\n- compare the performance of automatic methods on the segmentation of the left ventricular endocardium and epicardium as the right ventricular endocardium for both end diastolic and end systolic phase instances;\r\n- compare the performance of automatic methods for the classification of the examinations in five classes (normal case, heart failure with infarction, dilated cardiomyopathy, hypertrophic cardiomyopathy, abnormal right ventricle).\r\n\r\nThe overall **ACDC** dataset was created from real clinical exams acquired at the University Hospital of Dijon. Acquired data were fully anonymized and handled within the regulations set by the local ethical committee of the Hospital of Dijon (France). Our dataset covers several well-defined pathologies with enough cases to (1) properly train machine learning methods and (2) clearly assess the variations of the main physiological parameters obtained from cine-MRI (in particular diastolic volume and ejection fraction). The dataset is composed of 150 exams (all from different patients) divided into 5 evenly distributed subgroups (4 pathological plus 1 healthy subject groups) as described below. Furthermore, each patient comes with the following additional information : weight, height, as well as the diastolic and systolic phase instants.\r\n\r\nThe database is made available to participants through two datasets from the dedicated online evaluation website after a personal registration: i) a training dataset of 100 patients along with the corresponding manual references based on the analysis of one clinical expert; ii) a testing dataset composed of 50 new patients, without manual annotations but with the patient information given above. The raw input images are provided through the Nifti format.\r\n\r\nSource: [Automated Cardiac Diagnosis Challenge](https://acdc.creatis.insa-lyon.fr/description/databases.html)\r\n\r\nImage source: [Automated Cardiac Diagnosis Challenge](https://acdc.creatis.insa-lyon.fr/description/databases.html)", "variants": ["Automatic Cardiac Diagnosis Challenge (ACDC)", "ACDC"]}
{"id": "MoNuSeg", "title": "", "contents": "The dataset for this challenge was obtained by carefully annotating tissue images of several patients with tumors of different organs and who were diagnosed at multiple hospitals. This dataset was created by downloading H&E stained tissue images captured at 40x magnification from TCGA archive. H&E staining is a routine protocol to enhance the contrast of a tissue section and is commonly used for tumor assessment (grading, staging, etc.). Given the diversity of nuclei appearances across multiple organs and patients, and the richness of staining protocols adopted at multiple hospitals, the training datatset will enable the development of robust and generalizable nuclei segmentation techniques that will work right out of the box.\r\n\r\nSource: [MoNuSeg](https://monuseg.grand-challenge.org/Data/)", "variants": ["MoNuSeg"]}
{"id": "AbstRCT - Neoplasm", "title": "", "contents": "The AbstRCT dataset consists of randomized controlled trials retrieved from the MEDLINE database via PubMed search. The trials are annotated with argument components and argumentative relations.\r\n\r\nPaper: [Transformer-Based Argument Mining for Healthcare Applications](https://hal.archives-ouvertes.fr/hal-02879293/)", "variants": ["AbstRCT - Neoplasm"]}
{"id": "DRI Corpus", "title": "A Multi-Layered Annotated Corpus of Scientific Papers", "contents": "The **Dr. Inventor Multi-Layer Scientific Corpus** (**DRI Corpus**) includes 40 Computer Graphics papers, selected by domain experts. Each paper of the Corpus has been annotated by three annotators by providing the following layers of annotations, each one characterizing a core aspect of scientific publications:\r\n\r\n* Scientific discourse: each sentence has been associated to a specific scientific discourse category (Background, Approach, Challenge, Future Work, etc.).\r\n* Subjective statements and novelty: each sentence has been characterized with respect to advantages, disadvantages and novel aspects presented.\r\n* Citation purpose: to each citation has been associated a purpose specifying the reason why the authors of the paper cited the specific piece of research.\r\n* Summary relevance of sentences and hand written summaries: each sentence of the paper has been characterized by an integer score ranging from 1 to 5, to point out the relevance of the same sentence for its inclusion in the summary of the paper. Sentences rated as 5 are the most relevant ones to summarize a paper. For each paper three hand-written summaries (max 250 words) are provided.\r\n\r\nSource: [Dr. Inventor Multi-layer Scientific Corpus](http://sempub.taln.upf.edu/dricorpus)", "variants": ["DRI Corpus"]}
{"id": "PIPAL", "title": "PIPAL: a Large-Scale Image Quality Assessment Dataset for Perceptual Image Restoration", "contents": "PIPAL training set contains 200 reference images, 40 distortion types, 23k distortion images, and more than one million human ratings. Especially, we include GAN-based algorithms’ outputs as a new GAN-based distortion type. We employ the Elo rating system to assign the Mean Opinion Scores (MOS).", "variants": ["PIPAL"]}
{"id": "ReCAM", "title": "", "contents": "Tasks\r\nOur shared task has three subtasks. Subtask 1 and 2 focus on evaluating machine learning models' performance with regard to two definitions of abstractness (Spreen and Schulz, 1966; Changizi, 2008), which we call imperceptibility and nonspecificity, respectively. Subtask 3 aims to provide some insights to their relationships.\r\n\r\n• Subtask 1: ReCAM-Imperceptibility\r\n\r\nConcrete words refer to things, events, and properties that we can perceive directly with our senses (Spreen and Schulz, 1966; Coltheart 1981; Turney et al., 2011), e.g., donut, trees, and red. In contrast, abstract words refer to ideas and concepts that are distant from immediate perception. Examples include objective, culture, and economy. In subtask 1, the participanting systems are required to perform reading comprehension of abstract meaning for imperceptible concepts.\r\n\r\nBelow is an example. Given a passage and a question, your model needs to choose from the five candidates the best one for replacing @placeholder.\r\n\r\n\r\n\r\n \r\n\r\n• Subtask 2: ReCAM-Nonspecificity\r\n \r\nSubtask 2 focuses on a different type of definition. Compared to concrete concepts like groundhog and whale, hypernyms such as vertebrate are regarded as more abstract (Changizi, 2008). \r\n \r\n• Subtask 3: ReCAM-Intersection\r\nSubtask 3 aims to provide more insights to the relationship of the two views on abstractness, In this subtask, we test the performance of a system that is trained on one definition and evaluted on the other.", "variants": ["ReCAM"]}
{"id": "RSPECT", "title": "", "contents": "**The RSNA Pulmonary Embolism CT** (**RSPECT**) Dataset is composed of CT pulmonary angiogram images and annotations related to pulmonary embolism. It's part of the 2020 RSNA Pulmonary Embolism Detection Challenge which invited researchers to develop machine-learning algorithms to detect and characterize instances of pulmonary embolism (PE) on chest CT studies. The competition, conducted in collaboration with the Society of Thoracic Radiology (STR), involved creating the largest publicly available annotated PE dataset, comprised of more than 12,000 CT studies. Imaging data was contributed by five international research centers and labeled with detailed clinical annotations by a group of more than 80 expert thoracic radiologists. For the first time in an RSNA data challenge, the rules required competitors to submit and run their code in a standard shared environment, producing simpler, more readily usable models.\r\n\r\nSource: [](https://www.rsna.org/education/ai-resources-and-training/ai-image-challenge/rsna-pe-detection-challenge-2020)", "variants": ["RSPECT"]}
{"id": "SEP-28k", "title": "SEP-28k: A Dataset for Stuttering Event Detection From Podcasts With People Who Stutter", "contents": "Stuttering Events in Podcasts (SEP-28k) is a dataset containing over 28k clips labeled with five event types including blocks, prolongations, sound repetitions, word repetitions, and interjections. Audio comes from public podcasts largely consisting of people who stutter interviewing other people who stutter. \r\n\r\nSource: [Lea et al.](https://arxiv.org/pdf/2102.12394.pdf)\r\n\r\nImage source: [Lea et al.](https://arxiv.org/pdf/2102.12394.pdf)", "variants": ["SEP-28k"]}
{"id": "FluencyBank", "title": "", "contents": "**FluencyBank** is a shared database for the study of fluency development. Participants include typically-developing monolingual and bilingual children, children and adults who stutter (C/AWS) or who clutter (C/AWC), and second language learners.\r\n\r\n\r\nImage Source: [FluencyBank](https://fluency.talkbank.org/)", "variants": ["FluencyBank"]}
{"id": "MHIST", "title": "A Petri Dish for Histopathology Image Analysis", "contents": "The **m**inimalist **hist**opathology image analysis dataset (**MHIST**) is a binary classification dataset of 3,152 fixed-size images of colorectal polyps, each with a gold-standard label determined by the majority vote of seven board-certified gastrointestinal pathologists. MHIST also includes each image’s annotator agreement level. As a minimalist dataset, MHIST occupies less than 400 MB of disk space, and a ResNet-18 baseline can be trained to convergence on MHIST in just 6 minutes using approximately 3.5 GB of memory on a NVIDIA RTX 3090. As example use cases, the authors use MHIST to study natural questions that arise in histopathology image classification such as how dataset size, network depth, transfer learning, and high-disagreement examples affect model performance.\r\n\r\nSource: [Wei et al.](https://arxiv.org/pdf/2101.12355.pdf)\r\n\r\nImage source: [Wei et al.](https://arxiv.org/pdf/2101.12355.pdf)", "variants": ["MHIST"]}
{"id": "CC-News", "title": "", "contents": "**CommonCrawl News** is a dataset containing news articles from news sites all over the world. The dataset is available in form of Web ARChive (WARC) files that are released on a daily basis.\r\n\r\nSource: [https://commoncrawl.org/2016/10/news-dataset-available/](https://commoncrawl.org/2016/10/news-dataset-available/)", "variants": ["CC-News"]}
{"id": "MalNet", "title": "A Large-Scale Database for Graph Representation Learning", "contents": "MalNet is a large public graph database, representing a large-scale ontology of software function call graphs. MalNet contains over 1.2 million graphs, averaging over 17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696 families.\r\n\r\nImage Source: [Expore MalNet](https://mal-net.org/explore)", "variants": ["MalNet"]}
{"id": "Maintenance of Wakefulness Test (MWT) recordings", "title": "", "contents": "Maintenance of Wakefulness Test (MWT) is a dataset of recordings with microsleep episodes and drowsiness.\r\n\r\nCite as:\r\nHertig-Godeschalk Anneke, Skorucak Jelena, Malafeev Alexander, Achermann Peter, Mathis Johannes, & Schreier David R. (2019). Maintenance of Wakefulness Test (MWT) recordings (Version v1) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.325171\r\n\r\nEach file contains a MWT trial (first trial after noon) recording of a patient. The data contains occipital EEG and EOG data. All signals were bandpass filtered between 0.5-45 Hz.\r\n\r\nIn each file, the data is structured as the following:\r\n\r\n    fs:   sampling rate.\r\n    eeg_O1:   EEG channel O1-M2 where M2 is the mastoid electrode on the opposite side.\r\n    eeg_O2:   EEG channel O2-M1 where M1 is the mastoid electrode on the opposite side.\r\n    E1 and E2:   EOG channels for left and right eye, both referenced to M1.\r\n    labels_O1 and labels_O2:   arrays with expert scoring (0-wake, 1-MSE, 2-MSEc, 3-ED, according to the BERN scoring criteria published in Hertig-Godeschalk et al. doi:10.1093/sleep/zsz163.); length of the arrays is the same as for other signals, i.e. there is a label per sample.\r\n    prec:   amount of signal samples per label, in this case it is 1. variables prec and half_prec were not used.\r\n    num_Labels:   length of the signal in samples.\r\n\r\nFurther descriptions, details, and outcomes can be found in the related studies. The published studies which are based on this data and address the borderland between wakefulness and sleep, i.e. microsleep episodes, are listed under related/alternative identifiers.", "variants": ["Maintenance of Wakefulness Test (MWT) recordings"]}
{"id": "darpa_sd2_perovskites", "title": "Can Machines “Learn” Halide Perovskite Crystal Formation without Accurate Physicochemical Features?", "contents": "Included in this content:\r\n\r\n  * 0045.perovksitedata.csv - main dataset used in this article.  A more detailed description can be found in the “dataset overview” section below\r\n  * Chemical Inventory.csv - the hand curated file of all chemicals used in the construction of the perovskite dataset.  This file includes identifiers, chemical properties, and other information.\r\n  * ExcessMolarVolumeData.xlsx - record of experimental data, computations, and final dataset used in the generation of the excess molar volume plots.\r\n  * MLModelMetrics.xlsx - all of the ML metrics organized in one place (excludes reactant set specific breakdown, see ML_Logs.zip for those files).\r\n  * OrganoammoniumDensityDataset.xlsx - complete set of the data used to generate the density values.  Example calculations included.\r\n  * model_matchup_main.py - python pipeline used to generate all of the ML runs associated with the article.  More detailed instructions on the operation of this code is included in the “ML Code” Section below.  This file is also hosted on\r\n    * GIT: https://github.com/ipendlet/MLScripts/blob/master/temp_densityconc/model_matchup_main_20191231.py \r\n  * SolutionVolumeDataset - complete set of 219 solutions in the perovskite dataset.  Tabs include the automatically generated reagent information from ESCALATE, hand curated reagent information from early runs, and the generation of the dataset used in the creation of Figure 5.\r\n  * error_auditing.zip - code and historical datasets used for reporting the dataset auditing.\r\n  * “AllCode.zip” which contains:\r\n    * model_matchup_main_20191231.py - python pipeline used to generate all of the ML runs associated with the article.  More detailed instructions on the operation of this code is included in the “ML Code” Section below.  This file is also hosted on\r\n      * GIT: https://github.com/ipendlet/MLScripts/blob/master/temp_densityconc/0045.perovskitedata.csv \r\n    * VmE_CurveFitandPlot.py - python code for generating the third order polynomial fit to the VmE vs mole fraction of FAH included in the main text. Requires the ‘MolFractionResults.csv’ to function (also included).\r\n    * Calculation_Vm_Ve_CURVEFITTING.nb - mathematica code for generating the third order polynomial fit to the VmE vs mole fraction of FAH included in the main text.  \r\n    * Covariance_Analysis.py - python code for ingesting and plotting the covariance of features and volumes in the perovskite dataset.  Includes renaming dictionaries used for the publication.\r\n    * FeatureComparison_Plotting.py - python code for reading in and plotting features for the ‘GBT’ and ‘OHGBT’ folders in this directory.  The code parses the contents of these folders and generates feature comparison metrics used for Figure 9 and the associated Figure S8. Some assembly required.\r\n    * Requirements.txt - all of the packages used in the generation of this paper\r\n    * 0045.perovskitedata.csv - the main dataset described throughout the article.  This file is required to run some of the code and is therefore kept near the code. \r\n  * “ML_Logs.zip” which contains:\r\n    * A folder describing every model generated for this article.  In each folder there are a number of files:\r\n      * Features_named_important.csv and features_value_importance.csv - these files are linked together and describe the weighted feature contributions from features (only present for GBT models)\r\n      * AnalysisLog.txt - Log file of the run including all options, data curation and model training summaries        \r\n      * LeaveOneOut_Summary.csv - Results of the leave-one-reactant set-out studies on the model (if performed)\r\n      * LOOModelInfo.txt - Hyperparameter information for each model in the study (associated with the given dataset, sometimes includes duplicate runs).\r\n      * STTSModelInfo.txt - Hyperparameter information for each model in the study (associated with the given dataset, sometimes includes duplicate runs).\r\n      * StandardTestTrain_Summary.csv - Results of the 6 fold cross validation ML performance (for the hold out case)\r\n      * LeaveOneOut_FullDataset_ByAmine.csv - Results of the leave-one-reactant set-out studies performed on the full dataset (all experiments) specified by reactant set (delineated by the amine)\r\n      * LeaveOneOut_StratifiedData_ByAmine.csv -  Results of the leave-one-reactant set-out studies performed on a random stratified sample (96 random experiments) specified by reactant set (delineated by the amine)\r\n      * model_matchup_main_*.py - code used to generate all of the runs contained in a particular folder.  The code is exactly what was used at run time to generate a given dataset (requires 0045.perovskitedata.csv file to run).", "variants": ["darpa_sd2_perovskites"]}
{"id": "TREC-10", "title": "", "contents": "A question type classification dataset with 6 classes for questions about a person, location, numeric information, etc. The test split has 500 questions, and the training split has 5452 questions.\r\n\r\nPaper: [Learning Question Classifiers](https://www.aclweb.org/anthology/C02-1150/)", "variants": ["TREC-10"]}
{"id": "WIT", "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning", "contents": "**Wikipedia-based Image Text** (**WIT**) Dataset is a large multimodal multilingual dataset. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal machine learning models.\r\n\r\n**Key Advantages**\r\n\r\nA few unique advantages of WIT:\r\n\r\n- The largest multimodal dataset (time of this writing) by the number of image-text examples.\r\n- A massively multilingual (first of its kind) with coverage for over 100+ languages.\r\n- A collection of diverse set of concepts and real world entities.\r\n- Brings forth challenging real-world test sets.", "variants": ["WIT"]}
{"id": "Unsplash Dataset", "title": "", "contents": "The Unsplash Dataset is created by over 200,000 contributing photographers and billions of searches across thousands of applications, uses, and contexts. It contains over 2M Unsplash images.", "variants": ["Unsplash Dataset"]}
{"id": "IDRiD", "title": "", "contents": "Indian Diabetic Retinopathy Image Dataset (IDRiD) dataset consists of typical diabetic retinopathy lesions and normal retinal structures annotated at a pixel level. This dataset also provides information on the disease severity of diabetic retinopathy and diabetic macular edema for each image. This dataset is perfect for the development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.", "variants": ["IDRiD"]}
{"id": "HRWSI", "title": "Structure-Guided Ranking Loss for Single Image Depth Prediction", "contents": "The HRWSI dataset consists of about 21K diverse high-resolution RGB-D image pairs derived from the Web stereo images. Also, it provides sky segmentation masks, instance segmentation masks as well as invalid pixel masks.", "variants": ["HRWSI"]}
{"id": "Lens Flare Dataset", "title": "Automatic Flare Spot Artifacts Detection and Removal in Photographs", "contents": "The Lens Flare dataset is an internal dataset for Flare Spot detection used in the paper \"Automatic Flare Spot Artifact Detection and Removal in Photographs\" by Patricia Vitoria and Coloma Ballester.\r\n\r\nThe dataset consists of 405 natural images in which a minimum of one flare spot artifact appears. The sources of light can be the sun, light bulbs or specular surfaces, among others. The images have been captured by different cameras with different technical specifications.", "variants": ["Lens Flare Dataset"]}
{"id": "SARA motion", "title": "", "contents": "Sara motion is a 3D motion dataset, named Synthetic Actors and Real Actions (SARA), for training a model to produce motion embeddings suitable for reasoning about motion similarity. \r\n\r\nThe motion sequence data for this dataset was generated by combining 18 different actors (i.e., action performing characters). The characters were rendered in a skeleton shape with Adobe Fuse software. Four action categories were selected (Combat, Adventure, Sport, and Dance) comprising a number of motion variations, where each action has a frame length of 32 or more. There are 4,428 base motions (e.g., dancing, jumping) in the SARA dataset.", "variants": ["SARA motion"]}
{"id": "NTU RGB+D 120 motion similarity", "title": "A Body Part Embedding Model With Datasets for Measuring 2D Human Motion Similarity", "contents": "Motion similarity annotations for [NTU RGB+D 120 dataset](https://paperswithcode.com/dataset/ntu-rgb-d-120) to evaluate motion similarity in the real world.", "variants": ["NTU RGB+D 120 motion similarity"]}
{"id": "BU-BIL", "title": "", "contents": "**BU-BIL** is an image library which includes six datasets that represent three imaging modalities and six object types. Providers of the datasets are instructed to choose images that capture the various environmental conditions and imaging noise that arose in their studies. These experts are asked to then select objects from those images that reflect the natural diversity of shape and appearances that these objects can exhibit. The image subregions containing the identified objects are cropped to create the image library. The outcome was a library with 305 objects from 235 images. Authors verify by visual inspection that the image library includes a variety of object appearances, backgrounds, and properties distinguishing objects from the background.\r\n\r\nPaper: [How to Collect Segmentations for Biomedical Images? A Benchmark Evaluating the Performance of Experts, Crowdsourced Non-Experts, and Algorithms](https://www.cs.bu.edu/fac/betke/papers/Gurari-etal-WACV-2015.pdf)\r\n\r\nImage source: [How to Collect Segmentations for Biomedical Images? A Benchmark Evaluating the Performance of Experts, Crowdsourced Non-Experts, and Algorithms](https://www.cs.bu.edu/fac/betke/papers/Gurari-etal-WACV-2015.pdf)", "variants": ["BU-BIL"]}
{"id": "MTA-KDD'19", "title": "A Novel Resampling Technique for Imbalanced Dataset Optimization", "contents": "Malware Traffic Analysis Knowledge Dataset 2019 (MTA-KDD'19) is an updated and refined dataset specifically tailored to train and evaluate machine learning based malware traffic analysis algorithms. To generate it, that authors started from the largest databases of network traffic captures available online, deriving a dataset with a set of widely-applicable features and then cleaning and preprocessing it to remove noise, handle missing data and keep its size as small as possible. The resulting dataset is not biased by any specific application (although specifically addressed to machine learning algorithms), and the entire process can run automatically to keep it updated.\r\n\r\nSource: [Letteri et al.](http://ceur-ws.org/Vol-2597/paper-14.pdf)", "variants": ["MTA-KDD'19"]}
{"id": "Cuff-Less Blood Pressure Estimation", "title": "", "contents": "##Data Set Information:\r\n\r\nThe main goal of this data set is providing clean and valid signals for designing cuff-less blood pressure estimation algorithms. The raw electrocardiogram (ECG), photoplethysmograph (PPG), and arterial blood pressure (ABP) signals are originally collected from the physionet.org and then some preprocessing and validation performed on them. (For more information about the process please refer to our paper)\r\n\r\n##Attribute Information:\r\n\r\nThis database consists of a cell array of matrices, each cell is one record part. \r\nIn each matrix each row corresponds to one signal channel:\r\n\r\n1: PPG signal, FS=125Hz; photoplethysmograph from fingertip\r\n\r\n2: ABP signal, FS=125Hz; invasive arterial blood pressure (mmHg)\r\n\r\n3: ECG signal, FS=125Hz; electrocardiogram from channel II\r\n\r\nNote: dataset is splitted to multiple parts to make it easier to load on machines with low memory. Each cell is a record. There might be more than one record per patient (which is not possible to distinguish). However, records of the same patient appear next to each other. N-fold cross test and train is suggested to reduce the chance of trainset being contaminated by test patients.", "variants": ["Cuff-Less Blood Pressure Estimation"]}
{"id": "POTUS Corpus", "title": "The POTUS Corpus, a Database of Weekly Addresses for the Study of Stance in Politics and Virtual Agents", "contents": "The **POTUS Corpus** is a Database of Weekly Addresses for the Study of Stance in Politics and Virtual Agents.\r\n\r\nOne of the main challenges in the field of Embodied Conversational Agent (ECA) is to generate socially believable agents. The common strategy for agent behaviour synthesis is to rely on dedicated corpus analysis. Such a corpus is composed of multimedia files of socio-emotional behaviors which have been annotated by external observers. The underlying idea is to identify interaction information for the agent’s socio-emotional behavior by checking whether the intended socio-emotional behavior is actually perceived by humans. Then, the annotations can be used as learning classes for machine learning algorithms applied to the social signals. This paper introduces the POTUS Corpus composed of high-quality audio-video files of political addresses to the American people. Two protagonists are present in this database. First, it includes speeches of former president Barack Obama to the American people. Secondly, it provides videos of these same speeches given by a virtual agent named Rodrigue. The ECA reproduces the original address as closely as possible using social signals automatically extracted from the original one. Both are annotated for social attitudes, providing information about the stance observed in each file. It also provides the social signals automatically extracted from Obama’s addresses used to generate Rodrigue’s ones.", "variants": ["POTUS Corpus"]}
{"id": "ImageNet VIPriors subset", "title": "", "contents": "The training and validation data are subsets of the training split of the Imagenet 2012. The test set is taken from the validation split of the Imagenet 2012 dataset. Each data set includes 50 images per class.", "variants": ["ImageNet VIPriors subset"]}
{"id": "BiRD", "title": "Big BiRD: A Large, Fine-Grained, Bigram Relatedness Dataset for Examining Semantic Composition", "contents": "**Bigram Relatedness Dataset** (**BiRD**) is a large, fine-grained, bigram relatedness dataset, using a comparative annotation technique called Best Worst Scaling. Each of BiRD's 3,345 English term pairs involves at least one bigram. BiRD is made freely available to foster further research on how meaning can be represented and how meaning can be composed.\r\n\r\nImage source: [http://saifmohammad.com/WebPages/BiRD.html](http://saifmohammad.com/WebPages/BiRD.html)", "variants": ["BiRD"]}
{"id": "Shiny dataset", "title": "NeX: Real-time View Synthesis with Neural Basis Expansion", "contents": "The shiny folder contains 8 scenes with challenging view-dependent effects used in our paper. We also provide additional scenes in the shiny_extended folder. \r\nThe test images for each scene used in our paper consist of one of every eight images in alphabetical order.\r\n\r\nEach scene contains the following directory structure:\r\n```\r\n  scene/\r\n    dense/\r\n      cameras.bin\r\n      images.bin\r\n      points3D.bin\r\n      project.ini\r\n    images/\r\n      image_name1.png\r\n      image_name2.png\r\n      ...\r\n      image_nameN.png\r\n    images_distort/\r\n      image_name1.png\r\n      image_name2.png\r\n      ...\r\n      image_nameN.png\r\n    sparse/\r\n      cameras.bin\r\n      images.bin\r\n      points3D.bin\r\n      project.ini\r\n    database.db\r\n    hwf_cxcy.npy\r\n    planes.txt\r\n    poses_bounds.npy\r\n```\r\n\r\n- dense/ folder contains COLMAP's output [1] after the input images are undistorted.\r\n- images/ folder contains undistorted images. (We use these images in our experiments.)\r\n- images_distort/ folder contains raw images taken from a smartphone.\r\n- sparse/ folder contains COLMAP's sparse reconstruction output [1].\r\n\r\nOur poses_bounds.npy is similar to the LLFF[2] file format with a slight modification. This file stores a Nx14 numpy array, where N is the number of cameras. Each row in this array is split into two parts of sizes 12 and 2. The first part, when reshaped into 3x4, represents the camera extrinsic (camera-to-world transformation), and the second part with two dimensions stores the distances from that point of view to the first and last planes (near, far). These distances are computed automatically based on the scene’s statistics using LLFF’s code. (For details on how these are computed, see [this code](https://git.io/JqLKF)) \r\n\r\nhwf_cxcy.npy stores the camera intrinsic (height, width, focal length, principal point x, principal point y) in a 1x5 numpy array.\r\n\r\nplanes.txt stores information about the MPI planes. The first two numbers are the distances from a reference camera to the first and last planes (near, far). The third number tells whether the planes are placed equidistantly in the depth space (0) or inverse depth space (1). The last number is the padding size in pixels on all four sides of each of the MPI planes. I.e., the total dimension of each plane is (H + 2 * padding, W + 2 * padding).\r\n\r\n\r\nReferences:\r\n\r\n- [1]: [COLMAP structure from motion (Schönberger and Frahm, 2016)](https://colmap.github.io/).\r\n- [2]: [Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines (Mildenhall et al., 2019)](https://arxiv.org/abs/1905.00889).", "variants": ["Shiny dataset"]}
{"id": "MATH", "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "contents": "MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations.\r\n\r\nSource: [Hendrycks et al.](https://arxiv.org/pdf/2103.03874.pdf)\r\n\r\nImage source: [Hendrycks et al.](https://arxiv.org/pdf/2103.03874v1.pdf)", "variants": ["MATH"]}
{"id": "PhysioNet Challenge 2016", "title": "", "contents": "Introduction\r\nThe 2016 PhysioNet/CinC Challenge aims to encourage the development of algorithms to classify heart sound recordings collected from a variety of clinical or nonclinical (such as in-home visits) environments. The aim is to identify, from a single short recording (10-60s) from a single precordial location, whether the subject of the recording should be referred on for an expert diagnosis.\r\n\r\nDuring the cardiac cycle, the heart firstly generates the electrical activity and then the electrical activity causes atrial and ventricular contractions. This in turn forces blood between the chambers of the heart and around the body. The opening and closure of the heart valves is associated with accelerations-decelerations of blood, giving rise to vibrations of the entire cardiac structure (the heart sounds and murmurs) [1]. These vibrations are audible at the chest wall, and listening for specific heart sounds can give an indication of the health of the heart. The phonocardiogram (PCG) is the graphical representation of a heart sound recording. Figure 1 illustrates a short section of a PCG recording.", "variants": ["PhysioNet Challenge 2016"]}
{"id": "IXI Dataset", "title": "", "contents": "**IXI Dataset** is a collection of 600 MR brain images from normal, healthy subjects. The MR image acquisition protocol for each subject includes:\r\n\r\n * T1, T2 and PD-weighted images\r\n * MRA images\r\n * Diffusion-weighted images (15 directions)\r\n\r\nThe data has been collected at three different hospitals in London:\r\n\r\n * Hammersmith Hospital using a Philips 3T system (details of scanner parameters)\r\n * Guy’s Hospital using a Philips 1.5T system (details of scanner parameters)\r\n * Institute of Psychiatry using a GE 1.5T system (details of the scan parameters not available at the moment)\r\n\r\nThe data has been collected as part of the project:\r\n\r\n * IXI – Information eXtraction from Images (EPSRC GR/S21533/02)\r\n\r\nThe images in NIFTI format can be downloaded from [here](https://brain-development.org/ixi-dataset/):\r\n\r\nThis data is made available under the Creative Commons CC BY-SA 3.0 license. If you use the IXI data please acknowledge the source of the IXI data.", "variants": ["IXI Dataset"]}
{"id": "LIFULL HOME'S", "title": "", "contents": "The National Institute of Informatics provides LIFULL HOME'S Dataset to researchers, which was offered by [LIFULL Co., Ltd.](https://lifull.com/en/) for promoting research in informatics and the related fields.\r\n\r\nThe dataset contains the data of [LIFULL HOME'S](https://www.homes.co.jp/), a Real Estate Information Service in Japan.\r\n\r\n1. Snapshot Data of Rentals (snapshot of 2015-09)\r\nRental data (5.33 million all over Japan): rental fee, area, location, age, floor plan, structure, facilities, etc.; approx. 1.6GB .tsv format files.\r\nImage data (83 million files): floor plan image, room view, etc. of all the above items; approx. 210GB .jpg format files, max size: 120x120.\r\n\r\n2. High Resolution Floor Plan Image Data\r\nHigh resolution version data of floor plan image (5.31 million files) included in Snapshot Data of Rentals; approx. 140GB .jpg format files. Additional application required to use this data (see Application section below).\r\n\r\n3. Monthly Data of Rentals and Sales (2015-07 - 2017-06, 24 months)\r\nProperty data of rentals and sales (5.33 million all over Japan): rental fee/price, area, location, age, floor plan, structure, facilities, etc.; approx. 1.7 - 4.5GB .tsv format files for respective months.\r\nIn addition, LIFULL Co., Ltd. provides a sample script for classifying image types via Github:\r\nhttps://github.com/Littel-Laboratory/homes-dataset-tools", "variants": ["LIFULL HOME'S"]}
{"id": "Sketch2aia (Mobile User Interface Sketches)", "title": "Automatic code generation from sketches of mobile applications in end-user development using Deep Learning", "contents": "Dataset of 374 photos of hand-drawn sketches of App Inventor apps used for development of the Sketch2aia model for automatic generation of App Inventor wireframes from hand-drawn sketches.\r\n\r\nData format\r\nTraining:2 37 images in JPG (.jpg) format with 720×1280 pixels, each accompanied by a JSON (.json) file with manually attributed bounding box annotation for 10 different classes of UI elements (Screen, Label, Button, Switch, Slider, TextBox, CheckBox, ListPicker, Image and Map), used to train the Sketch2aia model.\r\n\r\nValidation: 42 images in JPG (.jpg) format with 720×1280 pixels, each accompanied by a JSON (.json) file with manually attributed bounding box annotation for 10 different classes of UI elements (Screen, Label, Button, Switch, Slider, TextBox, CheckBox, ListPicker, Image and Map), used to test the Sketch2aia model.\r\n\r\nAdditional Images: 95 images in JPG (.jpg) format with 720×1280 pixels. Some images are accompanied by a JSON (.json) file with manually attributed bounding box annotation for 10 different classes of UI elements (Screen, Label, Button, Switch, Slider, TextBox, CheckBox, ListPicker, Image and Map), while others have not yet been labeled. This portion of the dataset was collected during user evaluation of the Sketch2aia model, and have not been directly used to train or test the object detection model.", "variants": ["Sketch2aia (Mobile User Interface Sketches)"]}
{"id": "An Amharic News Text classification Dataset", "title": "An Amharic News Text classification Dataset", "contents": "In NLP, text classification is one of the primary problems we try to solve and its uses in language analyses are indisputable. The lack of labeled training data made it harder to do these tasks in low resource languages like Amharic. The task of collecting, labeling, annotating, and making valuable this kind of data will encourage junior researchers, schools, and machine learning practitioners to implement existing classification models in their language. In this short paper, we aim to introduce the Amharic text classification dataset that consists of more than 50k news articles that were categorized into 6 classes. This dataset is made available with easy baseline performances to encourage studies and better performance experiments.", "variants": ["An Amharic News Text classification Dataset"]}
{"id": "CUAD", "title": "CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review", "contents": "**Contract Understanding Atticus Dataset** (**CUAD**) is a dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project\r\nand consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review.", "variants": ["CUAD"]}
{"id": "BIKED", "title": "BIKED: A Dataset and Machine Learning Benchmarks for Data-Driven Bicycle Design", "contents": "**BIKED** is a dataset comprised of 4500 individually designed bicycle models sourced from hundreds of designers. BIKED enables a variety of data-driven design applications for bicycles and generally supports the development of data-driven design methods. The dataset is comprised of a variety of design information including assembly images, component images, numerical design parameters, and class labels.", "variants": ["BIKED"]}
{"id": "THEOStereo", "title": "A Study on the Influence of Omnidirectional Distortion on CNN-based Stereo Vision", "contents": "THEOStereo is a dataset providing synthetic stereo image pairs and their corresponding scene depth and will be published along with [1]. All images follow the omnidirectional camera model. In total, there are *31,250* omnidirectional images pairs. The training set contains *25,000* image pairs. For validation and testing there are *3,125* image pairs, respectively. For each pair, there is a ground truth depth map describing the pixel-wise distance of the object along the left camera's z-axis. The virtual omnidirectional cameras exhibit a FOV of *180* degrees and can be described using Kannala's camera model [2]. The distortion parameters are *k_1 = 1* and *k_2 = k_3 = k_4 = k_5 = 0*. The baseline of the stereo camera was *0.3* m. Please do not forget to cite [1] if you use the dataset in your work. Thank you.\r\n\r\n## Structure of the Dataset\r\n\r\n```\r\n.\r\n├── README.md\r\n├── test\r\n│   ├── depth_exr_abs\r\n│   ├── img_stereo_webp\r\n│   └── img_webp\r\n├── train\r\n│   ├── depth_exr_abs\r\n│   ├── img_stereo_webp\r\n│   └── img_webp\r\n└── valid\r\n    ├── depth_exr_abs\r\n    ├── img_stereo_webp\r\n    └── img_webp\r\n```\r\n\r\nThe directory `depth_exr_abs` contain the depth maps given in meters. The depth reference to the image of the left camera. All images of the left camera are stored in the `img_webp`. The right camera's images can be found in `img_stereo_webp`.\r\n\r\n## License\r\n\r\nThis dataset is licensed under CC BY 4.0.  \r\nFor details, please visit <https://creativecommons.org/licenses/by/4.0/>.\r\n\r\n[![CC BY 4.0 Logo](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by.svg)](https://creativecommons.org/licenses/by/4.0/)\r\n\r\n## Conference paper\r\nThe conference paper can be downloaded from [here](https://doi.org/10.5220/0010324808090816).\r\n\r\n## BibTex\r\nIf you use the dataset in your work, we would kindly ask you to cite [1].\r\nYou might want to use the following BibTex entry:\r\n```bibtex\r\n@inproceedings{seuffert_study_2021,\r\n\taddress = {Online Conference},\r\n\ttitle = {A {Study} on the {Influence} of {Omnidirectional} {Distortion} on {CNN}-based {Stereo} {Vision}},\r\n\tisbn = {978-989-758-488-6},\r\n\tdoi = {10.5220/0010324808090816},\r\n\tbooktitle = {Proceedings of the 16th {International} {Joint} {Conference} on {Computer} {Vision}, {Imaging} and {Computer} {Graphics} {Theory} and {Applications}, {VISIGRAPP} 2021, {Volume} 5: {VISAPP}},\r\n\tpublisher = {SciTePress},\r\n\tauthor = {Seuffert, Julian Bruno and Perez Grassi, Ana Cecilia and Scheck, Tobias and Hirtz, Gangolf},\r\n\tyear = {2021},\r\n\tmonth = {2},\r\n\tpages = {809--816}\r\n}\r\n```\r\n\r\n## References\r\n[1] J. B. Seuffert, A. C. Perez Grassi, T. Scheck, and G. Hirtz, “A Study on the Influence of Omnidirectional Distortion on CNN-based Stereo Vision,” in *Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2021, Volume 5: VISAPP*, Online Conference, Feb. 2021, pp. 809–816, doi: 10.5220/0010324808090816.\r\n\r\n[2] J. Kannala, J. Heikkilä, and S. S. Brandt, “Geometric Camera Calibration,” in *Wiley Encyclopedia of Computer Science\r\nand Engineering*, B. W. Wah, Ed. Hoboken, NJ, USA: John Wiley & Sons, Inc., 2008.", "variants": ["THEOStereo"]}
{"id": "ARCH", "title": "Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles", "contents": "**ARCH** is a computational pathology (CP) multiple instance captioning dataset to facilitate dense supervision of CP tasks. Existing CP datasets focus on narrow tasks; ARCH on the other hand contains dense diagnostic and morphological descriptions for a range of stains, tissue types and pathologies. \r\n\r\nSource: [Gamper et al.](https://arxiv.org/pdf/2103.05121v1.pdf)\r\n\r\nImage source: [Gamper et al.](https://arxiv.org/pdf/2103.05121v1.pdf)", "variants": ["ARCH"]}
{"id": "SUM", "title": "SUM: A Benchmark Dataset of Semantic Urban Meshes", "contents": "SUM is a new benchmark dataset of semantic urban meshes which covers about 4 km2 in Helsinki (Finland), with six classes: Ground, Vegetation, Building, Water, Vehicle, and Boat.\r\n\r\nThe authors used Helsinki 3D textured meshes as input and annotated them as a benchmark dataset of semantic urban meshes. The Helsinki's raw dataset covers about 12 km2 and was generated in 2017 from oblique aerial images that have about a 7.5 cm ground sampling distance (GSD) using an off-the-shelf commercial software namely ContextCapture.\r\n\r\nThe entire region of Helsinki is split into tiles, and each of them covers about 250 m2. \r\n\r\nImage source: [Gao et al.](https://arxiv.org/pdf/2103.00355v1.pdf)", "variants": ["SUM"]}
{"id": "BLURB", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing", "contents": "**BLURB** is a collection of resources for biomedical natural language processing. In general domains such as newswire and the Web, comprehensive benchmarks and leaderboards such as GLUE have greatly accelerated progress in open-domain NLP. In biomedicine, however, such resources are ostensibly scarce. In the past, there have been a plethora of shared tasks in biomedical NLP, such as BioCreative, BioNLP Shared Tasks, SemEval, and BioASQ, to name just a few. These efforts have played a significant role in fueling interest and progress by the research community, but they typically focus on individual tasks. The advent of neural language models such as BERTs provides a unifying foundation to leverage transfer learning from unlabeled text to support a wide range of NLP applications. To accelerate progress in biomedical pretraining strategies and task-specific methods, it is thus imperative to create a broad-coverage benchmark encompassing diverse biomedical tasks.\r\n\r\nInspired by prior efforts toward this direction (e.g., BLUE), BLURB (short for Biomedical Language Understanding and Reasoning Benchmark) was created. BLURB comprises of a comprehensive benchmark for PubMed-based biomedical NLP applications, as well as a leaderboard for tracking progress by the community. BLURB includes thirteen publicly available datasets in six diverse tasks. To avoid placing undue emphasis on tasks with many available datasets, such as named entity recognition (NER), BLURB reports the macro average across all tasks as the main score. The BLURB leaderboard is model-agnostic. Any system capable of producing the test predictions using the same training and development data can participate. The main goal of BLURB is to lower the entry barrier in biomedical NLP and help accelerate progress in this vitally important field for positive societal and human impact.\r\n\r\nSource: [BLURB](https://microsoft.github.io/BLURB/index.html)\r\n\r\nImage source: [BLURB](https://microsoft.github.io/BLURB/index.html)", "variants": ["BLURB"]}
{"id": "Kaggle EyePACS", "title": "", "contents": "Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people.\r\n\r\nretina\r\n\r\nThe US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment.\r\n\r\nCurrently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment.\r\n\r\nClinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient.\r\n\r\nThe need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible – ideally resulting in models with realistic clinical potential. The winning models will be open sourced to maximize the impact such a model can have on improving DR detection.\r\n\r\nAcknowledgements\r\nThis competition is sponsored by the California Healthcare Foundation.\r\n\r\n![img](https://storage.googleapis.com/kaggle-competitions/kaggle/4104/media/chcf.gif)\r\n\r\nRetinal images were provided by EyePACS, a free platform for retinopathy screening.\r\n\r\n![img](https://storage.googleapis.com/kaggle-competitions/kaggle/4104/media/eyepacs.png)", "variants": ["Kaggle EyePACS"]}
{"id": "THFOOD-50", "title": "NU-InNet: Thai Food Image Recognition Using Convolutional Neural Networks on Smartphone", "contents": "Fine-Grained Thai Food Image Classification Datasets\r\n\r\nTHFOOD-50 containing 15,770 images of 50 famous Thai dishes.", "variants": ["THFOOD-50"]}
{"id": "BL30K", "title": "Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion", "contents": "BL30K is a synthetic dataset rendered using Blender with ShapeNet's data. We break the dataset into six segments, each with approximately 5K videos. The videos are organized in a similar format as DAVIS and YouTubeVOS, so dataloaders for those datasets can be used directly. Each video is 160 frames long, and each frame has a resolution of 768*512. There are 3-5 objects per video, and each object has a random smooth trajectory -- we tried to optimize the trajectories in a greedy fashion to minimize object intersection (not guaranteed), with occlusions still possible (happen a lot in reality). See [MiVOS](https://github.com/hkchengrex/MiVOS) for details.", "variants": ["BL30K"]}
{"id": "BIG", "title": "CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement", "contents": "A high-resolution semantic segmentation dataset with 50 validation and 100 test objects. Image resolution in BIG ranges from 2048×1600 to 5000×3600. Every image in the dataset has been carefully labeled by a professional while keeping the same guidelines as PASCAL VOC 2012 without the void region.", "variants": ["BIG"]}
{"id": "COCO Object Detection VIPriors subset", "title": "", "contents": "The training and validation data are subsets of the training split of the MS COCO dataset (2017 release, bounding boxes only). The test set is taken from the validation split of the MS COCO dataset.", "variants": ["COCO Object Detection VIPriors subset"]}
{"id": "Cityscapes VIPriors subset", "title": "", "contents": "The training and validation data are subsets of the training split of the Cityscapes dataset. The test set is taken from the validation split of the Cityscapes dataset.", "variants": ["Cityscapes VIPriors subset"]}
{"id": "UCF-101 VIPriors subset", "title": "VIPriors 1: Visual Inductive Priors for Data-Efficient Deep Learning Challenges", "contents": "The VIriors Action Recognition Challenge uses a subset of the UCF101 action recognition dataset:\r\n\r\nTrain set: ~4.8K clips.\r\nValidation set: ~4.7K clips.\r\nTest set: ~3.8K clips.", "variants": ["UCF-101 VIPriors subset"]}
{"id": "CLEVR-Hans", "title": "Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations", "contents": "The CLEVR-Hans data set is a novel confounded visual scene data set, which captures complex compositions of different objects. This data set consists of [CLEVR](clevr) images divided into several classes. \r\n\r\nThe membership of a class is based on combinations of objects’ attributes and relations. Additionally, certain classes within the data set are confounded. Thus, within the data set, consisting of train, validation, and test splits, all train, and validation images of confounded classes will be confounded with a specific attribute or combination of attributes.\r\n\r\nEach class is represented by 3000 training images, 750 validation images, and 750 test images. The training, validation, and test set splits contain 9000, 2250, and 2250 samples, respectively, for CLEVR-Hans3 and 21000, 5250, and 5250 samples for CLEVR-Hans7. The class distribution is balanced for all data splits.\r\n\r\nFor CLEVR-Hans classes for which class rules contain more than three objects, the number of objects to be placed per scene was randomly chosen between the minimal required number of objects for that class and ten, rather than between three and ten, as in the original CLEVR data set.\r\n\r\nFinally, the images were created such that the exact combinations of the class rules did not occur in images of other classes. It is possible that a subset of objects from one class rule occur in an image of another class. However, it is not possible that more than one complete class rule is contained in an image.\r\n\r\nSource: [CLEVR-Hans](https://github.com/ml-research/CLEVR-Hans)\r\nImage Source: [CLEVR-Hans](https://github.com/ml-research/CLEVR-Hans)", "variants": ["CLEVR-Hans"]}
{"id": "Tsinghua Dogs", "title": "A new dataset of dog breed images and a benchmark for fine-grained classification", "contents": "Tsinghua Dogs is a fine-grained classification dataset for dogs, over 65% of whose images are collected from people's real life. Each dog breed in the dataset contains at least 200 images and a maximum of 7,449 images, basically in proportion to their frequency of occurrence in China, so it significantly increases the diversity for each breed over existing dataset. Furthermore, Tsinghua Dogs annotated bounding boxes of the dog’s whole body and head in each image, which can be used for supervising the training of learning algorithms as well as testing them.", "variants": ["Tsinghua Dogs"]}
{"id": "ADAM", "title": "", "contents": "ADAM is organized as a half day Challenge, a Satellite Event of the ISBI 2020 conference in Iowa City, Iowa, USA.\r\n\r\nThe ADAM challenge focuses on the investigation and development of algorithms associated with the diagnosis of Age-related Macular degeneration (AMD) and segmentation of lesions in fundus photos from AMD patients. The goal of the challenge is to evaluate and compare automated algorithms for the detection of AMD on a common dataset of retinal fundus images. We invite the medical image analysis community to participate by developing and testing existing and novel automated fundus classification and segmentation methods.\r\n\r\nInstructions: \r\nADAM: Automatic Detection challenge on Age-related Macular degeneration\r\n\r\nLink: https://amd.grand-challenge.org\r\n\r\nAge-related macular degeneration, abbreviated as AMD, is a degenerative disorder in the macular region. It mainly occurs in people older than 45 years old and its incidence rate is even higher than diabetic retinopathy in the elderly.  \r\n\r\nThe etiology of AMD is not fully understood, which could be related to multiple factors, including genetics, chronic photodestruction effect, and nutritional disorder. AMD is classified into Dry AMD and Wet AMD. Dry AMD (also called nonexudative AMD) is not neovascular. It is characterized by progressive atrophy of retinal pigment epithelium (RPE). In the late stage, drusen and the large area of atrophy could be observed under ophthalmoscopy. Wet AMD (also called neovascular or exudative AMD), is characterized by active neovascularization under RPE, subsequently causing exudation, hemorrhage, and scarring, and will eventually cause irreversible damage to the photoreceptors and rapid vision loss if left untreated.\r\n\r\nAn early diagnosis of AMD is crucial to treatment and prognosis. Fundus photo is one of the basic examinations. The current dataset is composed of AMD and non-AMD (myopia, normal control, etc.) photos. Typical signs of AMD that can be found in these photos include drusen, exudation, hemorrhage, etc. \r\n\r\nThe ADAM challenge has 4 tasks:\r\n\r\nTask 1: Classification of AMD and non-AMD fundus images.\r\n\r\nTask 2: Detection and segmentation of optic disc.\r\n\r\nTask 3: Localization of fovea.\r\n\r\nTask 4: Detection and Segmentation of lesions from fundus images.", "variants": ["ADAM"]}
{"id": "DiCOVA", "title": "DiCOVA Challenge: Dataset, task, and baseline system for COVID-19 diagnosis using acoustics", "contents": "The DiCOVA Challenge dataset is derived from the Coswara dataset, a crowd-sourced dataset of sound recordings from COVID-19 positive and non-COVID-19 individuals. The Coswara data is collected using a web-application2, launched in April-2020, accessible through the internet by anyone around the globe. The volunteering subjects are advised to record their respiratory sounds in a quiet environment. \r\n\r\nEach subject provides 9 audio recordings, namely, (a) shallow and deep breathing (2 nos.), (b) shallow and heavy cough (2 nos.), (c) sustained phonation of vowels [æ] (as in bat), [i] (as in beet), and [u] (as in boot) (3 nos.), and (d) fast and normal pace 1 to 20 number counting (2 nos.). \r\n\r\nThe DiCOVA Challenge has two tracks. The participants also provided metadata corresponding to their current health status (includes COVID19 status, any other respiratory ailments, and symptoms), demographic information, age and gender. From this Coswara dataset, two datasets have been created: \r\n\r\n(a) Track-1 dataset: composed of cough sound recordings. It t is composed of cough audio data from 1040 subjects.\r\n(b) Track-2 dataset: composed of deep breathing, vowel [i], and number counting (normal pace) speech recordings. It is composed of audio data from 1199 subjects.", "variants": ["DiCOVA"]}
{"id": "Digital Peter", "title": "Digital Peter: Dataset, Competition and Handwriting Recognition Methods", "contents": "Digital Peter is a dataset of Peter the Great's manuscripts annotated for segmentation and text recognition. The dataset may be useful for researchers to train handwriting text recognition models as a benchmark for comparing different models. It consists of 9,694 images and text files corresponding to lines in historical documents. The dataset includes Peter’s handwritten materials covering the period from 1709 to 1713. \r\n\r\nThe open machine learning competition Digital Peter was held based on the considered dataset.", "variants": ["Digital Peter"]}
{"id": "OGB-LSC", "title": "OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs", "contents": "OGB Large-Scale Challenge (OGB-LSC) is a collection of three real-world datasets for advancing the state-of-the-art in large-scale graph ML. OGB-LSC provides graph datasets that are orders of magnitude larger than existing ones and covers three core graph learning tasks -- link prediction, graph regression, and node classification. \r\n\r\nOGB-LSC consists of three datasets: MAG240M-LSC, WikiKG90M-LSC, and PCQM4M-LSC. Each dataset offers an independent task.\r\n\r\n* MAG240M-LSC is a heterogeneous academic graph, and the task is to predict the subject areas of papers situated in the heterogeneous graph (node classification).\r\n* WikiKG90M-LSC is a knowledge graph, and the task is to impute missing triplets (link prediction).\r\n* PCQM4M-LSC is a quantum chemistry dataset, and the task is to predict an important molecular property, the HOMO-LUMO gap, of a given molecule (graph regression).", "variants": ["OGB-LSC"]}
{"id": "TeachMyAgent", "title": "TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL", "contents": "TeachMyAgent (TA) is a benchmark for Automatic Curriculum Learning (ACL) algorithms leveraging procedural task generation. It includes 1) challenge-specific unit-tests using variants of a procedural Box2D bipedal walker environment, and 2) a new procedural Parkour environment combining most ACL challenges, making it ideal for global performance assessment.", "variants": ["TeachMyAgent"]}
{"id": "L1000", "title": "", "contents": "The **L1000** dataset consists of ~1,400,000 gene-expression profiles on the responses of ~50 human cell lines to one of ~20,000 compounds across a range of concentrations. The L1000 dataset and its normalization versions10 were recently widely used in drug repurposing and discovery.\r\n\r\nDescription from: [A deep learning framework for high-throughput mechanism-driven phenotype compound screening and its application to COVID-19 drug repurposing](https://www.nature.com/articles/s42256-020-00285-9)\r\n\r\nPublication introducing the dataset: [A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles](https://pubmed.ncbi.nlm.nih.gov/29195078/)", "variants": ["L1000"]}
{"id": "DSBEC", "title": "Machine-learning enhanced dark soliton detection in Bose-Einstein condensates", "contents": "The data set consists of 6257 labeled images of Bose-Einstein condensates (BECs) with and without solitonic excitations, including kink solitons and solitonic vortices. Each element of the data set contains a masked image (132x164 pixels) of 2D atomic density used to train the machine learning model used in the paper \"Machine-learning enhanced dark soliton detection in Bose-Einstein condensates,\" (https://arxiv.org/abs/2101.05404), and a label indicating the class a given image belongs to (0 indicates no solitons, 1 indicates a single soliton, and 2 indicates other excitations). The data structure file and project description are included with the data.\r\nThis data set was used to train a deep convolutional neural network to automatically recognize whether or not a lone dark soliton has been created in BECs that was then implemented within an automated soliton detection and positioning system (see https://arxiv.org/abs/2101.05404 for details).", "variants": ["DSBEC"]}
{"id": "VESSEL12", "title": "", "contents": "", "variants": ["VESSEL12"]}
{"id": "ConScenD", "title": "The ConScenD Dataset: Concrete Scenarios from the highD Dataset According to ALKS Regulation UNECE R157 in OpenX", "contents": "The ConScenD dataset consists of over 340 scenarios extracted from the naturalistic highway dataset highD. This scenarios can be used to test for the introduction of Level 3 Automated Lane Keeping Systems according to the UNECE R157 ALKS Regulation.", "variants": ["ConScenD"]}
{"id": "LDC2020T02", "title": "", "contents": "", "variants": ["LDC2020T02"]}
{"id": "KoDF", "title": "KoDF: A Large-scale Korean DeepFake Detection Dataset", "contents": "The Korean DeepFake Detection Dataset (KoDF) is a large-scale collection of synthesized and real videos focused on Korean subjects, used for the task of deepfake detection.\r\n\r\nThe dataset consists of 62,166 real videos and 175,776 fake videos from 403 subjects. The fake videos are created using 6 different methods: FaceSwap, DeepFaceLab, FSGAN, FOMM, ATFHP and Wav2Lip.", "variants": ["KoDF"]}
{"id": "HDA Facial Tattoo and Painting Database", "title": "Impact of Facial Tattoos and Paintings on Face Recognition Systems", "contents": "The Hochschule Darmstadt (HDA) facial tattoo and paintings database contains 500 pairs of facial images of individuals with and without facial tattoos or paintings. The database was collected from multiple online sources.", "variants": ["HDA Facial Tattoo and Painting Database"]}
{"id": "Gowalla", "title": "Friendship and Mobility: User Movement In Location-Based Social Networks", "contents": "Gowalla is a location-based social networking website where users share their locations by checking-in. The friendship network is undirected and was collected using their public API, and consists of 196,591 nodes and 950,327 edges. We have collected a total of 6,442,890 check-ins of these users over the period of Feb. 2009 - Oct. 2010.", "variants": ["Gowalla"]}
{"id": "DODa", "title": "", "contents": "Darija Open Dataset (**DODa**) is an open-source project for the Moroccan dialect. With more than 10,000 entries DODa is arguably the largest open-source collaborative project for Darija-English translation built for Natural Language Processing purposes. In fact, besides semantic categorization, DODa also adopts a syntactic one, presents words under different spellings, offers verb-to-noun and masculine-to-feminine correspondences, contains the conjugation of hundreds of verbs in different tenses, and many other subsets to help researchers better understand and study Moroccan dialect. \r\n\r\nSource: [Moroccan Dialect -Darija- Open Dataset](https://arxiv.org/pdf/2103.09687v1.pdf)\r\n\r\nImage source: [Moroccan Dialect -Darija- Open Dataset](https://arxiv.org/pdf/2103.09687v1.pdf)", "variants": ["DODa"]}
{"id": "LeT-Mi", "title": "", "contents": "Levantine Twitter dataset for Misogynistic language (LeT-Mi) is an Arabic Levantine Twitter dataset for misogynistic language to be the first benchmark dataset for Arabic misogyny.\r\n\r\n⚠️ Note: To be made publicly available on Github\r\n\r\nSource: [Let-Mi: An Arabic Levantine Twitter Dataset for Misogynistic Language](https://arxiv.org/abs/2103.10195)\r\n\r\nImage source: [Let-Mi: An Arabic Levantine Twitter Dataset for Misogynistic Language](https://arxiv.org/abs/2103.10195)", "variants": ["LeT-Mi"]}
{"id": "SVT", "title": "", "contents": "**The Street View Text** (**SVT**) dataset was harvested from Google Street View. Image text in this data exhibits high variability and often has low resolution. In dealing with outdoor street level imagery, we note two characteristics. (1) Image text often comes from business signage and (2) business names are easily available through geographic business searches. These factors make the SVT set uniquely suited for word spotting in the wild: given a street view image, the goal is to identify words from nearby businesses.\r\n\r\nNote: the dataset has undergone revision since the time it was evaluated in this publication. Please consult the [ICCV2011 paper](http://vision.ucsd.edu/~kai/pubs/wang_iccv2011.pdf) for most up-to-date results.\r\n\r\nSource: [Street View Text Dataset](http://vision.ucsd.edu/~kai/svt/)\r\n\r\nImage source: [Street View Text Dataset](http://vision.ucsd.edu/~kai/svt/)", "variants": ["SVT"]}
